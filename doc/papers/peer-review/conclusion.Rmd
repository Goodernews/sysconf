# Conclusion and Future Work {#sec:conclusion}

This paper presented a new survey of authors of computer systems conferences, looking at two peer-review policies and two diversity aspects.
Our data suggests that many conference variables and author experience metrics appear to be affected by the blindness policy of the conference. But on deeper examination, double-blind reviewing policies are often conflated with the academic reputation of a conference. The question of whether double-blind reviews contribute to the prestige of a conference (or are a consequence of it) remains open, and we plan to explore it in future work.

Most respondents found the opportunity to respond to reviewers very helpful, even if it did not change their review scores. The implication for PC chairs, and by extension, educators, may be that while a response process to technical feedback is of little value to experienced practitioners, novices do find it overwhelmingly helpful. Another finding is that longer feedback is generally perceived as more helpful, understanding, and fair, which in turn may serve as another factor in improving researchers' and students' experience.

Women represent an alarmingly small group of authors in systems research, and this paper looked at whether the peer-review process plays a role in this underrepresentation, as has been found in some grant and job evaluations. For female authors of accepted papers, we found little evidence of negative effects in the reviews they receive or experience they perceive, even when their identity is known to the reviewers. On the other hand, we found evidence that women do have to work harder for their publications and appreciate thorough reviews more than men. Non-native English speakers also appear to experience no specific adverse effects from peer review, and in fact often report more positively on their experiences than native speakers. These two negative results can help focus the diversity effort on other policies.


This data set remains rich for exploration of the many questions that fell outside the scope of this paper. Some of the questions we plan to address in future work include:

* Why is the representation of women in systems so low?

* What other publication differences and commonalities exist between systems and the rest of CS?

* How do review grade correlate across categories?

* How might reviewer workload affect our results?

* How do any of these factors affect the eventual success of a paper, as measured by awards or citations?

* How do we correct for, or address, the surviorship bias, so that the voices of rejected papers' authors can be incorporated into this data?
