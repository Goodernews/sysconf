# Background {-}

Peer review is a cornerstone of modern scientific research. And yet understanding and improving this process is challenging, because it can be hard to experiment with [@beverly13:findings].
For example, most CS conferences disallow parallel submission, and even within the same conference we cannot design an experiment where the same paper is reviewed multiple times with fully controlled variations in the process. Thus, decisions on peer-review policies are often based more on the opinions of editors or program chairs, and less based on hard data and evidence.
However, in the absence of experimental evidence, conference chairs have to rely more on their subjective experience and opinions, to make policy decisions, and often prioritize the technical quality of the conference. But these decisions can also have profound effects on authors, which are rarely evaluated across an entire field.

This article attempts to present such data and evidence from statistical observations on the peer-review process for a specific year (2017) and a specific subfield of research (computer systems or "Systems"). Like most of CS, the primary channel for publishing research results in Systems is peer-reviewed conferences [@franceschet10:role; @vardi09:conferences]. Reputable Systems conferences employ similar policies, such as giving each paper at least three blind reviews (where the identity of the specific reviewers is hidden from authors). But conferences can vary considerably in other aspects, such as double-blind reviews (authors' identities are hidden from reviewers), rebuttals, two-phase reviews, etc.

We combined data from multiple sources, including a large author survey ($n=918$), to analyze the effects of two specific policies from the _conference author's perspective_. We were interested in the question of how these two policies, author rebuttal and double-blind reviews, affect two populations in particular: women and non-native English speakers. By limiting our scope to conferences in a single subfield, we avoid the variability that might occur across a broader range of disciplines.^[Despite its large research output and enormous economic impact, we found no consensus definition for the field of "Systems". For the purposes of this article, we define it to be the study of computer hardware and software components, which includes research in operating systems, computer architectures, databases, parallel and distributed computing, and computer networks.]  This important subfield often exhibits poor gender diversity [@destefano18:micro], which gives us a lens by which we can examine any magnified effects of review policy on diversity. Despite this focus, we aimed to generalize our analysis and increase the statistical validity and robustness of our measurements by including a large number conferences and of authors in our study.

