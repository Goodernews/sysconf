# Introduction {#sec:intro}

Peer review is a cornerstone of modern scientific research. And yet understanding and improving this process is challenging, because it can be hard to experiment with [@beverly13:findings; @mahoney77:publication; @mcnutt90:blinding]. For example, most conferences disallow parallel submission, and even within the same conference we cannot design an experiment where the same paper is reviewed multiple times with fully controlled variations in the process. Thus, decisions on peer-review policies are often based more on the opinions of editors or program chairs, and less based on hard data and evidence.

This paper attempts to present such data and evidence from statistical observations on the peer-review process for a specific year (2017) and a specific subfield of research (computer systems or "systems"). Like most subfields of computer science, the primary channel for publishing research results in systems is peer-reviewed conferences [@patterson99:evaluating; @vardi09:conferences]. Reputable systems conferences employ some similar policies, such as giving each paper at least three blind reviews (where the identity of the specific reviewers is hidden from authors). But conferences can vary considerably in other aspects, such as double-blind reviews (authors' identities are hidden from reviewers), rebuttals, two-phase reviews, etc. These decisions can potentially have dramatic effects on both the quality of the conference and the experience of the authors, but there appear to be conflicting opinions on the effects and tradeoffs of these policies [@mainguy05:peer].

In this study we combine data from multiple sources, including a large author survey, to analyze the effects of two different policies from the _conference author's perspective_. We are interested in the research question of how these two policies, author rebuttal and double-blind reviews, affect two populations in particular: women and non-native English speakers. By limiting our scope to conferences in a single subfield, we avoid the variability that might occur across a broader range of disciplines.^[Despite its large research output and enormous economic impact, we found no consensus definition for the field of "systems". For the purposes of this paper, we define it to be the study of computer hardware and software components, which includes research in operating systems, computer architectures, databases, parallel and distributed computing, and computer networks.]  This important subfield is notorious for poor gender diversity [@destefano18:micro; @fox06:engineering], which gives us a lens by which we can examine any magnified effects of review policy on diversity. Despite this focus, we aim to generalize our analysis and increase the statistical validity and robustness of our measurements by including a large number conferences and of authors in our study.

As an exploratory observation, another contribution of this paper is an exposition and description of the survey. In addition to the data's relevance to PC chairs and early-career researchers working to better understand the publication process, we hope our results on reviews that authors appreciate can be extended to the classroom, where peer and mentor feedback can also impact diversity [@ginsberg09:diversity].

To the best of our knowledge, this is the first cross-sectional survey of authors across computer systems. Past studies has concentrated on either very wide journal authorship [@editage18:perspectives; @publons18:peer; @sense09:peer; @solomon14:survey] or a single conference [@beverly13:findings; @daume15:naacl; @papagiannaki07:author; @parno17:SPsurvey], including SIGCSE [@walker02:variability]. We cite these works throughout our study wherever relevant.

<!--
__Organization__ The next section details our experimental methodology and data sources, followed by a section describing the survey's responses. Then, in Sec. \@ref(sec:analysis), we combine these raw quantitative data in an attempt to offer meaningful responses to qualitative questions.
-->
