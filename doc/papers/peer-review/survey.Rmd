# Author survey results {-}

We designed the survey to address issues of bias and diversity in peer review. This section details its questions, responses, and initial interpretation.
We sent our survey to all 5919 valid email addresses during the summer of 2018, and `r nrow(raw_survey)` authors responded. We asked a few demographic questions, as well as questions about the review process, repeated for up to three distinct papers from our data set. Non-responses to a question were marked by NA and excluded from later analyses.

Of the `r nrow(unique(survey$paper_id))` papers,
`r survey %>% group_by(paper_id) %>% mutate(n = n_distinct(name)) %>% filter(n > 1) %>% select(paper_id) %>% unique() %>% nrow()`
had responses from more than one author. These responses were typically identical or off by one unit, and always tested statistically insignificant in aggregate. In five papers, the responses from different authors were so inconsistent across questions that we elided them from our data.
<!---
```{r code-to-see-discrepancies}
# survey %>% group_by(paper_id) %>% summarize(tmax = range(as.numeric(months_research), na.rm = T)[2], tmin = range(as.numeric(months_research), na.rm = T)[1], diff = tmax-tmin)
```
-->

## Demographic Questions {-}

We asked three demographic questions to evaluate their role in the review experience. The first two also serve for comparison against external data, to assess the likelihood of selection bias in our survey.

### Which best describes your position during 2017? {-}

As shown in Fig. \@ref(fig:position-dist), about one third (`r pct(nrow(filter(demographics, position == "Student")), nrow(demographics))`%) of the respondents were students in 2017, another third or so professors of various ranks (`r pct(str_extract(demographics$position, "Professor") %>% na.omit() %>% length(), nrow(demographics))`%), and the rest were distributed between all other categories, including unknown.
For comparison, we looked at the inferred affiliation of `r nrow(good_affils)` total authors with an identifiable email affiliation. Of these, `r pct(sum(good_affils$sector == "COM"), nrow(good_affils))`% had an industry affiliation, compared to `r pct(nrow(filter(demographics, position == "Industry Researcher")), nrow(good_positions))`% of the non-NA survey respondents (`r report_chi(chisq.test(com_table))`). The difference for government researchers is a little larger:
`r pct(nrow(filter(demographics, position == "Government Researcher")), nrow(good_positions))`% by affiliation vs.
`r pct(sum(good_affils$sector == "GOV"), nrow(good_affils))`% among survey respondents,
but not significantly so
(`r report_chi(chisq.test(gov_table))`).

```{r position-dist, echo = F, message = F, out.width = '100%', warning = F, fig.cap = "Distribution of respondent positions."}
ggplot(demographics, aes(x = position)) +
         geom_histogram(stat = "count") + 
         scale_x_discrete(limits = c(NA, "Other", "Government Researcher", "Industry Researcher", "Professor", "Associate Professor", "Assistant Professor", "Postdoctoral Researcher", "Student")) +
  coord_flip() +
  theme_light()
```

### What is your gender? {-}

Of the `r nrow(filter(demographics, !is.na(gender)))` non-NA responses,
`r nrow(filter(demographics, gender == "F"))`
(`r pct(nrow(filter(demographics, gender == "F")), sum(!is.na(demographics$gender)))`%)
chose "Female". In our hand-verified gender data of all authors,
`r pct(nrow(filter(good_genders, gender=="F")), nrow(good_genders))`%
were female. These two proportions are close
(`r report_chi(chisq.test(gender_table))`),
leading us to believe there is no significant selection bias by gender.


### What is your English level proficiency? {-}

Of the `r nrow(filter(demographics, !is.na(native_english)))` non-NA respondents,
`r pct(nrow(filter(demographics, native_english == T)), sum(!is.na(demographics$native_english)))`%
of respondents chose "Native" for their English level proficiency. There appears to be no gender or position difference in the response to this question.

We also asked (and checked) for each paper whether there was any native English speaker among its coauthors. From this question, we estimate that about `r pct(sum(survey$any_english == T, na.rm = T), sum(!is.na(survey$any_english)), 0)`% of papers had at least one native speaking author.

## Paper History {-}

We also asked for each unique paper, up to three per author, to understand its history.

### How many months did it take to research and write? {-}
\hfill

```{r mor.dist, echo = F, message = F, warning = F}
levels(survey$months_research)[5] <- "13+"
df <- survey %>%
  group_by(paper_id) %>%
  summarize(mor = max(ordered(months_research))) %>%
  select(mor) %>%
  freq_and_prop(usena = "ifany")
df$Response = ordered(df$Response, levels = c("1-3", "4-6", "7-9", "10-12", "13+", NA))
knitr::kable(t(arrange(df, Response)), booktabs = T, align = "rrrrr") %>% kable_styling(position = "center", font_size = 8)
```

The answers to this question varied most among different responses to the same paper, although typically by no more than 3 months. The response to this question was not significantly associated with the team size (number of coauthors), or lead author's experience, gender, or sector. It was, however, correlated to the next question:

### How many conferences/journals was it submitted to prior to this publication? {-}
\hfill

```{r subs_dist, echo = F, message = F, warning = F}
tmp <- survey
tmp[!is.na(tmp$prior_subs) & tmp$prior_subs>4,]$prior_subs = 9  # Make table a little smaller
tmp$prior_subs <- factor(tmp$prior_subs)
levels(tmp$prior_subs)[6] <- "5+"
df <- tmp %>%
  group_by(paper_id) %>%
  summarize(prior = max(ordered(prior_subs))) %>%
  select(prior) %>%
  freq_and_prop(usena = "ifany")
df$Response = ordered(df$Response, levels = c("0", "1", "2", "3", "4", "5+", NA))
knitr::kable(t(arrange(df, Response)), booktabs = T, align = "r") %>% kable_styling(position = "center", font_size = 8)
```

It is instructive to see that over 40% of papers with responses are rejected on their first submission [@solomon14:survey; @wallach11:rebooting], and can take as many as 12 attempts to reach publication. 
Authors were also asked to type in the names of the venues they had previously submitted to. We note that many papers were accepted after previously being rejected from equivalent (or even the very same) conferences, which can be interpreted both positively and negatively. Some authors expressed frustration in their comments that the peer-review process can appear arbitrary [@francois15:arbit; @vardi09:conferences]. On the other hand, other authors expressed that an effective peer review process provides feedback that improves the paper for the next submission. These mixed opinions, combined with the resubmission history of so many papers, suggests that selection bias in this survey played a lesser role in deciding authors' experience one way or another.


## Rebuttal Process {-}

We asked several questions specific to author feedback on reviews.

### Did the conference allow you to address reviewers concerns before final acceptance notice? {-}

Of the non-NA responses,
`r pct(nrow(filter(survey, allow_rebuttal == T)), nrow(filter(survey, !is.na(allow_rebuttal))))`%
chose "Yes".
Contrast this with the conferences, of which only `r sum(all_confs$rebuttal)` of our `r nrow(all_confs)` offered a formal rebuttal option
(`r pct(sum(all_confs$npapers * all_confs$rebuttal), sum(all_confs$npapers))`%
when weighted by papers).
The discrepancy may be explained by some authors who specifically mentioned answering "Yes" to this question despite the lack of a formal rebuttal policy, because the conference had a "provisional acceptance" policy or mandatory revisions guided by a PC "shepherd". Although this response type is clearly different than a formal rebuttal, limiting our analysis to formal rebuttals only does not meaningfully change their results.


`r rebutters <- filter(survey, allow_rebuttal == T) %>% group_by(name, paper_id) %>% summarize(use = first(use_rebuttal), grade = first(gr_overall), mor = first(months_research), gender = first(mapped_gender), native = first(native_english)); pct(nrow(filter(rebutters, use == T)), nrow(rebutters), 2)`%
of the "Yes" respondents also reported that they took advantage of the rebuttal option. The few who didn't received higher overall acceptance grades on average,
(`r pct(filter(rebutters, use == F)$grade %>% mean(na.rm = T), 1)`%
vs. 
`r pct(filter(rebutters, use == T)$grade %>% mean(na.rm = T), 1)`%),
possibly obviating the need to respond [@daume15:naacl].

### Did you find the response process helpful? {-}

Of the non-NA responses,
`r pct(nrow(filter(survey, rebuttal_helpful == T)), nrow(filter(survey, !is.na(rebuttal_helpful))))`%
were positive. This high percentage may be a little surprising, considering how many PC chairs and authors alike commented privately on how little difference rebuttals make [@daume15:naacl]. One cautionary reminder is that the survey and statistics exclude rejected papers, which could lead to survivorship bias. It is quite plausible that authors of rejected papers were less enthused about the rebuttal process.

## Review Quality Assessment {-}

The following questions (one for each review and paper) were designed to assess the quality of the reviews.

### How many reviews did this paper receive? {-}
\hfill

```{r reviews.dist, echo = F, message = F, warning = F}
df <- survey %>%
  group_by(paper_id) %>%
  summarize(reviews = first(reviews)) %>%
  select(reviews) %>%
  freq_and_prop(usena = "ifany")
knitr::kable(t(df), booktabs=TRUE, align=c("rrrrrrr")) %>% kable_styling(position = "center")
```

The papers in our dataset average more than four reviews per paper, far better than the typical 2+ reviews in an average CS journal [@publons18:peer, p. 21]). This could partially explain the attractiveness of conferences over journals, at least in Systems. Authors were also asked to qualitatively approximate _how long each review_ was. it is encouraging to find over half of the responses showing one or more pages per review
(`r pct(nrow(filter(survey, length == "A Page" | length == "Multiple Pages")), sum(!is.na(survey$length)))`%),
whereas only about
`r pct(nrow(filter(survey, length == "1-2 Paragraphs")), sum(!is.na(survey$length)))`%
of reviews were reported to be less than half a page.

### How well did the reviewer understand the paper, in your estimation? {-}
\hfill

```{r understanding.dist, echo = F, message = F, warning = F}
df <- freq_and_prop(survey$understanding, usena = "ifany")
knitr::kable(df, booktabs = T, align = "lrr") %>% kable_styling(position = "center", font_size = 8)

poor_understanding <- filter(survey_with_confs, understanding == "Probably didn't read it" | understanding == "Misunderstood major points")
good_understanding <- filter(survey_with_confs, understanding != "Probably didn't read it" & understanding != "Misunderstood major points")
```

`r pct(nrow(filter(poor_understanding, length == "1-2 Paragraphs" | length == "Half a Page")), nrow(filter(poor_understanding, !is.na(length))))`%
of reviews that missed major points or worse spanned half a page or shorter, demonstrating how these two factors are linked
(`r report_chi(chisq.test(table(survey$understanding, survey$length)))`) [@papagiannaki07:author]. But longer is not always better or necessary, as these short reviews still comprise
`r pct(nrow(filter(survey_with_confs, understanding == "Perfectly", length == "1-2 Paragraphs" | length == "Half a Page")), nrow(filter(survey_with_confs, understanding == "Perfectly", !is.na(length))))`%
of the "perfect understanding" reviews, whereas multi-page reviews only comprise
`r pct(nrow(filter(survey_with_confs, understanding == "Perfectly", length == "Multiple Pages")), nrow(filter(survey_with_confs, understanding == "Perfectly", !is.na(length))))`%
of these.
In terms of paper history, the better-understood papers appear to have had a longer history, in terms prior submissions
(`r report_t(t.test(poor_understanding$prior_subs, good_understanding$prior_subs))`),
and in terms of months researched as well.
It's conceivable that previous rejections have helped improve the communication of a resubmitted paper. 

### How helpful did you find this review for improving the paper? {-}
\hfill

```{r helpfulness.dist, echo = F, message = F, warning = F}
df <- freq_and_prop(survey$helpfulness)
knitr::kable(df, booktabs = T, align = "lrr") %>% kable_styling(position = "center", font_size = 8)
```

The helpfulness of a review is closely linked to its reported level of understanding
(`r report_chi(chisq.test(table(survey$helpfulness, survey$understanding)))`),
which in turn also implies that it's closely linked to the review's length
(`r report_chi(chisq.test(table(survey$helpfulness, survey$length)))`).
This result is consistent with other surveys of journal authors [@editage18:perspectives; @sense19:peer].

### How fair would you say the review was? {-}
\hfill

```{r fairness.dist, echo = F, message = F, warning = F}
df <- freq_and_prop(survey$fairness)
knitr::kable(df, booktabs = T, align = "lrr") %>% kable_styling(position = "center", font_size = 8)
```

Once more, the perception of a review's fairness is closely tied to that of its understanding
(`r report_chi(chisq.test(table(survey$fairness, survey$understanding)))`)
and helpfulness
(`r report_chi(chisq.test(table(survey$fairness, survey$helpfulness)))`).

## Review Scores {-}

We've asked respondents to upload their reviews' text, or alternately to fill in the actual grades they received in the reviews of up to six reviews per paper and in seven different categories, when applicable. All grades were then normalized, so that the lowest grade in a category always received 0 and the highest always 1. The transcription of reviews, scaling, and calibration process are error-prone, introducing some noise to these responses. The distributions of these normalized scores are depicted in Fig. \@ref(fig:review-grades).

```{r review-grades, echo = F, message = F, warning = F, out.width = '100%', fig.cap = "Normalized grades and response distribution. Diamonds represent mean scores. Bars represent median scores, with a notched 95-pct confidence. N is the number of scores received in each category." }
survey_with_confs %>%
  select(starts_with("gr_")) %>%
  rename(Overall = gr_overall, Technical = gr_technical, Presentation = gr_present, Impact = gr_impact, Originality = gr_originality, Relevance = gr_relevance, Confidence = gr_confidence) %>%
  melt() %>%
  rename(Category = variable, Grade = value) %>%
  filter(!is.na(Grade)) %>%
  group_by(Category) %>%
  add_count() %>%
  ungroup() %>%
  ggplot(aes(x = Category, y = Grade, color = Category)) +
         geom_text(aes(y = 1.0, label = paste0("N=",n)), vjust = -0.5) +
         geom_boxplot(notch = T) +
         stat_summary(fun.y=mean, geom="point", shape=23, size=4) +
         theme(legend.position = "none") +
         ylab("Normalized grade") 
```

It's not surprising that all papers average above 50% for all grades---after all, they've been accepted. The inter-quartile range for the overall grade is 0.5-0.75, meaning that half the papers got accepted with an overall recommendation somewhere between "weak accept" and "accept". Perhaps more surprisingly, a good proportion of papers were accepted despite fairly low reviewer confidence. Another observation is that the "Relevance" grade appears mostly irrelevant, both because of its narrow distribution, and because of the low number of conferences that ask for it. It is plausible that an out-of-scope paper would simply get a "reject" overall recommendation, instead of specifying its relevance.

