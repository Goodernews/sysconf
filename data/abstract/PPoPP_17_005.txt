
A core, but often neglected, aspect of a programming language design is its memory (consistency) model. Sequential consistency (SC) is the most intuitive memory model for
programmers as it guarantees sequential composition of instructions and provides a simple abstraction of shared memory as a single global store with atomic read and writes. Unfortunately, SC is widely considered to be impractical due to
its associated performance overheads.
Perhaps contrary to popular opinion, this paper demonstrates that SC is achievable with acceptable performance
overheads for mainstream languages that minimize mutable
shared heap. In particular, we modify the Glasgow Haskell
Compiler to insert fences on all writes to shared mutable
memory accessed in nonfunctional parts of the program. For
a benchmark suite containing 1,279 programs, SC adds a geomean overhead of less than 0.4% on an x86 machine.
The efficiency of SC arises primarily due to the isolation
provided by the Haskell type system between purely functional and thread-local imperative computations on the one
hand, and imperative computations on the global heap on the
other. We show how to use new programming idioms to further reduce the SC overhead; these create a virtuous cycle of
less overhead and even stronger semantic guarantees (static
data-race freedom)
