
Efficient implementations of HPC applications for parallel architectures generally rely on external software packages (e.g.,
BLAS, LAPACK, CUDNN). While these libraries provide
highly optimized routines for certain characteristics of inputs
(e.g., square matrices), they generally do not retain optimal
performance across the wide range of problems encountered
in practice. In this paper, we present an input-aware autotuning framework for matrix multiplications and convolutions,
ISAAC, which uses predictive modeling techniques to drive
highly parameterized PTX code templates towards not only
hardware-, but also application-specific kernels. Numerical
experiments on the NVIDIA Maxwell and Pascal architectures show up to 3x performance gains over both cuBLAS
and cuDNN after only a few hours of auto-tuning.

(auto-tuning). There, the performance-critical portions (kernels) of the application code are parameterized, and those
parameters optimized for the architecture – and inputs –
of interest [5, 21]. The wide adoption of this technique in
fields like Linear Algebra [15, 18, 19] and Machine Learning
[2, 20] has given rise to a plethora of hardware-oblivious
software libraries capable of efficiently adapting virtually
any underlying memory hierarchies and/or multi-threading
schemes.
