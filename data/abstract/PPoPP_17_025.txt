
The current design trend in large scale machine learning is to use distributed clusters of CPUs and GPUs with
MapReduce-style programming. Some have been led to believe that this type of horizontal scaling can reduce or even
eliminate the need for traditional algorithm development,
careful parallelization, and performance engineering. This
paper is a case study showing the contrary: that the benefits
of algorithms, parallelization, and performance engineering,
can sometimes be so vast that it is possible to solve “clusterscale” problems on a single commodity multicore machine.

Connectomics is an emerging area of neurobiology that
uses cutting edge machine learning and image processing to extract brain connectivity graphs from electron microscopy images. It has long been assumed that the processing of connectomics data will require mass storage, farms of
CPU/GPUs, and will take months (if not years) of processing time. We present a high-throughput connectomics-ondemand system that runs on a multicore machine with less
than 100 cores and extracts connectomes at the terabyte per
hour pace of modern electron microscopes.
