
In this paper, we argue that in many “Big Data” applica-
tions, getting data into the system correctly and at scale via
traditional ETL (Extract, Transform, and Load) processes is
a fundamental roadblock to being able to perform timely an-
alytics or make real-time decisions. The best way to address
this problem is to build a new architecture for ETL which
takes advantage of the push-based nature of a stream pro-
cessing system. We discuss the requirements for a streaming
ETL engine and describe a generic architecture which sat-
isfies those requirements. We also describe our implemen-
tation of streaming ETL using a scalable messaging system
(Apache Kafka), a transactional stream processing system
(S-Store), and a distributed polystore (Intel’s BigDAWG),
as well as propose a new time-series database optimized to
handle ingestion internally.
