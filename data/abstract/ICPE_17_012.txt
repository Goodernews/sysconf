

Benchmarking is a widely-used technique to quantify the
performance of software systems. However, the design and
implementation of a benchmarking study can face several
challenges. In particular, the time required to perform a
benchmarking study can quickly spiral out of control, owing to the number of distinct variables to systematically examine. In this paper, we propose IRIS, an IteRative and
Intelligent Experiment Selection methodology, to maximize
the information gain while minimizing the duration of the
benchmarking process. IRIS selects the region to place the
next experiment point based on the variability of both dependent, i.e., response, and independent variables in that
region. It aims to identify a performance function that minimizes the response variable prediction error for a constant
and limited experimentation budget. We evaluate IRIS for a
wide selection of experimental, simulated and synthetic systems with one, two and three independent variables. Considering a limited experimentation budget, the results show
IRIS is able to reduce the performance function prediction
error up to 4.3 times compared to equal distance experiment
point selection. Moreover, we show that the error reduction
can further improve through system-specific parameter tuning. Analysis of the error distributions obtained with IRIS
reveals that the technique is particularly effective in regions
where the response variable is sensitive to changes in the
independent variables.

