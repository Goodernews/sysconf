
Tuple-independent probabilistic databases (TI-PDBs) handle uncertainty by annotating each tuple with a probability
parameter; when the user submits a query, the database derives the marginal probabilities of each output-tuple, assuming input-tuples are statistically independent. While query
processing in TI-PDBs has been studied extensively, limited
research has been dedicated to the problems of updating or
deriving the parameters from observations of query results.
Addressing this problem is the main focus of this paper. We
introduce Beta Probabilistic Databases (B-PDBs), a generalization of TI-PDBs designed to support both (i) belief updating and (ii) parameter learning in a principled and scalable
way. The key idea of B-PDBs is to treat each parameter as
a latent, Beta-distributed random variable. We show how
this simple expedient enables both belief updating and parameter learning in a principled way, without imposing any
burden on regular query processing. We use this model to
provide the following key contributions: (i) we show how to
scalably compute the posterior densities of the parameters
given new evidence; (ii) we study the complexity of performing Bayesian belief updates, devising efficient algorithms for
tractable classes of queries; (iii) we propose a soft-EM algorithm for computing maximum-likelihood estimates of the
parameters; (iv) we show how to embed the proposed algorithms into a standard relational engine; (v) we support our
conclusions with extensive experimental results.
