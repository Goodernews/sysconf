
Plans for exascale computing have identified power
and energy as looming problems for simulations running at that
scale. In particular, writing to disk all the data generated by these
simulations is becoming prohibitively expensive due to the energy
consumption of the supercomputer while it idles waiting for data
to be written to permanent storage. In addition, the power cost
of data movement is also steadily increasing. A solution to this
problem is to write only a small fraction of the data generated
while still maintaining the cognitive fidelity of the visualization.
With domain scientists increasingly amenable towards adopting
an in-situ framework that can identify and extract valuable
data from extremely large simulation results and write them to
permanent storage as compact images, a large-scale simulation
will commit to disk a reduced dataset of data extracts that will
be much smaller than the raw results, resulting in a savings in
both power and energy.

The goal of this paper is two-fold: (i) to understand the role
of in-situ techniques in combating power and energy issues of
extreme-scale visualization and (ii) to create a model for performance, power, energy, and storage to facilitate what-if analysis.
Our experiments on a specially instrumented, dedicated 150-node
cluster show that while it is difficult to achieve power savings
in practice using in-situ techniques, applications can achieve
significant energy savings due to shorter write times for in-situ
visualization. We present a characterization of power and energy for in-situ visualization; an application-aware, architecturespecific methodology for modeling and analysis of such in-situ
workflows; and results that uncover indirect power savings in
visualization workflows for high-performance computing (HPC).


