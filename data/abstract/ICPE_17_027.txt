

The usage of open source (OS) software is wide-spread across
many industries. While the functional quality of OS projects
is considered to be similar to closed-source software, much
is unknown about the quality in terms of performance. One
challenge for OS developers is that, unlike for functional
testing, there is a lack of accepted best practices for performance testing. To reveal the state of practice of performance
testing in OS projects, we conduct an exploratory study on
111 Java-based OS projects from GitHub. We study the
performance tests of these projects from five perspectives:
(1) developers, (2) size, (3) test organization, (4) types of
performance tests and (5) used tooling. We show that writing performance tests is not a popular task in OS projects:
performance tests form only a small portion of the test suite,
are rarely updated, and are usually maintained by a small
group of core project developers. Further, even though many
projects are aware that they need performance tests, developers appear to struggle implementing them. We argue
that future performance testing frameworks should provider
better support for low-friction testing, for instance via nonparameterized methods or performance test generation, as
well as focus on a tight integration with standard continuous
integration tooling.

