
Shared Memory stands out as a sine qua non for
parallel programming of many commercial and emerging multicore processors. It optimizes patterns of communication that
benefit common programming styles. As parallel programming
is now mainstream, those common programming styles are
challenged with emerging applications that communicate often
and involve large amount of data. Such applications include
graph analytics and machine learning, and this paper focuses
on these domains. We retain the shared memory model and
introduce a set of lightweight in-hardware explicit messaging
instructions in the instruction set architecture (ISA). A set
of auxiliary communication models are proposed that utilize
explicit messages to accelerate synchronization primitives, and
efficiently move computation towards data. The results on a
256-core simulated multicore demonstrate that the proposed
communication models improve performance and dynamic energy by an average of 4x and 42% respectively over traditional
shared memory.

