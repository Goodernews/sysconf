

In this paper, we present CrowdDQÂ§, a system that uses
the most recent set of crowdsourced voting evidence to dynamically issue questions to workers on Amazon Mechanical
Turk (AMT). CrowdDQS posts all questions to AMT in a
single batch, but delays the decision of the exact question to
issue a worker until the last moment, concentrating votes on
uncertain questions to maximize accuracy. Unlike previous
works, CrowdDQS also (1) optionally can decide when it is
more beneficial to issue gold standard questions with known
answers than to solicit new votes (both can help us estimate worker accuracy, but gold standard questions provide
a less noisy estimate of worker accuracy at the expense of
not obtaining new votes), (2) estimates worker accuracies in
real-time even with limited evidence (with or without gold
standard questions), and (3) infers the distribution of worker
skill levels to actively block poor workers. We deploy our
system live on AMT to over 1000 crowdworkers, and find
that CrowdDQ$S can accurately answer questions using up
to 6x fewer votes than standard approaches. We also find
there are many non-obvious practical challenges involved in
deploying such a system seamlessly to crowdworkers, and
discuss techniques to overcome these challenges.

