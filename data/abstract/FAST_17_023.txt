
Ensuring stable performance for storage stacks is important, especially with the growth in popularity of
hosted services where customers expect QoS guarantees. The same requirement arises from benchmarking
settings as well. One would expect that repeated, carefully controlled experiments might yield nearly identical performance results—but we found otherwise. We
therefore undertook a study to characterize the amount
of variability in benchmarking modern storage stacks. In
this paper we report on the techniques used and the results of this study. We conducted many experiments using several popular workloads, file systems, and storage
devices—and varied many parameters across the entire
storage stack. In over 25% of the sampled configurations, we uncovered variations higher than 10% in storage performance between runs. We analyzed these variations and found that there was no single root cause: it
often changed with the workload, hardware, or software
configuration in the storage stack. In several of those
cases we were able to fix the cause of variation and reduce it to acceptable levels. We believe our observations
in benchmarking will also shed some light on addressing
stability issues in production systems.
