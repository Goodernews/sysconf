
Training Convolutional Neural Network (CNN)
is a computationally intensive task whose parallelization has
become critical in order to complete the training in an
acceptable time. However, there are two obstacles to developing
a scalable parallel CNN in a distributed-memory computing
environment. One is the high degree of data dependency
exhibited in the model parameters across every two adjacent
mini-batches and the other is the large amount of data to be
transferred across the communication channel. In this paper,
we present a parallelization strategy that maximizes the overlap
of inter-process communication with the computation. The
overlapping is achieved by using a thread per compute node
to initiate communication after the gradients are available.
The output data of backpropagation stage is generated at
each model layer, and the communication for the data can
run concurrently with the computation of other layers. To
study the effectiveness of the overlapping and its impact on
the scalability, we evaluated various model architectures and
hyperparameter settings. When training VGG-A model using
ImageNet data sets, we achieve speedups of 62.97 x and 77.97
on 128 compute nodes using mini-batch sizes of 256 and 512,
respectively.

