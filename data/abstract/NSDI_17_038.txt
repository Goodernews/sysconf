

TuX? is a new distributed graph engine that bridges
graph computation and distributed machine learning.
TuxX? inherits the benefits of an elegant graph computation model, efficient graph layout, and balanced parallelism to scale to billion-edge graphs; we extend and
optimize it for distributed machine learning to support
heterogeneity, a Stale Synchronous Parallel model, and
a new MEGA (Mini-batch, Exchange, GlobalSync, and
Apply) model.

We have developed a set of representative distributed
machine learning algorithms in TUX‚Äù, covering both supervised and unsupervised learning. Compared to implementations on distributed machine learning platforms,
writing these algorithms in TUX? takes only about 25%
of the code: Our graph computation model hides the detailed management of data layout, partitioning, and parallelism from developers. Our extensive evaluation of
TuX?, using large data sets with up to 64 billion edges,
shows that TUX? outperforms state-of-the-art distributed
graph engines PowerGraph and PowerLyra by an order of
magnitude, while beating two state-of-the-art distributed
machine learning systems by at least 48%.

