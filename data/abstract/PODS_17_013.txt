

Recent works on bounding the output size of a conjunctive
query with functional dependencies and degree bounds have
shown a deep connection between fundamental questions in
information theory and database theory. We prove analogous output bounds for disjunctive datalog rules, and answer
several open questions regarding the tightness and looseness
of these bounds along the way. The bounds are intimately
related to Shannon-type information inequalities. We devise the notion of a “proof sequence” of a specific class of
Shannon-type information inequalities called “Shannon flow
inequalities”. We then show how a proof sequence can be
used as symbolic instructions to guide an algorithm called
PANDA, which answers disjunctive datalog rules within the
size bound predicted. We show that PANDA can be used
as a black-box to devise algorithms matching precisely the
fractional hypertree width and the submodular width runtimes for aggregate and conjunctive queries with functional
dependencies and degree bounds.
Our results improve upon known results in three ways.
First, our bounds and algorithms are for the much more general class of disjunctive datalog rules, of which conjunctive
queries are a special case. Second, the runtime of PANDA
matches precisely the submodular width bound, while the
previous algorithm by Marx has a runtime that is polynomial in this bound. Third, our bounds and algorithms work
for queries with input cardinality bounds, functional dependencies, and degree bounds.
Overall, our results showed a deep connection between
three seemingly unrelated lines of research; and, our results
on proof sequences for Shannon flow inequalities might be
of independent interest.
