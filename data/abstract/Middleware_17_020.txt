
The quality of machine learning (ML) and deep learning (DL) models are very sensitive to many different adjustable parameters that
are set before training even begins, commonly called hyperparameters. Efficient hyperparameter exploration is of great importance
to practitioners in order to find high-quality models with affordable time and cost. This is however a challenging process due to
a huge search space, expensive training runtime, sparsity of good
configurations, and scarcity of time and resources. We develop a
scheduling algorithm POP that quickly identifies among promising,
opportunistic and poor configurations of hyperparameters. It infuses
probabilistic model-based classification with dynamic scheduling
and early termination to jointly optimize quality and cost. We also
build a comprehensive hyperparameter exploration infrastructure,
HyperDrive, to support existing and future scheduling algorithms
for a wide range of usage scenarios across different ML/DL frameworks and learning domains. We evaluate POP and HyperDrive
using complex and deep models. The results show that we speedup
the training process by up to 6.7x compared with basic approaches
like random/grid search and up to 2.1x compared with state-of-theart approaches while achieving similar model quality compared
with prior work.
