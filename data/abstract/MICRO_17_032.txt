

Large-scale deep neural networks (DNNs) are both compute and
memory intensive. As the size of DNNs continues to grow, it is
critical to improve the energy efficiency and performance while
maintaining accuracy. For DNNs, the model size is an important factor affecting performance, scalability and energy efficiency. Weight
pruning achieves good compression ratios but suffers from three
drawbacks: 1) the irregular network structure after pruning, which
affects performance and throughput; 2) the increased training complexity; and 3) the lack of rigirous guarantee of compression ratio
and inference accuracy.

To overcome these limitations, this paper proposes CIRCNN,
a principled approach to represent weights and process neural
networks using block-circulant matrices. CIRCNN utilizes the Fast
Fourier Transform (FFT)-based fast multiplication, simultaneously
reducing the computational complexity (both in inference and
training) from O(n?) to O(n log n) and the storage complexity from
O(n?) to O(n), with negligible accuracy loss. Compared to other
approaches, C1RCNN is distinct due to its mathematical rigor: the
DNNs based on C1rCNN can converge to the same “effectiveness”
as DNNs without compression. We propose the CIRCNN architecture, a universal DNN inference engine that can be implemented in
various hardware/software platforms with configurable network
architecture (e.g., layer type, size, scales, etc.). In CIRCNN architecture: 1) Due to the recursive property, FFT can be used as the
key computing kernel, which ensures universal and small-footprint
implementations. 2) The compressed but regular network structure
avoids the pitfalls of the network pruning and facilitates high performance and throughput with highly pipelined and parallel design.
To demonstrate the performance and energy efficiency, we test C1rCNN in FPGA, ASIC and embedded processors. Our results show
that CrRCNN architecture achieves very high energy efficiency and

