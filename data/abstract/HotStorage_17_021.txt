

High accuracy scientific simulations on high performance computing (HPC) platforms generate large
amounts of data. To allow data to be efficiently analyzed, simulation outputs need to be refactored, compressed, and properly mapped onto storage tiers. This
paper presents Canopus, a progressive data management
framework for storing and analyzing big scientific data.
Canopus allows simulation results to be refactored into
a much smaller dataset along with a series of deltas with
fairly low overhead. Then, the refactored data are compressed, mapped, and written onto storage tiers. For
data analytics, refactored data are selectively retrieved
to restore data at a specific level of accuracy that satisfies analysis requirements. Canopus enables end users to
make trade-offs between analysis speed and accuracy onthe-fly. Canopus is demonstrated and thoroughly evaluated using blob detection on fusion simulation data.

