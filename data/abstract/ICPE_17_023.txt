

Previous work has shown that benchmark and application performance in public cloud computing environments can be highly variable. Utilizing Amazon EC2 traces that include measurements affected by CPU, memory, disk, and network performance, we study
commonly used methodologies for comparing performance measurements in cloud computing environments. The results show
considerable flaws in these methodologies that may lead to incorrect conclusions. For instance, these methodologies falsely report
that the performance of two identical systems differ by 38% using a confidence level of 95%. We then study the efficacy of the
Randomized Multiple Interleaved Trials (RMIT) methodology using the same traces. We demonstrate that RMIT could be used
to conduct repeatable experiments that enable fair comparisons in
this cloud computing environment despite the fact that changing
conditions beyond the userâ€™s control make comparing competing
alternatives highly challenging.

