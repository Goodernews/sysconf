[1] OpenACC-Standard.org,
http://www.openacc.org.

[2] XcalableMP Speciﬁcation Working Group, “XcalableMP
WebSite,” http://www.xcalablemp.org.
[3] M. Nakao, H. Murai, T. Shimosaka, A. Tabuchi, T. Hanawa,
Y. Kodama, T. Boku, and M. Sato, “XcalableACC: Extension of XcalableMP PGAS Language Using OpenACC for
Accelerator Clusters,” in Proceedings of the First Workshop
on Accelerator Programming Using Directives, ser. WACCPD
’14, 2014, pp. 27–36.

[4] RIKEN AICS and University of Tsukuba, “Omni Compiler
Project,” http://omni-compiler.org.

[5] J. Lee, M. Tran, T. Odajima, T. Boku, and M. Sato, “An
Extension of XcalableMP PGAS Lanaguage for Multi-node
GPU Clusters,” in Euro-Par 2011: Parallel Processing Workshops, ser. Lecture Notes in Computer Science, 2012, vol.
7155, pp. 429–439.


[6] T. Nomizu, D. Takahashi, J. Lee, T. Boku, and M. Sato,
“Implementation of XcalableMP Device Acceleration Extention with OpenCL.” in IPDPS Workshops. IEEE Computer
Society, 2012, pp. 2394–2403.
[7] T. Hanawa, Y. Kodama, T. Boku, and M. Sato, “Interconnection Network for Tightly Coupled Accelerators Architecture,”
in High-Performance Interconnects (HOTI), 2013 IEEE 21st
Annual Symposium on, Aug 2013, pp. 79–82.
[8] Y. Zheng, C. Iancu, P. H. Hargrove, S.-J. Min, and K. Yelick,
“Extending Uniﬁed Parallel C for GPU Computing,” in SIAM
Conference on Parallel Processing for Scientiﬁc Computing
(SIAMPP), 2010.
[9] S. Potluri, D. Bureddy, H. Wang, H. Subramoni, and
D. Panda, “Extending OpenSHMEM for GPU Computing,”
Parallel and Distributed Processing Symposium, International, pp. 1001–1012, 2013.

[10] L. Chen, L. Liu, S. Tang, L. Huang, Z. Jing, S. Xu, D. Zhang,
and B. Shou, “Uniﬁed Parallel C for GPU Clusters: Language
Extensions and Compiler Implementation,” in Languages and
Compilers for Parallel Computing, ser. Lecture Notes in
Computer Science, 2011, vol. 6548, pp. 151–165.
[11] D. Cunningham, R. Bordawekar, and V. Saraswat, “GPU
Programming in a High Level Language: Compiling X10
to CUDA,” in Proceedings of the 2011 ACM SIGPLAN X10
Workshop, ser. X10 ’11, 2011, pp. 1–10.

[12] A. Sidelnik, B. L. Chamberlain, M. J. Garzaran, and D. Padua,
“Using the High Productivity Language Chapel to Target
GPGPU Architectures,” Cray, Tech. Rep., 2011.
[13] C. Rasmussen, M. Sottile, S. Rasmussen, D. Nagle, and
W. Dumas, “CAFe: Coarray Fortran Extensions for Heterogeneous Computing,” in 2016 IEEE International Parallel and
Distributed Processing Symposium Workshops (IPDPSW),
May 2016, pp. 357–365.
[14] V. Cardellini, A. Fanfarillo, S. Filippone, and D. Rouson,
“Hybrid coarrays: A PGAS feature for many-core architectures,” in International Conference on Parallel Computing
(ParCo 2015), 2015.
[15] A. Hart, R. Ansaloni, and A. Gray, “Porting and scaling OpenACC applications on massively-parallel, GPU-accelerated
supercomputers,” The European Physical Journal Special
Topics, vol. 210, no. 1, pp. 5–16, 2012.

[16] “PC Cluster Consortium,” http://www.pccluster.org/en/.
[17] C. H. Koelbel and M. E. Zosel, The High Performance
FORTRAN Handbook. Cambridge, MA, USA: MIT Press,
1993.
[18] J. Reid, “Coarrays in the next fortran standard,” ISO/IEC
JTC1/SC22/WG5 N1824, Apr. 2010.
[19] Network-Based Computing Laboratory,
Home,” http://mvapich.cse.ohio-state.edu/.

[20] Cray,
“Cray
fortran
reference
manual
(8.5),”
http://docs.cray.com/PDF/Cray Fortran Reference Manual 85.pdf.
[21] RIKEN Advanced Center for Computing and Communication, “Himeno Benchmark,” http://accc.riken.jp/en/supercom/
himenobmt/.

[22] NASA Advanced Supercomputing Division, “NAS Parallel Benchmarks,” https://www.nas.nasa.gov/publications/
npb.html.
[23] Center for Computational Sciences, University of Tsukuba,
“HA-PACS
Project,”
http://www.ccs.tsukuba.ac.jp/eng/
research-activities/projects/ha-pacs/.
[24] A. Tabuchi, M. Nakao, and M. Sato, “A Source-to-Source
OpenACC Compiler for CUDA,” in Euro-Par Workshops,
2013, pp. 178–187.
[25] K. Tsugane, T. Boku, H. Murai, M. Sato, W. Tang, and
B. Wang, “Hybrid-view programming of nuclear fusion simulation code in the PGAS parallel programming language
XcalableMP,” Parallel Computing, vol. 57, pp. 37–51, 2016.
