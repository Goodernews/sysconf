[1] E. Chan, M. Heimlich, A. Purkayastha, and R. van de Geijn. 2007. Collective
Communication: Theory, Practice, and Experience. Concurrency and Computation:
Practice and Experience 19 (2007), 1749-1783.

[2] Chih-Chung Chang and Chih-Jen Lin. 2000. LIBSVM: Introduction and Benchmarks.
Technical Report. Department of Computer Science and Information Engineering,
National Taiwan University, Taipei, Taiwan.

[3] E. Chang, K. Zhu, H. Wang, H. Bai, J. Li, Z. Qiu, and H. Cui. 2008. Parallelizing
Support Vector Machines on Distributed Computers. In NIPS.

[4] H. P. Graf, E. Cosatto, L. Bottou, I Dundanovic, and V. Vapnik. 2005. Parallel
Support Vector Machines: The Cascade SVM. In NIPS.

[5] C.J. Hsieh, S. Si, and I. S. Dhillon. 2014. A Divide-and-Conquer Solver for Kernel
Support Vector Machines. In ICML.

[6] Cho-Jui Hsieh, Si Si, and Inderjit $ Dhillon. 2014. Fast prediction for largescale kernel machines. In Advances in Neural Information Processing Systems.
3689-3697.

[7] C.-J. Hsieh, H. F. Yu, and L S. Dhillon. 2015. PASSCoDe: Parallel ASynchronous
Stochastic dual Coordinate Descent. In ICML.

[8] P.-S. Huang, H. Avron, T. Sainath, V. Sindhwani, and B. Ramabhadran. 2014.
Kernel Methods Match Deep Neural Networks on TIMIT. In ICASSP. 205-209.

[9] M. Jaggi, V. Smith, M. Takaé, J. Terhorst, T. Hofmann, and M. Jordan. 2014.
Communication-efficient Distributed Dual Coordinate ascent. In NIPS.

[10] Thorsten Joachims. 1998. Making Large-scale SVM Learning Practical. In Advances in Kernel Methods — Support Vector Learning.

[11] S. Sathiya Keerthi, Kaibo Duan, Shirish Shevade, and Aun Neow Poo. 2005. A
Fast Dual Algorithm for Kernel Logistic Regression. Machine Learning 61 (2005),
151-165.

[12] S. Kumar, M. Mohri, and A. Talwalkar. 2009. Ensemble Nystrém Method. In NIPS.
[13] Q. Le, T. Sarlés, and A. Smola. 2013. Fastfood — Approximating Kernel Expansions
in Loglinear Time. In ICML.

[14] C.-P. Lee and D. Roth. 2015. Distributed Box-Constrained Quadratic Optimization
for Dual Linear SVM. In ICML.

[15] Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I. Jordan, Peter Richtarik, and
Martin Takaé. 2015. Adding vs. Averaging in Distributed Primal-Dual Optimization. In ICML.

[16] Dhruv Mahajan, S. Sathiya Keerthi, and Sundararajan Sellamanickam. 2014. A
Distributed Algorithm for Training Nonlinear Kernel Machines. arXiv:1405.4543
(2014).

[17] Dhruv Mahajan, S. Sathiya Keerthi, and Sundararajan Sellamanickam. 2014. A
distributed block coordinate descent method for training L1 regularized linear
classifiers. arXiv:1405.4544 (2014).

[18] John C. Platt. 1998. Fast Training of Support Vector Machines using Sequential
Minimal Optimization. In Advances in Kernel Methods - Support Vector Learning.

[19] Ali Rahimi and Benjamin Recht. 2008. Random Features for Large-scale Kernel
Machines. In NIPS.

[20] P. Richtarik and M. Takaé. 2012. Parallel Coordinate Descent Methods for Big
Data Optimization. Mathematical Programming (2012).

[21] C. Scherrer, A. Tewari, M. Halappanavar, and D. Haglin. 2012. Feature Clustering
for Accelerating Parallel Coordinate Descent. In NIPS.

[22] Si Si, Kai-Yang Chiang, Cho-Jui Hsieh, Nikhil Rao, and Inderjit S. Dhillon. 2016.
Goal-Directed Inductive Matrix Completion. In KDD. 1165-1174.

[23] Si Si, Cho-Jui Hsieh, and Inderjit S. Dhillon. 2016. Computationally Efficient
Nystrém Approximation using Fast Transforms. In ICML. 2655-2663.

[24] S. Si, C. J. Hsieh, and I. S. Dhillon. 2017. Memory Efficient Kernel Approximation.
(2017).

[25] Stephen Tu, Rebecca Roelofs, Shivaram Venkataraman, and Benjamin Recht.
2016. Large Scale Kernel Learning using Block Coordinate Descent. CoRR
abs/1602.05310 (2016).

[26] P.-W. Wang and C.-J. Lin. 2014. Iteration Complexity of Feasible Descent Methods
for Convex Optimization. Journal of Machine Learning Research 15 (2014), 15231548.

[27] C.K. L Williams and M. Seeger. 2001. Using the Nystrém Method to Speed Up
Kernel Machines. In NIPS.

[28] T. Yang. 2013. Trading Computation for Communication: Distributed Stochastic
Dual Coordinate Ascent. In NIPS.

[29] Y. You, J. Demmel, K. Czechowski, L. Song, and R. Vuduc. 2015. CA-SVM:
Communication-Avoiding Support Vector Machines on Clusters. In IPDPS.

[30] Yang You, Xiangru Lian, Ji Liu, Hsiang-Fu Yu, Inderjit Dhillon, James Demmel,
and Cho-Jui Hsieh. 2016. Asynchronous Parallel Greedy Coordinate Descent. In
NIPS.

[31] H.-F. Yu, F.-L. Huang, and C.-J. Lin. 2011. Dual Coordinate Descent Methods for
Logistic Regression and Maximum Entropy Models. Machine Learning 85, 1-2
(2011), 41-75.

[32] Huan Zhang and Cho-Jui Hsieh. 2016. Fixing the convergence problems in
parallel asynchronous dual coordinate descent. In ICDM.

[33] K. Zhang, L. Lan, Z. Wang, and F. Moerchen. 2012. Scaling up Kernel SVM on
Limited Resources: A Low-rank Linearization Approach. In AISTATS.

[34] Y. Zhang and L. Xiao. 2015. DiSCO: Communication-Efficient Distributed Optimization of Self-Concordant Loss. In ICML.

[35] Zeyuan A. Zhu, Weizhu Chen, Gang Wang, Chenguang Zhu, and Zheng Chen.
2009. P-packSVM: Parallel Primal Gradient Descent Kemel SVM. In ICDM.
