1994.
[2] OpenMP ARB, “OpenMP Application Program Interface Version 4.5,” 2015.
[3] C. Liao, Y. Yan, B. R. de Supinski, D. J. Quinlan, and B. Chapman, “Early Experiences with the OpenMP Accelerator Model,” in OpenMP in the Era of
Low Power Devices and Accelerators. Springer, 2013, pp. 84–98.
[4] S. Wienke, P. Springer, C. Terboven, and D. an Mey, “OpenACC–First Expe- riences with Real-World Applications,” in Euro-Par 2012 Parallel Processing.
Springer, 2012, pp. 859–870.
[5] T. R. Scogland, J. Keasler, J. Gyllenhaal, R. Hornung, B. R. de Supinski,
and H. Finkel, “Supporting Indirect Data Mapping in OpenMP,” in OpenMP:
Heterogenous Execution and Data Movements. Springer, 2015, pp. 260–272.
[6] X. Yu, H. Wang, W.-c. Feng, H. Gong, and G. Cao, “cuart: Fine-grained algebraic reconstruction technique for computed tomography images on gpus,” in Cluster, Cloud and Grid Computing (CCGrid), 2016 16th IEEE/ACM International
Symposium on. IEEE, 2016, pp. 165–168.
[7] T. Hoshino, N. Maruyama, S. Matsuoka, and R. Takaki, “CUDA vs. OpenACC:
Performance Case Studies with Kernel Benchmarks and a Memory-Bound CFD Application,” in Cluster, Cloud and Grid Computing (CCGrid), 2013 13th IEEE/ACM International Symposium on. IEEE, 2013, pp. 136–143.
[8] K. Krommydas, T. R. Scogland, and W.-c. Feng, “On the Programmability and Performance of Heterogeneous Platforms,” in Parallel and Distributed Systems (ICPADS), 2013 International Conference on. IEEE, 2013, pp. 224–231.
[9] Y. Wang, Q. Qin, S. C. W. See, and J. Lin, “Performance Portability Evaluation for OpenACC on Intel Knights Corner and NVIDIA Kepler.”
[10] Y. Lin and D. Padua, “Compiler Analysis of Irregular Memory Accesses,” ACM SIGPLAN Notices, vol. 35, no. 5, pp. 157–168, 2000.
[11] J. A. Stratton, C. Rodrigues, I.-J. Sung, N. Obeid, L.-W. Chang, N. Anssari, G. D. Liu, and W.-m. W. Hwu, “Parboil: A Revised Benchmark Suite for Scientific and Commercial Throughput Computing,” Center for Reliable and High-Performance Computing, vol. 127, 2012.
[12] L.-N. Pouchet et al., “Polybenchmarks Benchmark Suite,” 2013.
[13] R. C. Gonzalez and R. E. Woods, “Digital Image Processing,” 2002.
[14] S. F. Boll, “Suppression of Acoustic Noise in Speech Using Spectral Subtrac-
tion,” Acoustics, Speech and Signal Processing, IEEE Transactions on, vol. 27,
no. 2, pp. 113–120, 1979.
[15] A. Corparation, “Amd stream profiler.”
[16] X. Cui, T. R. Scogland, B. R. de Supinski, and W.-C. Feng, “Directive-based
pipelining extension for openmp,” in Cluster Computing (CLUSTER), 2016 IEEE
International Conference on. IEEE, 2016, pp. 481–484.
[17] A. Duran, E. Ayguade ́, R. M. Badia, J. Labarta, L. Martinell,
X. Martorell, and J. Planas, “OmpSs: A Proposal for Program-
ming Heterogeneous Multi-Core Architectures,” Parallel Processing Letters, vol. 21, no. 2, pp. 173–193, 2011. [Online]. Available: http://www.worldscinet.com/abstract?id=pii:S0129626411000151
[18] J. Bueno, J. Planas, A. Duran, R. M. Badia, X. Martorell, E. Ayguade ́, and J. Labarta, “Productive Programming of GPU Clusters with OmpSs,” Interna- tional Parallel and Distributed Processing Symposium, pp. 557–568, 2012.
[19] J. Bueno, X. Martorell, R. M. Badia, E. Ayguade ́, and J. Labarta, “Implementing OmpSs Support for Regions of Data in Architectures with Multiple Address Spaces,” in ACM International Conference on Supercomputing. ACM, Jun. 2013.
[20] C. Augonnet, S. Thibault, and R. Namyst, “StarPU: a Runtime System for Scheduling Tasks over Accelerator-Based Multicore Machines,” Laboratoire Bordelais de Recherche en Informatique - LaBRI, RUNTIME - INRIA Bordeaux - Sud-Ouest, Tech. Rep. RR-7240, Mar. 2010. [Online]. Available: http://hal.inria.fr/inria-00467677
[21] M. Bauer, S. Treichler, E. Slaughter, and A. Aiken, “Legion: Expressing Locality and Independence with Logical Regions,” in High Performance Computing, Networking, Storage and Analysis (SC), 2012 International Conference for. IEEE Computer Society, 2012, pp. 1–11.
[22] B. L. Chamberlain, D. Callahan, and H. P. Zima, “Parallel Programmability and the Chapel Language,” International Journal of High Performance Computing Applications, vol. 21, no. 3, pp. 291–312, 2007.
[23] T. R. Scogland, W.-c. Feng, B. Rountree, and B. R. de Supinski, “CoreTSAR: Core Task-Size Adapting Runtime,” 2014.
[24] T. R. Scogland, B. Rountree, W.-c. Feng, and B. R. de Supinski, “Heterogeneous Task Scheduling for Accelerated OpenMP,” in IEEE 26th International Parallel & Distributed Processing Symposium (IPDPS). IEEE, 2012, pp. 144–155.
[25] H. Wang, S. Potluri, M. Luo, A. K. Singh, S. Sur, and D. K. Panda, “MVAPICH2-GPU: Optimized GPU to GPU Communication for InfiniBand Clusters,” Computer Science-Research and Development, vol. 26, no. 3-4, pp. 257–266, 2011.
[26] H. Wang, S. Potluri, M. Luo, A. K. Singh, X. Ouyang, S. Sur, and D. K. Panda, “Optimized Non-Contiguous MPI Datatype Communication for GPU Clusters: Design, Implementation and Evaluation with MVAPICH2,” in Cluster Computing (CLUSTER), 2011 IEEE International Conference on. IEEE, 2011, pp. 308–316.
[27] H. Wang, S. Potluri, D. Bureddy, C. Rosales, and D. K. Panda, “GPU-Aware MPI on RDMA-Enabled Clusters: Design, Implementation and Evaluation,” Parallel and Distributed Systems, IEEE Transactions on, vol. 25, no. 10, pp. 2595–2605, 2014.
[28] A. M. Aji, J. Dinan, D. Buntinas, P. Balaji, W.-c. Feng, K. R. Bisset, and R. Thakur, “MPI-ACC: An Integrated and Extensible Approach to Data Movement in Accelerator-Based Systems,” in High Performance Computing and Communication & 2012 IEEE 9th International Conference on Embedded Software and Systems (HPCC-ICESS), 2012 IEEE 14th International Conference
on. IEEE, 2012, pp. 647–654.
[29] J. Jenkins, J. Dinan, P. Balaji, T. Peterka, N. F. Samatova, and R. Thakur, “Processing MPI Derived Datatypes on Noncontiguous GPU-Resident Data,” Parallel and Distributed Systems, IEEE Transactions on, vol. 25, no. 10, pp. 2627–2637, 2014.
[30] A. Aji, L. Panwar, F. Ji, K. Murthy, M. Chabbi, P. Balaji, K. Bisset, J. Dinan, W.-c. Feng, J. Mellor-Crummey et al., “MPI-ACC: Accelerator-Aware MPI for Scientific Applications,” Parallel and Distributed Systems, IEEE Transactions on, vol. 27, no. 5, pp. 1401–1414, 2016.
[31] W. Wu, G. Bosilca, S. Jeaugey, and J. Dongarra, “GPU-Aware Non-Contiguous Data Movement In Open MPI.”
[32] I. Gelado, J. E. Stone, J. Cabezas, S. Patel, N. Navarro, and W.-m. W. Hwu, “An Asymmetric Distributed Shared Memory Model for Heterogeneous Parallel Systems,”inACMSIGARCHComputerArchitectureNews,vol.38,no.1. ACM, 2010, pp. 347–358.
[33] T. B. Jablin, P. Prabhu, J. A. Jablin, N. P. Johnson, S. R. Beard, and D. I. August, “Automatic CPU-GPU Communication Management and Optimization,” in ACM SIGPLAN Notices, vol. 46, no. 6. ACM, 2011, pp. 142–151.
[34] Y. Yuan, M. F. Salmi, Y. Huai, K. Wang, R. Lee, and X. Zhang, “Spark-gpu: An accelerated in-memory data processing engine on clusters,” in Big Data (Big Data), 2016 IEEE International Conference on. IEEE, 2016.
[35] F. Ji, H. Lin, and X. Ma, “RSVM: A Region-Based Software Virtual Memory for GPU,” in Parallel Architectures and Compilation Techniques (PACT), 2013 22nd International Conference on. IEEE, 2013, pp. 269–278.
