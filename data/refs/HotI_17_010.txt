[1] “Non-uniﬁed memory access (NUMA).” [Online]. Available: https:
//en.wikipedia.org/wiki/Non-uniform memory access
[2] M. S. Birrittella, M. Debbage, R. Huggahalli, J. Kunz, T.Lovett, T. Rimmer, K. D. Underwood, and R. C. Zak, “Intel omni-path architecture:
Enabling scalable, high performance fabrics,” in 2015 IEEE 23rd Annual
Symposium on High-Performance Interconnects, Aug 2015, pp. 1–9.
[3] L. Chai, Q. Gao, and D. K. Panda, “Understanding the impact of multicore architecture in cluster computing: A case study with intel dualcore system,” in Seventh IEEE International Symposium on Cluster
Computing and the Grid (CCGrid ’07), May 2007, pp. 471–478.
[4] E. Jeannot and G. Mercier, “Near-optimal placement of mpi processes
on hierarchical numa architectures,” in Proceedings of the 16th
International Euro-Par Conference on Parallel Processing: Part II, ser.
Euro-Par’10. Berlin, Heidelberg: Springer-Verlag, 2010, pp. 199–210.
[Online]. Available: http://dl.acm.org/citation.cfm?id=1885276.1885299
[5] “PSM2.” [Online]. Available: https://github.com/01org/opa-psm2
[6] H. Pourreza and P. Graham, “On the programming impact of multicore,multi-processor nodes in mpi clusters,” in High Performance Computing Systems and Applications, 2007. HPCS 2007. 21st International
Symposium on, May 2007, pp. 1–1.
[7] C. Zhang, X. Yuan, and A. Srinivasan, “Processor afﬁnity and mpi
performance on smp-cmp clusters,” in 2010 IEEE International Symposium on Parallel Distributed Processing, Workshops and Phd Forum
(IPDPSW), April 2010, pp. 1–8.
[8] E. R. Rodrigues, F. L. Madruga, P. O. A. Navaux, and J. Panetta, “Multicore aware process mapping and its impact on communication overhead
of parallel applications,” in 2009 IEEE Symposium on Computers and
Communications, July 2009, pp. 811–817.
[9] G. Mercier and J. Clet-Ortega, “Towards an efﬁcient process placement
policy for mpi applications in multicore environments,” in Proceedings
of the 16th European PVM/MPI Users’ Group Meeting on Recent
Advances in Parallel Virtual Machine and Message Passing Interface.
Berlin, Heidelberg: Springer-Verlag, 2009, pp. 104–115. [Online].
Available: http://dx.doi.org/10.1007/978-3-642-03770-2 17
[10] F. Pellegrini, Scotch and LibScotch 5.1 Users Guide, INRIA BordeauxSud-Ouest, ENSEIRB and LaBRI, UMR CNRS 5800, 2008.
[11] M. K. Ferreira, V. S. Cruz, and P. O. A. Navaux, “Static process mapping heuristics evaluation for mpi processes in multi-core clusters,” in
Latin American Conference on High Performance Computing (CLCAR),
September 2011.
[12] D. Li, Y. Wang, and W. Zhu, “Topology-aware process mapping on
clusters featuring numa and hierarchical network,” in 2013 IEEE 12th
International Symposium on Parallel and Distributed Computing, June
2013, pp. 74–81.
[13] “Open MPI.” [Online]. Available: https://www.open-mpi.org/
[14] F. Broquedis, J. Clet-Ortega, S. Moreaud, N. Furmento, B. Goglin,
G. Mercier, S. Thibault, and R. Namyst, “hwloc: A generic framework for managing hardware afﬁnities in hpc applications,” in 2010
18th Euromicro Conference on Parallel, Distributed and Network-based
Processing, Feb 2010, pp. 180–186.
[15] J. Hursey, J. M. Squyres, and T. Dontje, “Locality-aware parallel process
mapping for multi-core hpc systems,” in 2011 IEEE International
Conference on Cluster Computing, Sept 2011, pp. 527–531.
[16] J. Hursey and J. M. Squyres, “Advancing application process afﬁnity
experimentation: Open mpi’s lama-based afﬁnity interface,” in Proceedings of the 20th European MPI Users’ Group Meeting, ser. EuroMPI
’13. New York, NY, USA: ACM, 2013, pp. 163–168.
[17] “Process afﬁnity: Hop on the bus gus.” [Online]. Available:
http://blogs.cisco.com/performance/process-afﬁnity-hop-on-the-bus-gus
[18] “OSU micro benchmarks.” [Online]. Available: http://mvapich.cse.
ohio-state.edu/benchmarks/