[1] E. Angelino, N. Larus-Stone, D. Alabi, M. Seltzer, and C. Rudin. 2017. Learning
certifiably optimal rule lists for categorical data. Preprint at arXiv:1704.01701
(April 2017).

[2] K.P. Bennett and J. A. Blue. 1996. Optimal Decision Trees. Technical Report. R.P.1
Math Report No. 214, Rensselaer Polytechnic Institute.

[3] I. Bratko. 1997. Machine learning: Between accuracy and interpretability. In
Learning, Networks and Statistics. International Centre for Mechanical Sciences,
Vol. 382. Springer Vienna, 163-177.

[4] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. 1984. Classification and
Regression Trees. Wadsworth.

[5] C. Chen and C. Rudin. 2017. Optimized Falling Rule Lists and Softly Falling Rule
Lists. (2017). Work in progress.

[6] H. A. Chipman, E. I. George, and R. E. McCulloch. 1998. Bayesian CART model
search. J. Amer. Statist. Assoc. 93, 443 (1998), 935-948.

[7] H. A. Chipman, E. I. George, and R. E. McCulloch. 2002. Bayesian treed models.
Machine Learning 48, 1 (2002), 299-320.

[8] H. A. Chipman, E. 1. George, and R. E. McCulloch. 2010. BART: Bayesian additive
regression trees. The Annals of Applied Statistics 4, 1 (2010), 266-298.

[9] P. Clark and T. Niblett. 1989. The CN2 induction algorithm. Machine Learning 3
(1989), 261-283. Issue 4.

[10] W. W. Cohen. 1995. Fast Effective Rule Induction. In Twelfth International Conference on Machine Learning (ICML). 115-123.

[11] R. M. Dawes. 1979. The robust beauty of improper linear models in decision
making. American Psychologist 34, 7 (1979), 571-582.

[12] D. Dension, B. Mallick, and A.F.M. Smith. 1998. A Bayesian CART algorithm.
Biometrika 85, 2 (1998), 363-377.

[13] D. Dobkin, T. Fulton, D. Gunopulos, S. Kasif, and S. Salzberg. 1996. Induction of
shallow decision trees. (1996).

[14] A. Farhangfar, R. Greiner, and M. Zinkevich. 2008. A Fast Way to Produce Optimal
Fixed-Depth Decision Trees. In International Symposium on Artificial Intelligence
and Mathematics (ISAIM 2008).

[15] Eibe Frank and Jan H. Witten. 1998. Generating Accurate Rule Sets Without
Global Optimization. In Proceedings of the Fifteenth International Conference on
Machine Learning (ICML ’98), 144-151.

[16] A. A. Freitas. 2014. Comprehensible classification models: a position paper. ACM
SIGKDD Explorations Newsletter 15, 1 (2014), 1-10.

[17] M. Garofalakis, D. Hyun, R. Rastogi, and K. Shim. 2000. Efficient Algorithms
for Constructing Decision Trees with Constraints. In Proceedings of the Sixth

KDD’17, August 13-17, 2017, Halifax, NS, Canada

ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
(KDD’98), 335-339.

[18] C. Giraud-Carrier. 1998. Beyond predictive accuracy: What?. In Proceedings of
the ECML-98 Workshop on Upgrading Learning to Meta-Level: Model Selection and
Data Transformation. 78-85.

[19] B. Goodman and S. Flaxman. 2016. European Union regulations on algorithmic
decision-making and a “right to explanation”. In ICML Workshop on Human
Interpretability in Machine Learning (WHI).

[20] R.C. Holte. 1993. Very simple classification rules perform well on most commonly
used datasets. Machine Learning 11, 1 (1993), 63-91.

[21] J. Huysmans, K. Dejaeger, C. Mues, J. Vanthienen, and B. Baesens. 2011. An
empirical evaluation of the comprehensibility of decision table, tree and rule
based predictive models. Decision Support Systems 51, 1 (2011), 141-154.

[22] H. Lakkaraju and C. Rudin. 2017. Cost-sensitive and Interpretable Dynamic
Treatment Regimes Based on Rule Lists. In Proceedings of the Artificial Intelligence
and Statistics (AISTATS).

[23] J. Larson, S. Mattu, L. Kirchner, and J. Angwin. 2016. How We Analyzed the
COMPAS Recidivism Algorithm. ProPublica (2016).

[24] N. L. Larus-Stone. 2017. Learning Certifiably Optimal Rule Lists: A Case For
Discrete Optimization in the 21st Century. (2017). Undergraduate thesis, Harvard
College.

[25] B. Letham, C. Rudin, T. H. McCormick, and D. Madigan. 2015. Interpretable
classifiers using rules and Bayesian analysis: Building a better stroke prediction
model. Annals of Applied Statistics 9, 3 (2015), 1350-1371.

[26] W. Li, J. Han, and J. Pei. 2001. CMAR: Accurate and efficient classification based
on multiple class-association rules. IEEE International Conference on Data Mining
(2001), 369-376.

[27] B. Liu, W. Hsu, and Y. Ma. 1998. Integrating classification and association rule
mining. In Proceedings of the 4th International Conference on Knowledge Discovery
and Data Mining (KDD ’98). 80-96.

[28] M. Marchand and M. Sokolova. 2005. Learning with decision lists of datadependent features. Journal of Machine Learning Research 6 (2005), 427-451.

[29] R.S. Michalski. 1969. On the quasi-minimal solution of the general covering
problem. In Proceedings of the Fifth International Symposium on Information
Processing. 125-128.

[30] New York Civil Liberties Union. 2014. Stop-and-Frisk Data. (2014). http:
//www_nyclu.org/content/stop-and-frisk- data.

[31] S. Nijssen and E. Fromont. 2010. Optimal constraint-based decision tree induction
from itemset lattices. Data Mining and Knowledge Discovery 21, 1 (2010), 9-51.

[32] J. R. Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann.

[33] P. R. Rijnbeek and J. A. Kors. 2010. Finding a Short and Accurate Decision Rule
in Disjunctive Normal Form by Exhaustive Search. Machine Learning 80, 1 (July
2010), 33-62.

[34] R. L. Rivest. 1987. Learning Decision Lists. Machine Learning 2, 3 (Nov. 1987),
229-246.

[35] U. Riickert and L. De Raedt. 2008. An experimental evaluation of simplicity in
rule learning. Artificial Intelligence 172 (2008), 19-28.

[36] C. Rudin and S. Ertekin. 2015. Learning Optimized Lists of Rules with Mathematical Programming. (2015). Unpublished.

[37] C. Rudin, B. Letham, and D. Madigan. 2013. Learning Theory Analysis for
Association Rules and Sequential Event Prediction. Journal of Machine Learning
Research 14 (2013), 3384-3436.

[38] S. Ripping. 2006. Learning interpretable models. Ph.D. Dissertation. Universitat
Dortmund.

[39] G.Shmueli. 2010. To explain or to predict? Statist. Sci. 25, 3 (Aug. 2010), 289-310.

[40] M. Sokolova, M. Marchand, N. Japkowicz, and J. Shawe-Taylor. 2003. The Decision
List Machine. In Advances in Neural Information Processing Systems (NIPS 03),
Vol. 15. 921-928,

[41] K. Vanhoof and B. Depaire. 2010. Structure of association rule classifiers: A
review. In Proceedings of the International Conference on Intelligent Systems and
Knowledge Engineering (ISKE ’10). 9-12.

[42] A. Vellido, J. D. Martin-Guerrero, and P. J.G. Lisboa. 2012. Making machine
learning models interpretable. In Proceedings of the European Symposium on
Artificial Neural Networks, Computational Intelligence and Machine Learning.

[43] F. Wang and C. Rudin. 2015. Causal Falling Rule Lists. Preprint at arXiv:1510.05189
(Oct. 2015).

[44] F. Wang and C. Rudin. 2015. Falling Rule Lists. In Proceedings of Artificial Intelligence and Statistics (AISTATS).

[45] H. Yang, C. Rudin, and M. Seltzer. 2017. Scalable Bayesian Rule Lists. In Proceedings
of the 34th International Conference on Machine Learning (ICML °17).

[46] X. Yin and J. Han. 2003. CPAR: Classification based on predictive association
rules. In Proceedings of the 2003 SIAM International Conference on Data Mining
(ICDM ’03). 331-335.

[47] Y. Zhang, E. B. Laber, A. Tsiatis, and M. Davidian. 2015. Using decision lists to
construct interpretable and parsimonious treatment regimes. Biometrics 71, 4
(2015), 895-904.
