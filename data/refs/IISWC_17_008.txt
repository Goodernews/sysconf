[1] “Ceph documentation,” Pool, PG and CRUSH config reference,
Ceph, URL: http://docs.ceph.com/docs/master/rados/configuration/poolpg-config-ref/.
[2] “Ceph homepage,” Ceph 11.2.0-”Kraken Release”, Ceph, URL:
http://ceph.com/releases/v11-2-0-kraken-released/, 2017.
[3] J. Axboe, “Flexible io tester,” 2015.
[4] D. Borthakur et al., “Hdfs raid,” in Hadoop User Group Meeting, 2010.
[5] A. D. Brunelle, “Block i/o layer tracing: blktrace,” HP, GelatoCupertino, CA, USA, 2006.
[6] F. Chen et al., “Understanding intrinsic characteristics and system
implications of flash memory based solid state drives,” in SIGMETRICS,
2009.
[7] Dell, PowerEdge RAID Controller H730P, 2014.
[8] A. G. Dimakis et al., “A survey on network codes for distributed
storage,” IEEE, 2011.
[9] K. S. Esmaili et al., “Core: Cross-object redundancy for efficient data
repair in storage systems,” in IEEE bigdata, 2013.
[10] D. Ford et al., “Availability in globally distributed storage systems.” in
OSDI, 2010.
[11] S. Ghemawat et al., “The google file system,” in ACM SIGOPS operating
systems review, 2003.
[12] A. Greenberg et al., “The cost of a cloud: research problems in data
center networks,” ACM SIGCOMM computer communication review,
2008.
[13] X.-Y. Hu et al., “Write amplification analysis in flash-based solid state
drives,” in SYSTOR, 2009.
[14] Y. Hu et al., “Nccloud: applying network coding for the storage repair
in a cloud-of-clouds.” in FAST, 2012.
[15] C. Huang et al., “Erasure coding in windows azure storage.” in Usenix
ATC, 2012.
[16] M. Jung et al., “Revisiting widely-held expectations of ssd and rethinking implications for systems,” SIGMETRICS, 2013.
[17] M. Jung, “Exploring design challenges in getting solid state drives closer
to cpu,” TC, 2016.
[18] J. Lacan et al., “Systematic mds erasure codes based on vandermonde
matrices,” IEEE Communications Letters, 2004.
[19] C. Metz, “Google remakes online empire with colossus,” Wired [Online].
Available: http://www. wired. com/2012/07/google-colossus/[Accessed:
May 4, 2014], 2012.
[20] S. Mitra et al., “Partial-parallel-repair (ppr): a distributed technique for
repairing erasure coded storage,” in EuroSys, 2016.
[21] L. J. Mohan et al., “Benchmarking the performance of hadoop triple
replication and erasure coding on a nation-wide distributed cloud,” in
NetCod, 2015.
[22] J. K. Omura et al., “Computational method and apparatus for finite field
arithmetic,” May 6 1986, uS Patent 4,587,627.
[23] D. S. Papailiopoulos et al., “Simple regenerating codes: Network coding
for cloud storage,” in INFOCOM, 2012.
[24] J. S. Plank et al., “Jerasure: A library in c facilitating erasure coding for
storage applications–version 2.0,” Technical Report UT-EECS-14-721,
University of Tennessee, Tech. Rep., 2014.
[25] K. Rashmi et al., “A solution to the network challenges of data recovery
in erasure-coded distributed storage systems: A study on the facebook
warehouse cluster.” in HotStorage, 2013.
[26] K. RAshmi et al., “Having your cake and eating it too: Jointly optimal
erasure codes for i/o, storage, and network-bandwidth.” in FAST, 2015.
[27] K. RASHMI et al., “Ec-cache: load-balanced, low-latency cluster
caching with online erasure coding,” in OSDI, 2016.
[28] I. S. Reed et al., “Polynomial codes over certain finite fields,” SIAM
journal, 1960.
[29] J. Rosenthal et al., “Maximum distance separable convolutional codes,”
Applicable Algebra in Engineering, Communication and Computing,
1999.
[30] M. Sathiamoorthy et al., “Xoring elephants: Novel erasure codes for big
data,” in PVLDB, 2013.
[31] N. Shahidi et al., “Exploring the potentials of parallel garbage collection
in ssds for enterprise storage systems,” in SC, 2016.
[32] M. Silberstein et al., “Lazy means smart: Reducing repair bandwidth
costs in erasure-coded distributed storage,” in SYSTOR, 2014.
[33] A. S. Tanenbaum et al., Distributed systems: principles and paradigms.
Prentice-Hall, 2007.
[34] S. A. Weil et al., “Ceph: A scalable, high-performance distributed file
system,” in OSDI, 2006.
[35] S. A. WEil et al., “Crush: Controlled, scalable, decentralized placement
of replicated data,” in SC, 2006.
[36] S. A. WEIL et al., “Rados: a scalable, reliable storage service for
petabyte-scale storage clusters,” in international workshop on Petascale
data storage, 2007.
[37] J. Zhang et al., “Opennvm: An open-sourced fpga-based nvm controller
for low level memory characterization,” in ICCD, 2015.
