[1] GPUDirect
Async
support
for
IB
Verbs.
https://github.com/gpudirect/libgdsync.
[2] NVIDIA CUDA. http://docs.nvidia.com/cuda.
[3] NVIDIA GPUDirect RDMA. http://docs.nvidia.com/cuda/gpudirectrdma/.
[4] NVIDIA GPUDirect Technology. https://developer.nvidia.com/gpudirect.
[5] OpenMPI:
Open
Source
High
Performance
Computing.
http://www.open-mpi.org/.
[6] Ronald Babich, Michael A Clark, and Bálint Joó. Parallelizing the
QUDA Library for Multi-GPU Calculations in Lattice Quantum Chromodynamics. In 2010 ACM/IEEE International Conference for High
Performance Computing, Networking, Storage and Analysis, pages 1–
11. IEEE, 2010.
[7] Feras Daoud, Amir Watad, and Mark Silberstein. GPUrdma: GPUside Library for High Performance Networking from GPU Kernels.
In Proceedings of the 6th International Workshop on Runtime and
Operating Systems for Supercomputers, page 6. ACM, 2016.
[8] Torsten Hoeﬂer, Jeffrey M. Squyres, Wolfgang Rehm, and Andrew
Lumsdaine. Frontiers of High Performance Computing and Networking
– ISPA 2006 Workshops: ISPA 2006 International Workshops, FHPCN,
XHPC, S-GRACE, GridGIS, HPC-GTP, PDCE, ParDMCom, WOMP,
ISDF, and UPWN, Sorrento, Italy, December 4-7, 2006. Proceedings,
chapter A Case for Non-blocking Collective Operations, pages 155–164.
Springer Berlin Heidelberg, Berlin, Heidelberg, 2006.
[9] Sangman Kim, Seonggu Huh, Xinya Zhang, Yige Hu, Amir Wated, Emmett Witchel, and Mark Silberstein. GPUnet: Networking Abstractions
for GPU Programs. In 11th USENIX Symposium on Operating Systems
Design and Implementation (OSDI 14), pages 201–216, Broomﬁeld, CO,
2014. USENIX Association.
[10] Matthew J. Koop, Rahul Kumar, and Dhabaleswar K. Panda. Can Software Reliability Outperform Hardware Reliability on High Performance
Interconnects?: A Case Study with MPI over Inﬁniband. In Proceedings
of the 22Nd Annual International Conference on Supercomputing, ICS
’08, pages 145–154, New York, NY, USA, 2008. ACM.
[11] Lawrence
Livermore
National
Laboratory.
Sierra.
http://computation.llnl.gov/computers/sierra.
[12] Message Passing Interface Forum. http://www.mpi-forum.org/.
[13] MPICH. https://www.mpich.org/.
[14] MVAPICH. http://mvapich.cse.ohio-state.edu/.
[15] Nikolay Sakharnykh. HPCMG. https://bitbucket.org/nsakharnykh/hpgmgcuda.
[16] Oak
Ridge
National
Laboratory.
SUMMIT.
https://www.olcf.ornl.gov/summit/.
[17] Lena Oden and Holger Fröning. Inﬁniband Verbs on GPU: A Case
Study of Controlling an InﬁniBand Network Device from the GPU.
International Journal of High Performance Computing Applications,
page 1094342015588142, 2015.
[18] S. Potluri, K. Hamidouche, A. Venkatesh, D. Bureddy, and D.K. Panda.
Efﬁcient Inter-node MPI Communication Using GPUDirect RDMA for
InﬁniBand Clusters with NVIDIA GPUs. In Parallel Processing (ICPP),
2013 42nd International Conference on, pages 80–89, Oct 2013.
[19] Rong Shi, Xiaoyi Lu, S. Potluri, K. Hamidouche, Jie Zhang, and D.K.
Panda. HAND: A Hybrid Approach to Accelerate Non-contiguous Data
Movement Using MPI Datatypes on GPU Clusters. In 43rd International
Conference on Parallel Processing (ICPP), pages 221–230, Sept 2014.
[20] US Department of Energy.
FACT SHEET: COLLABORATION
OF OAK RIDGE, ARGONNE, AND LIVERMORE (CORAL).
http://energy.gov/downloads/fact-sheet-collaboration-oak-ridge-argonneand-livermore-coral.
[21] Hao Wang, S. Potluri, Miao Luo, A.K. Singh, Xiangyong Ouyang,
S. Sur, and D.K. Panda. Optimized Non-contiguous MPI Datatype
Communication for GPU Clusters: Design, Implementation and Evaluation with MVAPICH2. In Cluster Computing (CLUSTER), 2011 IEEE
International Conference on, pages 308–316, Sept 2011.
[22] Hao Wang, Sreeram Potluri, Devendar Bureddy, Carlos Rosales, and
Dhabaleswar K Panda. GPU-aware MPI on RDMA-enabled Clusters:
Design, Implementation and Evaluation. IEEE Transactions on Parallel
and Distributed Systems, 25(10):2595–2605, 2014.