[1] J. Meng, D. Tarjan, and K. Skadron, “Dynamic Warp Subdivi- sion for Integrated Branch and Memory Divergence Tolerance,” in Intl. Symp. on Computer Architecture, 2010, pp. 235–246.
[2] N. Chatterjee, M. O’Connor, G. H. Loh, N. Jayasena, and R. Balasubramonian, “Managing DRAM Latency Divergence in Irregular GPGPU Applications,” in Intl. Conf. for High Performance Computing, Networking, Storage and Analysis (SC), Nov 2014, pp. 128–139.
[3] Y. Yang, P. Xiang, M. Mantor, N. Rubin, and H. Zhou, “Shared Memory Multiplexing: A Novel Way to Improve GPGPU Throughput,” in Intl. Conf. on Parallel Architectures and Compilation Techniques (PACT), Sep 2012, pp. 283–292.
[4] J. S. Liptay, “Structural Aspects of the System/360 Model 85, Part II: The Cache,” IBM Systems Journal, vol. 7, pp. 15–21, Mar 1968.
[5] S. Kumar, H. Zhao, A. Shriraman, E. Matthews, S. Dwarkadas, and L. Shannon, “Amoeba-Cache: Adaptive Blocks for Elim- inating Waste in the Memory Hierarchy,” in Intl. Symp. on Microarchitecture (MICRO), Dec 2012, pp. 376–388.
[6] “Using Shared Memory in CUDA C/C++,” https://devblogs. nvidia.com/parallelforall/using- shared- memory- cuda- cc/.
[7] C. Shelor, “Logic and method for reading data from cache,” Nov. 4 2004, uS Patent App. 10/429,009. [Online]. Available: http://137.175.46.122/patents/US20040221117
[8] B. Sinharoy, J. Van Norstrand, R. Eickemeyer, H. Le, J. Leen- stra, et al, “IBM POWER8 Processor Core Microarchitecture,” IBM Journal of Research and Development, vol. 59, no. 1, pp. 2:1–2:21, Jan 2015.
[9] R. B. Naveen Muralimanohar and N. P. Jouppi, “CACTI 6.0: A Tool to Model Large Caches,” HP Lab., Tech. Rep., 2009.
[10] X. Chen, L.-W. Chang, C. Rodrigues, J. Lv, Z. Wang, and W. mei Hwu, “Adaptive Cache Management for Energy- Efficient GPU Computing,” in International Symposium on Microarchitecture (MICRO), Dec 2014, pp. 343–355.
[11] NVIDIA’s Next Generation CUDA Compute Architecture: Fermi, NVIDIA Corporation, 2009.
[12] M. Gebhart, S. W. Keckler, B. Khailany, R. Krashinsky, and W. J. Dally, “Unifying Primary Cache, Scratch, and Register File Memories in a Throughput Processor,” in Intl. Symp. on Microarchitecture (MICRO), Dec 2012, pp. 96–106.
[13] Q. Xu, H. Jeon, and M. Annavaram, “Graph Processing on GPUs: Where are the Bottlenecks?” in Intl. Symp. on Workload Characterization (IISWC), 2014, pp. 140–149.
[14] NVIDIA CUDA C Programming Guide, NVIDIA Corpora- tion, 2015.
[15] NVIDIA GeForce GTX 980, NVIDIA Corporation, 2014. [16] S. Che, M. Boyer, J. Meng, D. Tarjan, J. Sheaffer, S.- H. Lee, and K. Skadron, “Rodinia: A benchmark Suite for Heterogeneous Computing,” in IEEE International Symposium on Workload Characterization (IISWC), Oct 2009, pp. 44–54. [17] J. A. Stratton, C. Rodrigues, I.-J. Sung, N. Obeid, L.-W. Chang, et al, “Parboil: A Revised Benchmark Suite for Sci- entific and Commercial Throughput Computing,” Center for
Reliable and High-Performance Computing, 2012.
[18] K. P. Martin Burtscher, Rupesh Nasre, “A Quantitative Study of Irregular Programs on GPUs,” in Intl. Symp. on Workload
Characterization (IISWC), Nov 2012, pp. 141–151.
[19] B. He, W. Fang, Q. Luo, N. K. Govindaraju, and T. Wang, “Mars: A MapReduce Framework on Graphics Processors,” in International Conference on Parallel Architectures and
Compilation Techniques (PACT), Oct 2008, pp. 260–269.
[20] A. Bakhoda, G. Yuan, W. Fung, H. Wong, and T. Aamodt, “Analyzing CUDA Workloads Using a Detailed GPU Simula- tor,” in International Symposium on Performance Analysis of
Systems and Software (ISPASS), Apr 2009, pp. 163–174.
[21] “Inside Kepler,” http://gpu.cs.uct.ac.za/Slides/Kepler.pdf. [22] T. Rogers, M. O’Connor, and T. Aamodt, “Cache-Conscious
Wavefront Scheduling,” in IEEE/ACM International Sympo-
sium on Microarchitecture (MICRO), Dec 2012, pp. 72–83. [23] T. G. Rogers, M. O’Connor, and T. M. Aamodt, “Divergence- aware Warp Scheduling,” in IEEE/ACM International Sympo-
sium on Microarchitecture (MICRO), Dec 2013, pp. 99–110. [24] Z. Zheng, Z. Wang, and M. Lipasti, “Adaptive Cache and Concurrency Allocation on GPGPUs,” IEEE Computer Archi-
tecture Letters (CAL), vol. PP, no. 99, pp. 1–1, Sep 2014. [25] W. Jia, K. A. Shaw, and M. Martonosi, “MRPB: Memory request prioritization for massively parallel processors,” in IEEE International Symposium on High Performance Com-
puter Architecture (HPCA), Feb 2014, pp. 272–283.
[26] X. Chen, L.-W. Chang, C. Rodrigues, J. Lv, Z. Wang, and W.- M. Hwu, “Adaptive Cache Management for Energy-Efficient GPU Computing,” in IEEE/ACM International Symposium on
Microarchitecture (MICRO), Dec 2014, pp. 343–355.
[27] M.Rhu,M.Sullivan,J.Leng,andM.Erez,“ALocality-aware Memory Hierarchy for Energy-efficient GPU Architectures,” in
Intl. Symp. on Microarchitecture (MICRO), 2013, pp. 86–98.
