[1] Envytools.
envytools.
[2] AMD. clMath. https: //github.com/clMathLibraries.
[3] ACML AMD. AMD Core Math Library (ACML), 2014.

https://github.com/envytools/
[4] Alexandro Baldassin, Paulo Cesar Centoducatte, and Sandro
Rigo. Extending the archc language for automatic generation of assemblers. In 17th International Symposium on Computer Architecture and High Performance Computing (SBACPAD’05), pages 60-67. IEEE, 2005.

[5] Daniel J Bernstein, Hsieh-Chung Chen, Chen-Mou Cheng,
Tanja Lange, Ruben Niederhagen, Peter Schwabe, and Bo-Yin
Yang. Usable assembly language for GPUs: a success story.
IACR Cryptology ePrint Archive, 2012:137, 2012.

[6] Lung-Sheng Chien. Hand tuned SGEMM on GT200 gpu.
Technical report, Tech. rep., Department of Mathematics, Tsing Hua University, Taiwan, 2010.

[7] Jack Hilaire Choquette, Manuel Olivier Gautho, and John Erik
Lindholm. Methods and apparatus for source operand collector caching, January 28 2014. US Patent 8,639,882.

[8] Christian S Collberg. Reverse interpretation+ mutation analysis= automatic retargeting. In ACM SIGPLAN Notices, volume 32, pages 57-70. ACM, 1997.

[9] Agner Fog. Instruction tables: Lists of instruction latencies,
throughputs and micro-operation breakdowns for Intel, AMD
and VIA CPUs. Denmark (Lyngby): Technical University of
Denmark, 2012.

[10] Scott Gray. MaxAs.
NervanaSystems/maxas.

[11] Scott Gray. NervanaGPU. https ://github.com/
NervanaSystems/maxas/wiki/SGEMM.

https://github.com/

[12] Yunging Hou. AsFermi. https://code.google.com/
archive/p/asfermi/wikis.

[13] Wilson C Hsieh, Dawson R Engler, and Godmar Back.
Reverse-engineering instruction encodings. In USENIX Annual Technical Conference, General Track, pages 133-145,
2001.

[14] MKL Intel. Intel Math Kernel Library, 2007.

[15] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks.

In Advances in neural information processing systems, pages
1097-1105, 2012.

[16] Junjie Lai and André Seznec. Performance upper bound
analysis and optimization of sgemm on Fermi and Kepler
GPUs. In Code Generation and Optimization (CGO), 2013
IEEE/ACM International Symposium on, pages 1-10. IEEE,
2013.

[17] M. Lukyanov, B. Beylin, R.S. Glanville, and A. Grosul. Efficient placement of texture barrier instructions, February 20
2014. US Patent App. 13/590,075.

[18] Xinxin Mei, Kaiyong Zhao, Chengjian Liu, and Xiaowen Chu.
Benchmarking the memory hierarchy of modern GPUs. In
Network and Parallel Computing, pages 144-156. Springer,
2014.

[19] R Nath, S Tomov, and J Dongarra. An improved MAGMA
GEMM for Fermi GPUs, university of tennessee computer
science technical report. Technical report, UTCS-10-655,
July, 2010.

[20] Nvidia. Nvidias next generation CUDA compute architecture:
Kepler GK110, the fastest, most efficient HPC architecture

43

ever built. White Paper, 2012.
[21] NVidia. CUDA binary utilities. September 2015.

[22] NVIDIA. Parallel thread execution ISA v4.3. http://docs.
nvidia.com/cuda/parallel-thread-execution/
#axzz42f7ftJVy, September 2015.

[23] Pierre Sermanet, David Eigen, Xiang Zhang, Michaél Mathieu, Rob Fergus, and Yann LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. arXiv preprint arXiv: 1312.6229, 2013.

[24] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv
preprint arXiv: 1409. 1556, 2014.

[25] Guangming Tan, Linchuan Li, Sean Triechle, Everett Phillips,
Yungang Bao, and Ninghui Sun. Fast implementation of
dgemm on fermi gpu. In Proceedings of 2011 International
Conference for High Performance Computing, Networking,
Storage and Analysis, page 35. ACM, 2011.

[26] D. Tarjan and K. Skadron. Policy based allocation of register
file cache to threads in multi-threaded processor, June 12
2012. US Patent 8,200,949.

[27] Wladimir J. van der Laan. Decuda. https://github.com/
laanwj/decuda.

[28] Vasily Volkov. Better performance at lower occupancy. In
Proceedings of the GPU technology conference, GTC, volume 10, page 16. San Jose, CA, 2010.

[29] Vasily Volkov and James W Demmel. Benchmarking GPUs to
tune dense linear algebra. In High Performance Computing,
Networking, Storage and Analysis, 2008. SC 2008. International Conference for, pages 1-11. IEEE, 2008.

[30] Qian Wang, Xianyi Zhang, Yunquan Zhang, and Qing Yi.
Augem: automatically generate high performance dense linear algebra kernels on x86 cpus. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, page 25. ACM, 2013.

[31] Henry Wong, Misel-Myrto Papadopoulou, Maryam
Sadooghi-Alvandi, and Andreas Moshovos. Demystifying GPU microarchitecture through microbenchmarking. In
Performance Analysis of Systems & Software (ISPASS), 2010
IEEE International Symposium on, pages 235-246. IEEE,
2010.

[32] Jingyue Wu, Artem Belevich, Eli Bendersky, Mark Heffernan,
Chris Leary, Jacques Pienaar, Bjarke Roune, Rob Springer,
Xuetian Weng, and Robert Hundt. gpucc: an open-source
GPGPU compiler. In Proceedings of the 2016 International
Symposium on Code Generation and Optimization, pages
105-116. ACM, 2016.

[33] Xiuxia Zhang. KeplerAs.
PAA-NCIC/PPoPP2017 _artifact.

https://github.com/
