[1] Jorge Albericio, Patrick Judd, Tayler Hetherington, Tor Aamodt, Natalie Enright
Jerger, and Andreas Moshovos. 2016. Cnvlutin: Ineffectual-neuron-free Deep
Neural Network Computing. In Proceedings of the 43rd International Symposium
on Computer Architecture (ISCA ’16). IEEE Press, Piscataway, NJ, USA, 1-13.
DOT-hitps://doi.org/10.1109/ISCA.2016.11

[2] M. Alwani, H. Chen, M. Ferdman, and P. Milder. 2016. Fused-layer CNN accelerators. In Proceedings of the 49th Annual IEEE/ACM International Symposium
on Microarchitecture (MICRO ’16). TEEE Computer Society, Washington, DC,
USA, 1-12. DOL:https://doi.org/10.1109/MICRO.2016.7783725

[3] Srimat Chakradhar, Murugan Sankaradas, Venkata Jakkula, and Srihari Cadambi.
2010. A Dynamically Configurable Coprocessor for Convolutional Neural Networks. In Proceedings of the 37th Annual International Symposium on Computer Architecture (ISCA ’10), ACM, New York, NY, USA, 247-257. DOT:https:
//doi.org/10.1145/1815961.18 15993

[4] Tianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengyong Wu, Yunji Chen,
and Olivier Temam. 2014. DianNao: A Small-footprint High-throughput Accelerator for Ubiquitous Machine-learning. In Proceedings of the 19th International Conference on Architectural Support for Programming Languages
and Operating Systems (ASPLOS ’14). ACM, New York, NY, USA, 269-284.
DOT-hitps://doi.org/10.1145/2541940.2541967

[5] Yunji Chen, Tao Luo, Shaoli Liu, Shijin Zhang, Liqiang He, Jia Wang, Ling Li,
Tianshi Chen, Zhiwei Xu, Ninghui Sun, and Olivier Temam. 2014, DaDianNao: A
Machine-Learning Supercomputer. In Proceedings of the 47th Annual IEEE/ACM
International Symposium on Microarchitecture (MICRO ’ 14). TEEE Computer
Society, Washington, DC, USA, 609-622. DOT:https://doi.org/10.1109/MICRO.
2014.58

[6] Yu-Hsin Chen, Joel Emer, and Vivienne Sze. 2016. Eyeriss: A Spatial Architecture
for Energy-efficient Dataflow for Convolutional Neural Networks. In Proceedings
of the 43rd International Symposium on Computer Architecture (ISCA ’16). TEEE
Press, Piscataway, NJ, USA, 367-379. DOI:https://doi.org/10.1109/ISCA.2016.
40

[7] Yu-Hsin Chen, Tushar Krishna, Joel S Emer, and Vivienne Sze. 2017. Eyeriss:
An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural
Networks. [EEE Journal of Solid-State Circuits 52, 1 (Jan 2017), 127-138.
DOT:https://doi.org/10.1109/JSSC.2016.2616357

[8] Ping Chi, Shuangchen Li, Cong Xu, Tao Zhang, Jishen Zhao, Yongpan Liu, Yu
Wang, and Yuan Xie. 2016. PRIME: A Novel Processing-in-memory Architecture
for Neural Network Computation in RERAM-based Main Memory. In Proceedings
of the 43rd International Symposium on Computer Architecture (ISCA ’16), TEEE
Press, Piscataway, NJ, USA, 27-39. DOI:https://doi.org/10.1109/ISCA.2016.13
[9] Ronan Collobert and Jason Weston. 2008. A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. In Proceedings
of the 25th International Conference on Machine Learning (ICML ’08). ACM,
New York, NY, USA, 160-167. DOT:https://doi.org/10.1145/1390156.1390177
[10] Clément Farabet, Berin Martini, Polina Akselrod, Selguk Talay, Yann LeCun, and
Eugenio Culurciello. 2010. Hardware accelerated convolutional neural networks
for synthetic vision systems. In Proceedings of the 2010 IEEE International
Symposium on Circuits and Systems (ISCAS ’10). 257-260. DOT:https://doi.org/
10.1109/ISCAS .2010.5537908

[11] Clément Farabet, Berin Martini, Benoit Corda, Polina Akselrod, Eugenio Culurciello, and Yann LeCun. 2011. NeuFlow: A runtime reconfigurable dataflow
processor for vision. In CVPR 2011 WORKSHOPS. 109-116. DOT:https:
/Aoi.org/10.1109/CVPRW.2011.5981829

[12] Clément Farabet, Cyril Poulet, Jefferson Y Han, and Yann LeCun. 2009. CNP:
An FPGA-based processor for Convolutional Networks. In Proceedings of the
19th International Conference on Field Programmable Logic and Applications
(FPL 09). 32-37. DOT:https://doi.org/10.1109/FPL.2009.5272559

[13] Forrest N. Jandola, Matthew W. Moskewicz, Khalid Ashraf, Song Han, William J.
Dally, and Kurt Keutzer. 2016. SqueezeNet: AlexNet-level accuracy with 50x
fewer parameters and <1MB model size. CoRR abs/1602.07360 (2016).

[14] Patrick Judd, Jorge Albericio, Tayler Hetherington, Tor M. Aamodt, Natalie Enright Jerger, and Andreas Moshovos. 2016. Proteus: Exploiting Numerical Precision Variability in Deep Neural Networks. In Proceedings of the 2016 International Conference on Supercomputing (ICS ’16)., ACM, New York, NY, USA,
Article 23, 23:1-23:12 pages. DOI-https://doi.org/10.1145/2925426.2926294
[15] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012, ImageNet Classification with Deep Convolutional Neural Networks. In Proceedings of the 25th
International Conference on Neural Information Processing Systems (NIPS ’12).
Curran Associates Inc., Red Hook, NY, USA, 1097-1105.

[16] Huimin Li, Xitian Fan, Li Jiao, Wei Cao, Xuegong Zhou, and Lingli Wang.
2016. A high performance FPGA-based accelerator for large-scale convolutional
neural networks. In Proceedings of the 26th International Conference on Field
Programmable Logic and Applications (FPL ’16). IEEE Computer Society, Los
Alamitos, CA, USA, 1-9. DOT:https://doi.org/10.1109/FPL.2016.7577308
[17] Adron van den Oord, Sander Dieleman, and Benjamin Schrauwen. 2013. Deep
Content-based Music Recommendation. In Proceedings of the 26th International
Conference on Neural Information Processing Systems (NIPS ’13). Curran Associates Inc., Red Hook, NY, USA, 2643-2651.

[18] Maurice Peemen, Bart Mesman, and Henk Corporaal. 2015. Inter-tile Reuse
Optimization Applied to Bandwidth Constrained Embedded Accelerators. In
Proceedings of the 2015 Design, Automation & Test in Europe Conference &
Exhibition (DATE ’15). EDA Consortium, San Jose, CA, USA, 169-174.
[19] Maurice Peemen, Arnaud AA Setio, Bart Mesman, and Henk Corporaal. 2013.
Memory-centric accelerator design for Convolutional Neural Networks. In Proceedings of the 31st IEEE International Conference on Computer Design (ICCD
”13). 13-19. DOLhttps://doi.org/10.1109/ICCD.2013.6657019

[20] Andrew Putnam, Adrian M. Caulfield, Eric S. Chung, Derek Chiou, Kypros Constantinides, John Demme, Hadi Esmaeilzadeh, Jeremy Fowers, Gopi Prashanth
Gopal, Jan Gray, Michael Haselman, Scott Hauck, Stephen Heil, Amir Hormati,
Joo-Young Kim, Sitaram Lanka, James Larus, Eric Peterson, Simon Pope, Aaron
Smith, Jason Thong, Phillip Yi Xiao, and Doug Burger. 2014. A Reconfigurable
Fabric for Accelerating Large-scale Datacenter Services. In Proceedings of the
41st Annual International Symposium on Computer Architecture (ISCA ’14).
TEEE Press, Piscataway, NJ, USA, 13-24. DOI:https://doi.org/10.1145/2678373.
2665678

[21] Jiantao Qiu, Jie Wang, Song Yao, Kaiyuan Guo, Boxun Li, Erjin Zhou, Jincheng
Yu, Tiangi Tang, Ningyi Xu, Sen Song, Yu Wang, and Huazhong Yang. 2016.
Going Deeper with Embedded FPGA Platform for Convolutional Neural Network. In Proceedings of the 24th ACM/SIGDA International Symposium on FieldProgrammable Gate Arrays (FPGA ’16). ACM, New York, NY, USA, 26-35.
DOL-https://doi.org/10.1145/2847263.2847265
Maximizing CNN Accelerator Efficiency Through Resource Partitioning ISCA °17, June 24-28, 2017, Toronto, ON, Canada

[22] Murugan Sankaradas, Venkata Jakkula, Srihari Cadambi, Srimat Chakradhar, Igor
Durdanovic, Eric Cosatto, and Hans Peter Graf. 2009. A Massively Parallel
Coprocessor for Convolutional Neural Networks. In Proceedings of the 20th
IEEE International Conference on Application-specific Systems, Architectures
and Processors (ASAP ’09). IEEE Computer Society, Washington, DC, USA,
53-60. DOLhttps://doi.org/10.1109/ASAP.2009.25

[23] Ali Shafiee, Anirban Nag, Naveen Muralimanohar, Rajeev Balasubramonian,
John Paul Strachan, Miao Hu, R. Stanley Williams, and Vivek Srikumar. 2016.
ISAAC: A Convolutional Neural Network Accelerator with In-situ Analog Arithmetic in Crossbars. In Proceedings of the 43rd International Symposium on
Computer Architecture (ISCA ’16). IEEE Press, Piscataway, NJ, USA, 14-26.
DOT-hitps://doi.org/10.1109/ISCA.2016.12

[24] Hardik Sharma, Jongse Park, Divya Mahajan, Emmanuel Amaro, Joon Kyung
Kim, Chenkai Shao, Asit Mishra, and Hadi Esmaeilzadeh. 2016. From high-level
deep neural models to FPGAs. In Proceedings of the 49th Annual IEEE/ACM
International Symposium on Microarchitecture (MICRO ’ 16). TEEE Computer
Society, Washington, DC, USA, 1-12. DOI-https://doi.org/10.1109/MICRO.2016.
7183720

[25] Yongming Shen, Michael Ferdman, and Peter Milder. 2017. Escher: A CNN
Accelerator with Flexible Buffering to Minimize Off-Chip Transfer. In Proceedings of the 25th IEEE International Symposium on Field-Programmable Custom
Computing Machines (FCCM ’17). TEEE Computer Society, Los Alamitos, CA,
USA.

[26] Karen Simonyan and Andrew Zisserman. 2014. Very Deep Convolutional Networks for Large-Scale Image Recognition. CoRR abs/1409.1556 (2014).

[27] Lili Song, Ying Wang, Yinhe Han, Xin Zhao, Bosheng Liu, and Xiaowei Li.
2016, C-brain: A Deep Learning Accelerator That Tames the Diversity of CNNs
Through Adaptive Data-level Parallelization. In Proceedings of the 53rd Annual
Design Automation Conference (DAC ’16). ACM, New York, NY, USA, Article
123, 123:1-123:6 pages. DOI:https://doi.org/10.1145/2897937.2897995

[28] Naveen Suda, Vikas Chandra, Ganesh Dasika, Abinash Mohanty, Yufei Ma,
Sarma Vrudhula, Jae-sun Seo, and Yu Cao. 2016. Throughput-Optimized
OpenCL-based FPGA Accelerator for Large-Scale Convolutional Neural Networks. In Proceedings of the 24th ACM/SIGDA International Symposium on
Field-Programmable Gate Arrays (FPGA ’16). ACM, New York, NY, USA, 16—
25. DOT:https://doi.org/10.1145/2847263.2847276

[29] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed,
Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015. Going deeper with convolutions. In Proceedings of the 2015
JEEE Conference on Computer Vision and Pattern Recognition (CVPR ’15). 1-9.
DOT-hitps://doi.org/10.1109/CVPR.2015.7298594

[30] Ying Wang, Jie Xu, Yinhe Han, Huawei Li, and Xiaowei Li. 2016. DeepBurning: Automatic Generation of FPGA-based Learning Accelerators for the Neural
Network Family. In Proceedings of the 53rd Annual Design Automation Conference (DAC ’16). ACM, New York, NY, USA, Article 110, 110:1-110:6 pages.
DOT-https://doi.org/10.1145/2897937.2898003

[31] Xilinx. 2016. 7 Series FPGAs Memory Resources User Guide. (2016).

[32] Chen Zhang, Peng Li, Guangyu Sun, Yijin Guan, Bingjun Xiao, and Jason Cong.
2015. Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural
Networks. In Proceedings of the 23rd ACM/SIGDA International Symposium on
Field-Programmable Gate Arrays (FPGA ’15). ACM, New York, NY, USA, 161—
170. DOT:https://doi.org/10.1145/2684746.2689060
