[1] G. Cavallanti, N. Cesa-Bianchi, and C. Gentile. Tracking
the best hyperplane with a simple budget perceptron.
Machine Learning, 69(2-3):143-167, 2007.

[2] K. Crammer, J. Kandola, and Y. Singer. Online classification on a budget. In Advances in Neural Information
Processing Systems 16, MIT Press, 2004.

[3] K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and
Y. Singer. Online passive-aggressive algorithms. J. Mach.
Learn. Res., 7:551-585, 2006.

[4] Lehel Csaté and Manfred Opper. Sparse on-line gaussian

processes. Neural computation, 14(3):641-668, 2002.

[5] O. Dekel, S. Shalev-Shwartz, and Y. Singer. The for
getron: A kernel-based perceptron on a fixed budget.

In Advances in Neural Information Processing Systems,

pages 259-266, 2005.

[6] Y. Freund and R. E. Schapire. Large margin classification

using the perceptron algorithm. Mach. Learn., 37(3):

277-296, December 1999.

[7] GPy. GPy: A gaussian process framework in python.

http://github.com/SheffieldML/GPy, 2012.

[8] J. Hensman, M. Rattray, and N. D. Lawrence. Fast

variational inference in the conjugate exponential family.

In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q.

Weinberger, editors, Advances in Neural Information

Processing Systems, pages 2888-2896. 2012.

[9] J. Hensman, N. Fusi, and N. D. Lawrence. Gaussian

processes for big data. In Uncertainty in Artificial

Intelligence, page 282. Citeseer, 2013.

[10] T. N. Hoang, Q. M. Hoang, and B. Low. A unifying

framework of anytime sparse gaussian process regression

models with stochastic variational inference for big data.

In Proceedings of the 32nd International Conference on

Machine Learning, pages 569-578, 2015.

[11] T. N. Hoang, Q. M. Hoang, and B. K. H. Low. A

distributed variational inference framework for unifying

parallel sparse gaussian process regression models. In

Proc. ICML, pages 382-391, 2016.

[12] M. D. Hoffman, D. M. Blei, C. Wang, and J. Paisley.

Stochastic variational inference. J. Mach. Learn. Res.,

14(1):1303-1347, May 2013. ISSN 1532-4435.

[13] J. Kivinen, A. J. Smola, and R. C. Williamson. Online

Leaming with Kemels. IEEE Transactions on Signal

Processing, 52:2165-2176, August 2004.

[14] Neil Lawrence, Matthias Seeger, Ralf Herbrich, et al.

Fast sparse gaussian process methods: The informative

vector machine. Advances in neural information pro
cessing systems, pages 625-632, 2003.
[15] T. Le, P. Duong, M. Dinh, D. T Nguyen, V. Nguyen,
and D. Phung. Budgeted semi-supervised support vector
machine. In The 32th Conference on Uncertainty in
Artificial Intelligence, June 2016.
[16] T. Le, T. D. Nguyen, V. Nguyen, and D. Phung. Dual
space gradient descent for online learning. In Advances
in Neural Information Processing (NIPS), pages 45834591, December 2016.
[17] T. Le, V. Nguyen, T. D. Nguyen, and Dinh Phung. Nonparametric budgeted stochastic gradient descent. In The
19th International Conference on Artificial Intelligence
and Statistics, pages 654-572, May 2016.
[18] T. Le, T. D. Nguyen, V. Nguyen, and D. Phung. Approximation vector machines for large-scale online learning.
Journal of Machine Learning Research (JMLR), 2017.
[19] K. Nguyen, T. Le, V. Nguyen, T. D. Nguyen, and
D. Phung. Multiple kernel learning with data augmentation. In 8th Asian Conference on Machine Learning
(ACML), Nov. 2016.
[20] T. D. Nguyen, V. Nguyen, T. Le, and D. Phung. Distributed data augmented support vector machine on
spark. In 23rd International Conference on Pattern
Recognition (ICPR), pages 498-503, Dec 2016.
[21] T. D. Nguyen, T. Le, H. Bui, and D. Phung. Large-scale
online kernel learning with random feature reparameterization. In Proceedings of the 26th International Joint
Conference on Artificial Intelligence (IJCAI), 2017.
[22] F. Orabona, J. Keshet, and B. Caputo. Bounded kernelbased online learning. J. Mach. Learn. Res., 10:26432666, December 2009. ISSN 1532-4435.
[23] J. Quifionero-Candela and C. E. Rasmussen. A unifying
view of sparse approximate gaussian process regression.
J. Mach. Learn. Res., 6:1939-1959, 2005.
[24] C. E. Rasmussen and C. K. I. Williams. Gaussian
Processes for Machine Learning (Adaptive Computation
and Machine Learning). The MIT Press, 2005. ISBN
026218253X.
[25] F. Rosenblatt. The perceptron: A probabilistic model
for information storage and organization in the brain.
Psychological Review, 65(6):386-408, 1958.
[26] A. Schwaighofer and V. Tresp. Transductive and inductive methods for approximate Gaussian process regression. In Advances in Neural Information Processing
Systems, 2003.
[27] M. Seeger, C. K. I. Williams, and N. D. Lawrence. Fast
forward selection to speed up sparse gaussian process
regression. In Workshop on AI and Statistics 9, 2003.
[28] A. J. Smola and P. L. Bartlett. Sparse greedy gaussian
process regression. In Advances in Neural Information
Processing Systems 13, pages 619-625. MIT Press, 2001.
[29] E. Snelson and Z. Ghahramani. Sparse gaussian processes using pseudo-inputs. In Advances in Neural in
Information Processing Systems, pages 1257-1264. MIT
press, 2006.
[30] E. Snelson and Z. Ghahramani. Local and global sparse
gaussian process approximations. In Proceedings of the
Eleventh International Conference on Artificial Intelli
gence and Statistics, AISTATS 2007, San Juan, Puerto
Rico, March 21-24, 2007, pages 524-531, 2007.

[31] M. K. Titsias. Variational learning of inducing variables
in sparse gaussian processes. In Artificial Intelligence
and Statistics 12, pages 567-574, 2009.

[32] Z. Wang and S. Vucetic. Twin vector machines for
online learning on a budget. In Proceedings of the SIAM
International Conference on Data Mining, pages 906—
917, 2009.

[33] Z. Wang and S. Vucetic. Online passive-aggressive
algorithms on a budget. In AJSTATS, volume 9, pages
908-915, 2010.

[34] Z. Wang, K. Crammer, and S. Vucetic. Breaking the curse
of kernelization: Budgeted stochastic gradient descent for
large-scale svm training. J. Mach. Learn. Res., 13(1):
3103-3131, 2012.

[35] L. Wasserman. All of Statistics: A Concise Course
in Statistical Inference. Springer Publishing Company,
Incorporated, 2010. ISBN 1441923225, 9781441923226.
[36] M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Machine Learning,
Proceedings of the Twentieth International Conference
(ICML 2003, pages 928-936, 2003.
