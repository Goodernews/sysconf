
[1] Apache Mahout: Scalable machine learning and data

mining. http://mahout.apache.org/.
[2] Machine Learning Library (MLlib).

http://spark.apache.org/mllib/.

[3] M. Abadi et al. TensorFlow: A System for Large-Scale

Machine Learning. In OSDI, pages 265–283, 2016.
[4] D. Agrawal et al. Rheem: Enabling multi-platform

task execution. In SIGMOD, 2016.

[5] D. Agrawal et al. Road to Freedom in Big Data

Analytics. In EDBT, 2016.

[6] S. Ben-David and S. Shalev-Shwartz. Understanding

Machine Learning: From Theory to Algorithms.
Cambridge University Press, 2014.

[7] D. P. Bertsekas. Nonlinear programming. chapter 1.3.

Athena scientiﬁc Belmont, 1999.

[8] M. Boehm et al. SystemML: Declarative Machine

Learning on Spark. PVLDB, 9(13):1425–1436, 2016.

[9] M. Boehm, S. Tatikonda, B. Reinwald, P. Sen,

Y. Tian, D. R. Burdick, and S. Vaithyanathan. Hybrid
Parallelization Strategies for Large-scale Machine
Learning in SystemML. PVLDB, 7(7):553–564, Mar.
2014.

[10] L. Bottou. Stochastic Gradient Descent Tricks. In

Neural Networks: Tricks of the Trade. 2012.

[11] O. Bousquet and L. Bottou. The tradeoﬀs of large

scale learning. In NIPS, pages 161–168, 2008.

[12] X. Feng, A. Kumar, B. Recht, and C. R´e. Towards a

Uniﬁed Architecture for in-RDBMS Analytics. In
SIGMOD, pages 325–336, 2012.

[13] J. M. Hellerstein, C. R´e, F. Schoppmann, D. Z. Wang,
E. Fratkin, A. Gorajek, K. S. Ng, C. Welton, X. Feng,
K. Li, and A. Kumar. The MADlib Analytics Library
or MAD Skills, the SQL. PVLDB, 5(12):1700–1711,
2012.

[14] B. Huang, S. Babu, and J. Yang. Cumulon:

Optimizing Statistical Data Analysis in the Cloud. In
SIGMOD, 2013.

[15] R. Johnson and T. Zhang. Accelerating Stochastic

Gradient Descent using Predictive Variance
Reduction. In NIPS, 2013.

[16] T. Kraska, A. Talwalkar, J. C. Duchi, R. Griﬃth,

M. J. Franklin, and M. I. Jordan. MLbase: A
Distributed Machine-learning System. In CIDR, 2013.

[17] A. Kumar, J. Naughton, and J. M. Patel. Learning

Generalized Linear Models Over Normalized Data. In
SIGMOD, pages 1969–1984, 2015.

[18] J. Liu, S. J. Wright, C. R´e, V. Bittorf, and S. Sridhar.

An Asynchronous Parallel Stochastic Coordinate
Descent Algorithm. In ICML, pages 469–477, 2014.
[19] B. C. Ooi, K.-L. Tan, S. Wang, W. Wang, Q. Cai,
G. Chen, J. Gao, Z. Luo, A. K. H. Tung, Y. Wang,
Z. Xie, M. Zhang, and K. Zheng. SINGA: A
Distributed Deep Learning Platform. In ACM
Multimedia, 2015.

[20] B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A

Lock-Free Approach to Parallelizing Stochastic
Gradient Descent. In NIPS, pages 693–701, 2011.
[21] J. Shin, S. Wu, F. Wang, C. D. Sa, C. Zhang, and
C. R´e. Incremental Knowledge Base Construction
Using DeepDive. PVLDB, 8(11):1310–1321, 2015.

[22] E. P. Xing, Q. Ho, W. Dai, J. K. Kim, J. Wei, S. Lee,

X. Zheng, P. Xie, A. Kumar, and Y. Yu. Petuum: A
New Platform for Distributed Machine Learning on
Big Data. In KDD, 2015.

[23] C. Zhang, A. Kumar, and C. R´e. Materialization
optimizations for feature selection workloads. In
SIGMOD, pages 265–276, 2014.

[24] C. Zhang and C. Re. DimmWitted: A Study of

