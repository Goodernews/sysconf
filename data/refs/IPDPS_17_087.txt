[1] A. Gray and A. Moore, “N-body problems in statistical learning,” Advances in neural information processing systems, pp. 521–527, 2001.
[2] C. E. Rasmussen and C. Williams, Gaussian Processes for Machine Learning. MIT Press, 2006.
[3] L. Wasserman, All of Statistics: A Concise Course in Statis- tical Inference. Springer, 2004.
[4] T. Hofmann, B. Scho ̈lkopf, and A. J. Smola, “Kernel methods in machine learning,” The annals of statistics, pp. 1171–1220, 2008.
[5] Z. Lu et al., “How to scale up kernel methods to be as good as deep neural nets,” arXiv preprint arXiv:1411.4000, 2014.
[6] B. Dai, B. Xie, N. He, Y. Liang, A. Raj, M.-F. F. Balcan, and L. Song, “Scalable kernel methods via doubly stochastic gradients,” in Advances in Neural Information Processing Systems, 2014, pp. 3041–3049.
[7] C. D. Yu, W. B. March, B. Xiao, and G. Biros, “INV- ASKIT: a parallel fast direct solver for kernel matrices,” in Proceedings of the IPDPS16, Chicago, USA, May 2016.
[8] W. B. March, B. Xiao, and G. Biros, “ASKIT: Approximate skeletonization kernel-independent treecode in high dimensions,” SIAM Journal on Scientific Computing, vol. 37, no. 2, pp. 1089–1110, 2015. [Online]. Available: http://dx.doi.org/10.1.1/jpb001
[9] W. B. March, B. Xiao, S. Tharakan, C. D. Yu, and G. Biros, “A kernel-independent FMM in general dimensions,” in Proceedings of SC15, Austin, Texas, Nov 2015. [Online]. Available: http://dx.doi.org/10.1145/2807591.2807647
[10] C. Williams and M. Seeger, “Using the Nystro ̈m method to speed up kernel machines,” in Proceedings of the 14th Annual Conference on Neural Information Processing Systems, no. EPFL-CONF-161322, 2001, pp. 682–688.
[11] A. Gittens and M. Mahoney, “Revisiting the Nystrom method for improved large-scale machine learning,” in Proceedings of ICML13, 2013, pp. 567–575.
[12] C.-J. Hsieh, S. Si, and I. S. Dhillon, “Fast prediction for large- scale kernel machines,” in Advances in Neural Information Processing Systems 27, 2014, pp. 3689–3697.
[13] S. Si, C.-J. Hsieh, and I. Dhillon, “Memory efficient kernel approximation,” in Proceedings of The 31st International Conference on Machine Learning, 2014, pp. 701–709.
[14] R. Kondor, N. Teneva, and V. Garg, “Multiresolution matrix factorization,” in Proceedings of ICML14, 2014, pp. 1620– 1628.
[15] W. B. March, B. Xiao, S. Tharakan, C. D. Yu, and G. Biros, “Robust treecode approximation for kernel machines,” in Proceedings of the 21st ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Sydney, Australia, August 2015, pp. 1–10. [Online]. Available: http://dx.doi.org/10.1145/2783258.2783272
[16] R. Wang, Y. Li, M. W. Mahoney, and E. Darve, “Structured block basis factorization for scalable kernel matrix evalua- tion,” arXiv preprint arXiv:1505.00398, 2015.
[17] D. Lee, A. Gray, and A. Moore, “Dual-tree fast gauss trans- forms,” Advances in Neural Information Processing Systems, vol. 18, pp. 747–754, 2006.
[18] S. Ambikasaran, D. Foreman-Mackey, L. Greengard, D. W. Hogg, and M. O’Neil, “Fast direct methods for Gaussian processes and the analysis of NASA Kepler mission data,” arXiv preprint arXiv:1403.6015, 2014.
[19] S. Ambikasaran and E. Darve, “An O(nlogn) fast direct solver for partial hierarchically semi-separable matrices,” Journal of Scientific Computing, vol. 57, no. 3, pp. 477–501, 2013.
[20] M. Bebendorf, Hierarchical matrices.
Springer, 2008.
[21] W. Hackbusch, B. N. Khoromskij, and E. E. Tyrtyshnikov, “Approximate iterations for structured matrices,” Numerische Mathematik, vol. 109, no. 3, pp. 365–383, 2008.
[22] R. Kriemann, “Parallele Algorithmen fr H-Matrizen,” Disser- tation, Universitt Kiel, 2004.
[23] ——, “H-LU factorization on many-core systems,” Max Planck Institute for Mathematics in the Sciences, Preprint, 2014. [Online]. Available: http://www.mis.mpg. de/de/publications/preprints/2014/prepr2014- 5.html
[24] F.-H. Rouet, X. S. Li, P. Ghysels, and A. Napov, “A distributed-memory package for dense hierarchically semi- separable matrix computations using randomization,” ACM Transactions on Mathematical Software (TOMS), vol. 42, no. 4, p. 27, 2016.
[25] S. Wang, X. S. Li, J. Xia, Y. Situ, and M. V. De Hoop, “Efficient scalable algorithms for solving dense linear systems with hierarchically semiseparable structures,” SIAM Journal on Scientific Computing, vol. 35, no. 6, pp. C519–C544, 2013.
[26] M. Izadi Khaleghabadi et al., “Parallel H-matrix arithmetic on distributed-memory systems,” 2012.
[27] S. M. Omohundro, Five balltree construction algorithms. International Computer Science Institute Berkeley, 1989.
[28] N. Halko, P.-G. Martinsson, and J. Tropp, “Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions,” SIAM Review, vol. 53, pp. 217–288, 2011.
[29] W. B. March, B. Xiao, C. D. Yu, and G. Biros, “An algebraic parallel treecode in arbitrary dimensions,” in Proceedings of IPDPS15, Hyderabad, India, May 2015.
[30] C. D. Yu, J. Huang, W. Austin, B. Xiao, and G. Biros, “Performance optimization for the k-nearest neighbors kernel on x86 architectures,” in Proceedings of SC15. ACM, 2015, pp. 7:1–7:12. [Online]. Available: http: //doi.acm.org/10.1145/2807591.2807601
[31] F.G.VanZeeandR.A.VanDeGeijn,“Blis:Aframeworkfor rapidly instantiating blas functionality,” ACM Transactions on Mathematical Software (TOMS), vol. 41, no. 3, p. 14, 2015.
[32] W. W. Hager, “Updating the inverse of a matrix,” SIAM review, vol. 31, no. 2, pp. 221–239, 1989.
[33] A. Sodani et al., “Knights landing: Second-generation intel xeon phi product,” IEEE Micro, vol. 36, no. 2, pp. 34–46, 2016.
[34] S. Balay et al., “PETSc Web page,” 2014. [Online]. Available: http://www.mcs.anl.gov/petsc
[35] M. Lichman, “UCI machine learning repository,” 2013. [Online]. Available: http://archive.ics.uci.edu/ml
[36] C.-C. Chang and C.-J. Lin, “Libsvm: A library for support vector machines,” ACM Transactions on Intelligent Systems and Technology (TIST), vol. 2, no. 3, p. 27, 2011.
[37] Menze et al., “The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS),” IEEE Transactions on Medical Imaging, p. 33, 2014. [Online]. Available: https://hal.inria.fr/hal- 00935640
