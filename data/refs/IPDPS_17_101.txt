[1] W. S. McCulloch and W. Pitts, “A logical calculus of the ideas
immanent in nervous activity,” The bulletin of mathematical
biophysics, vol. 5, no. 4, pp. 115–133, 1943.
[2] F. Rosenblatt, The perceptron, a perceiving and recognizing
automaton Project Para. Cornell Aeronautical Laboratory,
1957.
[3] M. L. Minsky and S. A. Papert, Perceptrons - Expanded
Edition: An Introduction to Computational Geometry. MIT
press Boston, MA:, 1987.
[4] P. Werbos, Beyond regression: New tools for prediction and
analysis in the behavioral sciences.
Harvard University,
1974.
11 With the notation of theorems 2 and 3, let us consider a convolutional
network as a multilayer feed forward neural network such as for each
synapse connecting a neuron i in layer l to a neuron j in layer l − 1
(l)
not belonging to the receptive ﬁeld of i, we have wji = 0. Together with
the weight sharing property of convolutional networks this leads to the
(l)
equality between the maximal weight in absolute value over a layer wm
and the maximum weight in absolute value over a single receptive ﬁeld.

[5] Y. LeCun and Y. Bengio, “Convolutional networks for images,
speech, and time series,” The handbook of brain theory and
neural networks, vol. 3361, no. 10, 1995.
[6] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov, “Dropout: A simple way to prevent neural
networks from overﬁtting,” The Journal of Machine Learning
Research, vol. 15, no. 1, pp. 1929–1958, 2014.
[7] B. Klein, L. Wolf, and Y. Afek, “A dynamic convolutional
layer for short range weather prediction,” in IEEE Conference
on Computer Vision and Pattern Recognition, 2015.
[8] G. Chowdhary and E. Johnson, “Adaptive neural network
ﬂight control using both current and recorded data,” in AIAA
Guidance, Navigation, and Control Conference, AIAA-20076505. Hilton Head, 2007.
[9] M. E. Farmer, C. S. Jacobs, and S. Cong, “Neural network
radar processor,” Apr. 2 2002, US Patent 6,366,236.
[10] A. Diaz Alvarez et al., “Modeling the driving behavior of
electric vehicles using smartphones and neural networks,”
Intelligent Transportation Systems Magazine, IEEE, vol. 6,
no. 3, pp. 44–53, 2014.
[11] M. T. Hagan, H. B. Demuth, M. H. Beale, and O. De Jesús,
Neural network design. PWS publishing company Boston,
1996, vol. 20.
[12] F. B. Schneider, “Implementing fault-tolerant services using
the state machine approach: A tutorial,” ACM Computing
Surveys, vol. 22, no. 4, pp. 299–319, 1990.
[13] J. Dean et al., “Large scale distributed deep networks,” in
Advances in Neural Information Processing Systems, 2012,
pp. 1223–1231.
[14] N. A. Lynch, Distributed algorithms.
1996.

Morgan Kaufmann,

[15] J. Misra and I. Saha, “Artiﬁcial neural networks in hardware:
A survey of two decades of progress,” Neurocomputing,
vol. 74, no. 1, pp. 239–255, 2010.
[16] H. Markram, E. Muller, S. Ramaswamy et al., “Reconstruction and simulation of neocortical microcircuitry,” Cell, vol.
163, no. 2, pp. 456–492, 2015.
[17] G. Indiveri et al., “Neuromorphic silicon neuron circuits,”
Frontiers in neuroscience, vol. Vol. 5, 2011.
[18] S. K. Esser, P. A. Merolla, J. V. Arthur, A. S. Cassidy, R. Appuswamy, A. Andreopoulos, D. J. Berg, J. L. McKinstry,
T. Melano, D. R. Barch et al., “Convolutional networks for
fast, energy-efﬁcient neuromorphic computing,” Proceedings
of the National Academy of Sciences, p. 201604850, 2016.
[19] S. K. Esser, R. Appuswamy, P. Merolla, J. V. Arthur, and D. S.
Modha, “Backpropagation for energy-efﬁcient neuromorphic
computing,” in Advances in Neural Information Processing
Systems, 2015, pp. 1117–1125.
[20] V. Piuri, “Analysis of fault tolerance in artiﬁcial neural
networks,” Journal of Parallel and Distributed Computing,
vol. 61, no. 1, pp. 18–48, 2001.

[21] Y. Tan and T. Nanya, “Fault-tolerant back-propagation model
and its generalization ability,” in Neural Networks, 1993. Proceedings of 1993 International Joint Conference on, vol. 3.
IEEE, 1993, pp. 2516–2519.
[22] P. Kerlirzin and F. Vallet, “Robustness in multilayer perceptrons,” Neural computation, vol. 5, no. 3, pp. 473–482, 1993.
[23] S. Lawrence, C. L. Giles, and A. C. Tsoi, “What size neural
network gives optimal generalization? Convergence properties
of backpropagation.” Pennsylvania State University, 1998.
[24] K. Mehrotra, C. K. Mohan, S. Ranka, and C.-t. Chiu, “Fault
tolerance of neural networks,” DTIC Document, Tech. Rep.,
1994.
[25] C.-T. Chiu et al., “Robustness of feedforward neural networks,” in IEEE International Conference on Neural Networks. IEEE, 1993, pp. 783–788.
[26] S. Haykin, Neural networks and learning machines. Pearson
Education Upper Saddle River, 2009, vol. 3.
[27] H. T. Siegelmann and E. D. Sontag, “Analog computation
via neural networks,” Theoretical Computer Science, vol. 131,
no. 2, pp. 331–360, 1994.
[28] L. Lamport, R. Shostak, and M. Pease, “The Byzantine generals problem,” ACM Transactions on Programming Languages
and Systems, vol. 4, no. 3, pp. 382–401, 1982.
[29] D. M. MacKay and W. S. McCulloch, “The limiting information capacity of a neuronal link,” The bulletin of mathematical
biophysics, vol. 14, no. 2, pp. 127–135, 1952.
[30] C. Allen and C. F. Stevens, “An evaluation of causes for
unreliability of synaptic transmission,” Proceedings of the
National Academy of Sciences, vol. 91, no. 22, pp. 10 380–
10 383, 1994.
[31] P. Judd, J. Albericio, T. Hetherington, T. M. Aamodt, N. E.
Jerger, and A. Moshovos, “Proteus: Exploiting numerical
precision variability in deep neural networks,” in Proceedings
of the 2016 International Conference on Supercomputing.
ACM, 2016, p. 23.
[32] K. Hornik, “Approximation capabilities of multilayer feedforward networks,” Neural networks, vol. 4, no. 2, pp. 251–257,
1991.
[33] E. M. El Mhamdi and R. Guerraoui, “When neurons fail,”
EPFL, Tech. Rep., 2016.
[34] A. R. Barron, “Universal approximation bounds for superpositions of a sigmoidal function,” IEEE Transactions on
Information Theory, vol. 39, no. 3, pp. 930–945, 1993.
[35] S. Geman, E. Bienenstock, and R. Doursat, “Neural networks
and the bias/variance dilemma,” Neural computation, vol. 4,
no. 1, pp. 1–58, 1992.
[36] C. Neti, M. H. Schneider, and E. D. Young, “Maximally
fault tolerant neural networks,” IEEE Transactions on Neural
Networks, vol. 3, no. 1, pp. 14–23, 1992.

