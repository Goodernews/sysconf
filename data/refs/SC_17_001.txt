[1] C. Peterson, “Track finding with neural networks?” Nuclear Instruments and
Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and
Associated Equipment, vol. 279, no. 3, pp. 537 — 545, 1989.

[2] B. Denby, “Neural networks and cellular automata in experimental high energy
physics” Computer Physics Communications, vol. 49, no. 3, pp. 429 — 448, 1988.

[3] L. de Oliveira, M. Kagan, L. Mackey, B. Nachman, and A. Schwartzman, “Jetimages — deep learning edition,” JHEP, vol. 07, p. 069, 2016.

[4] P. T. Komiske, E. M. Metodiev, and M. D. Schwartz, “Deep learning in color:
towards automated quark/gluon jet discrimination? JHEP, vol. 01, p. 110, 2017.

[5] The ATLAS collaboration, “Search for massive supersymmetric particles in multijet final states produced in pp collisions at V/s = 13 TeV using the ATLAS detector
at the LHC? ATLAS-CONF-2016-057, 2016.

[6] T. Sjéstrand, S. Mrenna, and P. Skands, “A brief introduction to PYTHIA 8.1?
Computer Physics Communications, vol. 178, no. 11, pp. 852 — 867, 2008.

[7] J. de Favereau, C. Delaere, P. Demin, A. Giammanco, V. LemaAétre, A. Mertens,
and M. Selvaggi, “DELPHES 3, A modular framework for fast simulation of a
generic collider experiment,” JHEP, vol. 02, p. 057, 2014.

[8] M. Cacciari, G. P. Salam, and G. Soyez, “Fastjet user manual,’ The European
Physical Journal C, vol. 72, no. 3, p. 1896, 2012.

[9] M. Wehner, Prabhat, K. A. Reed, D. Stone, W. D. Collins, and J. Bacmeister,
“Resolution dependence of future tropical cyclone projections of cam5.1 in the
u.s. clivar hurricane working group idealized configurations,’ Journal of Climate,
vol. 28, no. 10, pp. 3905-3925, 2015.

[10] T. R. Knutson, J. L. McBride, J. Chan, K. Emanuel, G. Holland, C. Landsea, I. Held,
J. P. Kossin, A. Srivastava, and M. Sugi, “Tropical cyclones and climate change?
Nature Geoscience, vol. 3, no. 3, pp. 157-163, 2010.

[11] D. A. Lavers, G. Villarini, R. P. Allan, E. F. Wood, and A. J. Wade, “The detection
of atmospheric rivers in atmospheric reanalyses and their links to british winter
floods and the large-scale climatic circulation,” Journal of Geophysical Research:
Atmospheres, vol. 117, no. D20, 2012.

[12] U. Neu and et al., “Imilast: A community effort to intercompare extratropical cyclone detection and tracking algorithms; Bulletin of the American Meteorological
Society, vol. 94, no. 4, pp. 529-547, 2013.

[13] Y. Liu, E. Racah, Prabhat, J. Correa, A. Khosrowshahi, D. Lavers, K. Kunkel,
M. Wehner, and W. D. Collins, “Application of deep convolutional neural networks for detecting extreme weather in climate datasets,’ CoRR, vol.
abs/1605.01156, 2016.

[14] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B. Catanzaro,
and E. Shelhamer, “cuDNN: Efficient primitives for deep learning? CoRR, vol.
abs/1410.0759, 2014.

[15] “Introducing DNN primitives in Intel® Math Kernel Library,” https://software.
intel.com/en-us/articles/introducing-dnn-primitives-in-intelr-mkl, 2017.

[16] A. Heinecke, G. Henry, M. Hutchinson, and H. Pabst, “Libxsmm: Accelerating
small matrix multiplications by runtime code generation,’ in Proceedings of SC16.

[17] “Deepbench,” github.com/baidu-research/DeepBench, 2017.
[18] D. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper, B. Catanzaro, J. Chen,
M. Chrzanowski, A. Coates, G. Diamos et al., “Deep speech 2 : End-to-end speech
recognition in english and mandarin,” in Proceedings of ICML), 2016, pp. 173–182.
[19] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, A. Senior, P. Tucker,
K. Yang, Q. V. Le et al., “Large scale distributed deep networks,” in NIPS, 2012, pp.
1223–1231.
[20] F. N. Iandola, K. Ashraf, M. W. Moskewicz, and K. Keutzer, “Fireca�e: near-linear
acceleration of deep neural network training on compute clusters,” CoRR, vol.
abs/1511.00175, 2015.
[21] D. Das, S. Avancha, D. Mudigere, K. Vaidyanathan, S. Sridharan, D. D. Kalamkar,
B. Kaul, and P. Dubey, “Distributed deep learning using synchronous stochastic
gradient descent,” CoRR, vol. abs/1602.06709, 2016.
[22] S. Pathak, P. He, and W. Darling, “Scalable deep document / sequence
reasoning with cognitive toolkit,” in Proceedings of the 26th International
Conference on World Wide Web Companion, ser. WWW ’17 Companion.
Republic and Canton of Geneva, Switzerland: International World Wide
Web Conferences Steering Committee, 2017, pp. 931–934. [Online]. Available:
https://doi.org/10.1145/3041021.3051103
[23] “Scaling Deep Learning on 18,000 GPUs,” https://www.nextplatform.com/2017/
03/28/scaling-deep-learning-beyond-18000-gpus/, 2017.
[24] A. Anandkumar. Deep Learning at Scale on AWS. [Online]. Available: https://
ml-days-prd.s3.amazonaws.com/slides/speakers/slides/3/Anima-EPFL2017.pdf
[25] S. Hadjis, C. Zhang, I. Mitliagkas, D. Iter, and C. Ré, “Omnivore: An optimizer
for multi-device deep learning on cpus and gpus,” arXiv:1606.04487, 2016.
[26] N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang, “On
large-batch training for deep learning: Generalization gap and sharp minima,”
arXiv:1609.04836, 2016.
[27] J. Tsitsiklis, D. Bertsekas, and M. Athans, “Distributed asynchronous deterministic
and stochastic gradient optimization algorithms,” IEEE transactions on automatic
control, vol. 31, no. 9, pp. 803–812, 1986.
[28] F. Niu, B. Recht, C. Re, and S. Wright, “Hogwild: A lock-free approach to parallelizing stochastic gradient descent,” in NIPS, 2011, pp. 693–701.
[29] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, A. Senior, P. Tucker,
K. Yang, Q. V. Le et al., “Large scale distributed deep networks,” in NIPS, 2012, pp.
1223–1231.
[30] T. Chilimbi, Y. Suzue, J. Apacible, and K. Kalyanaraman, “Project adam: Building
an e�cient and scalable deep learning training system,” in 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14), 2014, pp.
571–582.
[31] I. Mitliagkas, C. Zhang, S. Hadjis, and C. Ré, “Asynchrony begets momentum,
with an application to deep learning,” arXiv:1605.09774, 2016.
[32] C. Zhang and C. Re, “Dimmwitted: A study of main-memory statistical analytics,”
PVLDB, vol. 7, no. 12, pp. 1283–1294, 2014.
[33] R. H. Hahnloser, R. Sarpeshkar, M. A. Mahowald, R. J. Douglas, and H. S. Seung,
“Digital selection and analogue ampli�cation coexist in a cortex-inspired silicon
circuit,” Nature, vol. 405, no. 6789, pp. 947–951, 2000.
[34] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into recti�ers: Surpassing
human-level performance on imagenet classi�cation,” in Proceedings of ICCV,
2015, pp. 1026–1034.
[35] D. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
arXiv:1412.6980, 2014.
[36] E. Racah, C. Beckham, T. Maharaj, C. Pal et al., “Semi-supervised detection of
extreme weather events in large climate datasets,” arXiv:1612.02095, 2016.
[37] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once: Uni�ed,
real-time object detection,” in CVPR, 2016, pp. 779–788.
[38] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg,
“Ssd: Single shot multibox detector,” in European Conference on Computer Vision.
Springer, 2016, pp. 21–37.
[39] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time object
detection with region proposal networks,” in NIPS, 2015, pp. 91–99.
[40] “Intel® distribution of Ca�e*,” https://github.com/intel/ca�e, 2017.
[41] “Intel® Machine Learning Scaling Library for Linux* OS,” https://github.com/
01org/MLSL, 2017.
[42] “Intel® Software Development Emulator,” https://software.intel.com/en-us/
articles/intel-software-development-emulator, 2017.
[43] A. Lavin and S. Gray, “Fast algorithms for convolutional neural networks,” CoRR,
vol. abs/1509.09308, 2015.
[44] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio, “Quantized
neural networks: Training neural networks with low precision weights and
activations,” CoRR, vol. abs/1609.07061, 2016.
[45] M. Courbariaux, Y. Bengio, and J. David, “Training deep neural networks with
low precision multiplications,” CoRR, vol. abs/1412.7024, 2014.
[46] S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan, “Deep learning
with limited numerical precision,” CoRR, vol. abs/1502.02551, 2015.

Deep Learning at 15PF
[47] P. Gysel, M. Motamedi, and S. Ghiasi, “Hardware-oriented approximation of
convolutional neural networks,” CoRR, vol. abs/1604.03168, 2016.
[48] J. Zhang, I. Mitliagkas, and C. Ré, “Yellow�n and the art of momentum tuning,”
arXiv preprint arXiv:1706.03471, 2017.
[49] J. Snoek, H. Larochelle, and R. P. Adams, “Practical bayesian optimization of
machine learning algorithms,” in NIPS, 2012, pp. 2951–2959.
[50] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,”
in Proceedings of the IEEE conference on computer vision and pattern recognition,
2016, pp. 770–778.
[51] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
Computation, vol. 9, no. 8, pp. 1735–1780, 1997. [Online]. Available:
http://dx.doi.org/10.1162/neco.1997.9.8.1735
[52] F. A. Gers, J. Schmidhuber, and F. Cummins, “Learning to forget: Continual
prediction with lstm,” Neural Computation, vol. 12, no. 10, pp. 2451–2471, 2000.
[Online]. Available: http://dx.doi.org/10.1162/089976600300015015
