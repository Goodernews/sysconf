[1] T. G. Kolda and B. W. Bader, “Tensor decompositions and applications,” SIAM review, vol. 51, no. 3, pp. 455–500, 2009.
[2] Y. Wang, R. Chen, J. Ghosh, J. C. Denny, A. Kho, Y. Chen, B. A. Malin, and J. Sun, “Rubik: Knowledge guided tensor factorization and completion for health data analytics,” in Pro- ceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2015, pp.
1265–1274.
[3] H. Fanaee-T and J. Gama, “Tensor-based anomaly detec-
tion: An interdisciplinary survey,” Knowledge-Based Systems,
vol. 98, pp. 130–147, 2016.
[4] Y. Shi, A. Karatzoglou, L. Baltrunas, M. Larson, A. Hanjalic,
and N. Oliver, “Tfmap: optimizing map for top-n context- aware recommendation,” in Proceedings of the 35th interna- tional ACM SIGIR conference on Research and development in information retrieval. ACM, 2012, pp. 155–164.
[5] A. Sodani, R. Gramunt, J. Corbal, H.-S. Kim, K. Vinod, S. Chinthamani, S. Hutsell, R. Agarwal, and Y.-C. Liu, “Knights landing: Second-generation intel xeon phi product,” IEEE Micro, vol. 36, no. 2, pp. 34–46, 2016.
[6] J. D. McCalpin, “Stream: Sustainable memory bandwidth in high performance computers,” University of Virginia, Charlottesville, Virginia, Tech. Rep., 1991-2007, a continually updated technical report. http://www.cs.virginia.edu/stream/. [Online]. Available: http://www.cs.virginia.edu/stream/
[7] S. Smith, N. Ravindran, N. D. Sidiropoulos, and G. Karypis, “SPLATT: Efficient and parallel sparse tensor-matrix multi- plication,” in International Parallel & Distributed Processing Symposium (IPDPS’15), 2015.
[8] S. Smith and G. Karypis, “Tensor-matrix products with a compressed sparse tensor,” in Proceedings of the 5th Work- shop on Irregular Applications: Architectures and Algorithms. ACM, 2015, p. 7.
[9] A. Heinecke, A. Breuer, M. Bader, and P. Dubey, “High order seismic simulations on the intel xeon phi processor (knights landing),” in International Conference on High Performance Computing. Springer, 2016, pp. 343–362.
[10] N. Bell and M. Garland, “Efficient sparse matrix-vector multiplication on cuda,” Nvidia Technical Report NVR-2008- 004, Nvidia Corporation, Tech. Rep., 2008.
[11] X. Liu, M. Smelyanskiy, E. Chow, and P. Dubey, “Efficient sparse matrix-vector multiplication on x86-based many-core processors,” in Proceedings of the 27th international ACM conference on International conference on supercomputing. ACM, 2013, pp. 273–282.
[12] W. Liu and B. Vinter, “CSR5: An efficient storage format for cross-platform sparse matrix-vector multiplication,” in Proceedings of the 29th ACM on International Conference on Supercomputing. ACM, 2015, pp. 339–350.
[13] M. Baskaran, B. Meister, and R. Lethin, “Low-overhead load- balanced scheduling for sparse tensor computations,” in High Performance Extreme Computing Conference (HPEC), 2014 IEEE. IEEE, 2014, pp. 1–6.
[14] N. Ravindran, N. D. Sidiropoulos, S. Smith, and G. Karypis, “Memory-efficient parallel computation of tensor and matrix products for big tensor decomposition,” in Proceedings of the Asilomar Conference on Signals, Systems, and Computers, 2014.
[15] J. H. Choi and S. Vishwanathan, “DFacTo: Distributed fac- torization of tensors,” in Advances in Neural Information Processing Systems, 2014, pp. 1296–1304.
[16] O. Kaya and B. Uc ̧ar, “Scalable sparse tensor decomposi- tions in distributed memory systems,” in Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. ACM, 2015, p. 77.
[17] S. Smith and G. Karypis, “A medium-grained algorithm for distributed sparse tensor factorization,” in 30th IEEE International Parallel & Distributed Processing Symposium (IPDPS’16), 2016.
[18] T. B. Rolinger, T. A. Simon, and C. D. Krieger, “Performance evaluation of parallel sparse tensor decomposition implemen- tations,” in Proceedings of the 6th Workshop on Irregular Applications: Architectures and Algorithms. IEEE, 2016.
[19] S. Smith and G. Karypis, “SPLATT: The Surprisingly Paral- leL spArse Tensor Toolkit,” http://cs.umn.edu/⇠splatt/.
[20] H. Bae, J. Cownie, M. Klemm, and C. Terboven, “A user-
guided locking api for the openmp* application program interface,” in International Workshop on OpenMP. Springer, 2014, pp. 173–186.
[21] Center for Medicare and Medicaid Services. (2010) CMS data entrepreneurs synthetic public use file (DE-SynPUF). [Online]. Available: https://www.cms.gov/Research-Statistics-Data-and-Systems/ Downloadable- Public- Use- Files/SynPUFs/index.html
[22] J. Bennett and S. Lanning, “The netflix prize,” in Proceedings of KDD cup and workshop, vol. 2007, 2007, p. 35.
[23] G. Dror, N. Koenigstein, Y. Koren, and M. Weimer, “The yahoo! music dataset and kdd-cup’11.” in KDD Cup, 2012, pp. 8–18.
[24] O. Go ̈rlitz, S. Sizov, and S. Staab, “Pints: peer-to-peer infras- tructure for tagging systems.” in IPTPS, 2008, p. 19.
[25] A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. Hr- uschka, and T. M. Mitchell, “Toward an architecture for never- ending language learning,” in In AAAI, 2010.
[26] J. Baumgartner. (2015) Reddit comment dataset. [On- line]. Available: https://www.reddit.com/r/datasets/comments/ 3bxlg7/i have every publicly available reddit comment/
[27] J. McAuley and J. Leskovec, “Hidden factors and hidden topics: understanding rating dimensions with review text,” in Proceedings of the 7th ACM conference on Recommender systems. ACM, 2013, pp. 165–172.
[28] S. Smith, J. W. Choi, J. Li, R. Vuduc, J. Park, X. Liu, and G. Karypis. (2017) FROSTT: The formidable repository of open sparse tensors and tools. [Online]. Available: http://frostt.io/
