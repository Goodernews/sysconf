
[1] Ricardo A. Baeza-Yates and Berthier A. Ribeiro-Neto. 1999. Modern Information Retrieval. ACM Press / Addison-Wesley.
[2] Yaxin Bi, David Bell, Hui Wang, Gongde Guo, and Jiwen Guan. 2007. COMBIN- ING MULTIPLE CLASSIFIERS USING DEMPSTER’S RULE FOR TEXT CATEGO- RIZATION. Appl. Artif. Intell. 21, 3 (March 2007), 211–239.
[3] Leo Breiman. 1996. Bagging Predictors. Mach. Learn. 24, 2 (Aug. 1996), 123–140.
[4] A. Danesh, B. Moshiri, and O. Fatemi. 2007. Improve text classification accuracy based on classifier fusion methods. In Information Fusion, 2007 10th International
Conference on. IEEE, 1–6.
[5] Clebson C. A. de Sa ́, Marcos Andre ́ Gon ̧calves, Daniel Xavier de Sousa, and
Thiago Salles. 2016. Generalized BROOF-L2R: A General Framework for Learning to Rank Based on Boosting and Random Forests. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR 2016, Pisa, Italy, July 17-21, 2016. 95–104.
[6] Yan-Shi Dong and Ke-Song Han. 2004. A comparison of several ensemble methods for text categorization. In Services Computing, 2004. (SCC 2004). Proceedings. 2004 IEEE International Conference on. IEEE, 419–422.
[7] Yan-Shi Dong and Ke-Song Han. 2004. A Comparison of Several Ensemble Methods for Text Categorization. In IEEE International Conference on Services Computing. IEEE.
[8] Manuel Ferna ́ndez-Delgado, Eva Cernadas, Sene ́n Barro, and Dinani Amorim. 2014. Do We Need Hundreds of Classifiers to Solve Real World Classification Problems? J. Mach. Learn. Res. 15, 1 (Jan. 2014), 3133–3181.
[9] Yoav Freund and Robert E Schapire. 1997. A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting. J. Comput. Syst. Sci. 55, 1 (Aug. 1997), 119–139.
[10] Pierre Geurts, Damien Ernst, and Louis Wehenkel. 2006. Extremely randomized trees. Machine Learning 63, 1 (2006), 3–42.
[11] Giorgio Giancinto and Fabio Roli. 2001. Design of Effective Neural Network Ensembles for Image Classification Purposes. IMAGE VISION AND COMPUTING JOURNAL 19 (2001), 699–707.
[12] T. Hastie, R. Tibshirani, and J. H. Friedman. 2009. The Elements of Statistical Learning. Springer.
[13] Monisha Kanakaraj and Ram Mohana Reddy Guddeti. 2015. Performance analysis of Ensemble methods on Twitter sentiment analysis using NLP techniques. In IEEE International Conference on Semantic Computing (ICSC). IEEE.
[14] Aurangzeb Khan, Baharum Baharudin, Lam Hong Lee, Khairullah Khan, and Universiti Teknologi Petronas Tronoh. 2010. A Review of Machine Learning Algo- rithms for Text-Documents Classification. In Journal of Advances In Information Technology. Academy Publisher.
[15] Ludmila I. Kuncheva, James C. Bezdek, and Robert P. W. Duin. 2001. Decision templates for multiple classifier fusion: an experimental comparison. Pattern Recognition 34 (2001), 299–314.
[16] Ludmila I. Kuncheva and Christopher J. Whitaker. 2003. Measures of Diversity in Classifier Ensembles and Their Relationship with the Ensemble Accuracy. Mach. Learn. 51, 2 (May 2003), 181–207.
[17] Christopher J. Merz. 1999. Using Correspondence Analysis to Combine Classifiers. Mach. Learn. 36, 1-2 (July 1999), 33–58.
[18] Tien Thanh Nguyen, Thi Thu Thuy Nguyen, Xuan Cuong Pham, and Alan Wee- Chung Liew. 2016. A Novel Combining Classifier Method Based on Variational Inference. Pattern Recogn. 49, C (Jan. 2016), 198–212.
[19] Aytug ̆ Onan, Serdar Korukog ̆lu, and Hasan Bulut. 2016. Ensemble of Keyword Extraction Methods and Classifiers in Text Classification. Expert Syst. Appl. 57, C (Sept. 2016), 232–247.
[20] Gabriel Pui, Cheong Fung, Jeffrey Xu Yu, Haixun Wang, David W. Cheung, and Huan Liu. 2006. A Balanced Ensemble Approach to Weighting Classifiers for Text Classification. In ICDM ’06. Sixth International Conference on Data Mining. IEEE.
[21] Lior Rokach. 2009. Taxonomy for characterizing ensemble methods in classifi- cation tasks: A review and annotated bibliography. In Computational Statistics Data Analysis, In Press, Corrected Proof.
[22] Thiago Salles, Marcos Gon ̧calves, and Leonardo Rocha. 2017. PhD Dissertation: Random Forest based Classifiers for Classification Tasks with Noisy Data. (2017). Federal University of Minas Gerais.
[23] Thiago Salles, Marcos Gon ̧calves, Victor Rodrigues, and Leonardo Rocha. 2015. BROOF: Exploiting Out-of-Bag Errors, Boosting and Random Forests for Effective Automated Classification. In Proc. of the 38th International ACM SIGIR Conference on Inf. Retrieval. ACM, 353–362.
[24] M. R. Segal. 2004. Machine Learning Benchmarks and Random Forest Regression. Technical Report. University of California.
[25] Kai Ming Ting and Ian H. Witten. 1997. Stacking Bagged and Dagged Models. In Proceedings of the Fourteenth International Conference on Machine Learning (ICML ’97). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 367–375.
[26] David Wolpert and William G. Macready. 1996. Combining Stacking With Bagging To Improve A Learning Algorithm. Technical Report.
[27] David H. Wolpert. 1992. Stacked Generalization. Neural Networks 5 (1992), 241–259.
