[1] “Cifar.torch.”
[Online].
Available:
https://github.com/
szagoruyko/cifar.torch
[2] D. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper,
B. Catanzaro, J. Chen, M. Chrzanowski, A. Coates, G. Diamos,
E. Elsen, J. Engel, L. Fan, C. Fougner, T. Han, A. Y. Hannun,
B. Jun, P. LeGresley, L. Lin, S. Narang, A. Y. Ng, S. Ozair,
R. Prenger, J. Raiman, S. Satheesh, D. Seetapun, S. Sengupta,
Y. Wang, Z. Wang, C. Wang, B. Xiao, D. Yogatama, J. Zhan,
and Z. Zhu, “Deep speech 2: End-to-end speech recognition
in english and mandarin,” CoRR, vol. abs/1512.02595, 2015.
[Online]. Available: http://arxiv.org/abs/1512.02595
[3] J. Axboe, “Flexible I/O Tester,” 2016. [Online]. Available:
http://freecode.com/projects/ﬁo
[4] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu,
G. Desjardins, J. Turian, D. Warde-Farley, and Y. Bengio,
“Theano: a CPU and GPU math expression compiler,” in Proceedings of the Python for Scientiﬁc Computing Conference
(SciPy), Jun. 2010, oral Presentation.
[5] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran,
B. Catanzaro, and E. Shelhamer, “cudnn: Efﬁcient primitives
for deep learning,” CoRR, vol. abs/1410.0759, 2014. [Online].
Available: http://arxiv.org/abs/1410.0759
[6] T. Chilimbi, Y. Suzue, J. Apacible, and K. Kalyanaraman,
“Project adam: Building an efﬁcient and scalable deep
learning training system,” in 11th USENIX Symposium on
Operating Systems Design and Implementation (OSDI 14).
Broomﬁeld, CO: USENIX Association, Oct. 2014, pp. 571–582.
[Online]. Available: https://www.usenix.org/conference/osdi14/
technical-sessions/presentation/chilimbi
[7] A. Coates, B. Huval, T. Wang, D. Wu, B. Catanzaro, and N. Andrew, “Deep learning with cots hpc systems,” in Proceedings of
the 30th ICML, 2013, pp. 1337–1345.
[8] R. Collobert, K. Kavukcuoglu, and C. Farabet, “Torch7: A
matlab-like environment for machine learning,” in BigLearn,
NIPS Workshop, 2011.
[9] H. Cui, H. Zhang, G. R. Ganger, P. B. Gibbons, and E. P. Xing,
“Geeps: Scalable deep learning on distributed gpus with a gpuspecialized parameter server,” in Proceedings of the Eleventh
European Conference on Computer Systems, ser. EuroSys ’16.
New York, NY, USA: ACM, 2016, pp. 4:1–4:16. [Online].
Available: http://doi.acm.org/10.1145/2901318.2901323
[10] D. Dalessandro, A. Devulapalli, and P. Wyckoff, “iSER storage target for object-based storage devices,” in Proceedings of
Fourth International Workshop on Storage Netwo rk Architecture
and Parallel I/Os, September 2007.
[11] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin,
M. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang,
Q. V. Le, and A. Y. Ng, “Large scale distributed deep
networks,” in Advances in Neural Information Processing
Systems 25, P. Bartlett, F. Pereira, C. Burges, L. Bottou, and
K. Weinberger, Eds., 2012, pp. 1232–1240. [Online]. Available:
http://books.nips.cc/papers/ﬁles/nips25/NIPS2012_0598.pdf
[12] S. Gupta, W. Zhang, and F. Wang, “Model accuracy and runtime tradeoff in distributed deep learning,” IEEE International
Conference on Data Mining 2016, 2016.
[13] Q. Ho, J. Cipar, H. Cui, S. Lee, J. K. Kim, P. B. Gibbons, G. A.
Gibson, G. Ganger, and E. P. Xing, “More effective distributed
ML via a stale synchronous parallel parameter server,” in NIPS
26, 2013, pp. 1223–1231.

[14] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long,
R. Girshick, S. Guadarrama, and T. Darrell, “Caffe:
Convolutional architecture for fast feature embedding,” in
Proceedings of the 22Nd ACM International Conference
on Multimedia, ser. MM ’14.
New York, NY,
USA: ACM, 2014, pp. 675–678. [Online]. Available:
http://doi.acm.org/10.1145/2647868.2654889
[15] A. Krizhevsky, “Learning multiple layers of features from tiny
images,” Tech. Rep., 2009.
[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation with deep convolutional neural networks,” in Advances
in neural information processing systems, 2012, pp. 1097–1105.
[17] M. Li, D. G. Andersen, J. W. Park, A. J. Smola,
A. Ahmed, V. Josifovski, J. Long, E. J. Shekita, and
B.-Y. Su, “Scaling distributed machine learning with
the parameter server,” in 11th USENIX Symposium on
Operating Systems Design and Implementation (OSDI 14).
Broomﬁeld, CO: USENIX Association, Oct. 2014, pp. 583–598.
[Online]. Available: https://www.usenix.org/conference/osdi14/
technical-sessions/presentation/li_mu
[18] J. Liu, J. Wu, and D. K. Panda, “High performance rdma-based
mpi implementation over inﬁniband,” Int. J. Parallel Program.,
vol. 32, no. 3, pp. 167–198, Jun. 2004. [Online]. Available:
http://dx.doi.org/10.1023/B:IJPP.0000029272.69895.c1
[19] B. Recht, C. Re, S. Wright, and F. Niu, “Hogwild: A lock-free
approach to parallelizing stochastic gradient descent,” in Advances in Neural Information Processing Systems 24, J. ShaweTaylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q.
Weinberger, Eds. Curran Associates, Inc., 2011, pp. 693–701.
[20] Y. Ren, T. Li, D. Yu, S. Jin, and T. Robertazzi, “Design and
performance evaluation of numa-aware rdma-based end-to-end
data transfer systems,” in Proceedings of the International
Conference on High Performance Computing, Networking,
Storage and Analysis, ser. SC ’13. New York, NY,
USA: ACM, 2013, pp. 48:1–48:10. [Online]. Available:
http://doi.acm.org/10.1145/2503210.2503260
[21] Y. Ren, T. Li, D. Yu, S. Jin, T. Robertazzi, B. L. Tierney,
and E. Pouyoul, “Protocols for wide-area data-intensive
applications: Design and performance issues,” in Proceedings of
the International Conference on High Performance Computing,
Networking, Storage and Analysis, ser. SC ’12. Los Alamitos,
CA, USA: IEEE Computer Society Press, 2012, pp. 34:1–34:11.
[Online]. Available: http://dl.acm.org/citation.cfm?id=2388996.
2389043
[22] S. Zhang, A. Choromanska, and Y. LeCun, “Deep learning with
elastic averaging SGD,” CoRR, vol. abs/1412.6651, 2014.
[23] W. Zhang, S. Gupta, X. Lian, and J. Liu, “Staleness-aware
async-sgd for distributed deep learning,” in Proceedings of
the Twenty-Fifth International Joint Conference on Artiﬁcial
Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July 2016,
2016, pp. 2350–2356.
[24] M. Zinkevich, M. Weimer, L. Li, and A. J. Smola, “Parallelized
stochastic gradient descent,” in Advances in Neural Information
Processing Systems 23. Curran Associates, Inc., 2010,
pp. 2595–2603. [Online]. Available: http://papers.nips.cc/paper/
4006-parallelized-stochastic-gradient-descent.pdf