[1] S. Toledo, “Improving the memory-system performance of sparsematrix vector multiplication,” IBM Journal of research and development,
vol. 41, no. 6, pp. 711–725, 1997.
[2] A. Pinar and M. T. Heath, “Improving performance of sparse matrixvector multiplication,” in Proceedings of the 1999 ACM/IEEE conference
on Supercomputing. ACM, 1999, p. 30.
[3] J. C. Pichel, D. B. Heras, J. C. Cabaleiro, and F. F. Rivera, “Improving
the locality of the sparse matrix-vector product on shared memory multiprocessors,” in Parallel, Distributed and Network-Based Processing,
2004. Proceedings. 12th Euromicro Conference on. IEEE, 2004, pp.
66–71.
[4] R. W. Vuduc and H.-J. Moon, “Fast sparse matrix-vector multiplication
by exploiting variable block structure,” in High Performance Computing
and Communications. Springer, 2005, pp. 807–816.
[5] J. Willcock and A. Lumsdaine, “Accelerating sparse matrix computations
via data compression,” in Proceedings of the 20th annual international
conference on Supercomputing. ACM, 2006, pp. 307–316.
[6] K. Kourtis, G. Goumas, and N. Koziris, “Optimizing sparse matrixvector multiplication using index and value compression,” in Proceedings of the 5th conference on Computing frontiers. ACM, 2008, pp.
87–96.
[7] S. Williams, L. Oliker, R. Vuduc, J. Shalf, K. Yelick, and J. Demmel,
“Optimization of sparse matrix–vector multiplication on emerging multicore platforms,” Parallel Computing, vol. 35, no. 3, pp. 178–194, 2009.
[8] A. Buluç, J. T. Fineman, M. Frigo, J. R. Gilbert, and C. E. Leiserson,
“Parallel sparse matrix-vector and matrix-transpose-vector multiplication
using compressed sparse blocks,” in Proceedings of the twenty-ﬁrst
annual symposium on Parallelism in algorithms and architectures.
ACM, 2009, pp. 233–244.
[9] M. Belgin, G. Back, and C. J. Ribbens, “Pattern-based sparse matrix
representation for memory-efﬁcient smvm kernels,” in Proceedings of
the 23rd international conference on Supercomputing. ACM, 2009,
pp. 100–109.
[10] K. Kourtis, V. Karakasis, G. Goumas, and N. Koziris, “Csx: an extended
compression format for spmv on shared memory systems,” in ACM
SIGPLAN Notices, vol. 46, no. 8. ACM, 2011, pp. 247–256.
[11] A. Buluç, S. Williams, L. Oliker, and J. Demmel, “Reduced-bandwidth
multithreaded algorithms for sparse matrix-vector multiplication,” in
Parallel & Distributed Processing Symposium (IPDPS), 2011 IEEE
International. IEEE, 2011, pp. 721–733.
[12] M. Kreutzer, G. Hager, G. Wellein, H. Fehske, and A. R. Bishop, “A
uniﬁed sparse matrix data format for efﬁcient general sparse matrixvector multiplication on modern processors with wide simd units,” SIAM
Journal on Scientiﬁc Computing, vol. 36, no. 5, pp. C401–C423, 2014.
[13] R. Li and Y. Saad, “Gpu-accelerated preconditioned iterative linear
solvers,” The Journal of Supercomputing, vol. 63, no. 2, pp. 443–466,
2013.
[14] J. L. Greathouse and M. Daga, “Efﬁcient sparse matrix-vector
multiplication on gpus using the csr storage format,” in Proceedings of
the International Conference for High Performance Computing,
Networking, Storage and Analysis, ser. SC ’14.
Piscataway,
NJ, USA: IEEE Press, 2014, pp. 769–780. [Online]. Available:
https://doi.org/10.1109/SC.2014.68
[15] W. Liu and B. Vinter, “Csr5: An efﬁcient storage format for
cross-platform sparse matrix-vector multiplication,” in Proceedings of
the 29th ACM on International Conference on Supercomputing, ser.
ICS ’15. New York, NY, USA: ACM, 2015, pp. 339–350. [Online].
Available: http://doi.acm.org/10.1145/2751205.2751209
[16] Y. Saad, Numerical methods for large eigenvalue problems. SIAM,
1992, vol. 158.
[17] S. Williams, A. Waterman, and D. Patterson, “Rooﬂine: an insightful
visual performance model for multicore architectures,” Communications
of the ACM, vol. 52, no. 4, pp. 65–76, 2009.
[18] J. Mellor-Crummey and J. Garvin, “Optimizing sparse matrix–vector
product computations using unroll and jam,” International Journal of
High Performance Computing Applications, vol. 18, no. 2, pp. 225–236,
2004.
[19] W. T. Tang, R. Zhao, M. Lu, Y. Liang, H. P. Huyng, X. Li, and
R. S. M. Goh, “Optimizing and auto-tuning scale-free sparse matrixvector multiplication on intel xeon phi,” in Code Generation and
Optimization (CGO), 2015 IEEE/ACM International Symposium on.
IEEE, 2015, pp. 136–145.
[20] E. D. Lazowska, J. Zahorjan, G. S. Graham, and K. C. Sevcik, Quantitative system performance: computer system analysis using queueing
network models. Prentice-Hall, Inc., 1984.
[21] T. A. Davis and Y. Hu, “The university of ﬂorida sparse matrix collection,” ACM Transactions on Mathematical Software (TOMS), vol. 38,
no. 1, p. 1, 2011.
[22] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, “Scikit-learn: Machine learning in Python,” Journal of Machine
Learning Research, vol. 12, pp. 2825–2830, 2011.
[23] U. W. Pooch and A. Nieder, “A survey of indexing techniques for sparse
matrices,” ACM Computing Surveys (CSUR), vol. 5, no. 2, pp. 109–133,
1973.
[24] L. Dagum and R. Enon, “Openmp: an industry standard api for sharedmemory programming,” Computational Science & Engineering, IEEE,
vol. 5, no. 1, pp. 46–55, 1998.
[25] J. D. McCalpin, “Stream: Sustainable memory bandwidth in high
performance computers,” 1995.
[26] R. Vuduc, J. W. Demmel, and K. A. Yelick, “Oski: A library of
automatically tuned sparse matrix kernels,” in Journal of Physics:
Conference Series, vol. 16, no. 1. IOP Publishing, 2005, p. 521.
[27] E.-J. Im, K. Yelick, and R. Vuduc, “Sparsity: Optimization framework
for sparse matrix kernels,” International Journal of High Performance
Computing Applications, vol. 18, no. 1, pp. 135–158, 2004.
[28] J. W. Choi, A. Singh, and R. W. Vuduc, “Model-driven autotuning of
sparse matrix-vector multiply on gpus,” in ACM Sigplan Notices, vol. 45,
no. 5. ACM, 2010, pp. 115–126.
[29] B.-Y. Su and K. Keutzer, “clspmv: A cross-platform opencl spmv
framework on gpus,” in Proceedings of the 26th ACM international
conference on Supercomputing. ACM, 2012, pp. 353–364.
[30] P. Guo, L. Wang, and P. Chen, “A performance modeling and optimization analysis tool for sparse matrix-vector multiplication on gpus,”
Parallel and Distributed Systems, IEEE Transactions on, vol. 25, no. 5,
pp. 1112–1123, 2014.
[31] K. Li, W. Yang, and K. Li, “Performance analysis and optimization for
spmv on gpu using probabilistic modeling,” Parallel and Distributed
Systems, IEEE Transactions on, vol. 26, no. 1, pp. 196–205, 2015.
[32] J. Li, G. Tan, M. Chen, and N. Sun, “Smat: an input adaptive auto-tuner
for sparse matrix-vector multiplication,” in ACM SIGPLAN Notices,
vol. 48, no. 6. ACM, 2013, pp. 117–126.
