[1] Jorge Albericio, Patrick Judd, Tayler Hetherington, Tor Aamodt, Natalie Enright
Jerger, and Andreas Moshovos. 2016. CNVLUTIN: Ine�ectual-Neuron-Free Deep
Neural Network Computing. In International Symposium on Computer Architecture (ISCA).
[2] Lukas Cavigelli, Michele Magno, and Luca Benini. 2015. Accelerating real-time
embedded scene labeling with convolutional networks. In Design Automation
Conference (DAC).
[3] Srimat Chakradhar, Murugan Sankaradas, Venkata Jakkula, and Srihari Cadambi.
2010. A dynamically con�gurable coprocessor for convolutional neural networks.
In International Symposium on Computer Architecture (ISCA).
[4] Tianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengyong Wu, Yunji Chen,
and Olivier Temam. 2014. Diannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning. In Architecture Support for Programming
Languages and Operating Systems (ASPLOS).
[5] Yunji Chen, Tao Luo, Shaoli Liu, Shijin Zhang, Liqiang He, Jia Wang, Ling Li,
Tianshi Chen, Zhiwei Xu, Ninghui Sun, and Olivier Temam. 2014. Dadiannao: A
machine-learning supercomputer. In International Symposium on Microarchitecture (MICRO).
[6] Yu-Hsin Chen, Joel Emer, and Vivienne Sze. 2016. Eyeriss: A Spatial Architecture for Energy-E�cient Data�ow for Convolutional Neural Networks. In
International Symposium on Computer Architecture (ISCA).
[7] Sharan Chetlur, Cli� Woolley, Philippe Vandermersch, Jonathan Cohen, John
Tran, Bryan Catanzaro, and Evan Shelhamer. 2014. cuDNN: E�cient primitives
for deep learning. arXiv:1410.0759.
[8] Ronan Collobert, Clement Farabet, Koray Kavukcuoglu, and Soumith Chintala.
2015. torch. (2015). Retrieved August 25, 2017 from http://torch.ch/
[9] Francesco Conti and Luca Benini. 2015. A ultra-low-energy convolution engine
for fast brain-inspired vision in multicore clusters. In Design, Automation & Test
in Europe Conference & Exhibition (DATE).
[10] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. 2014. Low precision arithmetic for deep learning. arXiv:1412.7024.
[11] Je�rey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
Marc extquotesingle aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang,
Quoc V. Le, and Andrew Y. Ng. 2012. Large scale distributed deep networks. In
Neural Information Processing Systems (NIPS).
[12] Zidong Du, Avinash Lingamneni, Yunji Chen, Krishna Palem, Olivier Temam,
and Chengyong Wu. 2014. Leveraging the error resilience of machine-learning
applications for designing highly energy e�cient accelerators. In Asia and South
Paci�c Design Automation Conference (ASP-DAC).
[13] Clément Farabet, Berin Martini, Benoit Corda, Polina Akselrod, Eugenio Culurciello, and Yann LeCun. 2011. Neu�ow: A runtime recon�gurable data�ow
processor for vision. In Computer Vision and Pattern Recognition Workshops
(CVPRW).
[14] Klint Finley. 2015. Facebook open-sources a trove of AI tools. (2015).
Retrieved August 25, 2017 from https://www.wired.com/2015/01/
facebook-open-sources-trove-ai-tools/
[15] Wilson W. L. Fung, Ivan Sham, George Yuan, and Tor M. Aamodt. 2007. Dynamic
Warp Formation and Scheduling for E�cient GPU Control Flow. In International
Symposium on Microarchitecture (MICRO).
[16] Ross Girshick. 2015. Fast R-CNN. arXiv:1504.08083.
[17] Íñigo Goiri, Ricardo Bianchini, Santosh Nagarakatte, and Thu D Nguyen. 2015.
ApproxHadoop: Bringing Approximations to MapReduce Frameworks. In International Conference on Architectural Support for Programming Languages and
Operating Systems (ASPLOS).
[18] Google. 2015. TensorFlow. (2015). Retrieved August 25, 2017 from http://www.
tensor�ow.org/
[19] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.
2015. Deep Learning with Limited Numerical Precision. arXiv:1502.02551.
[20] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A Horowitz,
and William J Dally. 2016. EIE: e�cient inference engine on compressed deep
neural network. In International Symposium on Computer Architecture (ISCA).
[21] Song Han, Huizi Mao, and William J Dally. 2015. Deep compression: Compressing
deep neural network with pruning, trained quantization and hu�man coding.
International Conference on Learning Representations (ICLR).
[22] Song Han, Je� Pool, John Tran, and William Dally. 2015. Learning both Weights
and Connections for E�cient Neural Network. Neural Information Processing
Systems (NIPS).
[23] Awni Y. Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich
Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, and
Andrew Y. Ng. 2014. DeepSpeech: Scaling up end-to-end speech recognition.
arXiv:1412.5567.
[24] Johann Hauswald, Yiping Kang, Michael A Laurenzano, Quan Chen, Cheng Li,
Trevor Mudge, Ronald G Dreslinski, Jason Mars, and Lingjia Tang. 2015. DjiNN
and Tonic: DNN as a service and its implications for future warehouse scale
computers. In International Symposium on Computer Architecture (ISCA).
[25] Johann Hauswald, Michael A. Laurenzano, Yunqi Zhang, Cheng Li, Austin Rovinski, Arjun Khurana, Ronald G. Dreslinski, Trevor Mudge, Vinicius Petrucci,
Lingjia Tang, and Jason Mars. 2015. Sirius: An open end-to-end voice and vision
personal assistant and its implications for future warehouse scale computers. In
Architectural Support for Programming Languages and Operating Systems (ASPLOS).
[26] Sunpyo Hong and Hyesoon Kim. 2009. An Analytical Model for a GPU Architecture with Memory-level and Thread-level Parallelism Awareness. In International
Symposium on Computer Architecture (ISCA).
[27] Intel. 2015. neon. (2015). Retrieved August 25, 2017 from https://github.com/
NervanaSystems/neon
[28] Animesh Jain, Parker Hill, Shih-Chieh Lin, Muneeb Khan, Md E. Haque, Michael A.
Laurenzano, Scott Mahlke, Lingjia Tang, and Jason Mars. 2016. Concise Loads
and Stores: The Case for an Asymmetric Compute-Memory Architecture for
Approximation. In International Symposium on Microarchitecture (MICRO).
[29] Yangqing Jia, Evan Shelhamer, Je� Donahue, Sergey Karayev, Jonathan Long,
Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Ca�e: Convolutional
Architecture for Fast Feature Embedding. arXiv:1408.5093.
[30] Patrick Judd, Jorge Albericio, Tayler Hetherington, Tor Aamodt, Natalie Enright
Jerger, and Andreas Moshovos. 2016. Proteus: Exploiting Numerical Precision
Variability in Deep Neural Networks. In International Conference on Supercomputing (ICS).
[31] Sergey Karayev, Matthew Trentacoste, Helen Han, Aseem Agarwala, Trevor
Darrell, Aaron Hertzmann, and Holger Winnemoeller. 2013. Recognizing image
style. arXiv:1311.3715.
[32] Joo-Young Kim, Minsu Kim, Seungjin Lee, Jinwook Oh, Kwanho Kim, and Hoi-Jun
Yoo. 2010. A 201.4 GOPS 496 mW real-time multi-object recognition processor
with bio-inspired neural perception engine. In Journal of Solid-State Circuits
(JSSC).
[33] Alex Krizhevsky and Geo�rey Hinton. 2009. Learning multiple layers of features
from tiny images. Tech report, University of Toronto.
[34] Alex Krizhevsky, Ilya Sutskever, and Geo�rey E Hinton. 2012. Imagenet classi�cation with deep convolutional neural networks. In Neural Information Processing
Systems (NIPS).
[35] LISA lab. 2015. theano. (2015). Retrieved August 25, 2017 from http://deeplearning.
net/software/theano/
[36] Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua
Bengio. 2007. An empirical evaluation of deep architectures on problems with
many factors of variation. In International Conference on Machine learning (ICML).
[37] Michael A Laurenzano, Parker Hill, Mehrzad Samadi, Scott Mahlke, Jason Mars,
and Lingjia Tang. 2016. Input responsiveness: using canary inputs to dynamically steer approximation. In Programming Language Design and Implementation
(PLDI).
[38] Andrew Lavin. 2015. maxDNN: An E�cient Convolution Kernel for Deep Learning with Maxwell GPUs. arXiv:1501.06633.
[39] Yann LeCun, Yoshua Bengio, and Geo�rey Hinton. 2015. Deep learning. Nature.
[40] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Ha�ner. 1998. Gradientbased learning applied to document recognition. In Proceedings of the IEEE.
[41] Yann LeCun, Corinna Cortes, and Christopher JC Burges. 1998. The MNIST
database of handwritten digits. (1998). Retrieved August 25, 2017 from http:
//yann.lecun.com/exdb/mnist/
[42] Yongsoon Lee, Younhee Choi, Seok-Bum Ko, and Moon Ho Lee. 2009. Performance
analysis of bit-width reduced �oating-point arithmetic units in FPGAs: a case
study of neural network-based face detector. In EURASIP Journal on Embedded
Systems.
[43] Boxun Li, Yuzhi Wang, Yu Wang, Yuanfeng Chen, and Huazhong Yang. 2014.
Training itself: Mixed-signal training acceleration for memristor-based neural
network. In Asia and South Paci�c Design Automation Conference (ASP-DAC).
[44] Divya Mahajan, Amir Yazdanbakhsh, Jongse Park, Bradley Thwaites, and Hadi Esmaeilzadeh. 2016. Towards statistical guarantees in controlling quality tradeo�s
for approximate acceleration. In International Symposium on Computer Architecture (ISCA).
[45] Veynu Narasiman, Michael Shebanow, Chang Joo Lee, Rustam Miftakhutdinov,
Onur Mutlu, and Yale N. Patt. 2011. Improving GPU Performance via Large Warps
and Two-level Warp Scheduling. In International Symposium on Microarchitecture
(MICRO).
[46] Rajib Nath, Stanimire Tomov, and Jack Dongarra. 2010. Accelerating GPU kernels
for dense linear algebra. In High Performance Computing for Computational Science
(VECPAR).
[47] Jiquan Ngiam, Adam Coates, Ahbik Lahiri, Bobby Prochnow, Quoc V Le, and
Andrew Y Ng. 2011. On optimization methods for deep learning. In International
Conference on Machine Learning (ICML).
[48] M-E. Nilsback and A. Zisserman. 2008. Automated Flower Classi�cation over a
Large Number of Classes. In Indian Conference on Computer Vision, Graphics and
Image Processing (ICVGIP).
[49] Nvidia. 2017. cuBLAS. (2017). Retrieved August 25, 2017 from developer.nvidia.
com/cublas
[50] Nvidia. 2017. cuSPARSE. (2017). Retrieved August 25, 2017 from developer.nvidia.
com/cusparse
[51] Nvidia. 2017. GeForce GTX TITAN X, Speci�cations. (2017). Retrieved August 25,
2017 from http://www.geforce.com/hardware/desktop-gpus/geforce-gtx-titan-x/
speci�cations
[52] Nvidia. 2017. Parallel Thread Execution ISA Version 5.0. (2017). Retrieved August
25, 2017 from http://docs.nvidia.com/cuda/parallel-thread-execution
[53] Kalin Ovtcharov, Olatunji Ruwase, Joo-Young Kim, Jeremy Fowers, Karin
Strauss, and Eric Chung. 2015. Accelerating Deep Convolutional Neural Networks Using Specialized Hardware.
(2015).
Retrieved August
25, 2017 from https://www.microsoft.com/en-us/research/publication/
accelerating-deep-convolutional-neural-networks-using-specialized-hardware
[54] Eustace Painkras, Luis A Plana, Jim Garside, Steve Temple, Simon Davidson,
Je�rey Pepper, David Clark, Cameron Patterson, and Steve Furber. 2012. Spinnaker: a multi-core system-on-chip for massively-parallel neural net simulation.
In Custom Integrated Circuits Conference (CICC).
[55] Robert Preissl, Theodore M Wong, Pallab Datta, Myron Flickner, Raghavendra
Singh, Steven K Esser, William P Risk, Horst D Simon, and Dharmendra S Modha.
2012. Compass: a scalable simulator for an architecture for cognitive computing.
In International Conference on High Performance Computing, Networking, Storage
and Analysis (SC).
[56] Brandon Reagen, Paul Whatmough, Robert Adolf, Saketh Rama, Hyunkwang Lee,
Sae Kyu Lee, José Miguel Hernández-Lobato, Gu-Yeon Wei, and David Brooks.
2016. Minerva: Enabling low-power, highly-accurate deep neural network accelerators. In International Symposium on Computer Architecture (ISCA).
[57] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C.
Berg, and Li Fei-Fei. 2015. ImageNet Large Scale Visual Recognition Challenge.
International Journal of Computer Vision (IJCV).

[58] Mehrzad Samadi, Janghaeng Lee, D Anoushe Jamshidi, Amir Hormati, and Scott
Mahlke. 2013. SAGE: Self-tuning approximation for graphics engines. In International Symposium on Microarchitecture (ISCA).
An in-depth look
[59] Kaz Sato, Cli� Young, and David Patterson. 2017.
at Google’s �rst Tensor Processing Unit (TPU).
(2017).
Retrieved
August 25, 2017 from https://cloud.google.com/blog/big-data/2017/05/
an-in-depth-look-at-googles-�rst-tensor-processing-unit-tpu
[60] Johannes Schemmel, Johannes Fieres, and Karlheinz Meier. 2008. Wafer-scale
integration of analog neural networks. In International Joint Conference on Neural
Networks (IJCNN).
[61] Ali Sha�ee, Anirban Nag, Naveen Muralimanohar, Rajeev Balasubramonian,
John Paul Strachan, Miao Hu, R Stanley Williams, and Vivek Srikumar. 2016.
ISAAC: A Convolutional Neural Network Accelerator with In-Situ Analog Arithmetic in Crossbars. In International Symposium on Computer Architecture (ISCA).
[62] Olivier Temam. 2012. A defect-tolerant accelerator for emerging highperformance applications. In International Symposium on Computer Architecture
(ISCA).
[63] Vincent Vanhoucke, Andrew Senior, and Mark Z Mao. 2011. Improving the speed
of neural networks on CPUs. In Deep Learning and Unsupervised Feature Learning
Workshop.
[64] Jason Yosinski, Je� Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable
are features in deep neural networks?. In Neural Information Processing Systems
(NIPS).
[65] Jianming Zhang, Shuga Ma, Mehrnoosh Sameki, Stan Sclaro�, Margrit Betke,
Zhe Lin, Xiaohui Shen, Brian Price, and Radomír Mĕch. 2015. Salient Object
Subitizing. In Computer Vision and Pattern Recognition (CVPR).
