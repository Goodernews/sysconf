[1] A. R. Benson and G. Ballard, “A framework for practical parallel fast matrix multiplication,” in Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP 2015). ACM, 2015, pp. 42– 53.
[2] J. Huang, T. M. Smith, G. M. Henry, and R. A. van de Geijn, “Strassen’s algorithm reloaded,” in Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC 16). IEEE, 2016, pp. 59:1–59:12.
[3] F. G. Van Zee and R. A. van de Geijn, “BLIS: A framework for rapidly instantiating BLAS functionality,” ACM Trans. Math. Soft., vol. 41, no. 3, pp. 14:1–14:33, June 2015.
[4] C.-H. Huang, J. R. Johnson, and R. W. Johnson, “A tensor product formulation of Strassen’s matrix multiplication algo- rithm,” Applied Mathematics Letters, vol. 3, no. 3, pp. 67–71, 1990.
[5] J. Li, A. Skjellum, and R. D. Falgout, “A poly-algorithm for parallel dense matrix multiplication on two-dimensional pro- cess grid topologies,” Concurrency: Practice and Experience, vol. 9, no. 5, pp. 345–389, May 1997.
[6] K. Goto and R. A. van de Geijn, “Anatomy of a high- performance matrix multiplication,” ACM Trans. Math. Soft., vol. 34, no. 3, p. 12, May 2008.
[7] T. M. Low, F. D. Igual, T. M. Smith, and E. S. Quintana-Orti, “Analytical modeling is enough for high-performance BLIS,” ACM Trans. Math. Softw., vol. 43, no. 2, pp. 12:1–12:18, August 2016.
[8] N. J. Higham, Accuracy and Stability of Numerical Algo- rithms, 2nd ed. Philadelphia, PA, USA: SIAM, 2002.
[9] J. Demmel, I. Dumitriu, O. Holtz, and R. Kleinberg, “Fast matrix multiplication is stable,” Numerische Mathematik, vol. 106, no. 2, pp. 199–224, 2007.
[10] G. Ballard, A. R. Benson, A. Druinsky, B. Lipshitz, and O. Schwartz, “Improving the numerical stability of fast ma- trix multiplication,” SIAM Journal on Matrix Analysis and Applications, vol. 37, no. 4, pp. 1382–1418, 2016.
[11] D. Bini, M. Capovani, F. Romani, and G. Lotti, “O (n2.7799) complexity for n × n approximate matrix multiplication,” Information Processing Letters, vol. 8, no. 5, pp. 234–235, 1979.
[12] A. Graham, “Kronecker products and matrix calculus: With applications.” John Wiley & Sons, Inc., 605 3rd Ave., New York, NY 10158, 1982.
[13] V. Strassen, “Gaussian elimination is not optimal,” Nu- merische Mathematik, vol. 13, no. 4, pp. 354–356, August 1969.
[14] A. V. Smirnov, “The bilinear complexity and practical algo- rithms for matrix multiplication,” Computational Mathemat- ics and Mathematical Physics, vol. 53, no. 12, pp. 1781–1795, 2013.
[15] E. Elmroth, F. Gustavson, I. Jonsson, and B. Ka ̊gstro ̈m, “Recursive blocked algorithms and hybrid data structures for dense matrix library software,” SIAM review, vol. 46, no. 1, pp. 3–45, 2004.
[16] M. Thottethodi, S. Chatterjee, and A. R. Lebeck, “Tuning Strassen’s matrix multiplication for memory efficiency,” in Proceedings of the 1998 ACM/IEEE Conference on Super- computing (SC 98). IEEE, 1998, pp. 1–14.
[17] B.GraysonandR.vandeGeijn,“Ahighperformanceparallel Strassen implementation,” Parallel Processing Letters, vol. 6, no. 1, pp. 3–12, 1996.
[18] B. Lipshitz, G. Ballard, J. Demmel, and O. Schwartz, “Communication-avoiding parallel Strassen: Implementation and performance,” in Proceedings of the International Confer- ence on High Performance Computing, Networking, Storage and Analysis (SC 12). IEEE, 2012, pp. 101:1–101:11.
[19] “Intel MKL,” https://software.intel.com/en-us/intel-mkl.
[20] T. M. Smith, R. A. van de Geijn, M. Smelyanskiy, J. R. Ham- mond, and F. G. Van Zee, “Anatomy of high-performance many-threaded matrix multiplication,” in 28th IEEE Inter- national Parallel and Distributed Processing Symposium (IPDPS 2014), 2014.
[21] P. D’Alberto, M. Bodrato, and A. Nicolau, “Exploiting par- allelism in matrix-computation kernels for symmetric multi- processor systems: Matrix-multiplication and matrix-addition algorithm optimizations by software pipelining and threads allocation,” ACM Trans. Math. Softw., vol. 38, no. 1, pp. 2:1– 2:30, December 2011.
[22] D. E. Knuth, The Art of Computer Programming, Volume 2 (3rd Ed.): Seminumerical Algorithms. Boston, MA, USA: Addison-Wesley Longman Publishing Co., Inc., 1997.
[23] D. Irony, S. Toledo, and A. Tiskin, “Communication lower bounds for distributed-memory matrix multiplication,” Jour- nal of Parallel and Distributed Computing, vol. 64, no. 9, pp. 1017–1026, 2004.
[24] G. Ballard, J. Demmel, O. Holtz, B. Lipshitz, and O. Schwartz, “Communication-optimal parallel algorithm for Strassen’s matrix multiplication,” in Proceedings of the 24th ACM Symposium on Parallelism in Algorithms and Architec- tures (SPAA 12). ACM, 2012, pp. 193–204.
[25] J. Scott, O. Holtz, and O. Schwartz, “Matrix multiplication I/O-complexity by path routing,” in Proceedings of the 27th ACM Symposium on Parallelism in Algorithms and Architec- tures (SPAA 15). ACM, 2015, pp. 35–45.
[26] G. Bilardi and L. De Stefani, “The I/O complexity of Strassen’s matrix multiplication with recomputation,” arXiv preprint arXiv:1605.02224, 2016.
