
[1] http://highscalability.com/blog/2016/3/16/
jeff-dean-on-large-scale-deep-learning-at-google. html.
Accessed: 2016-08-10.

[2] M. Abadi, A. Agarwal, P. Barham, E. Brevdo,
Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean,
M. Devin, 8. Ghemawat, I. Goodfellow, A. Harp,
G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser,
M. Kudlur, J. Levenberg, D. Mané, R. Monga,
S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens,
B. Steiner, I. Sutskever, K. Talwar, P. Tucker,
V. Vanhoucke, V. Vasudevan, F. Viégas, O. Vinyals,
P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and
X. Zheng. 2015. Software available from
tensorflow.org.

[3] P. Bailis. Coordination Avoidance in Distributed
Databases. PhD thesis, EECS Department, University
of California, Berkeley, Oct 2015.

[4] P. Bailis, A. Fekete, M. J. Franklin, A. Ghodsi, J. M.

Hellerstein, and I. Stoica. Feral concurrency control:

An empirical investigation of modern application

integrity. SIGMOD ’15, pp. 1827-1342, 2015.

[5] P. Bailis, A. Fekete, A. Ghodsi, J. M. Hellerstein, and

I. Stoica. HAT, Not CAP: Towards highly available

transactions. HotOS ’13, 2013.

[6] A. Bernstein, P. Lewis, and S. Lu. Semantic conditions

for correctness at different isolation levels. ICDE ’00,

pp. 57-66, 2000.

[7] P. Boldi, A. Marino, M. Santini, and S. Vigna.

BUbiNG: Massive crawling for the masses. WWW ’04,

pp. 227-228, 2014.

[8] P. Boldi, M. Santini, and S. Vigna. A large time-aware

graph. SIGIR Forum, 42(2):33-38, 2008.

[9] T. Chilimbi, Y. Suzue, J. Apacible, and

K. Kalyanaraman. Project Adam: Building an
efficient and scalable deep learning training system.
OSDI ’14, pp. 571-582, Oct. 2014.

[10] H. Cui, H. Zhang, G. R. Ganger, P. B. Gibbons, and
E. P. Xing. GeePS: Scalable deep learning on
distributed gpus with a gpu-specialized parameter
server. HuroSys 716, pp. 4:1-4:16, 2016.

[11] J. Dean, G. S. Corrado, R. Monga, K. Chen,

M. Devin, Q. V. Le, M. Z. Mao, M. Ranzato,
A. Senior, P. Tucker, K. Yang, and A. Y. Ng. Large
scale distributed deep networks. NIPS, 2012.

[12] J. Dean and S. Ghemawat. MapReduce: Simplified
data processing on large clusters. OSDI ’04, pp.
10-10, 2004.

[13] S. N. Dorogovtsev, J. F. F. Mendes, and A. N.
Samukhin. Structure of growing networks with
preferential linking. Phys. Rev. Lett., 85:4633—4636,
Nov 2000.

[14] J. Duchi, E. Hazan, and Y. Singer. Adaptive
subgradient methods for online learning and stochastic
optimization. J. Mach. Learn. Res., 12:2121—2159,
July 2011.

[15] D. Engler and K. Ashcraft. RacerX: Effective, static
detection of race conditions and deadlocks. SOSP ’03,
pp. 237-252, 2003.

[16] A. Fekete, S. N. Goldrei, and J. P. Asenjo.
Quantifying isolation anomalies. Proc. VEDB Endow.,
2(1):467-478, Aug. 2009.

[17] A. Fekete, D. Liarokapis, E. O’Neil, P. O’Neil, and
D. Shasha. Making snapshot isolation serializable.
ACM Trans. Database Syst., 30(2):492-528, June 2005.

[18] J. E. Gonzalez, P. Bailis, M. I. Jordan, M. J. Franklin,
J. M. Hellerstein, A. Ghodsi, and I. Stoica.
Asynchronous complex analytics in a distributed
dataflow architecture. CoRR, abs/1510.07092, 2015.

[19] J. E. Gonzalez, Y. Low, H. Gu, D. Bickson, and
C. Guestrin. PowerGraph: Distributed graph-parallel
computation on natural graphs. OSDI ’12, pp. 17-30,
2012.

[20] M. Han and K. Daudjee. Giraph Unchained:

Barrierless asynchronous parallel execution in

 
pregel-like graph processing systems. Proc. VEDB
Endow., 8(9):950-961, May 2015.

[21] T. A. Henzinger, C. M. Kirsch, H. Payer, A. Sezgin,

and A. Sokolova. Quantitative relaxation of concurrent

data structures. POPL ’13, pp. 317-328, 2013.

[22] B. Holt, J. Bornholt, I. Zhang, D. Ports, M. Oskin,

and L. Ceze. Disciplined inconsistency with

consistency types. SoCC ’16, pp. 279-293, 2016.

[23] C. Hsieh, H. Yu, and I. 8. Dhillon. PASSCoDe:

Parallel asynchronous stochastic dual co-ordinate

descent. ICML 715, pp. 2370-2379, 2015.

[24] D. B. Johnson. Finding all the elementary circuits of a

directed graph. SIAM Journal on Computing,

4(1):77-84, 1975.

[25] P. Joulani, A. Gyérgy, and C. Szepesvari.

Delay-tolerant online convex optimization: Unified

analysis and adaptive-gradient algorithms. AAAI ’16,

pp. 1744-1750, 2016.

C. Kirsch, M. Lippautz, and H. Payer. Fast and

scalable, lock-free k-FIFO queues. PaC'T ’13, pp.

208-223, 2013.

[27] T. Kraska, M. Hentschel, G. Alonso, and
D. Kossmann. Consistency rationing in the cloud: Pay
only when it matters. Proc. VLDB Endow.,
2(1):253-264, Aug. 2009.

[28] J. Leskovec and A. Krevl.
http://snap.stanford.edu/data, June 2014.

[29] C. Li, J. Leitao, A. Clement, N. Preguica,

R. Rodrigues, and V. Vafeiadis. Automating the
choice of consistency levels in replicated systems.
USENIX ATC ’14, pp. 281-292, June 2014.

[30] C. Li, D. Porto, A. Clement, J. Gehrke, N. Preguiga,
and R. Rodrigues. Making geo-replicated systems fast
as possible, consistent when necessary. OSDI ’12, pp.
265-278, 2012.

[31] H. Li, A. Kadav, E. Kruus, and C. Ungureanu.
MALT: Distributed data-parallelism for existing ml
applications. FuroSys 715, pp. 3:1-3:16, 2015.

[32] M. Li, D. G. Andersen, J. W. Park, A. J. Smola,

A. Ahmed, V. Josifovski, J. Long, E. J. Shekita, and
B.-Y. Su. Scaling distributed machine learning with
the parameter server. OSDI ’14, pp. 583-598, Oct.
2014.

[33] J. Liu, T. Magrino, O. Arden, M. D. George, and
A. C. Myers. Warranties for faster strong consistency.
NSDI ’14, pp. 503-517, 2014.

[34] J. Liu, S. J. Wright, C. Ré, V. Bittorf, and S. Sridhar.
An asynchronous parallel stochastic coordinate
descent algorithm. J. Mach. Learn. Res.,
16(1):285-322, Jan. 2015.

[35] H. B. McMahan and M. Streeter. Delay-tolerant
algorithms for asynchronous distributed online
learning. NIPS ’14, pp. 2915-2923, 2014.

[36] R. Meusel, S. Vigna, O. Lehmberg, and C. Bizer.
Graph structure in the web — revisited: A trick of the
heavy tail. WWW ’14 Companion, pp. 427-432, 2014.

[37] I. Mitliagkas, C. Zhang, S. Hadjis, and C. Ré.
Asynchrony begets momentum, with an application to
deep learning. CoRR, abs/1605.09774, 2016.

[38] S. Narayanasamy, Z. Wang, J. Tigani, A. Edwards,
and B. Calder. Automatically classifying benign and

 


harmful data races using replay analysis. PLDI ’07,
pp. 22-31, 2007.

[39] X. Pan, M. Lam, S. Tu, D. Papailiopoulos, C. Zhang,
M. I. Jordan, K. Ramchandran, C. Re, and B. Recht.
Cyclades: Conflict-free asynchronous machine
learning. NIPS ’16. 2016.

[40] K. Ramamritham and C. Pu. A formal
characterization of epsilon serializability. IEEE Trans.
on Knowl. and Data Eng., 7(6):997-1007, Dec. 1995.

[41] B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A
lock-free approach to parallelizing stochastic gradient
descent. NIPS ’11, pp. 693-701. 2011.

[42] S. J. Reddi, A. Hefny, S. Sra, B. Péczos, and
A. Smola. On variance reduction in stochastic gradient
descent and its asynchronous variants. NIPS’15, pp.
2647-2655, 2015.

[43] M. C. Rinard and P. C. Diniz. Commutativity
analysis: A new analysis technique for parallelizing
compilers. ACM Trans. Program. Lang. Syst.,
19(6):942-991, Nov. 1997.

[44] S. Roy, L. Kot, N. Foster, J. Gehrke, H. Hojjat, and
C. Koch. Writes that fall in the forest and make no
sound: Semantics-based adaptive data consistency.
CoRR, abs/1403.2307, 2014.

[45] C. D. Sa, C. Re, and K. Olukotun. Global convergence

of stochastic gradient descent for some non-convex

matrix problems. ICML ’15, pp. 2332-2341, 2015.

[46] Z. Shang, F. Li, J. X. Yu, Z. Zhang, and H. Cheng.

Graph analytics through fine-grained parallelism.

SIGMOD ’16, pp. 463-478, 2016.

[47] M. Shapiro, N. Preguiga, C. Baquero, and

M. Zawirski. Conflict-free replicated data types. SSS

11, pp. 386-400, 2011.

[48] K. Vora, S. C. Koduru, and R. Gupta. ASPIRE:

Exploiting asynchronous parallelism in iterative

algorithms using a relaxed consistency based DSM.

OOPSLA ’14, pp. 861-878, 2014.

[49] G. Wang, W. Xie, A. Demers, and J. Gehrke.

Asynchronous large-scale graph processing made easy.

CIDR. ’13, 2013.

[50] Z. Wang, Y. Gu, Y. Bao, G. Yu, and J. X. Yu. Hybrid
pulling/pushing for I/O-efficient distributed and
iterative graph computing. STIGMOD 716, pp. 479-494,
2016.

[51] G. Weikum and G. Vossen. Transactional Information
Systems: Theory, Algorithms, and the Practice of
Concurrency Control and Recovery. Morgan
Kaufmann Publishers Inc., 2001.

[52] C. Xie, R. Chen, H. Guan, B. Zang, and H. Chen.
SYNC or ASYNC: Time to fuse for distributed
graph-parallel computation. PPoPP 2015, pp.
194-204, 2015.

[53] C. Xie, C. Su, M. Kapritsos, Y. Wang,

N. Yaghmazadeh, L. Alvisi, and P. Mahajan. Salt:
Combining ACID and BASE in a distributed
database. OSDI ’14, pp. 495-509, Oct. 2014.

[54] E. P. Xing, Q. Ho, W. Dai, J.-K. Kim, J. Wei, S. Lee,
X. Zheng, P. Xie, A. Kumar, and Y. Yu. Petuum: A
new platform for distributed machine learning on big
data. KDD ’15, pp. 1335-1344, 2015.

[55] W. Xiong, S. Park, J. Zhang, Y. Zhou, and Z. Ma. Ad

 hoc synchronization considered harmful. OSDI ’10,
pp. 163-176, 2010.

[56] F. Yang, J. Li, and J. Cheng. Husky: Towards a more
efficient and expressive distributed computing
framework. Proc. VLDB Endow., 9(5):420—431, Jan.
2016.

[57] K. Zellag and B. Kemme. How consistent is your cloud
application? SoCC ’12, pp. 6:1-6:14, 2012.

[58] K. Zellag and B. Kemme. Consistency anomalies in
multi-tier architectures: Automatic detection and
prevention. The VEDB Journal, 23(1):147-172, Feb.
2014.

[59] S. Zhang, A. Choromanska, and Y. LeCun. Deep
learning with elastic averaging sgd. NIPS' ’15, pp.
685-693, 2015.

[60] W. Zhang, S. Gupta, X. Lian, and J. Liu.
Staleness-aware async-SGD for distributed deep
learning. IJCAI ’16, pp. 2350-2356, 2016.

[61] C. Zhou, J. Gao, B. Sun, and J. X. Yu. MOCgraph:
Scalable distributed graph processing using message
online computing. Proc. VLDB Endow., 8(4):377-388,
Dec. 2014.

