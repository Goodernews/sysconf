[1] H. Sutter, “The free lunch is over: A fundamental turn toward concurrency
in software,” 2005.
[2] K. Bergman, S. Borkar, D. Campbell et al., “Exascale computing study:
Technology challenges in achieving exascale systems,” 2008.
[3] D. Jeon, S. Garcia, C. Louie, and M. B. Taylor, “Kismet: Parallel
speedup estimates for serial programs,” in Proceedings of the 2011
ACM International Conference on Object Oriented Programming Systems
Languages and Applications, ser. OOPSLA ’11, 2011, pp. 519–536.
[4] M. Kim, P. Kumar, H. Kim, and B. Brett, “Predicting potential speedup
of serial code via lightweight profiling and emulations with memory
performance model,” in Proceedings of the 2012 IEEE 26th International
Parallel and Distributed Processing Symposium, ser. IPDPS ’12, 2012,
pp. 1318–1329.
[5] S. Lee, J. S. Meredith, and J. S. Vetter, “Compass: A framework for
automated performance modeling and prediction,” in Proceedings of the
29th ACM on International Conference on Supercomputing, 2015.
[6] V. Kuncak and M. Rinard, “Existential heap abstraction entailment is
undecidable,” in Proceedings of the 10th International Conference on
Static Analysis, ser. SAS’03, 2003, pp. 418–438.
[7] G. Ramalingam, “The undecidability of aliasing,” ACM Trans. Program.
Lang. Syst., vol. 16, no. 5, pp. 1467–1471, Sep. 1994.
[8] N. Ardalani, C. Lestourgeon, K. Sankaralingam, and X. Zhu, “Crossarchitecture performance prediction (xapp) using cpu code to predict gpu
performance,” in Proceedings of the 48th International Symposium on
Microarchitecture, ser. MICRO-48, 2015, pp. 725–737.
[9] I. Baldini, S. J. Fink, and E. Altman, “Predicting gpu performance from
cpu runs using machine learning,” in Proceedings of IEEE 26th International Symposium on Computer Architecture and High Performance
Computing, ser. SBAC-PAD’14, 2014, pp. 254–261.
[10] V. W. Lee, C. Kim, J. Chhugani et al., “Debunking the 100x gpu vs.
cpu myth: An evaluation of throughput computing on cpu and gpu,” in
Proceedings of the 37th Annual International Symposium on Computer
Architecture, ser. ISCA ’10, 2010, pp. 451–460.
[11] J. A. Stratton, C. Rodrigues, I.-J. Sung et al., “Parboil: A revised
benchmark suite for scientific and commercial throughput computing,”
Center for Reliable and High-Performance Computing, vol. 127, 2012.
[12] A. E. Helal, P. Sathre, and W.-c. Feng, “Metamorph: A library framework
for interoperable kernels on multi- and many-core clusters,” in Proceedings of the International Conference for High Performance Computing,
Networking, Storage and Analysis, ser. SC ’16, 2016.
[13] H. C. Edwards, C. R. Trott, and D. Sunderland, “Kokkos: Enabling
manycore performance portability through polymorphic memory access
patterns,” Journal of Parallel and Distributed Computing, vol. 74, no. 12,
pp. 3202–3216, 2014.
[14] R. Hornung, J. Keasler et al., “The raja portability layer: overview and
status,” 2014.
[15] A. Aggarwal and S. Vitter, Jeffrey, “The input/output complexity of
sorting and related problems,” Commun. ACM, vol. 31, no. 9, pp. 1116–
1127, Sep. 1988.
[16] K. Czechowski, C. Battaglino, C. McClanahan, A. Chandramowlishwaran, and R. Vuduc, “Balance principles for algorithm-architecture codesign,” in Proceedings of the 3rd USENIX Conference on Hot Topic in
Parallelism, ser. HotPar’11, 2011, pp. 9–9.

[17] S. Xiao and W. c. Feng, “Inter-block gpu communication via fast barrier
synchronization,” in Parallel Distributed Processing (IPDPS), 2010 IEEE
International Symposium on, 2010, pp. 1–12.
[18] W. c. Feng and S. Xiao, “To gpu synchronize or not gpu synchronize?”
in Proceedings of 2010 IEEE International Symposium on Circuits and
Systems, 2010, pp. 3801–3804.
[19] Y. J. Lo, S. Williams, B. Van Straalen et al., “Roofline model toolkit:
A practical tool for architectural and program analysis,” in International
Workshop on Performance Modeling, Benchmarking and Simulation of
High Performance Computer Systems, 2014, pp. 129–148.
[20] L. McVoy and C. Staelin, “lmbench: portable tools for performance
analysis,” in Proceedings of the 1996 annual conference on USENIX
Annual Technical Conference, 1996, pp. 23–23.
[21] H. Wong, M.-M. Papadopoulou, M. Sadooghi-Alvandi, and A. Moshovos,
“Demystifying gpu microarchitecture through microbenchmarking,” in
Performance Analysis of Systems & Software (ISPASS), 2010 IEEE
International Symposium on, 2010, pp. 235–246.
[22] A. E. Helal, A. M. Bayoumi, and Y. Y. Hanafy, “Parallel circuit simulation using the direct method on a heterogeneous cloud,” in 52nd
ACM/EDAC/IEEE Design Automation Conference (DAC), June 2015.
[23] D. E. Culler, J. P. Singh, and A. Gupta, Parallel Computer Architecture,
A Hardware/Software Approach. Morgan Kaufmann, 1996.
[24] C. Lattner and V. Adve, “LLVM: A Compilation Framework for Lifelong
Program Analysis and Transformation,” 2004, pp. 75–86.
[25] S. Muchnick, Advanced Compiler Design Implementation.
Morgan
Kaufmann Publishers, 1997.
[26] G. E. Blelloch, “Programming parallel algorithms,” Commun. ACM,
vol. 39, no. 3, pp. 85–97, Mar. 1996.
[27] M. Mock, M. Das, C. Chambers, and S. J. Eggers, “Dynamic pointsto sets: A comparison with static analyses and potential applications in
program understanding and optimization,” in Proceedings of the 2001
ACM SIGPLAN-SIGSOFT Workshop on Program Analysis for Software
Tools and Engineering, ser. PASTE ’01, 2001, pp. 66–72.
[28] M. Kim, H. Kim, and C.-K. Luk, “Sd3: A scalable approach to dynamic
data-dependence profiling,” in Proceedings of the 43rd Annual IEEE/ACM
International Symposium on Microarchitecture, 2010.
[29] R. L. Mattson, J. Gecsei, D. R. Slutz, and I. L. Traiger, “Evaluation
techniques for storage hierarchies,” IBM Systems Journal, vol. 9, no. 2,
pp. 78–117, 1970.
[30] G. E. Blelloch, P. B. Gibbons, and H. V. Simhadri, “Low depth cacheoblivious algorithms,” in Proceedings of the ACM Symposium on Parallelism in Algorithms and Architectures, ser. SPAA ’10, 2010.
[31] R. A. Sugumar and S. G. Abraham, “Efficient simulation of caches under
optimal replacement with applications to miss characterization,” in In
Proceedings of the ACM SIGMETRICS Conference on Measurement &
Modeling Computer Systems, 1993.
[32] T. A. Davis and Y. Hu, “The university of florida sparse matrix collection,”
ACM Transactions on Mathematical Software (TOMS), vol. 38, no. 1, p. 1,
2011.
[33] S. Che, M. Boyer, J. Meng et al., “Rodinia: A benchmark suite for
heterogeneous computing,” in Workload Characterization, 2009. IISWC
2009. IEEE International Symposium on, 2009, pp. 44–54.
[34] T. M. Chilimbi, “Cache-conscious data structures: design and implementation,” Ph.D. dissertation, 1999.
[35] Y. Saad, “Krylov subspace methods on supercomputers,” SIAM Journal
on Scientific and Statistical Computing, vol. 10, no. 6, 1989.
[36] H. Wu, G. Diamos, J. Wang et al., “Optimizing data warehousing applications for gpus using kernel fusion/fission,” in Parallel and Distributed
Processing Symposium Workshops & PhD Forum (IPDPSW), 2012.
[37] R. F. Barrett, C. T. Vaughan, and M. A. Heroux, “Minighost: a miniapp
for exploring boundary exchange strategies using stencil computations in
scientific parallel computing,” Tech. Rep., 2011.
[38] N. Ozisik, Finite difference methods in heat transfer. CRC press, 1994.
[39] A. E. Helal, W.-c. Feng, C. Jung, and Y. Y. Hanafy, “Commanalyzer:
Automated estimation of communication cost on hpc clusters using
sequential code,” Tech. Rep., 2017.
[40] S. Hong and H. Kim, “An analytical model for a gpu architecture with
memory-level and thread-level parallelism awareness,” in Proceedings of
the 36th Annual International Symposium on Computer Architecture, ser.
ISCA ’09, 2009, pp. 152–163.
[41] J. Shen, A. L. Varbanescu, Y. Lu, P. Zou, and H. Sips, “Workload
partitioning for accelerating applications on heterogeneous platforms,”
IEEE Transactions on Parallel and Distributed Systems, vol. 27, no. 9,
pp. 2766–2780, 2016.
