[1] Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C.,
Corrado, G.S., Davis, A., Dean, J., Devin, M. Ghemawat, S., et al.
2016. TensorFlow: Large-scale machine learning on heterogeneous
distributed systems. arXiv:1603.04467.

[2] Albericio, J., Judd, P., Hetherington, T., Aamodt, T., Jerger, N.E.
and Moshovos, A., 2016 Cnvlutin: Ineffectual-Neuron-Free Deep
Neural Network Computing. Proc. Int’l Symp. on Computer
Architecture.

[3] Adolf, R., Rama, S., Reagen, B., Wei, G.Y. and Brooks, D., 2016,
September. Fathom: reference workloads for modern deep learning
methods. IEEE Int'l Symp. on Workload Characterization (IISWC).

[4] Asanovié, K. 2002. Programmable Neurocomputing, in The
Handbook of Brain Theory and Neural Networks: Second Edition,
M. A. Arbib (Ed.), MIT Press, ISBN 0-262-01197-2, November
2002.

[5] Asanovié, K. 1998. Asanovié, K., Beck, Johnson, J., Wawrzynek, J.,
Kingsbury, B. and Morgan, N., November 1998. Training Neural
Networks with Spert-II. Chapter 11 in Parallel Architectures for
Artificial Networks: Paradigms and Implementations, N.
Sundararajan and P. Saratchandran (Eds.), IEEE Computer Society
Press, ISBN 0-8186-8399-6. _https://people.cecs.berkeley.edu/
~krste/papers/annbook.pdf

[6] Barroso, L.A. and Hélzle, U., 2007. The case for energyproportional computing. IEEE Computer, vol. 40.

[7] Barr, J. September 29, 2016, New P2 Instance Type for Amazon
EC2 — Up to 16 GPUs. https://aws.amazon.com/blogs /aws/new-p2instance-type-for-amazon-ec2-up-to-16-gpus/

[8] Brooks, D. November 4, 2016. Private communication.

[9] Caulfield, A.M., Chung, E.S., Putnam, A., Angepat, H., Fowers, J.,

Haselman, M., Heil, S., Humphrey, M., Kaur, P., Kim, J.Y. and Lo,

D.2016. A Cloud-Scale Acceleration Architecture. MICRO-49

conference.

[10] Cavigelli, L., Gschwend, D., Mayer, C., Willi, S., Muheim, B. and

Benini, L., 2015, May. Origami: A convolutional network

accelerator. Proc. 25th edition on Great Lakes Symp. on VLSI.

[11] Chen, Y.H., Emer, J. and Sze, V., 2016. Eyeriss: A Spatial

Architecture for Energy-Efficient Dataflow for Convolutional Neural

Networks. Proc. Int’! Symp. on Computer Architecture.

[12] Chen, Y., Chen, T., Xu, Z., Sun, N., and Teman, O., 2016. DianNao

Family: Energy-Efficient Hardware Accelerators for Machine

Learning, Research Highlight, CACM, 59(11).

[13] Chi, P., Li, S., Qi, Z., Gu, P., Ku, C., Zhang, T., Zhao, J., Liu, Y.,

Wang, Y. and Xie, Y., 2016. PRIME: A Novel Processing-In
Memory Architecture for Neural Network Computation in RERAM
based Main Memory. Proc. Int’! Symp. on Computer Architecture.

[14] Clark, J. October 26, 2015, Google Turning Its Lucrative Web

Search Over to AI Machines. Bloomberg Technology,

http://www .bloomberg.com.

[15] Dally, W. February 9, 2016. High Performance Hardware for

Machine Learning, Cadence ENN Summit.

[16] Dean, J. and Barroso, L.A., 2013. The tail at scale. CACM, 56(2).

[17] Dean, J. July 7, 2016 Large-Scale Deep Learning with TensorFlow

for Building Intelligent Systems, ACM Webinar.

[18] Gupta, S., Agrawal, A., Gopalakrishnan, K., and Narayanan, P.,

2015, July. Deep Learning with Limited Numerical Precision. ICML.

[19] Hammerstrom, D., 1990, June. A VLSI architecture for high
performance, low-cost, on-chip learning. 1990 IJCNN Int'l Joint

Conference on Neural Networks.

[20] Han, S.; Pool, J.; Tran, J.; and Dally, W., 2015. Learning both

weights and connections for efficient neural networks. In Advances

in Neural Information Processing Systems.

[21] Han, S., Liu, X., Mao, H., Pu, J., Pedram, A., Horowitz, M.A. and

Dally, W.J., 2016. EIE: efficient inference engine on compressed

deep neural network. Proc. Int’l Symp. on Computer Architecture.

[22] He, K., Zhang, X., Ren, S. and Sun, J., 2016. Identity mappings in

deep residual networks. Also in arXiv preprint arXiv: 1603.05027.

[23] Hennessy, J.L. and Patterson, D.A., 2018. Computer architecture: a
quantitative approach, 6th edition, Elsevier.

[24] Hélzle, U. and Barroso, L., 2009. The datacenter as a computer.
Morgan and Claypool.

[25] Tenne, P., Cornu, T. and Kuhn, G., 1996. Special-purpose digital
hardware for neural networks: An architectural survey. Journal of
VLSI signal processing systems for signal, image and video
technology, 13(1).

[26] Intel, 2016, Intel® Xeon® Processor E5-4669 v3,
http://ark intel.com/products/85766/Intel-Xeon-Processor-E5-4669v3-45M-Cache-2_10-GHz.

[27] Jouppi, N. May 18, 2016. Google supercharges machine learning
tasks with TPU custom chip. https://cloudplatform.googleblog.com.
[28] Keutzer, K., 2016. If I could only design one circuit...: technical
perspective. CACM, 59(11).

[29] Kim, D., Kung, J.H., Chai, S., Yalamanchili, S. and Mukhopadhyay,
S., 2016. Neurocube: A Programmable Digital Neuromorphic
Architecture with High-Density 3D Memory. Proc. Int’l Symp. on
Computer Architecture.

[30] Krizhevsky, A., Sutskever, I. and Hinton, G., 2012. Imagenet
classification with deep convolutional neural networks. Advances in
neural information processing systems.

[31] Kung, H.T. and Leiserson, C.E., 1980. Algorithms for VLSI
processor artays. Introduction to VLSI systems.

[32] Lange, K.D., 2009. Identifying shades of green: The SPECpower
benchmarks. JEEE Computer, 42(3).

[33] Larabel, M. March 10, 2016, Google Looks To Open Up
StreamExecutor To Make GPGPU Programming Easier, Phoronix,
https://www.phoronix.com/ scan.php?page=news_item&px=GoogleStreamExec-Parallel.

[34] LiKamWa, R., Hou, Y., Gao, J., Polansky, M. and Zhong, L., 2016.
RedEye: Analog ConvNet Image Sensor Architecture for Continuous
Mobile Vision. Proc. Int’! Symp. on Computer Architecture.

[35] Liu, S., Du, Z.D., Tao, J.H., Han, D., Luo, T., Xie, Y., Chen, Y. and
Chen, T., 2016. Cambricon: An instruction set architecture for neural
networks. Proc. Int’! Symp. on Computer Architecture.

[36] Metz, C. September 26, 2016, Microsoft Bets Its Future On A
Reprogrammable Computer Chip, Wired Magazine,
https://www.wired.com/2016/09/microsoft-bets-future-chipreprogram-fly/

[37] Nvidia, January 2015. Tesla K80 GPU Accelerator. Board
Specification https://images.nvidia.ccom/ content/pdf/kepler/TeslaK80-BoardSpec-07317-001-v05 pdf.

[38] Nvidia, 2016. Tesla GPU Accelerators
http://www nvidia.com/object/tesla-servers html.
[39] Ovtcharov, K., Ruwase, O., Kim, J.Y., Fowers, J., Strauss, K. and
Chung, E.S., February 2, 2015. Accelerating deep convolutional
neural networks using specialized hardware. Microsoft Research
Whitepaper. www microsoft.com/en-us/research/publication/
accelerating-deep-convolutional-neural-networks-using-specializedhardware/

[40] Ovtcharov, K., Ruwase, O., Kim, J.Y., Fowers, J., Strauss, K. and
Chung, E.S., 2015, August. Toward accelerating deep learning at
scale using specialized hardware in the datacenter. 20/5 IEEE Hot
Chips 27 Symp.

[41] Patterson, D.A. and Ditzel, D.R., 1980. The case for the reduced
instruction set computer. ACM SIGARCH Computer Architecture
News, 8(6), pp. 25-33.

[42] Caulfield, A.M., Chung, E.S., Chiou, D.,
Constantinides, K., Demme, J., Esmaeilzadeh, H., Fowers, J., Gopal,
GP., Gray, J., Haselman, M., Hauck, S., Heil, S., Hormati, A., Kim,
J-Y., Lanka, S., Larus, J., Peterson, E., Pope, S., Smith, A., Thong,
J., Xiao, P.Y., Burger, D. 2016. A Reconfigurable Fabric for
Accelerating Large-Scale Datacenter Services. CACM, 59(11).

[43] Qadeer, W., Hameed, R., Shacham, O., Venkatesan, P., Kozyrakis,
C. and Horowitz, M.A., 2013, June. Convolution engine: balancing
efficiency & flexibility in specialized computing. Proc. Int’l Symp.
on Computer Architecture.

[44] Ramacher, U., Beichter, J., Raab, W., Anlauf, J., Bruels, N.,
Hachmann, U. and Wesseling, M., 1991. Design of a 1st Generation
Neurocomputer. In VLSI Design of Neural Networks. Springer US.
Reagen, B., Whatmough, P., Adolf, R., Rama, S., Lee, H., Lee, S.K.,
Hernandez-Lobato, J.M., Wei, G.Y. and Brooks, D., 2016. Minerva:
Enabling low-power, highly-accurate deep neural network
accelerators. Proc. Int’] Symp. on Computer Architecture.

[45] Ross, J., Jouppi, N., Phelps, A., Young, C., Norrie, T., Thorson, G.,
Luu, D., 2015. Neural Network Processor, Patent Application No.
62/164,931.

[46] Ross, J., Phelps, A., 2015. Computing Convolutions Using a Neural
Network Processor, Patent Application No. 62/164,902.

[47] Ross, J., 2015. Prefetching Weights for a Neural Network Processor,
Patent Application No. 62/164,981.

[48] Ross, J., Thorson, G., 2015. Rotating Data for Neural Network
Computations, Patent Application No. 62/164,908.

[49] Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S.,
Huang, Z., Karpathy, A., Khosla, A., Bernstein, M. and Berg, A.C.,
2015. Imagenet large scale visual recognition challenge. Int'l Journal
of Computer Vision, 115(3).

[50] Schurman, E. and Brutlag, J., 2009, June. The user and business
impact of server delays, additional bytes, and HTTP chunking in
web search. In Velocity Web Performance and Operations
Conference.

[51] Shafiee, A., Nag, A., Muralimanohar, N., Balasubramonian, R.,
Strachan, J.P., Hu, M., Williams, R.S. and Srikumar, V., 2016.
ISAAC: A Convolutional Neural Network Accelerator with In-Situ
Analog Arithmetic in Crossbars. Proc. Int’l Symp. on Computer
Architecture.

[52] Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., Van Den
Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V.,
Lanctot, M. and Dieleman, S., 2016. Mastering the game of Go with
deep neural networks and tree search. Nature, 529(7587).

[53] Smith, J.E., 1982, April. Decoupled access/execute computer
architectures. Proc. Int’! Symp. on Computer Architecture.

[54] Steinberg, D., 2015. Full-Chip Simulations, Keys to Success. Proc.
Synopsys Users Group (SNUG) Silicon Valley 2015.

[55] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D.,
Erhan, D., Vanhoucke, V. and Rabinovich, A., 2015. Going deeper
with convolutions. Proc. IEEE Conference on Computer Vision and
Pattern Recognition.

[56] Thorson, G., Clark, C., Luu, D., 2015. Vector Computation Unit in a
Neural Network Processor, Patent Application No. 62/165,022.
[57] Williams, S., Waterman, A. and Patterson, D., 2009. Roofline: an
insightful visual performance model for multicore architectures.
CACM, 52(4).

[58] Wu, Y., Schuster, M., Chen, Z., Le, Q., Norouzi, M., Macherey, W.,
Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A.,
Johnson, M., Liu, X., Kaiser, L., Gouws, S., Kato, Y., Kudo, T.,
Kazawa, H., Stevens, K., Kurian, G., Patil, N., Wang, W., Young,
C., Smith, J., Riesa, J., Rudnick, A., Vinyals, O., Corrado, G.,
Hughes, M., and Dean, J. September 26, 2016, Google's Neural
Machine Translation System: Bridging the Gap between Human and
Machine Translation, http://arxiv.org/abs/1609.08144.

[59] Young, C., 2015. Batch Processing in a Neural Network Processor,
Patent Application No. 62/165,020.

[60] Zhang, C., Li, P., Sun, G., Guan, Y., Xiao, B. and Cong, J., 2015,
February. Optimizing FPGA-based accelerator design for deep
convolutional neural networks. Proceedings of the 2015
ACMISIGDA International Symposium on Field-Programmable
Gate Arrays.