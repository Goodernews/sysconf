[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,
C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, et al.
Tensorflow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.
org.
[2] A. Agarwal, S. Bird, M. Cozowicz, L. Hoang, J. Langford, S. Lee, J. Li, D. Melamed, G. Oshri, O. Ribas, et al.
A multiworld testing decision service. arXiv preprint
arXiv:1606.03966, 2016.
[3] D. Agarwal, B. Long, J. Traupman, D. Xin, and L. Zhang.
Laser: A scalable response prediction platform for online
advertising. In WSDM, pages 173–182, 2014.
[4] S. Agarwal and J. R. Lorch. Matchmaking for online
games and other latency-sensitive p2p systems. In ACM
SIGCOMM Computer Communication Review, volume 39,
pages 315–326. ACM, 2009.
[5] A. Ahmed, M. Aly, J. Gonzalez, S. Narayanamurthy, and
A. J. Smola. Scalable inference in latent variable models.
In WSDM, pages 123–132, 2012.
[6] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire.
The nonstochastic multiarmed bandit problem. SIAM J.
Comput., 32(1):48–77, Jan. 2003.
[7] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley, and
Y. Bengio. Theano: a cpu and gpu math expression compiler. In Proceedings of the Python for scientific computing
conference (SciPy), volume 4, page 3. Austin, TX, 2010.
[8] C. M. Bishop. Pattern Recognition and Machine Learning.
Springer-Verlag, Secaucus, NJ, USA, 2006.
[9] L. Breiman. Bagging predictors. Mach. Learn., 24(2):123–
140, Aug. 1996.
[10] C. Chelba, D. Bikel, M. Shugrina, P. Nguyen, and S. Kumar. Large scale language modeling in automatic speech
recognition. arXiv preprint arXiv:1210.8440, 2012.
[11] J. Chen, R. Monga, S. Bengio, and R. Jozefowicz. Revisiting Distributed Synchronous SGD. arXiv.org, Apr.
2016.
[12] T. Chen and C. Guestrin. XGBoost: A Scalable Tree
Boosting System. arXiv.org, Mar. 2016.
[13] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao,
B. Xu, C. Zhang, and Z. Zhang. Mxnet: A flexible and
efficient machine learning library for heterogeneous distributed systems. arXiv preprint arXiv:1512.01274, 2015.
[14] T. Chilimbi, Y. Suzue, J. Apacible, and K. Kalyanaraman.
Project adam: Building an efficient and scalable deep learning training system. In OSDI, pages 571–582, 2014.
[15] D.-M. Chiu and R. Jain. Analysis of the increase and
decrease algorithms for congestion avoidance in computer
networks. Comput. Netw. ISDN Syst., 17(1):1–14, June
1989.
[16] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7:
A matlab-like environment for machine learning. In
BigLearn, NIPS Workshop, number EPFL-CONF-192376,
2011.
[17] F. J. Corbato. A paging experiment with the multics system. 1968.
[18] Microsoft Cortana. https://www.microsoft.com/
en-us/mobile/experiences/cortana/.
[19] D. Crankshaw, P. Bailis, J. E. Gonzalez, H. Li, Z. Zhang,
M. J. Franklin, A. Ghodsi, and M. I. Jordan. The missing
piece in complex analytics: Low latency, scalable model
management and serving with velox. In CIDR 2015, 2015.
[20] J. Davidson, B. Liebald, J. Liu, P. Nandy, T. Van Vleet,
U. Gargi, S. Gupta, Y. He, M. Lambert, B. Livingston, and
D. Sampath. The YouTube video recommendation system.
RecSys, pages 293–296, 2010.
[21] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin,
M. Mao, M. aurelio Ranzato, A. Senior, P. Tucker, K. Yang,
Q. V. Le, and A. Y. Ng. Large scale distributed deep networks. In NIPS, pages 1223–1231. 2012.
[22] J. Donahue. Caffenet. https://github.com/BVLC/
caffe/tree/master/models/bvlc_reference_
caffenet.
[23] A. Ganjam, F. Siddiqui, J. Zhan, X. Liu, I. Stoica, J. Jiang,
V. Sekar, and H. Zhang. C3: Internet-Scale Control Plane
for Video Quality Optimization. NSDI ’15, pages 131–144,
2015.
[24] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus,
D. S. Pallett, and N. L. Dahlgren. Darpa timit acoustic
phonetic continuous speech corpus cdrom, 1993.
[25] J. E. Gonzalez, Y. Low, H. Gu, D. Bickson, and C. Guestrin.
Powergraph: Distributed graph-parallel computation on
natural graphs. OSDI, pages 17–30, 2012.
[26] Google Now. https://www.google.com/landing/
now/.
[27] T. Graepel, J. Q. Candela, T. Borchert, and R. Herbrich.
Web-Scale Bayesian Click-Through rate Prediction for
Sponsored Search Advertising in Microsoft’s Bing Search
Engine. ICML, pages 13–20, 2010.
[28] h20. http://www.h2o.ai.
[29] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. arXiv preprint
arXiv:1512.03385, 2015.
[30] G. Hinton, O. Vinyals, and J. Dean. Distilling the Knowledge in a Neural Network. arXiv.org, Mar. 2015.
[31] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long,
R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the ACM International Conference on Multimedia, pages 675–678. ACM, 2014.
[32] A. Krizhevsky and G. Hinton. Cifar-10 dataset. https:
//www.cs.toronto.edu/~kriz/cifar.html.
[33] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
classification with deep convolutional neural networks. In
NIPS, pages 1097–1105, 2012.
[34] J. Langford, L. Li, and A. Strehl. Vowpal wabbit online
learning project, 2007.
[35] Y. LeCun, C. Cortes, and C. J. Burges. MNIST handwritten
digit database. 1998.
[36] X. Lei, A. W. Senior, A. Gruenstein, and J. Sorensen. Accurate and compact large vocabulary speech recognition
on mobile devices. INTERSPEECH, pages 662–665, 2013.
[37] R. Lerallut, D. Gasselin, and N. Le Roux. Large-Scale
Real-Time Product Recommendation at Criteo. In RecSys,
pages 232–232, 2015.
[38] M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed,
V. Josifovski, J. Long, E. J. Shekita, and B.-Y. Su. Scaling
distributed machine learning with the parameter server. In
OSDI, pages 583–598, 2014.
[39] H. B. McMahan, G. Holt, D. Sculley, M. Young, D. Ebner,
J. Grady, L. Nie, T. Phillips, E. Davydov, D. Golovin,
S. Chikkerur, D. Liu, M. Wattenberg, A. M. Hrafnkelsson,
T. Boulos, and J. Kubica. Ad click prediction: a view from
the trenches. In KDD, page 1222, 2013.
[40] X. Meng, J. Bradley, B. Yavuz, E. Sparks, S. Venkataraman, D. Liu, J. Freeman, D. Tsai, M. Amde, S. Owen,
D. Xin, R. Xin, M. J. Franklin, R. Zadeh, M. Zaharia, and
A. Talwalkar. Mllib: Machine learning in apache spark.
Journal of Machine Learning Research, 17(34):1–7, 2016.
[41] V. Mnih, N. Heess, A. Graves, et al. Recurrent models of
visual attention. In NIPS, pages 2204–2212, 2014.
[42] Deep MNIST for Experts. https://www.tensorflow.
org/versions/r0.10/tutorials/mnist/pros/index.
html.
[43] K. P. Murphy. Machine Learning: A Probabilistic Perspective. The MIT Press, 2012.
[44] J. Nagle. Congestion control in ip/tcp internetworks. SIGCOMM Comput. Commun. Rev., 14(4):11–17, Oct. 1984.
[45] nginx [engine x]. http://nginx.org/en/.
[46] V. S. Pai, P. Druschel, and W. Zwaenepoel. Flash: An efficient and portable Web server. USENIX Annual Technical
Conference, General Track, pages 199–212, 1999.
[47] Portable Format for Analytics (PFA). http://dmg.org/
pfa/index.html.
[48] PMML 4.2. http://dmg.org/pmml/v4-2-1/
GeneralStructure.html.
[49] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual
Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015.
[50] D. C. Schmidt. Pattern languages of program design. chapter Reactor: An Object Behavioral Pattern for Concurrent
Event Demultiplexing and Event Handler Dispatching,
pages 529–545. 1995.
[51] Scikit-Learn machine learning in python. http://
scikit-learn.org.
[52] D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips,
D. Ebner, V. Chaudhary, M. Young, J.-F. Crespo, and
D. Dennison. Hidden Technical Debt in Machine Learning
Systems. NIPS, 2015.
[53] J. Sill, G. Takács, L. Mackey, and D. Lin. Feature-weighted
linear stacking, 2009.
[54] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014.
[55] Apple Siri. http://www.apple.com/ios/siri/.
[56] Skype real time translator. https://www.skype.com/en/
features/skype-translator/.
[57] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, pages 1–9,
2015.
[58] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer
vision. arXiv preprint arXiv:1512.00567, 2015.
[59] TensorFlow Serving. https://tensorflow.github.io/
serving.
[60] Turi. https://turi.com.
[61] M. Welsh, D. E. Culler, and E. A. Brewer. SEDA: An
Architecture for Well-Conditioned, Scalable Internet Services. SOSP, pages 230–243, 2001.
[62] E. P. Xing, Q. Ho, W. Dai, J.-K. Kim, J. Wei, S. Lee,
X. Zheng, P. Xie, A. Kumar, and Y. Yu. Petuum: A new
platform for distributed machine learning on big data. In
KDD, pages 1335–1344. ACM, 2015.
[63] S. J. Young, G. Evermann, M. J. F. Gales, T. Hain,
D. Kershaw, G. Moore, J. Odell, D. Ollason, D. Povey,
V. Valtchev, and P. C. Woodland. The HTK Book, version 3.4. Cambridge University Engineering Department,
Cambridge, UK, 2006.
[64] J.-M. Yun, Y. He, S. Elnikety, and S. Ren. Optimal aggregation policy for reducing tail latency of web search. In
SIGIR, pages 63–72, 2015.
[65] M. Zaharia et al. Resilient distributed datasets: A faulttolerant abstraction for in-memory cluster computing. In
NSDI, 2012.

