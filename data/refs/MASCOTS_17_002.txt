[1] Acl 2014 workshop on statistical machine translation.
[2] CIFAR-10. https://www.cs.toronto.edu/ kriz/cifar.html.
[3] Torch. http://torch.ch/.
[4] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine
translation by jointly learning to align and translate.
CoRR’14.
[5] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley, and
Y. Bengio. Theano: a CPU and GPU math expression
compiler. SciPy’10, June.
[6] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen,
J. Tran, B. Catanzaro, and E. Shelhamer. cudnn: Efﬁcient
primitives for deep learning. CoRR’14.
[7] T. Chilimbi, Y. Suzue, J. Apacible, and K. Kalyanaraman. Project adam: Building an efﬁcient and scalable
deep learning training system. OSDI’14.
[8] A. Coates, B. Huval, T. Wang, D. Wu, B. Catanzaro,
and N. Andrew. Deep learning with cots hpc systems.
In S. Dasgupta and D. Mcallester, editors, ICML 2013.
[9] H. Cui, H. Zhang, G. R. Ganger, P. B. Gibbons, and E. P.
Xing. Geeps: Scalable deep learning on distributed gpus
with a gpu-specialized parameter server. In EuroSys’16.
[10] G. E. Dahl, D. Yu, L. Deng, and A. Acero. Contextdependent pre-trained deep neural networks for largevocabulary speech recognition. Trans. Audio, Speech and
Lang. Proc., 2012.
[11] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin,
M. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang,
Q. V. Le, and A. Y. Ng. Large scale distributed deep
networks. NIPS’12.
[12] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long,
R. Girshick, S. Guadarrama, and T. Darrell. Caffe:
Convolutional architecture for fast feature embedding.
ACM MM’14.
[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
classiﬁcation with deep convolutional neural networks.
In NIPS 2012.
[14] M. Li, D. G. Andersen, J. W. Park, A. J. Smola,
A. Ahmed, V. Josifovski, J. Long, E. J. Shekita, and
B.-Y. Su. Scaling distributed machine learning with the
parameter server. OSDI’14.
[15] M. Lin, Q. Chen, and S. Yan. Network in network.
CoRR’13.
[16] B. T. Polyak and A. B. Juditsky. Acceleration of
stochastic approximation by averaging. SIAM J. Control
Optim., 1992.
[17] B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A lockfree approach to parallelizing stochastic gradient descent.
NIPS’11.
[18] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual
Recognition Challenge. IJCV 2015.
[19] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. CoRR’14.
[20] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. CVPR’15.
[21] S. Zhang, A. E. Choromanska, and Y. LeCun. Deep
learning with elastic averaging sgd. NIPS’14.
[22] W. Zhang, S. Gupta, X. Lian, and J. Liu. Staleness-aware
async-sgd for distributed deep learning. IJCAI’16.
[23] M. Zinkevich, J. Langford, and A. J. Smola. Slow
learners are fast. NIPS’09.
[24] M. Zinkevich, M. Weimer, L. Li, and A. J. Smola.
Parallelized stochastic gradient descent. NIPS’10.
