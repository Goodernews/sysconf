
REFERENCES

[1] A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky,
“Tensor decompositions for learning latent variable models.” Journal
of Machine Learning Research, vol. 15, no. 1, pp. 2773-2832, 2014.

[2] F. Huang, U. Niranjan, M. U. Hakeem, and A. Anandkumar, “Online
tensor methods for learning latent variable models,” Journal of Machine
Learning Research, vol. 16, pp. 2797-2835, 2015.

[3] U. Kang, E. Papalexakis, A. Harpale, and C. Faloutsos, “Gigatensor:
scaling tensor analysis up by 100 times-algorithms and discoveries,”
in Proceedings of the 18th ACM SIGKDD international conference on
Knowledge discovery and data mining. ACM, 2012, pp. 316-324.

[4] T. G. Kolda and J. Sun, “Scalable tensor decompositions for multiaspect data mining,” in Data Mining, 2008. ICDM’08. Eighth IEEE
International Conference on. YEEE, 2008, pp. 363-372.

[5] M. A. O. Vasilescu and D. Terzopoulos, “Multilinear analysis of image
ensembles: Tensorfaces,” in European Conference on Computer Vision.
Springer, 2002, pp. 447-460.

[6] M. A. O. Vasilescu, “Multilinear projection for face recognition via
canonical decomposition,” in Automatic Face & Gesture Recognition
and Workshops (FG 2011), 2011 IEEE International Conference on.
TEEE, 2011, pp. 476-483.

[7] Y. Shi, A. Karatzoglou, L. Baltrunas, M. Larson, A. Hanjalic, and
N. Oliver, “Tfmap: optimizing map for top-n context-aware recommendation,” in Proceedings of the 35th international ACM SIGIR conference
on Research and development in information retrieval. ACM, 2012,
pp. 155-164.

[8] V. Khoromskaia and B. N. Khoromskij, “Tensor numerical methods in
quantum chemistry: from hartree—fock to excitation energies,” Physical
Chemistry Chemical Physics, vol. 17, no. 47, pp. 31491-31509, 2015.

[9] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.
Corrado, A. Davis, J. Dean, M. Devin et al., ““Tensorflow: Large-scale
machine learning on heterogeneous distributed systems,” arXiv preprint
arXiv: 1603.04467, 2016.

[10] R. Collobert, K. Kavukcuoglu, and C. Farabet, “Torch7: A matlab-like
environment for machine learning,” in BigLearn, NIPS Workshop, no.
EPFL-CONF- 192376, 2011.

[11] S. Smith, N. Ravindran, N. D. Sidiropoulos, and G. Karypis, “Splatt: Efficient and parallel sparse tensor-matrix multiplication,” in Parallel and
Distributed Processing Symposium (IPDPS), 2015 IEEE International.
TEEE, 2015, pp. 61-70.

[12] S. Smith and G. Karypis, “Tensor-matrix products with a compressed
sparse tensor,” in Proceedings of the 5th Workshop on Irregular Applications: Architectures and Algorithms. ACM, 2015, p. 7.

[13] J. Li, Y. Ma, C. Yan, and R. Vuduc, “Optimizing sparse tensor times
matrix on multi-core and many-core architectures,” in Proceedings of the
Sixth Workshop on Irregular Applications: Architectures and Algorithms.
IEEE Press, 2016, pp. 26-33.

[14] I. Jeon, E. E. Papalexakis, U. Kang, and C. Faloutsos, “Haten2:
Billion-scale tensor decompositions,” in 2015 IEEE 31st International
Conference on Data Engineering. YEEE, 2015, pp. 1047-1058.

[15] J. H. Choi and S. Vishwanathan, “Dfacto: Distributed factorization of
tensors,” in Advances in Neural Information Processing Systems, 2014,
pp. 1296-1304.

[16] O. Kaya and B. Ugar, “High performance parallel algorithms for the
tucker decomposition of sparse tensors,” in Parallel Processing (ICPP),
2016 45th International Conference on. YEEE, 2016, pp. 103-112.

[17] -, “Scalable sparse tensor decompositions in distributed memory
systems,” in Proceedings of the International Conference for High
Performance Computing, Networking, Storage and Analysis. ACM,
2015, p. 77.

[18] J. Li, Y. Ma, C. Yan, J. Sun, and R. Vuduc, “Parti a parallel tensor
infrastructure for data analysis,” in NIPS, Tensor-Learn Workshop, 2016.

[19] T. G. Kolda and B. W. Bader, “Tensor decompositions and applications,”
SIAM review, vol. 51, no. 3, pp. 455-500, 2009.

[20] W. Austin, G. Ballard, and T. G. Kolda, “Parallel tensor compression
for large-scale scientific data,’ arXiv preprint arXiv:1510.06689, 2015.

[21] B. W. Bader, T. G. Kolda et al., “Matlab tensor toolbox version 2.6,
available online, february 2015,” 2015.

[22] C. A. Andersson and R. Bro, “The n-way toolbox for matlab,” Chemometrics and intelligent laboratory systems, vol. 52, no. 1, pp. 1-4, 2000.

[23] E. Solomonik, D. Matthews, J. Hammond, and J. Demmel, “Cyclops
tensor framework: Reducing communication and eliminating load imbalance in massively parallel contractions,” in Parallel & Distributed
Processing (IPDPS), 2013 IEEE 27th International Symposium on.
TEEE, 2013, pp. 813-824.

[24] M. Baskaran, B. Meister, N. Vasilache, and R. Lethin, “Efficient and
scalable computations with sparse tensors,” in High Performance Extreme Computing (HPEC), 2012 IEEE Conference on. YEEE, 2012,
pp. 1-6.

[25] N. Bell and M. Garland, “Implementing sparse matrix-vector multiplication on throughput-oriented processors,” in Proceedings of the
Conference on High Performance Computing Networking, Storage and
Analysis. ACM, 2009, p. 18.

[26] S. Sengupta, M. Harris, and M. Garland, “Efficient parallel scan algorithms for gpus,” NVIDIA, Santa Clara, CA, Tech. Rep. NVR-2008-003,
no. 1, pp. 1-17, 2008.

[27] S. Yan, G. Long, and Y. Zhang, “Streamscan: fast scan algorithms for
gpus without global barrier synchronization,” in ACM SIGPLAN Notices,
vol. 48, no. 8. ACM, 2013, pp. 229-238.

[28] S. Smith, J. W. Choi, J. Li, R. Vuduc, J. Park, and G. Karypis. (2017)
FROSTT: The formidable repository of open sparse tensors and tools.
[Online]. Available: http://frostt.io/

[29] A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. Hruschka Jr,
and T. M. Mitchell, “Toward an architecture for never-ending language
learning.” in AAAJ, vol. 5, 2010, p. 3.

[30] T. M. Mitchell, S. V. Shinkareva, A. Carlson, K.-M. Chang, V. L.
Malave, R. A. Mason, and M. A. Just, “Predicting human brain activity
associated with the meanings of nouns,” science, vol. 320, no. 5880, pp.
1191-1195, 2008.

[31] O. Gorlitz, S. Sizov, and S. Staab, “Pints: peer-to-peer infrastructure for
tagging systems.” in JPTPS. Citeseer, 2008, p. 19.

[32] C. Nvidia, “Cublas library,’ NVIDIA Corporation, Santa Clara, California, vol. 15, p. 27, 2008.