[1] “National center for biotechnology information,” http://www.
ncbi.nlm.nih.gov, online, 2016.
[2] V. Adhinarayanan et al., “Measuring and Modeling On-Chip
Interconnect Power on Real Hardware,” in IISWC, 2016.
[3] AMD, “AMD APP SDK OpenCL Optimization Guide,” 2013.
[4] AMD, “AMD APP SDK OpenCL User Guide,” 2013.
[5] A. Bakhoda et al., “Analyzing CUDA workloads using a
detailed GPU simulator,” in ISPASS, 2009.
[6] M. Burtscher et al., “A Quantitative Study of Irregular Programs on GPUs,” in IISWC, 2012.
[7] S. Che et al., “Pannotia: Understanding Irregular GPGPU
Graph Applications,” in IISWC, 2013.
[8] G. Chen and X. Shen, “Free Launch: Optimizing GPU Dynamic Kernel Launches Through Thread Reuse,” in MICRO,
2015.
[9] H. Cheng et al., “BitMapper: an efficient all-mapper based
on bit-vector computing,” in BMC Bioinformatics, 2015.
[10] G. Diamos et al., “Relational Algorithms for Multi-bulksynchronous Processors,” in PPoPP, 2013.
[11] W. Ding et al., “Optimizing Off-chip Accesses in Multicores,”
in PLDI, 2015.
[12] S. Hong et al., “Accelerating CUDA Graph Algorithms at
Maximum Warp,” in PPoPP, 2011.
[13] A. Jog et al., “Anatomy of GPU Memory System for MultiApplication Execution,” in MEMSYS, 2015.
[14] A. Jog et al., “Exploiting Core Criticality for Enhanced
Performance in GPUs,” in SIGMETRICS, 2016.
[15] M. Kandemir et al., “Memory Row Reuse Distance and Its
Role in Optimizing Application Performance,” in SIGMETRICS, 2015.

[16] O. Kayiran et al., “uC-States: Fine-grained GPU Datapath
Power Management,” in PACT, 2016.
[17] J. Y. Kim and C. Batten, “Accelerating Irregular Algorithms
on GPGPUs Using Fine-Grain Hardware Worklists,” in MICRO, 2014.
[18] A. Kuhl, “Thermodynamic States in Explosion Fields,” in
IDS, 2010.
[19] M. Kulkarni et al., “Optimistic Parallelism Requires Abstractions,” in PLDI, 2007.
[20] G. Liu et al., “FlexBFS: A Parallelism-aware Implementation
of Breadth-first Search on GPU,” in PPoPP, 2012.
[21] M. Mendez-Lojo et al., “A GPU Implementation of Inclusionbased Points-to Analysis,” in PPoPP, 2012.
[22] D. Merrill et al., “Scalable GPU Graph Traversal,” in PPoPP,
2012.
[23] L. Nai et al., “GraphBIG: Understanding Graph Computing
in the Context of Industrial Solutions,” in SC, 2015.
[24] NVIDIA, “CUDA C/C++ SDK Code Samples.”
[25] NVIDIA, “JP Morgan Speeds Risk Calculations with
NVIDIA GPUs,” 2011.
[26] NVIDIA, “Dynamic Parallelism in CUDA,” 2012.
[27] NVIDIA, “Next Generation CUDA Compute Architecture:
Kepler GK110,” 2012.
[28] NVIDIA, “CUDA C Programming Guide,” 2015.
[29] NVIDIA, “Profiler User’s Guide,” 2015.
[30] S. I. Park et al., “Low-Cost, High-Speed Computer Vision
using NVIDIA’s CUDA Architecture,” in AIPR, 2008.
[31] A. Pattnaik et al., “Scheduling Techniques for GPU Architectures with Processing-In-Memory Capabilities,” in PACT,
2016.
[32] G. Pratx and L. Xing, “GPU Computing in Medical Physics:
A Review,” in Medical physics, 2011.
[33] S. Puthoor et al., “Implementing Directed Acyclic Graphs
with the Heterogeneous System Architecture,” in GPGPU,
2016.
[34] T. G. Rogers et al., “Cache-Conscious Wavefront Scheduling,” in MICRO, 2012.
[35] P. Sanders and C. Schulz, “10th Dimacs Implementation
Challenge-Graph Partitioning and Graph Clustering,” 2012.
[36] I. Schmerken, “Wall Street Accelerates Options Analysis with
GPU Technology,” 2009.
[37] J. Shen et al., “Improving Performance by Matching Imbalanced Workloads with Heterogeneous Platforms,” in ICS,
2014.
[38] S. S. Stone et al., “Accelerating advanced MRI reconstructions on GPUs,” J. Parallel Distributed Computing, 2008.
[39] X. Tang et al., “A Video Coding Benchmark Suite for
Evaluation of Processor Capability,” in SNPD, 2013.
[40] X. Tang et al., “Improving Bank-Level Parallelism for Irregular Applications,” in MICRO, 2016.
[41] Y. Ukidave et al., “NUPAR: A Benchmark Suite for Modern
GPU Architectures,” in ICPE, 2015.
[42] J. Wang et al., “Dynamic Thread Block Launch: A
Lightweight Execution Mechanism to Support Irregular Applications on GPUs,” in ISCA, 2015.
[43] J. Wang et al., “LaPerm: Locality Aware Scheduler for
Dynamic Parallelism on GPUs,” in ISCA, 2016.
[44] J. Wang and Y. Sudhakar, “Characterization and Analysis of
Dynamic Parallelism in Unstructured GPU Applications,” in
IISWC, 2014.
[45] H. Wu et al., “Compiler-Assisted Workload Consolidation For
Efficient Dynamic Parallelism on GPU”,” in IPDPS, 2016.
[46] Y. Yang and H. Zhou, “CUDA-NP: Realizing Nested Threadlevel Parallelism in GPGPU Applications,” in PPoPP, 2014.
