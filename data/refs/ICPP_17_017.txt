[1] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell, “Caffe: Convolutional
Architecture for Fast Feature Embedding,” arXiv preprint
arXiv: 1408.5093, 2014.

[2] NVIDIA, “Caffe: a fast open framework for deep learning,”

Accessed: June 6, 2017. [Online]. Available: https://github.

com/NVIDIA/caffe

[3] A. A. Awan, K. Hamidouche, J. M. Hashmi, and D. K. Panda,

“S-Caffe: Co-designing MPI Runtimes and Caffe for Scalable

Deep Learning on Modern GPU Clusters,” in Proceedings

of the 22Nd ACM SIGPLAN Symposium on Principles and

Practice of Parallel Programming, ser. PPoPP ’17. ACM,

2017, pp. 193-205.

[4] Facebook, “A New Lightweight, Modular, and Scalable Deep

Learning Framework,” Accessed: June 6, 2017. [Online].

Available: https://caffe2.ai/

[5] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,

C. Citro, G. S. Corrado et al., “TensorFlow: Large-Scale

Machine Learning on Heterogeneous Systems,’ Accessed:

June 6, 2017. [Online]. Available: http://tensorflow.org/

[6] Microsoft, “The Microsoft Cognitive Toolkit,’ Accessed:

June 6, 2017. [Online]. Available: http://www.cntk.ai/

[7] A. Venkatesh, H. Subramoni, K. Hamidouche, and D. K.

Panda, “A High Performance Broadcast Design with Hard
ware Multicast and GPUDirect RDMA for Streaming Appli
cations on Infiniband Clusters,’ in 20/4 21st International

Conference on High Performance Computing (HiPC), Dec

2014, pp. 1-10.

[8] C.-H. Chu, K. Hamidouche, H. Subramoni, A. Venkatesh,

B. Elton, and D. K. Panda, “Designing High Performance

Heterogeneous Broadcast for Streaming Applications on GPU

Clusters,” in 20/6 28th International Symposium on Com
puter Architecture and High Performance Computing (SBAC
PAD), Oct 2016, pp. 59-66.

[9] “InfiniBand Trade Association,’ Accessed: June 6, 2017.
[Online]. Available: http://www.infinibandta.org
[10] “NVIDIA GPUDirect RDMA,” Accessed: June 6, 2017. [On
line]. Available: http://docs.nvidia.com/cuda/gpudirect-rdma/

170

[11] S. Potluri, K. Hamidouche, A. Venkatesh, D. Bureddy, and
D. Panda, “Efficient Internode MPI Communication Using
GPUDirect RDMA for InfiniBand Clusters with NVIDIA
GPUs,” in Parallel Processing (ICPP), 2013 42nd International Conference on, Oct 2013, pp. 80-89.

[12] D. M. Wadsworth and Z. Chen, “Performance of MPI Broad
cast Algorithms,” in 2008 IEEE International Symposium on

Parallel and Distributed Processing, April 2008, pp. 1-7.

[13] D. S. Banerjee, K. Hamidouche, and D. K. Panda, “Re
Designing CNTK Deep Learning Framework on Modern

GPU Enabled Clusters,” in 20/6 IEEE International Confer
ence on Cloud Computing Technology and Science (Cloud
Com), Dec 2016, pp. 144-151.

[14] C. H. Chu, K. Hamidouche, H. Subramoni, A. Venkatesh,

B. Elton, and D. K. Panda, “Efficient Reliability Support

for Hardware Multicast-Based Broadcast in GPU-enabled

Streaming Applications,” in 2016 First International Work
shop on Communication Optimizations in HPC (COMHPC),

Nov 2016, pp. 29-38.

[15] D. Bureddy, H. Wang, A. Venkatesh, S. Potluri, and D. K.

Panda, “OMB-GPU: A Micro-benchmark Suite for Evaluating

MPI Libraries on GPU Clusters,” in Proceedings of the 19th

European Conference on Recent Advances in the Message

Passing Interface (EuroMPI!), 2012, pp. 110-120.

[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet

Classification with Deep Convolutional Neural Networks,” in

Advances in neural information processing systems, 2012, pp.

1097-1105.

[17] K. Simonyan and A. Zisserman, “Very Deep Convolutional

Networks for Large-Scale Image Recognition,” arXiv preprint

arXiv: 1409.1556, 2014.

[18] “Top 500 Supercomputer sites?’ Accessed: June 6, 2017.

[Online]. Available: http://www.top500.org/

[19] NVIDIA, “Optimized Primitives for Collective Multi
GPU Communication,’ Accessed: June 6, 2017. [Online].

Available: https://github.com/NVIDIA/nccl

[20] J. Zhou, X.-Y. Lin, and Y.-C. Chung, “Hardware Supported

Multicast in Fat-tree-based InfiniBand Networks,” J. Super
comput., vol. 40, no. 3, pp. 333-352, Jun. 2007.

[21] M. Sourouri, T. Gillberg, S. B. Baden, and X. Cai, “Effective

Multi-GPU Communication using Multiple CUDA Streams

and Threads,” in 2014 20th IEEE International Conference

on Parallel and Distributed Systems (ICPADS), Dec 2014,

pp. 981-986.

[22] T. Hoefler, C. Siebert, and W. Rehm, “A Practically Constant
time MPI Broadcast Algorithm for Large-scale InfiniBand

Clusters with Multicast,” in Proceedings of the 21st IEEE

International Parallel & Distributed Processing Symposium

(CAC’07 Workshop), Mar. 2007, p. 232.

[23] A. R. Mamidala, L. Chai, H.-W. Jin, and D. K. Panda,

“Efficient SMP-aware MPI-level Broadcast over InfiniBand’s

Hardware Multicast,” in Proceedings 20th IEEE International

Parallel Distributed Processing Symposium, April 2006, p. 8.