[1] “Microsoft Azure —- HPC & Batch,” 2017, https://azure.microsoft.com/
en-us/solutions/big-compute/.

[2] “Amazon Web Services — High performance computing,” 2017, https:
/faws.amazon.com/hpe/.

[3] “BDEC -— Big Data and Extreme-Scale Computing,” 2017, http://www.
exascale.org/bdec/.

[4] D. A. Reed and J. Dongarra, “Exascale computing and big data,”
Communications of the ACM, vol. 58, no. 7, pp. 56-68, jun 2015.

[5] G. Aloisio, S$. Fiore, I. Foster, and D. Williams, “Scientific big data
analytics challenges at large scale,” BDEC, Tech. Rep., 2013.

[6] J. Cope, K. Iskra, D. Kimpe, and R. B. Ross, “Bridging HPC and grid
file /O with IOFSL,” in Applied Parallel and Scientific Computing 10th International Conference, PARA 2010, Reykjavik, Iceland, June
6-9, 2010, Revised Selected Papers, Part II, 2010, pp. 215-225.

[7] C. Pei, X. Shi, and H. Jin, “Improving the memory efficiency of inmemory mapreduce based HPC systems,” in Algorithms and Architectures for Parallel Processing - 15th International Conference, ICA3PP
2015, Zhangjiajie, China, November 18-20, 2015, Proceedings, Part I,
2015, pp. 170-184.

[8] J. F. Lofstead and R. Ross, “Insights for exascale IO APIs from building
a petascale IO API,” in International Conference for High Performance
Computing, Networking, Storage and Analysis, SC’13, Denver, CO, USA
- November 17 - 21, 2013, 2013, pp. 87:1-87:12.

[9] D. Kimpe and R. Ross, “Storage models: Past, present, and future,”
High Performance Parallel I/O, pp. 335-345, 2014.

[10] R. Latham, R. B. Ross, and R. Thakur, “The impact of file systems
on MPI-IO scalability,” in Recent Advances in Parallel Virtual Machine
and Message Passing Interface, 11th European PVM/MPI Users’ Group
Meeting, Budapest, Hungary, September 19-22, 2004, Proceedings,
2004, pp. 87-96.

[11] J. Shafer, §. Rixner, and A. Cox, “The Hadoop distributed filesystem:
Balancing portability and performance,” in Performance Analysis of
Systems Software (ISPASS), 2010 IEEE International Symposium on,
March 2010, pp. 122-133.

[12] M. Vilayannur, §. Lang, R. Ross, R. Klundt, and L. Ward, “Extending
the POSIX I/O interface: A parallel file system perspective,” Argonne
National Laboratory, Tech. Rep. ANL/MCS-TM-302, 10 2008.

[13] M. Kuhn, J. M. Kunkel, and T. Ludwig, “Dynamically Adaptable /O
Semantics for High Performance Computing,” in High Performance
Computing, set. Lecture Notes in Computer Science, no. 9137. Switzerland: Springer International Publishing, 06 2015, pp. 240-256.

[14] P. Matri, A. Costan, G. Antoniu, J. Montes, and M. S. Pérez, “Tyr: Blob
storage meets built-in transactions,” in SC16: International Conference
for High Performance Computing, Networking, Storage and Analysis,
Nov 2016, pp. 573-584.

[15] S. A. Weil, A. W. Leung, S. A. Brandt, and C. Maltzahn, “Rados: A
scalable, reliable storage service for petabyte-scale storage clusters,”
in Proceedings of the 2nd international workshop on petascale data
storage: held in conjunction with Supercomputing’07. ACM, 2007,
pp. 35-44.

[16] “The Lustre File System,” 2017, http://lustre.org/.

[17] M. Moore, D. Bonnie, W. Ligon, N. Mills, , S. Yang, B. Ligon,
M. Marshall, E. Quarles, S. Sampson, and B. Wilson, “OrangeFS:
Advancing PVFS,” in 2011 9th USENIX Conference on File and Storage
Technologies (FAST), 2011.

[18] D. Huang, J. Yin, J. Wang, X. Zhang, J. Zhang, and J. Zhou, “UNIO:
A unified I/O system framework for hybrid scientific workflow,” in
Cloud Computing and Big Data - Second International Conference,
CloudCom-Asia 2015, Huangshan, China, June 17-19, 2015, Revised
Selected Papers, 2015, pp. 99-114.

[19] S. Patil and G. A. Gibson, “Scale and concurrency of GIGA+: file
system directories with millions of files,” in 9th USENIX Conference
on File and Storage Technologies, San Jose, CA, USA, February 15-17,
2011, 2011, pp. 177-190.

[20] M. Kuhn, “A Semantics-Aware I/O Interface for High Performance
Computing,” in Supercomputing, ser. Lecture Notes in Computer Science, J. M. Kunkel, T. Ludwig, and H. W. Meuer, Eds., no. 7905.
Berlin, Heidelberg: Springer, 06 2013, pp. 408-421.

[21] T. Sterling, E. Lusk, and W. Gropp, Beowulf Cluster Computing with
Linux, 2nd ed. Cambridge, MA, USA: MIT Press, 2003.

[22] P. Corbett, D. Feitelson, §. Fineberg, Y. Hsu, B. Nitzberg, J.-P. Prost,
M. Snir, B. Traversat, and P. Wong, Overview of the MPI-IO Parallel
I/O Interface. Boston, MA: Springer US, 1996, pp. 127-146.

[23] “Hierarchical data format version 5,” 2017, https://hdfgroup.org/HDFS.

[24] C. Bartz, K. Chasapis, M. Kuhn, P. Nerge, and T. Ludwig, “A best practice analysis of HDF5 and NetCDF-4 using lustre,” in High Performance
Computing, set. Lecture Notes in Computer Science, J. M. Kunkel
and T. Ludwig, Eds., no. 9137. Switzerland: Springer International
Publishing, 06 2015, pp. 274-281.

[25] “The adaptable io system (ADIOS),” 2017, https:/Awww.olcf.ornl.gov/
center-projects/adios/.

[26] Q. Liu, J. Logan, Y. Tian, H. Abbasi, N. Podhorszki, J. Y. Choi,
8. Klasky, R. Tchoua, J. F. Lofstead, R. Oldfield, M. Parashar, N. F.
Samatova, K. Schwan, A. Shoshani, M. Wolf, K. Wu, and W. Yu, “Hello
ADIOS: the challenges and lessons of developing leadership class I/O
frameworks,” Concurrency and Computation: Practice and Experience,
vol. 26, no. 7, pp. 1453-1473, 2014.

[27] 8. Ghemawat, H. Gobioff, and S.-T. Leung, “The Google file system,”
in ACM SIGOPS Operating Systems Review, vol. 37, no. 5. ACM,
2003, pp. 29-43.

[28] K. Shvachko, H. Kuang, S. Radia, and R. Chansler, “The Hadoop
distributed file system,” in 2010 IEEE 26th symposium on Mass Storage
Systems and Technologies (MSST). EEE, 2010, pp. 1-10.

[29] 8. A. Weil, S. A. Brandt, E. L. Miller, D. D. E. Long, and C. Maltzahn,
“Ceph: A scalable, high-performance distributed file system,” in Proceedings of the 7th Symposium on Operating Systems Design and Implementation, set. OSDI 06. Berkeley, CA, USA: USENIX Association,
2006, pp. 307-320.

[30] S. Mikami, K. Ohta, and O. Tatebe, “Using the gfarm file system
as a POSIX compatible storage platform for Hadoop MapReduce
applications,” in Proceedings of the 2011 IEEE/ACM 12th International
Conference on Grid Computing. EEE Computer Society, 2011, pp.
181-189.

[31] Z. Zhang, K. Barbary, F. A. Nothaft, E. R. Sparks, O. Zahn, M. J.
Franklin, D. A. Patterson, and S. Perlmutter, “Scientific computing
meets big data technology: An astronomy use case,” in 2015 IEEE
International Conference on Big Data, Big Data 2015, Santa Clara,
CA, USA, October 29 - November 1, 2015, 2015, pp. 918-927.

[32] W. Lu, J. Jackson, and R. S. Barga, “AzureBlast: A case study of
developing science applications on the cloud,” in Proceedings of the
19th ACM International Symposium on High Performance Distributed
Computing, HPDC 2010, Chicago, Illinois, USA, June 21-25, 2010,
2010, pp. 413-420.

[33] J. L. Vazquez-Poletti, D. Santos-Mujioz, I. M. Llorente, and F. Valero,
“A cloud for clouds: Weather research and forecasting on a public
cloud infrastructure,” in Cloud Computing and Services Sciences International Conference in Cloud Computing and Services Sciences,
CLOSER 2014, Barcelona, Spain, April 3-5, 2014, Revised Selected
Papers, 2014, pp. 3-11.

[34] H. A. Duran-Limon, J. Flores-Contreras, N. Parlavantzas, M. Zhao, and
A. Meulenert-Pefia, “Efficient execution of the WRF model and other
HPC applications in the cloud,” Earth Science Informatics, vol. 9, no. 3,
pp. 365-382, 2016.

[35] E. D. Carrefio, E. Roloff, and P. O. A. Navaux, “Porting a numerical
atmospheric model to a cloud service,” in High Performance Computing
- Second Latin American Conference, CARLA 2015, Petrépolis, Brazil,
August 26-28, 2015, Proceedings, 2015, pp. 50-61.

[36] B. Langmead, M. C. Schatz, J. Lin, M. Pop, and S. L. Salzberg,
“Searching for SNPs with cloud computing,” Genome Biology, vol. 10,
no. 11, p. R134, 2009.

[37] A. Jaikar and S. Noh, “Cloud computing: Read before use,” T. LargeScale Data- and Knowledge-Centered Systems, vol. 30, pp. 1-22, 2016.

[38] A. Gupta and D. Milojicic, “Evaluation of HPC applications on cloud,”
in Open Cirrus Summit (OCS), 2011 Sixth. YEEE, 2011, pp. 22-26.

[39] A. Gupta, L. V. Kale, F. Gioachin, V. March, C. H. Suen, B.-S. Lee,
P. Faraboschi, R. Kaufmann, and D. Milojicic, “The who, what, why,
and how of high performance computing in the cloud,” in 2013 IEEE Sth
International Conference on Cloud Computing Technology and Science
(CloudCom), vol. 1. EEE, 2013, pp. 306-314.

[40] R. Ledyayev and H. Richter, “High performance computing in a cloud
using OpenStack,” Cloud Computing, pp. 108-113, 2014.

[41] P. Jakovits, 8. N. Srirama, and I. Kromonov, “Stratus: A distributed
computing framework for scientific simulations on the cloud,” in 14th
IEEE International Conference on High Performance Computing and
Communication & 9th IEEE International Conference on Embedded
Software and Systems, HPCC-ICESS 2012, Liverpool, United Kingdom,
June 25-27, 2012, pp. 1053-1059.

[42] A. Pan, J. P. Walters, V. 8. Pai, D. I. D. Kang, and S. P. Crago, “Integrating high performance file systems in a cloud computing environment,”
in 2012 SC Companion: High Performance Computing, Networking
Storage and Analysis, Nov. 2012, pp. 753-759.

[43] Y. Abe and G. Gibson, “pwalrus: Towards better integration of parallel
file systems into cloud storage,” in Cluster Computing Workshops and
Posters (Cluster Workshops), 2010 IEEE International Conference on.
TEEE, 2010, pp. 1-7.

[44] “mpiBLAST: Open-Source Parallel BLAST,”
mpiblast.org/.

[45] “Ocean circulation 
models,” 2017, https://Awww.gfdl.noaa.gov/
ocean-model/.

[46] U. H. Institute of Oceanography, “ECOHAM,” https://wiki.zmaw.de/
ifm/ECOHAM, 2015.

[47] L. Wang, J. Zhan, C. Luo, Y. Zhu, Q. Yang, Y. He, W. Gao, Z. Jia,
Y. Shi, S. Zhang, C. Zheng, G. Lu, K. Zhan, X. Li, and B. Qiu,
“BigDataBench: A big data benchmark suite from Internet services,”
in 2014 IEEE 20th International Symposium on High Performance
Computer Architecture (HPCA). TEEE, feb 2014.

[48] B. Nicolae, G. Antoniu, L. Bougé, D. Moise, and A. Carpen-Amarie,
“BlobSeer: Next-generation data management for large scale infrastructures,” J, Parallel Distrib. Comput., vol. 71, no. 2, pp. 169-184, 2011.

[49] B. Nicolae and F, Cappello, “BlobCR: Efficient checkpoint-restart for
HPC applications on IaaS clouds using virtual disk image snapshots,”
in Proceedings of 2011 International Conference for High Performance
Computing, Networking, Storage and Analysis, set. SC’11. New York,
NY, USA: ACM, 2011, pp. 34:1-34:12.

[50] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauley,
M. J. Franklin, 8. Shenker, and I. Stoica, “Resilient distributed datasets:
A fault-tolerant abstraction for in-memory cluster computing,” in Proceedings of the 9th USENIX conference on Networked Systems Design
and Implementation. USENIX Association, 2012, pp. 2-2.

[51] M. Li, J. Tan, Y. Wang, L. Zhang, and V. Salapura, “Sparkbench: a
comprehensive benchmarking suite for in memory data analytic platform Spark,” in Proceedings of the 12th ACM International Conference
on Computing Frontiers. ACM, 2015, p. 53.

[52] D. Balouek, A. Carpen Amarie, G. Charrier, F. Desprez, E. Jeannot,
E. Jeanvoine, A. Lébre, D. Margery, N. Niclausse, L. Nussbaum,
O. Richard, C. Pérez, F Quesnel, C. Rohr, and L. Sarzyniec, “Adding
virtualization capabilities to the Grid’5000 testbed,” in Cloud Computing and Services Science, ser. Communications in Computer and
Information Science, I. Ivanov, M. Sinderen, F, Leymann, and T. Shan,
Eds. Springer International Publishing, 2013, vol. 367, pp. 3-20.

[53] “Running Spark on YARN,” 2017, https://spark.apache.org/docs/latest/
running-on-yarn.html.