[1] R. Bellman. On a routing problem. Quarterly of Applied
Mathematics, 16(1):87-90, 1958.

[2] L. Bougé and J.-L. Levaire. Control structures for dataparallel SIMD languages: semantics and implementation. Future Generation Computer Systems, 8(4):363-378, 1992.

[3] J. Brodman, D. Babokin, I. Filippov, and P. Tu. Writing
scalable SIMD programs with ISPC. In WPMVP, pages 2532. ACM, 2014.

[4] G. Chen and X. Shen. Free launch: optimizing GPU dynamic
kernel launches through thread reuse. In Micro, pages 407-—
419. ACM, 2015.

[5] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein.
Introduction to Algorithms. McGraw-Hill, 2nd edition, 2001.

[6] B. Coutinho, D. Sampaio, F M. Q. Pereira, and W. M. Jr.
Divergence analysis and optimizations. In PACT, pages 320329. IEEE, 2011.

[7] B. Coutinho, D. Sampaio, F. M. Q. Pereira, and W. M. Jr.
Profiling divergences in GPU applications. Concurrency and
Computation: Practice and Experience, 1(10.1002/cpe.28515):1-15, 2012.

[8] J. DiMarco and M. Taufer. Performance impact of dynamic
parallelism on different clustering algorithms. Modeling and
Simulation for Defense Systems and Applications, 8752(VIM):
87520E-87520E:8, 2013.

[9] P. Erdos and A. Renyi. On random graphs. I. Publicationes
Mathematicae, 6(1):290-297, 1959.

[10] C. A. Farrell and D. H. Kieronska. Formal specification of
parallel SIMD execution. Theo. Comp. Science, 169(1):3965, 1996.

[11] J. Ferrante, K. J. Ottenstein, and J. D. Warren. The program
dependence graph and its use in optimization. TOPLAS, 9(3):
319-349, 1987.

[12] M. Garland and D. B. Kirk. Understanding throughputoriented architectures. Commun. ACM, 53:58-66, 2010.

[13] B. Gaster. An execution model for OpenCL 2.0. Technical
Report 2014-02, Computer Sciences, 2014.

[14] T. D. Han and T. S. Abdelrahman. Reducing divergence in
GPGPU programs with loop merging. In GPGPU, pages 1223. ACM, 2013.

[15] P. Hoogvorst, R. Keryell, N. Paris, and P. Matherat. POMP
or how to design a massively parallel machine with small
developments. In PARLE, pages 83-100. Springer, 1991.

[16] S. Jones. Introduction to dynamic parallelism — Invited Talk.
In GPU Technology Conference, pages 1-33. NVIDIA, 2014.

[17] F. Khorasani, R. Gupta, and L. N. Bhuyan. Efficient warp
execution in presence of divergence with collaborative context
collection. In Micro, pages 204-215. ACM, 2015.

[18] A. Klockner, N. Pinto, Y. Lee, B. Catanzaro, P. Ivanov, and
A. Fasih. Pycuda and pyopencl: A scripting-based approach
to gpu run-time code generation. Parallel Comput., 38(3):
157-174, Mar. 2012.

[19] D.E. Knuth, J. H. M. Jr., and V.R. Pratt. Fast pattern matching
in strings. Journal of Computing, 6(2):323-350, 1977.

[20] MasPar. MasPar Programming Language (ANSI C compatible MPL) Reference Manual, 1992.

[21] D. Merrill and A. Grimshaw. High performance and scalable
radix sorting: A case study of implementing dynamic parallelism for GPU computing. Parallel Proceessing Letters, 21
(2):245-272, 2011.

[22] A. Munshi, B. Gaster, T. G. Mattson, J. Fung, and D. Ginsburg. OpenCL Programming Guide. Addison-Wesley, 1st
edition, 2011.

[23] J. Nickolls and W. J. Dally. The GPU computing era. JEEE
Micro, 30:56-69, 2010.

[24] R. Novak. Loop optimization for divergence reduction on
GPUs with SIMT architecture. IEEE Transactions on Parallel
and Distributed Systems, 26(6):1633-1642, 2015.

[25] M. Pharr and W. R. Mark. ISPC: A SPMD compiler for highperformance CPU programming. In InPar, pages 1-13. IEEE,
2012.

[26] J. Rose and G. Steele. C*: An extended C language for data
parallel programming. In JCS, 1987.

[27] D. Sampaio, R. Martins, S. Collange, and F. M. Q. Pereira.
Divergence analysis with affine constraints. In SBAC-PAD,
pages 67-74. IEEE, 2012.

[28] D. Sampaio, R. M. de Souza, S. Collange, and F. M. Q.
Pereira. Divergence analysis. ACM Trans. Program. Lang.
Syst., 35(4):13, 2013.

[29] J. Sanders and E. Kandrot. CUDA by Example: An Introduction to General-Purpose GPU Programming. AddisonWesley, 1st edition, 2010.

[30] T. Schaub, S. Moll, R. Karrenberg, and S. Hack. The impact
of the simd width on control-flow and memory divergence.
TACO, 11(4):54:1-54:25, 2015.

[31] A. Sodani, R. Gramunt, J. Corbal, H.-S. Kim, K. Vinod,
S. Chinthamani, S. Hutsell, R. Agarwal, and Y.-C. Liu.
Knights Landing: Second-generation Intel Xeon Phi product.
IEEE Micro, 36(2):34—46, 2016.

[32] J. Wang and S. Yalamanchili. Characterization and analysis
of dynamic parallelism in unstructured GPU applications. In
IISWC, pages 51-60. IEEE, 2014.

[33] J. Wang, N. Rubin, A. Sidelnik, and S. Yalamanchili. Dynamic thread block launch: A lightweight execution mechanism to support irregular applications on gpus. In JSCA, pages
528-540. ACM, 2015.

[34] J. Wang, N. Rubin, A. Sidelnik, and S. Yalamanchili. LaPerm:
Locality aware scheduler for dynamic parallelism on GPUs.
In ISCA, 2016.

[35] J. Wu, A. Belevich, E. Bendersky, M. Heffernan, C. Leary,
J. Pienaar, B. Roune, R. Springer, X. Weng, and R. Hundt.
Gpucc: An open-source gpgpu compiler. In CGO, pages 105116. ACM, 2016.

[36] Y. Yang and H. Zhou. CUDA-NP: realizing nested threadlevel parallelism in GPGPU applications. In PPoPP, pages
93-106. ACM, 2014.

[37] E. Z. Zhang, Y. Jiang, Z. Guo, K. Tian, and X. Shen. On-thefly elimination of dynamic irregularities for GPU computing.
In ASPLOS, pages 369-380. ACM, 2011.
A. Artifact Evaluation
