[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Advances in neural information processing systems, 2012, pp. 1097–1105.
[2] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” CoRR, vol. abs/1409.1556, 2014.
[3] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies for accurate object detection and semantic segmentation,” in
Computer Vision and Pattern Recognition, 2014.
[4] O. Abdel-Hamid, A. r. Mohamed, H. Jiang, L. Deng, G. Penn, and D. Yu,
“Convolutional neural networks for speech recognition,” IEEE/ACM
Transactions on Audio, Speech, and Language Processing, vol. 22, no. 10,
pp. 1533–1545, Oct 2014.
[5] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by
jointly learning to align and translate,” CoRR, vol. abs/1409.0473, 2014.
[Online]. Available: http://arxiv.org/abs/1409.0473
[6] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE, vol. 86,
no. 11, pp. 2278–2324, Nov 1998.
[7] O. R. et al., “Imagenet large scale visual recognition
challenge,” CoRR, vol. abs/1409.0575, 2014. [Online]. Available:
http://arxiv.org/abs/1409.0575
[8] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, “The pascal visual object classes (voc) challenge,” International
Journal of Computer Vision, vol. 88, no. 2, pp. 303–338, Jun. 2010.
[9] R. Al-Rfou, G. Alain, A. Almahairi, and et al., “Theano:
A python framework for fast computation of mathematical
expressions,” CoRR, vol. abs/1605.02688, 2016. [Online]. Available:
http://arxiv.org/abs/1605.02688
[10] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for
fast feature embedding,” arXiv preprint arXiv:1408.5093, 2014.
[11] M. A. et al., “TensorFlow: Large-scale machine learning on heterogeneous
systems,” 2015, software available from tensorﬂow.org. [Online].
Available: http://tensorﬂow.org/
[12] R. Collobert, K. Kavukcuoglu, and C. Farabet, “Torch7: A matlab-like
environment for machine learning,” in BigLearn, NIPS Workshop, 2011.
[13] D. Y. et al., “An introduction to computational networks and the
computational network toolkit,” Tech. Rep., October 2014. [Online].
Available:
https://www.microsoft.com/en-us/research/publication/anintroduction-to-computational-networks-and-the-computational-networktoolkit/
[14] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran,
B. Catanzaro, and E. Shelhamer, “cudnn: Efﬁcient primitives for
deep learning,” CoRR, vol. abs/1410.0759, 2014. [Online]. Available:
http://arxiv.org/abs/1410.0759
[15] M. Mathieu, M. Henaff, and Y. LeCun, “Fast training of convolutional
networks through ffts,” CoRR, vol. abs/1312.5851, 2013. [Online].
Available: http://arxiv.org/abs/1312.5851

[16] N. Vasilache, J. Johnson, M. Mathieu, S. Chintala, S. Piantino, and
Y. LeCun, “Fast convolutional nets with fbfft: A GPU performance
evaluation,” CoRR, vol. abs/1412.7580, 2014. [Online]. Available:
http://arxiv.org/abs/1412.7580
[17] A. Lavin, “Fast algorithms for convolutional neural networks,” CoRR, vol. abs/1509.09308, 2015. [Online]. Available:
http://arxiv.org/abs/1509.09308
[18] O. Yadan, K. Adams, Y. Taigman, and M. Ranzato, “Multi-gpu training
of convnets,” CoRR, vol. abs/1312.5853, 2013. [Online]. Available:
http://arxiv.org/abs/1312.5853
[19] S. Bahrampour, N. Ramakrishnan, L. Schott, and M. Shah,
“Comparative study of caffe, neon, theano, and torch for deep
learning,” CoRR, vol. abs/1511.06435, 2015. [Online]. Available:
http://arxiv.org/abs/1511.06435
[20] S. Shi, Q. Wang, P. Xu, and X. Chu, “Benchmarking state-of-the-art deep
learning software tools,” CoRR, vol. abs/1608.07249, 2016. [Online].
Available: http://arxiv.org/abs/1608.07249
[21] Github. [Online]. Available: https://github.com
[22] cuda-convnet. [Online]. Available: https://code.google.com/p/cudaconvnet/
[23] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei,
“ImageNet Large Scale Visual Recognition Challenge,” International
Journal of Computer Vision (IJCV), vol. 115, no. 3, pp. 211–252, 2015.
[24] I. J. Goodfellow, D. Warde-Farley, P. Lamblin, V. Dumoulin, M. Mirza,
R. Pascanu, J. Bergstra, F. Bastien, and Y. Bengio, “Pylearn2: a
machine learning research library,” arXiv preprint arXiv:1308.4214,
2013. [Online]. Available: http://arxiv.org/abs/1308.4214
[25] F. Chollet, “Keras,” https://github.com/fchollet/keras, 2015.
[26] Lasagne
documentation.
[Online].
Available:
https://lasagne.readthedocs.io/
[27] M. B. et al., “End to end learning for self-driving
cars,” CoRR, vol. abs/1604.07316, 2016. [Online]. Available:
http://arxiv.org/abs/1604.07316
[28] V. e. a. Mnih, “Human-level control through deep reinforcement learning,”
Nature, vol. 518, no. 7540, pp. 529–533, Feb. 2015.
[29] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed, D. Anguelov,
D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with
convolutions,” CoRR, vol. abs/1409.4842, 2014. [Online]. Available:
http://arxiv.org/abs/1409.4842
[30] L. Bottou, “Online algorithms and stochastic approximations,” in
Online Learning and Neural Networks, D. Saad, Ed. Cambridge,
UK: Cambridge University Press, 1998, revised, oct 2012. [Online].
Available: http://leon.bottou.org/papers/bottou-98x
[31] C. Nvidia, “Cublas library,” NVIDIA Corporation, Santa Clara, California, vol. 15, p. 27, 2008.
[32] S. Winograd, Arithmetic complexity of computations.
Siam, 1980,
vol. 33.
[33] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, M. aurelio
Ranzato, A. Senior, P. Tucker, K. Yang, Q. V. Le, and A. Y. Ng, “Large
scale distributed deep networks,” in Advances in Neural Information
Processing Systems 25, F. Pereira, C. J. C. Burges, L. Bottou, and K. Q.
Weinberger, Eds. Curran Associates, Inc., 2012, pp. 1223–1231.
[34] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing
deep neural network with pruning, trained quantization and huffman
coding,” CoRR, vol. abs/1510.00149, 2015. [Online]. Available:
http://arxiv.org/abs/1510.00149
[35] F. Seide, H. Fu, J. Droppo, G. Li, and D. Yu, “1-bit stochastic gradient
descent and application to data-parallel distributed training of speech
dnns,” in Interspeech 2014, September 2014.
[36] S.
Chintala.
convnet-benchmarks.
[Online].
Available:
https://github.com/soumith/convnetbenchmark://github.com/soumith/convnet-benchmarks
[37] M. Harris et al., “Optimizing parallel reduction in cuda,” NVIDIA
Developer Technology, vol. 2, no. 4, 2007.
