[1] Top500 list. http://www.top500.org/lists/2015/11/.

[2] 1O-Watchdog. https://code.google.com/p/io-watchdog/.

[3] Bug occurs after 12 hours. https://github.com/open-mpi/ompi/issues/81/.

[4] Bug occurs after 200 iterations. https://github.com/open-mpi/ompi/issues/99.

[5] NAS parallel benchmarks. https://www.nas.nasa.gov/publications/npb.html.

[6] Probability theory and mathematical statistics: normal approximation to binomial. https://onlinecourses.science.psu.edu/stat414/node/179.

[7] HPL: a portable implementation of the high-performance Linpack
benchmark for distributed-memory computers. http://www.netlib.org/
benchmark/hpl/.

[8] Paradyn project: Dyninst. http://www.paradyn.org/html/manuals.html#dyninst

[9] Ohio Supercomputer Center’s charging policy. https://www.osc.edu/
supercomputing/software/general#charging

[10] San Diego Supercomputer Center’s charging policy. http://www.sdsc.edu/
support/user_guides/comet.html#charging

[11] D-H. Ahn, B.R. De Supinski, I Laguna, G_L. Lee, B. Liblit, B.P. Miller, and M. Schulz.
Scalable temporal order analysis for large scale debugging. In ACM/IEEE Conference on High Performance Computing Networking, Storage and Analysis (SC),
Article No. 44, Nov. 2009.

[12] D.C. Arnold, D-H. Ahn, B.R. de Supinski, G.L. Lee, B.P. Miller, and M. Schulz.
Stack trace analysis for large scale debugging. In IEEE International Parallel and
Distributed Processing Symposium (IPDPS), pages 1-10, March 2007.

[13] G. Bronevetsky, I Laguna, S. Bagchi, B. R. de Supinski, D. Ahn, and M. Schulz.
Automaded: Automata-based debugging for dissimilar parallel tasks. In IEEE/IFIP
International Conference on Dependable Systems and Networks (DSN), pages 231240, July 2010.

[14] Z. Chen, J. Dinan, Z. Tang, P. Balaji, H. Zhong, J. Wei, T. Huang, and F. Qin. Mcchecker: Detecting memory consistency errors in mpi one-sided applications. In
ACMAEEE International Conference for High Performance Computing, Networking,
Storage and Analysis (SC), pages 499-510, 2014.

[15] Z. Chen, Q. Gao, W. Zhang, and F Qin. Flowchecker: Detecting bugs in mpi
libraries via message flow checking. In ACM/IEEE International Conference for
High Performance Computing, Networking, Storage and Analysis (SC), 2010.

[16] Z. Chen, X. Li, J-Y. Chen, H. Zhong, and F. Qin. Syncchecker: Detecting synchronization errors between mpi applications and libraries. In IEEE International
Parallel and Distributed Processing Symposium (IPDPS), pages 342-353, May 2012.
[17] J. Coyle, J. Hoekstra, G. R. Luecke, Y. Zou, and M. Kraeva. Deadlock detection in
MPI programs. Concurrency and Computation: Practice and Experience, 14(11):911–
932, 2002.
[18] J. DeSouza, B. Kuhn, B. R. de Supinski, V. Samofalov, S. Zheltov, and S. Bratanov.
Automated, scalable debugging of mpi programs with intel&reg; message checker.
In SE-HPCS Workshop, pages 78–82, 2005.
[19] Q. Gao, F. Qin, and D. K. Panda. Dmtracker: Finding bugs in large-scale parallel
programs by detecting anomaly in data movements. In ACM/IEEE Conference on
Supercomputing (SC), Article No. 15, 2007.
[20] W. Haque. Concurrent deadlock detection in parallel programs. International
Journal of Computers and Applications, 28(1):19–25, Jan 2006.
[21] M. A Heroux and J. Dongarra. Toward a new metric for ranking high performance
computing systems. TR SAND2013-4744, June 2013.
[22] T. Hilbrich, B. R. de Supinski, W. E. Nagel, J. Protze, C. Baier, and M. S. Müller.
Distributed wait state tracking for runtime mpi deadlock detection. In ACM/IEEE
International Conference on High Performance Computing, Networking, Storage
and Analysis (SC), 2013.
[23] T. Hilbrich, B. R. de Supinski, Martin Schulz, and Matthias S. Müller. A graph
based approach for mpi deadlock detection. In ACM/IEEE International Conference
on Supercomputing (SC), pages 296–305, 2009.
[24] T. Hoefler and R. Belli. Scientific benchmarking of parallel computing systems:
twelve ways to tell the masses when reporting performance results. In ACM/IEEE
International Conference for High Performance Computing, Networking, Storage
and Analysis (SC), page 73. ACM, 2015.
[25] B. Krammer, K. Bidmon, M. S. Müller, and M. M. Resch. Marmot: An mpi analysis
and checking tool. In PARCO, pages 493–500, 2003.
[26] I. Laguna, T. Gamblin, B. R. de Supinski, S. Bagchi, G. Bronevetsky, D. H. Anh, M.
Schulz, B. Rountree. Large scale debugging of parallel tasks with AutomaDeD. In
ACM/IEEE International Conference for High Performance Computing, Networking,
Storage and Analysis (SC), pages 50:1–50:10, 2011.
[27] I. Laguna, D. H. Ahn, B. R. de Supinski, S. Bagchi, and T. Gamblin. Probabilistic
diagnosis of performance faults in large-scale parallel applications. In International Conference on Parallel Architectures and Compilation Techniques (PACT),
pages 213–222, 2012.
[28] I. Laguna, D. H. Ahn, B. R. de Supinski, S. Bagchi, and T. Gamblin. Diagnosis
of performance faults in large scale mpi applications via probabilistic progressdependence inference. In IEEE Transactions on Parallel and Distributed Systems
(TPDS), 26(5):1280–1289, 2015.
[29] I. Laguna, D. H. Ahn, B. R. de Supinski, T. Gamblin, G. L. Lee, M. Schulz, S. Bagchi,
M. Kulkarni, B. Zhou, Z. Chen, et al. Debugging high-performance computing
applications at massive scales. CACM, 58(9):72–81, 2015.
[30] A.V. Mirgorodskiy, N. Maruyama, and B.P. Miller. Problem diagnosis in largescale computing environments. In ACM/IEEE Supercomputing Conference (SC),
Nov 2006.
[31] S. Mitra, I. Laguna, D. H. Ahn, S. Bagchi, M. Schulz, and T. Gamblin. Accurate
application progress analysis for large-scale parallel debugging. In ACM SIGPLAN
Conference on Programming Language Design and Implementation (PLDI), pages
193–203, 2014.
[32] P. Ohly and W. Krotz-Vogel. Automated mpi correctness checking: What if there
was a magic option? In LCI HPCC, 2007.
[33] F. Petrini, D.J. Kerbyson, and S. Pakin. The case of the missing supercomputer
performance: Achieving optimal performance on the 8,192 processors of ASCI
Q. In ACM/IEEE Conference on Supercomputing (SC), page 55. ACM, 2003.
[34] X. Song, H. Chen, and B. Zang. Why software hangs and what can be done with
it. In IEEE/IFIP International Conference on Dependable Systems and Networks
(DSN), June 2010.
[35] F. S. Swed and C. Eisenhart. Tables for testing randomness of grouping in a
sequence of alternatives. The Annals of Mathematical Statistics, 14(1):66–87, 03
1943.
[36] M. Snir, R. W. Wisniewski, J. A. Abraham, S. V. Adve, S. Bagchi, P. Balaji, J. Belak,
P. Bose, F. Cappello, B. Carlson, et al. Addressing failures in exascale computing.
International Journal of High Performance Computing Applications (IJHPCA), page
1094342014522573, 2014.
[37] S. S. Vakkalanka, S. Sharma, G. Gopalakrishnan, and R. M. Kirby. Isp: A tool for
model checking mpi programs. In ACM SIGPLAN Symposium on Principles and
Practice of Parallel Programming (PPoPP), 2008.
[38] J. S. Vetter and B. R. de Supinski. Dynamic software testing of mpi applications
with umpire. In ACM/IEEE Conference on Supercomputing (SC), Article No. 51,
2000.
[39] B. Zhou, M. Kulkarni, and S. Bagchi. Vrisha: using scaling properties of parallel
programs for bug detection and localization. In International Symposium on
High-Performance Distributed Computing (HPDC), pages 85–96. ACM, 2011.
[40] B. Zhou, J. Too, M. Kulkarni, and S. Bagchi. Wukong: automatically detecting and
localizing bugs that manifest at large system scales. In International Symposium
on High-performance Parallel and Distributed Computing (HPDC), pages 131–142.
ACM, 2013.
