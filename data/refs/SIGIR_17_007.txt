[1] Nima Asadi, Donald Metzler, Tamer Elsayed, and Jimmy Lin. 2011. Pseudo test collections for learning web search ranking functions. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. ACM, 1073–1082.
[2] Pierre Baldi. 2012. Autoencoders, unsupervised learning, and deep architectures. ICML unsupervised and transfer learning 27 (2012), 37–50.
[3] Lidong Bing, Sneha Chaudhari, Richard C Wang, and William W Cohen. 2015. Improving Distant Supervision for Information Extraction Using Label Propagation Through Lists. In EMNLP ’15. 524–529.
[4] Alexey Borisov, Ilya Markov, Maarten de Rijke, and Pavel Serdyukov. 2016. A Neural Click Model for Web Search. In WWW ’16. 531–541.
[5] Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard Sa ̈ckinger, and Roopak Shah. 1993. Signature Verification Using a ”Siamese” Time Delay Neural Network. In NIPS ’93. 737–744.
[6] Daniel Cohen and W. Bruce Croft. 2016. End to End Long Short Term Memory Networks for Non-Factoid Question Answering. In ICTIR ’16. 143–146.
[7] Gordon V. Cormack, Mark D. Smucker, and Charles L. Clarke. 2011. Efficient and Effective Spam Filtering and Re-ranking for Large Web Datasets. Inf. Retr. 14, 5 (2011), 441–465.
[8] Thomas Desautels, Andreas Krause, and Joel W Burdick. 2014. Parallelizing exploration-exploitation tradeoffs in Gaussian process bandit optimization. Journal of Machine Learning Research 15, 1 (2014), 3873–3923.
[9] Fernando Diaz. 2016. Learning to Rank with Labeled Features. In ICTIR ’16. 41–44.
[10] Fernando Diaz, Bhaskar Mitra, and Nick Craswell. 2016. Query Expansion with
Locally-Trained Word Embeddings. In ACL ’16.
[11] Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol,
Pascal Vincent, and Samy Bengio. 2010. Why does unsupervised pre-training
help deep learning? Journal of Machine Learning Research 11 (2010), 625–660.
[12] Mart ́ın Abadi et al. 2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. (2015). http://tensorflow.org/ Software available from
tensorflow.org.
[13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial
nets. In NIPS. 2672–2680.
[14] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Croft. 2016. A Deep Relevance
Matching Model for Ad-hoc Retrieval. In CIKM ’16. 55–64.
[15] Xianpei Han and Le Sun. 2016. Global Distant Supervision for Relation Extraction.
In AAAI’16. 2950–2956.
[16] Ralf Herbrich, Thore Graepel, and Klaus Obermayer. 1999. Support Vector Learning for Ordinal Regression. In ICANN ’99. 97–102.
[17] Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-based Weak Supervision for Information Extraction of Overlapping Relations. In ACL ’11. 541–550.
[18] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning Deep Structured Semantic Models for Web Search Using Clickthrough Data. In CIKM ’13. 2333–2338.
[19] Kalervo Ja ̈rvelin and Jaana Keka ̈la ̈inen. 2002. Cumulated Gain-based Evaluation of IR Techniques. ACM Trans. Inf. Syst. 20, 4 (Oct. 2002), 422–446.
[20] Thorsten Joachims. 2002. Optimizing Search Engines Using Clickthrough Data. In KDD ’02. 133–142.
[21] Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014).
[22] Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. In NIPS ’15. 3294–3302.
[23] Quoc V Le and Tomas Mikolov. 2014. Distributed Representations of Sentences and Documents. In ICML ’14, Vol. 14. 1188–1196.
[24] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep Learning. Nature 521, 7553 (2015), 436–444.
[25] Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-yi Wang.
2015. Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval. In NAACL ’15. 912–921.
[26] Zhengdong Lu and Hang Li. 2013. A Deep Architecture for Matching Short Texts. In NIPS ’13. 1367–1375.
[27] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed Representations of Words and Phrases and their Compositionality. In NIPS ’13. 3111–3119.
[28] Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to Match Using Local and Distributed Representations of Text for Web Search. In WWW ’17. 1291–1299.
[29] Kezban Dilek Onal, Ismail Sengor Altingovde, Pinar Karagoz, and Maarten de Rijke. 2016. Getting Started with Neural Models for Semantic Matching in Web Search. arXiv preprint arXiv:1611.03305 (2016).
[30] Greg Pass, Abdur Chowdhury, and Cayley Torgeson. 2006. A Picture of Search. In InfoScale ’06.
[31] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global Vectors for Word Representation. In EMNLP ’14. 1532–1543.
[32] Navid Rekabsaz, Mihai Lupu, Allan Hanbury, and Hamed Zamani. 2017. Word Embedding Causes Topic Shifting; Exploit Global Context!. In SIGIR’17.
[33] Navid Rekabsaz, Mihai Lupu, Allan Hanbury, and Guido Zuccon. 2016. General- izing translation models in the probabilistic relevance framework. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management. ACM, 711–720.
[34] Aliaksei Severyn and Alessandro Moschitti. 2015. Twitter Sentiment Analysis with Deep Convolutional Neural Networks. In SIGIR ’15. 959–962.
[35] Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Gre ́goire Mesnil. 2014. Learning Semantic Representations Using Convolutional Neural Networks for Web Search. In WWW ’14. 373–374.
[36] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. J. Mach. Learn. Res. 15, 1 (2014), 1929–1958.
[37] Yuan Tang. 2016. TF.Learn: TensorFlow’s High-level Module for Distributed Machine Learning. arXiv preprint arXiv:1612.04251 (2016).
[38] Christophe Van Gysel, Maarten de Rijke, and Evangelos Kanoulas. 2016. Learning latent vector spaces for product search. In CIKM’16. 165–174.
[39] Christophe Van Gysel, Maarten de Rijke, and Marcel Worring. 2016. Unsupervised, efficient and semantic expertise retrieval. In WWW’16. 1069–1079.
[40] Fabian L. Wauthier, Michael I. Jordan, and Nebojsa Jojic. 2013. Efficient Ranking from Pairwise Comparisons. In ICML’13. 109–117.
[41] Liu Yang, Qingyao Ai, Jiafeng Guo, and W. Bruce Croft. 2016. aNMM: Ranking Short Answer Texts with Attention-Based Neural Matching Model. In CIKM ’16. 287–296.
[42] Hamed Zamani, Michael Bendersky, Xuanhui Wang, and Mingyang Zhang. 2017.
Situational Context for Ranking in Personal Search. In WWW ’17. 1531–1540.
[43] Hamed Zamani and W. Bruce Croft. 2016. Embedding-based Query Language
Models. In ICTIR ’16. 147–156.
[44] Hamed Zamani and W. Bruce Croft. 2016. Estimating Embedding Vectors for
Queries. In ICTIR ’16. 123–132.
[45] Hamed Zamani and W. Bruce Croft. 2017. Relevance-based Word Embedding.
In SIGIR ’17.
[46] Ye Zhang, Md Mustafizur Rahman, Alex Braylan, Brandon Dang, Heng-Lu Chang,
Henna Kim, Quinten McNamara, Aaron Angert, Edward Banner, Vivek Khetan, and others. 2016. Neural Information Retrieval: A Literature Review. arXiv preprint arXiv:1611.06792 (2016).
[47] Guoqing Zheng and Jamie Callan. 2015. Learning to Reweight Terms with Distributed Representations. In SIGIR ’15. 575–584.
