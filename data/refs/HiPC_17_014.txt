[1] F. Gaud, B. Lepers, J. Funston, M. Dashti, A. Fedorova,
V. Quéma, R. Lachaize, and M. Roth, “Challenges of memory
management on modern numa systems,” Queue, vol. 13, no. 8,
p. 70, 2015.
[2] D. Molka, D. Hackenberg, and R. Schöne, “Main memory
and cache performance of intel sandy bridge and amd
bulldozer,” in Proceedings of the Workshop on Memory
Systems Performance and Correctness, ser. MSPC ’14.
New York, NY, USA: ACM, 2014, pp. 4:1–4:10. [Online].
Available: http://doi.acm.org/10.1145/2618128.2618129
[3] D. Ziakas, A. Baum, R. A. Maddox, and R. J. Safranek,
“Intel quickpath interconnect architectural features supporting
scalable system architectures,” in 2010 18th IEEE Symposium
on High Performance Interconnects, Aug 2010, pp. 1–6.

[4] F. Broquedis, J. Clet-Ortega, S. Moreaud, N. Furmento,
B. Goglin, G. Mercier, S. Thibault, and R. Namyst, “hwloc:
A generic framework for managing hardware afﬁnities in hpc
applications,” in 2010 18th Euromicro Conference on Parallel,
Distributed and Network-based Processing, Feb 2010, pp.
180–186.

[5] M. P. I. Forum, “MPI: A Message-Passing Interface Standard,” http://www.mpi-forum.org, Sept. 2012.
[6] F. Cappello and D. Etiemble, “Mpi versus mpi+openmp on
the ibm sp for the nas benchmarks,” in Supercomputing,
ACM/IEEE 2000 Conference, Nov 2000, pp. 12–12.

[7] E. Jeannot, G. Mercier, and F. Tessier, “Process placement
in multicore clusters:algorithmic issues and practical techniques,” IEEE Transactions on Parallel and Distributed Systems, vol. 25, no. 4, pp. 993–1002, April 2014.

[8] H. Chen, W. Chen, J. Huang, B. Robert, and H. Kuhn,
“Mpipp: an automatic proﬁle-guided parallel process placement toolset for smp clusters and multiclusters,” in Proceedings of the 20th annual international conference on
Supercomputing. ACM, 2006, pp. 353–360.

[9] J. Zhang, J. Zhai, W. Chen, and W. Zheng, “Process mapping
for mpi collective communications,” in European Conference
on Parallel Processing. Springer, 2009, pp. 81–92.

[10] M. Dashti, A. Fedorova, J. Funston, F. Gaud, R. Lachaize,
B. Lepers, V. Quema, and M. Roth, “Trafﬁc management: a
holistic approach to memory placement on numa systems,”
in ACM SIGPLAN Notices, vol. 48, no. 4. ACM, 2013, pp.
381–394.

[11] X. Liu and J. Mellor-Crummey, “A tool to analyze the performance of multithreaded programs on numa architectures,”
ACM Sigplan Notices, vol. 49, no. 8, pp. 259–272, 2014.

[12] C. Ma, Y. M. Teo, V. March, N. Xiong, I. R. Pop, Y. X.
He, and S. See, “An approach for matching communication
patterns in parallel applications,” in 2009 IEEE International
Symposium on Parallel Distributed Processing, May 2009,
pp. 1–12.

[13] A. Faraj and X. Yuan, “Communication characteristics in the
nas parallel benchmarks.” in IASTED PDCS, 2002, pp. 724–
729.

[14] I. Lee, “Characterizing communication patterns of nas-mpi
benchmark programs,” in Southeastcon, 2009. SOUTHEASTCON’09. IEEE. IEEE, 2009, pp. 158–163.

[15] Intel xeon platinum 8180 processor. https://ark.intel.com/,
visited on October 12, 2017.
[16] E. Gabriel, G. E. Fagg, G. Bosilca, T. Angskun, J. J. Dongarra,
J. M. Squyres, V. Sahay, P. Kambadur, B. Barrett, A. Lumsdaine et al., “Open mpi: Goals, concept, and design of a next
generation mpi implementation,” in European Parallel Virtual
Machine/Message Passing Interface Users Group Meeting.
Springer, 2004, pp. 97–104.

[17] G. Bosilca, C. Foyer, E. Jeannot, G. Mercier, and G. Papauré,
“Online dynamic monitoring of mpi communications,” in
European Conference on Parallel Processing.
Springer,
2017, pp. 49–62.

[18] C. Elkan, “Using the triangle inequality to accelerate kmeans,” in ICML, vol. 3, 2003, pp. 147–153.


[19] F. Declerq, “Choropleth map accuracy and the number of
class intervals,” in Proceedings of the 17th Conference and
the 10th General Assembly of the International Cartographic
Association, vol. 1.
Institute Cartogràﬁc de Catalunya
Barcelona, 1995, pp. 918–22.
[20] D. H. Bailey, E. Barszcz, J. T. Barton, D. S. Browning,
R. L. Carter, L. Dagum, R. A. Fatoohi, P. O. Frederickson,
T. A. Lasinski, R. S. Schreiber et al., “The nas parallel
benchmarks,” The International Journal of Supercomputing
Applications, vol. 5, no. 3, pp. 63–73, 1991.
[21] (2015) Perf. linux proﬁling with performance counters.
[Online]. Available: https://perf.wiki.kernel.org/
[22] D. L. Hill, D. Bachand, S. Bilgin, R. Greiner, P. Hammarlund,
T. Huff, S. Kulick, and R. Safranek, “The uncore: A modular approach to feeding the high-performance cores.” Intel
Technology Journal, vol. 14, no. 3, 2010.
[23] K. Fujiwara and H. Casanova, “Speed and accuracy of network simulation in the simgrid framework,” in Proceedings of
the 2nd international conference on Performance evaluation
methodologies and tools.
ICST (Institute for Computer
Sciences, Social-Informatics and Telecommunications Engineering), 2007, p. 12.
[24] A. Degomme, A. Legrand, G. Markomanolis, M. Quinson,
M. Stillwell, and F. Suter, “Simulating mpi applications:
the smpi approach,” IEEE Transactions on Parallel and
Distributed Systems, 2017.
[25] V. Viswanathan et al., “Intel memory latency checker v3.0,”
URL: https://software.intel.com/en-us/articles/intelr-memorylatency-checker, Mar. 2016.
[26] B. Hendrickson and R. Leland, “The chaco users guide: Version 2.0,” Technical Report SAND95-2344, Sandia National
Laboratories, Tech. Rep., 1995.

[27] T. Ma, T. Herault, G. Bosilca, and J. J. Dongarra, “Process
distance-aware adaptive mpi collective communications,” in
2011 IEEE International Conference on Cluster Computing,
Sept 2011, pp. 196–204.

[28] A. Bhatelé, L. V. Kalé, and S. Kumar, “Dynamic topology
aware load balancing algorithms for molecular dynamics
applications,” in Proceedings of the 23rd international conference on Supercomputing. ACM, 2009, pp. 110–116.
[29] L. L. Pilla, C. P. Ribeiro, D. Cordeiro, C. Mei, A. Bhatele,
P. O. A. Navaux, F. Broquedis, J. F. Mhaut, and L. V. Kale,
“A hierarchical approach for load balancing on parallel multicore systems,” in 2012 41st International Conference on
Parallel Processing, Sept 2012, pp. 118–127.

[30] L. L. Pilla, P. O. A. Navaux, C. P. Ribeiro, P. Coucheney,
F. Broquedis, B. Gaujal, and J. F. Mhaut, “Asymptotically
optimal load balancing for hierarchical multi-core systems,”
in 2012 IEEE 18th International Conference on Parallel and
Distributed Systems, Dec 2012, pp. 236–243.



[31] E. Jeannot, E. Meneses, G. Mercier, F. Tessier, and G. Zheng,
“Communication and topology-aware load balancing in
charm++ with treematch,” in 2013 IEEE International Conference on Cluster Computing (CLUSTER), Sept 2013, pp.
1–8.