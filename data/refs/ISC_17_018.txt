[1] Mellanox Technologies. http://www.mellanox.com

[2] Quadrics Supercomputers World Ltd. http://www.quadrics.com/

[3] Barrett, B.W., Brightwell, R., Hemmert, K.S., Wheeler, K.B., Underwood, K.D.:

Using triggered operations to oﬄoad rendezvous messages. In: Cotronis, Y.,

Danalis, A., Nikolopoulos, D.S., Dongarra, J. (eds.) EuroMPI 2011. LNCS, vol.

6960, pp. 120–129. Springer, Heidelberg (2011). doi:10.1007/978-3-642-24449-0 15

[4] Boden, N.J., Cohen, D., Felderman, R.E., Kulawik, A.E., Seitz, C.L., Seizovic,

J.N., Su, W.: Myrinet: a gigabit-per-second local area network. IEEE Micro 15,

29–36 (1995)

[5] Brightwell, R., Underwood, K.: Evaluation of an eager protocol optimization

for MPI. In: Dongarra, J., Laforenza, D., Orlando, S. (eds.) EuroPVM/MPI

2003. LNCS, vol. 2840, pp. 327–334. Springer, Heidelberg (2003). doi:10.1007/

978-3-540-39924-7 46

[6] Brunet, É., Trahay, F., Denis, A., Namyst, R.: A Sampling-based approach for

communication libraries auto-tuning. In: 2011 IEEE International Conference on

Cluster Computing (CLUSTER), pp. 299–307. IEEE (2011)

[7] Case, D.A., Darden, T.A., Cheatham, T.E., Simmerling, C.L., Wang, J., Duke, R.E.,

Luo, R., Walker, R.C., Zhang, W., Merz, K.M., Roberts, B.P., Wang, B., Hayik,

S., Roitberg, A., Seabra, G., Kolossváry, I., Wong, K.F., Paesani, F., Vanicek, J.,

Wu, X., Brozell, S.R., Steinbrecher, T., Gohlke, H., Cai, Q., Ye, X., Wang, J., Hsieh,

M.-J., Cui, G., Roe, D.R., Mathews, D.H., Seetin, M.G., Sagui, C., Babin, V., Luchko,

T., Gusarov, S., Kovalenko, A., Kollman, P.A.: Amber 2016, University of California,

San Francisco (2016)

[8] Open MPI: Open Source High Performance Computing. http://www.open-mpi.

org

[9] Cui, Y., Moore, R., Olsen, K., Chourasia, A., Maechling, P., Minster, B., Day, S.,

Hu, Y., Zhu, J., Majumdar, A., Jordan, T.: Toward petascale earthquake simulations. Acta Geotech. 4, 79–93 (2008). Springer

[10]  Derradji, S., Palfer-Sollier, T., Panziera, J.P., Poudes, A., Atos, F.W.: The BXI

interconnect architecture. In: 2015 IEEE 23rd Annual Symposium on HighPerformance Interconnects, pp. 18–25, August 2015

[11]  Gropp, W., Lusk, E., Doss, N., Skjellum, A.: A high-performance, portable implementation of the MPI, message passing interface standard. Technical report,

Argonne National Laboratory and Mississippi State University

[12]  Heroux, M.A., Doerﬂer, D.W., Crozier, P.S., Willenbring, J.M., Edwards, H.C.,

Williams, A., Rajan, M., Keiter, E.R., Thornquist, H.K., Numrich, R.W.: Improving performance via mini-applications. Technical report SAND2009-5574, Sandia

National Laboratories (2009)

[13]  Islam, T., Mohror, K., Schulz, M.: Exploring the capabilities of the new MPI T

interface. In: Proceedings of the 21st European MPI Users’ Group Meeting, p. 91.

ACM (2014)

[14]  Liu, J., Jiang, W., Wyckoﬀ, P., Panda, D.K., Ashton, D., Buntinas, D., Gropp, W.,

Toonen, B.: Design and implementation of MPICH2 over InﬁniBand with RDMA

support. In: Proceedings of Int’l Parallel and Distributed Processing Symposium

(IPDPS 2004), April 2004

[15]  Lu, Y., Yang, C., Du, Y.: HPCG on Tianhe2

[16]  Message Passing Interface Forum. MPI: A Message-Passing Interface Standard,

March 1994

[17]  Miceli, R., et al.: AutoTune: a plugin-driven approach to the automatic tuning of

parallel applications. In: Manninen, P., Öster, P. (eds.) PARA 2012. LNCS, vol.

7782, pp. 328–342. Springer, Heidelberg (2013). doi:10.1007/978-3-642-36803-5 24

[18]  MPI-3 Standard Document. http://www.mpi-forum.org/docs/mpi-3.0/mpi30report.pdf

[19]  Pimenta, A., Cesar, E., Sikora, A.: Methodology for MPI applications autotuning.

In: Proceedings of the 20th European MPI Users’ Group Meeting, pp. 145–146.

ACM (2013)

[20]  Portals Network Programming Interface. http://www.cs.sandia.gov/Portals/

[21]  San Diego Supercomputing Center. Gordon Supercomputer. http://www.sdsc.edu/

services/hpc/hpc systems.html#gordon

[22]  Shipman, G.M., Woodall, T.S., Graham, R.L., Maccabe, A.B., Bridges, P.G.:

InﬁniBand scalability in open MPI. In: Proceedings of the 20th International Conference on Parallel and Distributed Processing, IPDPS 2006, p. 100. IEEE Computer Society, Washington, DC (2006)

[23]  Sikora, A., César, E., Comprés, I., Gerndt, M.: Autotuning of MPI applications

using PTF. In: Proceedings of the ACM Workshop on Software Engineering Methods for Parallel and High Performance Applications, pp. 31–38. ACM (2016)

[24]  Sur, S., Chai, L., Jin, H., Panda, D.K.: Shared receive queue based scalable MPI

design for InﬁniBand clusters. In: Proceedings of the 20th International Conference on Parallel and Distributed Processing, IPDPS 2006, p. 101. IEEE Computer

Society, Washington, DC (2006)

[25]  Texas Advanced Computing Center. Stampede Supercomputer. http://www.tacc.

utexas.edu/

[26]  The MIMD Lattice Computation (MILC) Collaboration. http://physics.indiana.

edu/∼sg/milc.html

[27]  Wu, J., Liu, J., Wyckoﬀ, P., Panda, D.: Impact of on-demand connection management in MPI over via. In: Proceedings of the 2002 IEEE International Conference

on Cluster Computing, pp. 152–159 (2002)

[28]  Yu, W., Gao, Q., Panda, D.K.: Adaptive connection management for scalable

MPI over InﬁniBand. In: Proceedings 20th IEEE International Parallel Distributed Processing Symposium, p. 10, April 2006
