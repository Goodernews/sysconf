[1] J. Dongarra, P. Beckman et al., “The International Exascale Software Project Roadmap,” International Journal of High Performance Comput- ing Applications, 2011.
[2] K. B. Wheeler, R. C. Murphy, and D. Thain, “Qthreads: An API for pro- gramming with millions of lightweight threads,” in IEEE International Symposium on Parallel and Distributed Processing (IPDPS), 2008.
[3] S. Seo, A. Amer, P. Balaji, P. Beckman et al., “Argobots: A light- weight low-level threading/tasking framework,” https://collab.cels.anl. gov/display/ARGOBOTS/.
[4] S. Pronk, S. Pall, R. Schulz, P. Larsson et al., “GROMACS 4.5: A high-throughput and highly parallel open source molecular simulation toolkit,” Bioinformatics, 2013.
[5] M. Krone, J. E. Stone, T. Ertl, and K. Schulten, “Fast visualization of gaussian density surfaces for molecular dynamics and particle system trajectories,” in EuroVis Short Papers, 2012.
[6] M.DreherandB.Raffin,“Aflexibleframeworkforasynchronousinsitu and in transit analytics for scientific simulations,” in IEEE/ACM Interna- tional Symposium on Cluster, Cloud and Grid Computing (CLUSTER), 2014.
[7] “Docker – build, ship, and run any app, anywhere,” https://www.docker. com/.
[8] “APPC app container specification,” https://github.com/appc/spec.
[9] “Linux control groups,” https://www.kernel.org/doc/Documentation/
cgroup- v1/cgroups.txt.
[10] B. Van Essen, H. Hsieh, S. Ames, R. Pearce, and M. Gokhale, “DI-
MMAP: A scalable memory map runtime for out-of-core data-intensive
applications,” Cluster Computing, 2015.
[11] M. Jiang, B. Van Essen, C. Harrison, and M. Gokhale, “Multi-threaded
streamline tracing for data-intensive architectures,” in IEEE Symposium
on Large Data Analysis and Visualization (LDAV), 2014.
[12] R. Gioiosa, F. Petrini, K. Davis, and F. Lebaillif-Delamare, “Analysis of system overhead on parallel computers,” in IEEE Int. Symp. on Signal
Processing and Information Technology (ISSPIT), 2004.
[13] A. Morari, R. Gioiosa, R. W. Wisniewski et al., “A quantitative analysis of OS noise,” in IEEE International Parallel and Distributed Processing
Symposium (IPDPS), 2011.
[14] ——, “Evaluating the impact of TLB misses on future HPC systems,”
in IEEE International Parallel and Distributed Processing Symposium
(IPDPS), 2012.
[15] S. Rostedt, “Finding origins of latencies using ftrace,” in Real Time
Linux Workshop (RTLWS), 2009.
[16] A.B.Maccabe,K.S.McCurley,R.Riesen,andS.R.Wheat,“SUNMOS
for the Intel Paragon—A brief user’s guide,” in Intel Supercomputing
Users Group Meeting, 1994.
[17] R. Riesen, R. Brightwell, P. G. Bridges et al., “Designing and imple-
menting lightweight kernels for capability computing,” Concurrency and
Computation: Practice and Experience, 2009.
[18] J. A. Colmenares, S. Bird, H. Cook, P. Pearce, D. Zhu, J. Shalf,
S. Hofmeyr, K. Asanovic, and J. Kubiatowicz, “Resource management in the Tessellation manycore OS,” in USENIX Conference on Hot Topics in Parallelism, (HotPAR), 2010.
[19] B. Rhoden, K. Klues, D. Zhu, and E. Brewer, “Improving per-node effi- ciency in the datacenter with new OS abstractions,” in ACM Symposium on Cloud Computing (SOCC), 2011.
[20] M. Giampapa, T. Gooding, T. Inglett, and R. W. Wisniewski, “Experi- ences with a lightweight supercomputer kernel: Lessons learned from Blue Gene’s CNK,” in ACM/IEEE Conference on Supercomputing (SC), 2010.
[21] Y. Park, E. Van Hensbergen, M. Hillenbrand et al., “FusedOS: Fusing LWK performance with FWK functionality in a heterogeneous environ- ment,” in IEEE International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD), 2012.
[22] B. Gerofi, A. Shimada, A. Hori, and Y. Ishikawa, “Partially separated page tables for efficient operating system assisted hierarchical memory management on heterogeneous architectures,” in IEEE/ACM Interna- tional Symposium on Cluster, Cloud and Grid Computing (CCGrid), 2013.
[23] R. W. Wisniewski, T. Inglett, P. Keppel, R. Murty, and R. Riesen, “mOS: An architecture for extreme-scale operating systems,” in Interna- tional Workshop on Runtime and Operating Systems for Supercomputers (ROSS), 2014.
[24] E. Shmueli, G. Alma ́si, J. Brunheroto, J. Castan ̃os, G. Do ́zsa, S. Kumar, and D. Lieber, “Evaluating the effect of replacing CNK with Linux on the compute-nodes of Blue Gene/L,” in ACM International Conference on Supercomputing (ICS), 2008.
[25] K. Yoshii, K. Iskra, H. Naik, P. Beckman, and P. C. Broekema, “Performance and scalability evaluation of “Big Memory” on Blue Gene Linux,” International Journal of High Performance Computing Applications, 2011.
[26] B. Kocoloski and J. Lange, “HPMMAP: Lightweight memory manage- ment for commodity operating systems,” in IEEE International Parallel and Distributed Processing Symposium (IPDPS), 2014.
[27] P. Beckman, K. Iskra, K. Yoshii, and S. Coghlan, “Operating system issues for petascale systems,” ACM SIGOPS Operating Systems Review, 2006.
[28] A. Nataraj, A. Morris, A. D. Malony, M. Sottile, and P. Beckman, “The ghost in the machine: Observing the effects of kernel operation on parallel application performance,” in ACM/IEEE Conference on Supercomputing (SC), 2007.
[29] K. B. Ferreira, P. Bridges, and R. Brightwell, “Characterizing application sensitivity to OS interference using kernel-level noise injection,” in ACM/IEEE Conference on Supercomputing (SC), 2008.
[30] F. Petrini, D. J. Kerbyson, and S. Pakin, “The case of the missing su- percomputer performance: Achieving optimal performance on the 8,192 processors of ASCI Q,” in ACM/IEEE Conference on Supercomputing (SC), 2003.
[31] P. Beckman, K. Iskra, K. Yoshii, S. Coghlan, and A. Nataraj, “Bench- marking the effects of operating system interference on extreme-scale parallel machines,” Cluster Computing, 2008.
[32] R. Gioiosa, S. A. McKee, and M. Valero, “Designing OS for HPC applications: Scheduling,” in IEEE International Conference on Cluster Computing (CLUSTER), 2010.
[33] D. Wallace, “Compute Node Linux: Overview, progress to date & roadmap,” in Cray User Group Annual Technical Conference, 2007.
[34] J. Lange, K. Pedretti, T. Hudson, P. Dinda et al., “Palacios and Kitten: New high performance operating systems for scalable virtualized and native supercomputing,” in IEEE International Symposium on Parallel and Distributed Processing (IPDPS), 2010.
[35] M. G. Xavier, M. V. Neves, F. D. Rossi, T. C. Ferreto, T. Lange, and C. A. F. De Rose, “Performance evaluation of container-based virtual- ization for high performance computing environments,” in Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP), 2013.
[36] D. Beserra, E. D. Moreno, P. T. Endo, J. Barreto, D. Sadok, and S. Fernandes, “Performance analysis of LXC for HPC environments,” in International Conference on Complex, Intelligent, and Software Intensive Systems (CISIS), 2015.
[37] D. M. Jacobsen and R. S. Canon, “Contain this, unleashing Docker for HPC,” in Cray User Group Annual Technical Conference, 2015.
[38] “rkt – a security-minded, standards-based container engine,” https: //coreos.com/rkt/.
[39] H. Yu, C. Wang, R. Grout, J. Chen, and K.-L. Ma, “In situ visualization for large-scale combustion simulations,” IEEE Computer Graphics and Applications, 2010.
[40] B. Whitlock, J. M. Favre, and J. S. Meredith, “Parallel in situ coupling of simulation with a fully featured visualization system,” in Eurographics Conference on Parallel Graphics and Visualization (EGPGV), 2011.
[41] M. Dorier, G. Antoniu, F. Cappello, M. Snir, and L. Orf, “Damaris: How to efficiently leverage multicore parallelism to achieve scalable, jitter-free I/O,” in IEEE International Conference on Cluster Computing (CLUSTER), 2012.
[42] H. Abbasi, G. Eisenhauer, M. Wolf, K. Schwan, and S. Klasky, “Just in Time: Adding value to the IO pipelines of high performance applications with JITStaging,” in International Symposium on High Performance Distributed Computing (HPDC), 2011.
[43] F. Zheng, H. Zou, G. Eisenhauer, K. Schwan, M. Wolf et al., “FlexIO: I/O middleware for location-flexible scientific data analytics,” in IEEE International Symposium on Parallel Distributed Processing (IPDPS), 2013.
