[1] N. A. C. Cressie and C. K. Wikle, Statistics for spatio-temporal data, ser.
Wiley series in probability and statistics. Hoboken, N.J. Wiley, 2011.
[2] M. T. Bahadori, Q. R. Yu, and Y. Liu, “Fast multivariate spatio-temporal
analysis via low rank tensor learning,” in Advances in Neural Information
Processing Systems, 2014, pp. 3491–3499.
[3] H. Koppula and A. Saxena, “Learning spatio-temporal structure from rgbd videos for human activity detection and anticipation,” in Proceedings
of ICML, 2013.
[4] Y. Bengio, “Neural net language models,” Scholarpedia, vol. 3, no. 1, p.
3881, 2008.
[5] J. Chung, K. Kastner, L. Dinh, K. Goel, A. C. Courville, and Y. Bengio,
“A recurrent latent variable model for sequential data,” in Advances in
neural information processing systems, 2015, pp. 2962–2970.
[6] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel, “Gated graph sequence
neural networks,” arXiv preprint arXiv:1511.05493, 2015.
[7] X. SHI, Z. Chen, H. Wang, D.-Y. Yeung, W.-k. Wong, and W.-c.
WOO, “Convolutional lstm network: A machine learning approach for
precipitation nowcasting,” in Advances in Neural Information Processing
Systems 28, C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and
R. Garnett, Eds. Curran Associates, Inc., 2015, pp. 802–810.
[8] N. Srivastava, E. Mansimov, and R. Salakhudinov, “Unsupervised learning
of video representations using lstms,” in Proceedings of the 32nd ICML15, D. Blei and F. Bach, Eds., 2015.
[9] N. Kalchbrenner, A. v. d. Oord, K. Simonyan, I. Danihelka, O. Vinyals,
A. Graves, and K. Kavukcuoglu, “Unsupervised learning of video
representations using lstms,” in Proceedings of the 34nd ICML-17, 2017.
[10] M. L. Stein, Interpolation of spatial data: some theory for kriging.
Springer Science & Business Media, 2012.
[11] J. G. De Gooijer and R. J. Hyndman, “25 years of time series forecasting,”
International journal of forecasting, vol. 22, no. 3, pp. 443–473, 2006.
[12] K. R. Muller, A. J. Smola, G. Ratsch, B. Scholkopf, J. Kohlmorgen, and
V. Vapnik, “Using support vector machines for time series prediction,”
Advances in kernel methodssupport vector learning, 1999.
[13] J. T. Connor, R. D. Martin, and L. E. Atlas, “Recurrent neural networks
and robust time series prediction,” Neural Networks, IEEE Transactions
on, vol. 5, no. 2, pp. 240–254, 1994.
[14] A. Graves, A.-r. Mohamed, and G. Hinton, “Speech recognition with
deep recurrent neural networks,” in IIIE ICASSP, 2013.
[15] I. Sutskever, J. Martens, and G. E. Hinton, “Generating text with recurrent
neural networks,” in Proceedings of the 28th International Conference
on Machine Learning, ICML 2011, 2011.
[16] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares,
H. Schwenk, and Y. Bengio, “Learning phrase representations using
rnn encoder-decoder for statistical machine translation,” arXiv preprint
arXiv:1406.1078, 2014.
[17] P. Mirowski and Y. LeCun, “Dynamic factor graphs for time series
modeling,” in Machine Learning and Knowledge Discovery in Databases.
Springer, 2009, pp. 128–143.
[18] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,”
in Proceedings of the 2nd International Conference on Learning
Representations (ICLR), no. 2014, 2013.
[19] J. Bayer and C. Osendorfer, “Learning stochastic recurrent networks,”
arXiv preprint arXiv:1411.7610, 2014.
[20] J. Chung, K. Kastner, L. Dinh, K. Goel, A. C. Courville, and Y. Bengio,
“A recurrent latent variable model for sequential data,” in Advances in
Neural Information Processing Systems 28, C. Cortes, N. D. Lawrence,
D. D. Lee, M. Sugiyama, and R. Garnett, Eds. Curran Associates, Inc.,
2015, pp. 2980–2988.
[21] R. G. Krishnan, U. Shalit, and D. Sontag, “Deep kalman ﬁlters,” arXiv
preprint arXiv:1511.05121, 2015.
[22] C. K. Wikle and M. B. Hooten, “A general science-based framework for
dynamical spatio-temporal models,” Test, vol. 19, no. 3, pp. 417–451,
2010.
[23] C. K. Wikle, “Modern perspectives on statistics for spatio-temporal data,”
Wiley Interdisciplinary Reviews: Computational Statistics, vol. 7, no. 1,
pp. 86–98, 2015.
[24] H. Rue and L. Held, Gaussian Markov random ﬁelds: theory and
applications. CRC Press, 2005.
[25] M. Ceci, R. Corizzo, F. Fumarola, D. Malerba, and A. Rashkovska,
“Predictive modeling of pv energy production: How to set up the learning
task for a better prediction?” IEEE Transactions on Industrial Informatics,
vol. 13, no. 3, pp. 956–966, 2017.
[26] G. Dornhege, B. Blankertz, M. Krauledat, F. Losch, G. Curio, and
K. robert Muller, “Optimizing spatio-temporal ﬁlters for improving braincomputer interfacing,” in in NIPS, 2005.
[27] Y. Ren and Y. Wu, “Convolutional deep belief networks for feature
extraction of eeg signal,” in Neural Networks (IJCNN), 2014 International
Joint Conference on. IEEE, 2014, pp. 2850–2853.
[28] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
computation, vol. 9, no. 8, pp. 1735–1780, 1997.
[29] I. Sutskever, J. Martens, G. Dahl, and G. Hinton, “On the importance of
initialization and momentum in deep learning,” in Proceedings of the
30th ICML, 2013.
[30] S. Ben Taieb and R. Hyndman, “Boosting multi-step autoregressive
forecasts,” in Proceedings of The 31st International Conference on
Machine Learning, 2014, pp. 109–117.
[31] G. Ganeshapillai, J. Guttag, and A. Lo, “Learning connections in ﬁnancial
time series,” in Proceedings of the 30th International Conference on
Machine Learning (ICML-13), 2013, pp. 109–117.
[32] J. Yuan, Y. Zheng, X. Xie, and G. Sun, “Driving with knowledge from
the physical world,” in Proceedings of the 17th ACM SIGKDD. ACM,
2011, pp. 316–324.
[33] J. Yuan, Y. Zheng, C. Zhang, W. Xie, X. Xie, G. Sun, and Y. Huang,
“T-drive: driving directions based on taxi trajectories,” in Proceedings of
the 18th SIGSPATIAL. ACM, 2010, pp. 99–108.