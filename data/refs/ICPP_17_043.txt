[1] J. J. Dongarra, J. Du Croz, S. Hammarling, and I. S. Duff,
“A set of level 3 basic linear algebra subprograms,” ACM
Transactions on Mathematical Software (TOMS), vol. 16,
no. 1, pp. 1-17, 1990.

[2] J. J. Dongarra, P. Luszczek, and A. Petitet, “The LINPACK
benchmark: past, present and future,” Concurrency and Computation: Practice and Experience, vol. 15, no. 9, pp. 803-—
820, 2003.

[3] J. Choi, J. J. Dongarra, L. S. Ostrouchov, A. P. Petitet, D. W.
Walker, and R. C. Whaley, “Design and implementation of the
ScaLAPACK LU, QR, and Cholesky factorization routines,”
Scientific Programming, vol. 5, no. 3, pp. 173-184, 1996.
[4] https://software.intel.com/en-us/intel-mkl.
[5] http://developer.amd.com/tools-and-sdks/archive/compute/
amd-core-math-library-acml/acml-downloads-resources/.
[6] http://www- 03.ibm.com/systems/power/software/essl/.

[7] H. M. Klie, H. H. Sudan, R. Li, Y. Saad et al., “Exploiting capabilities of many core platforms in reservoir simulation,” in
SPE Reservoir Simulation Symposium. Society of Petroleum
Engineers, 2011.

[8] Y. Wu, W. Jia, L. W. Wang, W. Gao, L. Wang, and X. Chi,
GPU Tuning for First-Principle Electronic Structure Simulations. Springer Berlin Heidelberg, 2013.

[10] J. Dongarra, M. Gates, A. Haidar, Y. Jia, K. Kabir,
P. Luszczek, and S. Tomov, “Portable HPC programming on
intel many-integrated-core hardware with MAGMA port to
Xeon Phi,” in International Conference on Parallel Processing and Applied Mathematics. Springer, 2013, pp. 571-581.
[11] S. Tomov, R. Nath, H. Ltaief, and J. Dongarra, “Dense
linear algebra solvers for multicore with GPU accelerators,” in
Parallel & Distributed Processing, Workshops and Phd Forum
(IPDPSW), 2010 IEEE International Symposium on. YEEE,
2010, pp. 1-8.

[12] K. Chellapilla, S. Puri, and P. Simard, “High performance
convolutional neural networks for document processing,” in
Tenth International Workshop on Frontiers in Handwriting
Recognition. Suvisoft, 2006.

[13] H. Cui, L. Wang, J. Xue, Y. Yang, and X. Feng, “Automatic library generation for BLAS3 on GPUs,” in Parallel
& Distributed Processing Symposium (IPDP§S), 2011 IEEE
International. JYEEE, 2011, pp. 255-265.

[14] J. Dem&ar, T. Curk, A. Erjavec, C. Gorup, T. Hoéevar,
M. Milutinovié, M. MoZina, M. Polajnar, M. Toplak, A. Starié
et al., “Orange: data mining toolbox in Python,” Journal of
Machine Learning Research, vol. 14, no. 1, pp. 2349-2353,
2013.

[15] D. E. King, “Dlib-ml: A machine learning toolkit,” Journal of
Machine Learning Research, vol. 10, no. Jul, pp. 1755-1758,
2009.

[16] S. Balay, K. Buschelman, W. D. Gropp, D. Kaushik, M. G.
Knepley, L. C. McInnes, B. F. Smith, and H. Zhang, “PETSc,”
See http://www. mcs. anl. gov/petsc, 2001.

[17] A. Heinecke, K. Vaidyanathan, M. Smelyanskiy, A. Kobotov,
R. Dubtsov, G. Henry, A. G. Shet, G. Chrysos, and P. Dubey,
“Design and implementation of the Linpack benchmark for
single and multi-node systems based on Intel Xeon Phi
coprocessor,” in Parallel & Distributed Processing (IPDPS),
2013 IEEE 27th International Symposium on. YEEE, 2013,
pp. 126-137.

[18] M. Bach, M. Kretz, V. Lindenstruth, and D. Rohr, “Optimized
HPL for AMD GPU and multi-core CPU usage,” Computer
Science-Research and Development, vol. 26, no. 3-4, p. 153,
2011.

[19] https://www.top500.org/lists/2016/11/.

[20] V. Volkov and J. W. Demmel, “Benchmarking GPUs to tune
dense linear algebra,” in 2008 SC-International Conference
for High Performance Computing, Networking, Storage and
Analysis. JEEE, 2008, pp. 1-11.

[21] R. Nath, S. Tomov, and J. Dongarra, “An improved MAGMA
GEMM for Fermi GPUs,” 2010.

[22] N. Nakasato, “A fast GEMM implementation on the Cypress
GPU,” ACM SIGMETRICS Performance Evaluation Review,
vol. 38, no. 4, pp. 50-55, 2011.

[23] G. Tan, L. Li, S. Triechle, E. Phillips, Y. Bao, and N. Sun,
“Fast implementation of DGEMM on Fermi GPU,” in Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis. ACM,
2011, p. 35.

[24] H. Fu, J. Liao, J. Yang, L. Wang, Z. Song, X. Huang,
C. Yang, W. Xue, F. Liu, F. Qiao et al., “The Sunway
TaihuLight supercomputer: system and applications,” Science
China Information Sciences, vol. 59, no. 7, p. 072001, 2016.
[25] F. Zheng, H.-L. Li, H. Lv, F. Guo, X.-H. Xu, and X.-H. Xie,
“Cooperative computing techniques for a deeply fused and
heterogeneous many-core processor architecture,” Journal of
Computer Science and Technology, vol. 30, no. 1, pp. 145162, 2015.

[26] J. Dongarra, “Sunway TaihuLight supercomputer makes its
appearance,” National Science Review, vol. 3, no. 3, pp. 265—
266, 2016.

[27] J. A. Gunnels, G. M. Henry, and R. A. van de Geijn, “A family
of high-performance matrix multiplication algorithms,” in [nternational Conference on Computational Science. Springer,
2001, pp. 51-60.

[28] K. Goto and R. A. van de Geijn, “Anatomy of highperformance matrix multiplication,’ ACM Transactions on
Mathematical Software (TOMS), vol. 34, no. 3, p. 12, 2008.
[29] R. C. Whaley and J. J. Dongarra, “Automatically tuned linear
algebra software,” in Proceedings of the 1998 ACM/IEEE
conference on Supercomputing. _TEEE Computer Society,
1998, pp. 1-27.

[30] Q. Wang, X. Zhang, Y. Zhang, and Q. Yi, “AUGEM: automatically generate high performance dense linear algebra
kernels on x86 CPUs,” in Proceedings of the International
Conference on High Performance Computing, Networking,
Storage and Analysis. ACM, 2013, p. 25.

[31] Y. Li, J. Dongarra, and S. Tomov, “A note on auto-tuning
GEMM for GPUs,” in International Conference on Computational Science. Springer, 2009, pp. 884-892.

[32] P. Gepner, V. Gamayunov, D. L. Fraser, E. Houdard, L. Sauge,
D. Declat, and M. Dubois, “Evaluation of DGEMM implementation on Intel Xeon Phi coprocessor,’ Journal of
Computers, vol. 9, no. 7, pp. 1566-1571, 2014.