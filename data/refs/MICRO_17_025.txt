[1] 2015. IC Cost and Price Model, Revision 1506, IC Knowledge LLC. (2015).
http://www.icknowledge.com/

[2] 2016. 8Gb B-die DDR4 SDRAM. (2016). http://www.samsung.com/semiconductor/

global/file/product/2016/06/DS_K4A8G085WB-B_Rev1_61-0.pdf

[3] 2016. NVIDIA TITAN X (pascal). (2016). http://www.geforce.com/hardware/

10series/titan-x-pascal

[4] 2017. Design Compiler, Synopsys Inc. (2017).

[5] 2017. Intel Instruction Set Architecture Extensions. (2017). https://software.intel.
com/en-us/intel-isa- extensions

[6] 2017. Micron Automata Processor. (2017). https://www.micronautomata.com/

[7] 2017. NVIDIA cuDNN. (2017). https://developer.nvidia.com/cudnn
[8] 2017. NVIDIA System Management Interface. (2017). https://developer.nvidia.

com/nvidia-system-management-interface
[9] 2017. Torch 7. (2017). http://torch.ch/

[10] Junwhan Ahn, Sungpack Hong, Sungjoo Yoo, Onur Mutlu, and Kiyoung Choi.
2015. A scalable processing-in-memory accelerator for parallel graph processing.
In International Symposium on Computer Architecture (ISCA). ACM Press, New
York, New York, USA, 105-117.

[11] Junwhan Ahn, Sungjoo Yoo, and Kiyoung Choi. 2016. AIM: Energy-Efficient
Aggregation Inside the Memory Hierarchy. ACM Transactions on Architecture
and Code Optimization 13, 4 (oct 2016), 1-24.

[12] Junwhan Ahn, Sungjoo Yoo, Onur Mutlu, and Kiyoung Choi. 2015. PIM-enabled
Instructions: A Low-overhead, Locality-aware Processing-in-memory Architecture. In International Symposium on Computer Architecture (ISCA). ACM, 336-348.

[13] A. Akerib, O. AGAM, E. Ehrman, and M. Meyassed. 2014. Using storage cells to
perform computation. (dec 2014). US Patent 8,908,465.

[14] Avidan Akerib and Eli Ehrman. 2014. In-memory computational device. (nov
2014). US Patent App. 14/555,638.

[15] A. Akerib and E. Ehrman. 2015. Non-volatile in-memory computing device. (may
2015). US Patent App. 14/588,419.

[16] Berkin Akin, Franz Franchetti, and James C Hoe. 2015. Data Reorganization
in Memory Using 3D-stacked DRAM. In International Symposium on Computer
Architecture (ISCA). ACM, 131-143.

[17] Hadi Asghari-Moghaddam, Young Hoon Son, Jung Ho Ahn, and Nam Sung Kim.
2016. Chameleon: Versatile and practical near-DRAM acceleration architecture
for large memory systems. In International Symposium on Microarchitecture
(MICRO)}. ACM, 1-13.

[18] Rajeev Balasubramonian, Jichuan Chang, Troy Manning, Jaime H Moreno,

Richard Murphy, Ravi Nair, and Steven Swanson. 2014. Near-Data Processing:

Insights from a MICRO-46 Workshop. In Micro, IEEE, Vol. 34. IEEE, 36-42.

[19] Mahdi Nazm Bojnordi and Engin Ipek. 2016. Memristive Boltzmann machine:

A hardware accelerator for combinatorial optimization and deep learning. In

International Symposium on High Performance Computer Architecture (HPCA).

IEEE, 1-13.

[20] Amirali Boroumand, Saugata Ghose, Brandon Lucia, Kevin Hsieh, Krishna Mal
ladi, Hongzhong Zheng, and Onur Mutlu. 2016. LazyPIM: An Efficient Cache

Coherence Mechanism for Processing-in-Memory. Computer Architecture Letters

(2016), 1-1.

[21] Kevin Chen and Lior Pachter. 2005. Bioinformatics for whole-genome shotgun

sequencing of microbial communities. PLoS Comput Biol 1, 2 (2005), e24.

[22] Yunji Chen, Tao Luo, Shaoli Liu, Shijin Zhang, Ligiang He, Jia Wang, Ling Li,

Tianshi Chen, Zhiwei Xu, Ninghui Sun, and Olivier Temam. 2014. DaDianNao:

A Machine-Learning Supercomputer. In International Symposium on Microarchi
tecture (MICRO). IEEE, 609-622.

[23] Ping Chi, Shuangchen Li, Cong Xu, Tao Zhang, Jishen Zhao, Yongpan Liu, Yu

Wang, and Yuan Xie. 2016. PRIME: a novel processing-in-memory architecture

for neural network computation in ReRAM-based main memory. In international

Symposium on Computer Architecture (ISCA), Vol. 44. 27-39.

[24] Matthieu Courbariaux and Yoshua Bengio. 2016. BinaryNet: Training Deep

Neural Networks with Weights and Activations Constrained to +1 or -1. arXiv:

1602.02830 (2016).

[25] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. 2015. BinaryCon
nect: Training Deep Neural Networks with binary weights during propagations.

arXiv: 1511.00363 (2015).

[26] Bill Dally. 2015. The Path to Exascale. http://images.nvidia.com/events/sc15/
pdfs/SC5102-path- exascale- computing pdf. (2015).

[27] Paul Dlugosch, Dave Brown, Paul Glendenning, Michael Leventhal, and Harold

Noyes. 2014. An efficient and scalable semiconductor architecture for parallel

automata processing. In Parallel and Distributed Systems, IEEE Transactions on.

IEEE, 99.

[28] Zidong Du, Robert Fasthuber, Tianshi Chen, Paolo Ienne, Ling Li, Tao Luo,

Xiaobing Feng, Yunji Chen, and Olivier Temam. 2015. ShiDianNao: shifting

vision processing closer to the sensor. In International Symposium on Computer

Architecture (ISCA). ACM Press, New York, New York, USA, 92-104.

[29] Steve K. Esser, Alexander Andreopoulos, Rathinakumar Appuswamy, Pallab

Datta, Davis Barch, Arnon Amir, John Arthur, Andrew Cassidy, Myron Flickner,

Paul Merolla, Shyamal Chandra, Nicola Basilico, Stefano Carpin, Tom Zimmer
man, Frank Zee, Rodrigo Alvarez-Icaza, Jeffrey A. Kusnitz, Theodore M. Wong,

William P. Risk, Emmett McQuinn, Tapan K. Nayak, Raghavendra Singh, and
Dharmendra S. Modha. 2013. Cognitive computing systems: Algorithms and applications for networks of neurosynaptic cores. In International Joint Conference
on Neural Networks (IJCNN). IEEE, 1-10.

[30] A Farmahini-Farahani, Jung Ho Ahn, K Morrow, and Nam Sung Kim. 2015. NDA:

Near-DRAM acceleration architecture leveraging commodity DRAM devices and

standard memory modules. In International Symposium on High Performance

Computer Architecture (HPCA). 283-295.

[31] G. Fredeman, D. W. Plass, A. Mathews, J. Viraraghavan, K. Reyer, T. J. Knips, T.

Miller, E. L. Gerhard, D. Kannambadi, C. Paone, D. Lee, D. J. Rainey, M. Sperling,

M. Whalen, S. Burns, R. R. Tummuru, H. Ho, A. Cestero, N. Arnold, B. A. Khan,

T. Kirihata, and S. S. Iyer. 2016. A 14 nm 1.1 Mb Embedded DRAM Macro With 1

ns Access. IEEE Journal of Solid-State Circuits 51, 1 (jan 2016), 230-239.

[32] Mingyu Gao, Grant Ayers, and Christos Kozyrakis. 2015. Practical Near-Data

Processing for In-memory Analytics Frameworks. Parallel Archit. Compil. Tech.

(PACT), 2015 IEEE Int. Conf, (2015), 113-124.

[33] Mingyu Gao, Christina Delimitrou, Dimin Niu, Krishna T. Malladi, Hongzhong

Zheng, Bob Brennan, and Christos Kozyrakis. 2016. DRAF: A Low-Power DRAM
Based Reconfigurable Acceleration Fabric. In International Symposium on Com
puter Architecture (ISCA). IEEE, 506-518.

[34] Mingyu Gao and Christos Kozyrakis. 2016. HRL: Efficient and flexible recon
figurable logic for near-data processing. In International Symposium on High

Performance Computer Architecture (HPCA), IEEE, 126-137.

[35] Qing Guo, Xiaochen Guo, Ravi Patel, Engin Ipek, and Eby G Friedman. 2013.

AC-DIMM: associative computing with STT-MRAM. In nternational Symposium

on Computer Architecture (ISCA). ACM, 189-200.

[36] Qi Guo, Tze-Meng Low, Nikolaos Alachiotis, Berkin Akin, Larry Pileggi, James C.

Hoe, and Franz Franchetti. 2015. Enabling portable energy efficiency with mem
ory accelerated library. In International Symposium on Microarchitecture (MICRO).

ACM Press, New York, New York, USA, 750-761.

[37] Linley Gwennap. 2015. Skylake speedshifts to next gear. Microprocessor Report

29, 9 (2015), 6-10.

[38] Fatih Hamzaoglu, Umut Arslan, Nabhendra Bisnik, Swaroop Ghosh, Manoj B.

Lal, Nick Lindert, Mesut Meterelliyoz, Randy B. Osborne, Joodong Park, Shigeki

Tomishima, Yih Wang, and Kevin Zhang. 2014. 13.1 A 1Gb 2GHz embedded

DRAM in 22nm tri-gate CMOS technology. In International Solid-State Circuits

Conference Digest of Technical Papers (ISSCC). IEEE, 230-231.

[39] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A. Horowitz,

and William J. Dally. 2016. EIE: Efficient Inference Engine on Compressed Deep

Neural Network. In International Symposium on Computer Architecture (ISCA).

TEEE, 243-254.

[40] Song Han, Huizi Mao, and William J. Dally. 2015. Deep Compression: Compress
ing Deep Neural Network with Pruning, Trained Quantization and Huffman

Coding. arXiv: 1510.00149 (2015).

[41] Kaiming He, Xiangyu Zhang, Shaoging Ren, and Jian Sun. 2015. Deep Residual

Learning for Image Recognition. arXiv: 1512.03385 (2015).

[42] Byungchul Hong, Gwangsun Kim, Jung Ho Ahn, Yongkee Kwon, Hongsik Kim,

and John Kim. 2016. Accelerating Linked-list Traversal Through Near-Data

Processing. In International Conference on Parallel Architectures and Compilation

(PACT). ACM Press, New York, New York, USA, 113-124.

[43] Wei Huang, S. Ghosh, S. Velusamy, K. Sankaranarayanan, K. Skadron, and M.R.

Stan. 2006. HotSpot: a compact thermal modeling methodology for early-stage

VLSI design. IEEE ransactions on Very Large Scale Integration (VLSI) Systems 14, 5

(2006), 501-513.

[44] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua

Bengio. 2016. Quantized Neural Networks: Training Neural Networks with Low

Precision Weights and Activations. arXiv: 1609.07061 (2016).

[45] Sergey loffe and Christian Szegedy. 2015. Batch Normalization: Accelerating

Deep Network Training by Reducing Internal Covariate Shift. arXiv: 1502.03167

(2015).

[46] J. Lee and J. H. Ahn and K. Choi. 2016. Buffered compares: Excavating the
hidden parallelism inside DRAM architectures with lightweight logic. In Design,
Automation Test in Europe Conference Exhibition (DATE). 1243-1248.

[47] Jan Van Lunteren. 2016. Programmable Near-Memory Acceleration on ConTutto.

In OpenPower Summit.

[48] Y. Ji, Y. Zhang, S. Li, P. Chi, C. Jiang, P. Qu, Y. Xie, and W. Chen. 2016. NEUTRAMS:

Neural network transformation and co-design under neuromorphic hardware

constraints. In Annual IEEE/ACM International Symposium on Microarchitecture

(MICRO). 1-13.

[49] Sung-Mo Kang and Yusuf Leblebici. 2003. CMOS digital integrated circuits. Tata

McGraw-Hill Education.

[50] Ke Chen, Sheng Li, Naveen Muralimanohar, Jung Ho Ahn, Jay B Brockman, and

Norman P Jouppi. 2012. CACTI-3DD: Architecture-level modeling for 3D die
stacked DRAM main memory. In Design, Automation & Test in Europe Conference

& Exhibition (DATE), EDA Consortium, IEEE, 33-38.

[51] Brent Keeth, R. Jacob Baker, Brian Johnson, and Feng Lin. 2007. DRAM Circuit

Design: Fundamental and High-Speed Topics (2nd ed.). Wiley-IEEE Press.

[52] Saugata Ghose Kevin Hsieh, Samira Khan, Nandita Vijaykumar, Kevin K. Chang,

Amirali Boroumand and Onur Mutlu. 2016. Accelerating Pointer Chasing in
3D-Stacked Memory: Challenges, Mechanisms, Evaluation. In International Conference on Computer Design (ICCD).

[53] Duckhwan Kim, Jaeha Kung, Sek Chai, Sudhakar Yalamanchili, and Saibal
Mukhopadhyay. 2016. Neurocube: A Programmable Digital Neuromorphic Architecture with High-Density 3D Memory. In International Symposium on Computer
Architecture (ISCA). IEEE, 380-392.

[54] Yoongu Kim, Vivek Seshadri, Donghyuk Lee, Jamie Liu, and Onur Mutlu. 2012. A
Case for Exploiting Subarray-level Parallelism (SALP) in DRAM. In International
Symposium on Computer Architecture (ISCA). IEEE Computer Society, 368-379.

[55] Ytong-Bin Kim and Tom W. Chen. 1999. Assessing merged DRAM/Logic technology. Integration, the VLSI Journal 27, 2 (1999), 179-194.

[56] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. ImageNet Clas
sification with Deep Convolutional Neural Networks. In Advances in Neural

Information Processing Systems, F Pereira, C J C Burges, L Bottou, and K Q Wein
berger (Eds.). Curran Associates, Inc., 1097-1105.

[57] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. Nature

521, 7553 (2015), 436-444,

[58] Dong Uk Lee, Kyung Whan Kim, Kwan Weon Kim, Hongjung Kim, Ju Young

Kim, Young Jun Park, Jae Hwan Kim, Dae Suk Kim, Heat Bit Park, Jin Wook

Shin, Jang Hwan Cho, Ki Hun Kwon, Min Jeong Kim, Jaejin Lee, Kun Woo Park,

Byongtae Chung, and Sungjoo Hong. 2014. A 1.2V 8Gb 8-channel 128GB/s

high-bandwidth memory (HBM) stacked DRAM with effective microbump I/O

test methods using 29nm process and TSV. In International Solid-State Circuits

Conference Digest of Technical Papers (ISSCC). 432-433.

[59] Fengfu Li and Bin Liu. 2016. Ternary Weight Networks. arXiv: 1605.04711 (2016).

[60] Shuangchen Li, Cong Xu, Qiaosha Zou, Jishen Zhao, Yu Lu, and Yuan Xie. 2016.

Pinatubo: A processing-in-memory architecture for bulk bitwise operations in

emerging non-volatile memories. In Design Automation Conference (DAC). ACM

Press, New York, New York, USA, 1-6.

[61] Shaoli Liu, Zidong Du, Jinhua Tao, Dong Han, Tao Luo, Yuan Xie, Yunji Chen,

and Tianshi Chen. 2016. Cambricon: An Instruction Set Architecture for Neural

Networks. In International Symposium on Computer Architecture (ISCA). IEEE,

393-405.

[62] Paul Merolla, John Arthur, Filipp Akopyan, Nabil Imam, Rajit Manohar, and

Dharmendra S. Modha. 2011. A digital neurosynaptic core using embedded

crossbar memory with 45p] per spike in 45nm. In Custom Integrated Circuits

Conference (CICC), IEEE, 1-4.

[63] Paul A Merolla, John V Arthur, Rodrigo Alvarez-Icaza, Andrew S Cassidy, Jun

Sawada, Filipp Akopyan, Bryan L Jackson, Nabil Imam, Chen Guo, Yutaka Naka
mura, Bernard Brezzo, Ivan Vo, Steven K Esser, Rathinakumar Appuswamy,

Brian Taba, Arnon Amir, Myron D Flickner, William P Risk, Rajit Manohar, and

Dharmendra S Modha. 2014. A million spiking-neuron integrated circuit with a

scalable communication network and interface. Science 345, 6197 (2014), 668-673.

[64] Mu-Tien Chang, P. Rosenfeld, Shih-Lien Lu, and B. Jacob. 2013. Technology

comparison for large last-level caches (L3Cs): Low-leakage SRAM, low write
energy STT-RAM, and refresh-optimized eDRAM. In International Symposium

on High Performance Computer Architecture (HPCA). IEEE, 143-154.

[65] Norman P Muralimanohar, Naveen and Balasubramonian, Rajeev and Jouppi.

2009. CACTI 6.0: A tool to model large caches. HP Lab. (2009), 22-31.

[66] Prashant J. Nair, Dae-Hyun Kim, and Moinuddin K. Qureshi. 2013. ArchShield:

architectural framework for assisting DRAM scaling by tolerating high error

rates. In International Symposium on Computer Architecture. ACM Press, New

York, New York, USA, 72-83.

[67] R Nair, $ F Antao, C Bertolli, P Bose, J R Brunheroto, T Chen, C Cher, CH A

Costa, J Evangelinos, B M Fleischer, T W Fox, D S Gallo, L Grinberg, J A Gunnels,

ACJacob, P Jacob, H M Jacobson, T Karkhanis, C Kim, J H Moreno, J K O’Brien,

M Ohmacht, Y Park, D A Prener, B S Rosenburg, K D Ryu, O Sallenave, M J

Serrano, P D M Siegl, K Sugavanam, and Z Sura. 2015. Active Memory Cube: A

processing-in-memory architecture for exascale systems. IBM Journal of Research

and Development 59, 2/3 (mar 2015), 17:1-17:14.

[68] David Harris Neil Weste. 2006. CMOS VLSI Design: A Circuits And Systems
Perspective, 3/E. Pearson.

[69] Joachim Ott, Zhouhan Lin, Ying Zhang, Shih-Chii Liu, and Yoshua Bengio. 2016.
Recurrent Neural Networks With Limited Numerical Precision. arXiv: 1608.06902
(2016).

[70] J. M. Park, Y. S. Hwang, S. W. Kim, S. Y. Han, J. S. Park, J. Kim, J. W. Seo, B. S.
Kim, S. H. Shin, C. H. Cho, S. W. Nam, H. S. Hong, K. P. Lee, G. Y. Jin, and E. S.
Jung. 2015. 20nm DRAM: A new beginning of another revolution. In 2015 IEEE
International Electron Devices Meeting (IEDM). 26.5.1-26.5.4.

[71] David Patterson, Thomas Anderson, Neal Cardwell, Richard Fromm, Kimberly

Keeton, Christoforos Kozyrakis, Randi Thomas, and Katherine Yelick. 1997. A

case for intelligent RAM. Micro, IEEE 17, 2 (1997), 34-44.

[72] David A Patterson and John L Hennessy. 2013. Computer organization and design:

the hardware/software interface. Newnes.

[73] A, Pattnaik, X. Tang, A. Jog, O. Kayiran, A. K. Mishra, M. T. Kandemir, O. Mutlu,

and C. R. Das. 2016. Scheduling techniques for GPU architectures with processing
in-memory capabilities. In International Conference on Parallel Architecture and

Compilation Techniques (PACT). 31-44.

[74] ] Thomas Pawlowski. 2011. Hybrid memory cube (HMC). In Hot Chips, Vol. 23.
[75] Seth H. Pugsley, Jeffrey Jestes, Rajeev Balasubramonian, Vijayalakshmi Srinivasan, Alper Buyuktosunoglu, Al Davis, and Feifei Li. 2014. Comparing Implementations of Near-Data Computing with In-Memory MapReduce Workloads.
In Micro, IEEE, Vol. 34. IEEE, 44-52.

[76] Seth H Pugsley, Jeffrey Jestes, Huihui Zhang, Rajeev Balasubramonian, Vijayalakshmi Srinivasan, A Buyuktosunoglu, A Davis, and F Li. 2014. NDC: Analyzing
the Impact of 3D-Stacked Memory+ Logic Devices on MapReduce Workloads.
In International Symposium on Performance Analysis of Systems and Software
(ISPASS).

[77] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. 2016.
XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks. arXiv: 1603.05279 (2016).

[78] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C.
Berg, and Li Fei-Fei. 2015. ImageNet Large Scale Visual Recognition Challenge.
International Journal of Computer Vision (IJCV) 115, 3 (2015), 211-252.

[79] Vivek Seshadri. 2016. Simple DRAM and Virtual Memory Abstractions to Enable
Highly Efficient Memory Systems. arXiv: 1605.06483 (2016).

[80] V Seshadri, K Hsieh, A Boroumand, D Lee, M A Kozuch, O Mutlu, P B Gibbons,
and T C Mowry. 2015. Fast Bulk Bitwise AND and OR in DRAM. Computer
Architecture Letters PP, 99 (2015), 1.

[81] Vivek Seshadri, Michael A. Kozuch, Todd C. Mowry, Yoongu Kim, Chris Fallin,
Donghyuk Lee, Rachata Ausavarungnirun, Gennady Pekhimenko, Yixin Luo,
Onur Mutlu, and Phillip B. Gibbons. 2013. RowClone: fast and energy-efficient
in-DRAM bulk data copy and initialization. In International Symposium on Microarchitecture (MICRO). ACM Press, New York, New York, USA, 185-197.
[82] Vivek Seshadri, Donghyuk Lee, Thomas Mullins, Hasan Hassan, Amirali
Boroumand, Jeremie Kim, Michael A. Kozuch, Onur Mutlu, Phillip B. Gibbons, and
Todd C. Mowry. 2016. Buddy-RAM: Improving the Performance and Efficiency
of Bulk Bitwise Operations Using DRAM. arXiv: 1611.09988 (2016).

[83] Hardik Sharma, Jongse Park, Divya Mahajan, Emmanuel Amaro, Joon Kyung
Kim, Chenkai Shao, Asit Mishra, and Hadi Esmaeilzadeh. 2016. From High-Level
Deep Neural Models to FPGAs. In International Symposium on Microarchitecture
(MICRO), IEEE.

[84] George Sideris. 1973. INTEL 1103-MOS memory taht defied cores. ELECTRONICS
46, 9 (1973), 108-113.

[85] Karen Simonyan and Andrew Zisserman. 2014. Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv: 1409.1556 (2014).

[86] Young Hoon Son, O. Seongil, Yuhwan Ro, Jae W. Lee, and Jung Ho Ahn. 2013.
Reducing memory access latency with asymmetric DRAM bank organizations.
International Symposium on Computer Architecture (ISCA) 41, 3 (jul 2013), 380.
[87] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed,
Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2014. Going Deeper with Convolutions. arXiv: 1409.4842 (2014).

[88] Pedro Trancoso. 2015. Moving to Memoryland: In-memory Computation for
Existing Applications. In International Conference on Computing Frontiers. ACM,
32:1—-32:6.

[89] G. Venkatesh, E. Nurvitadhi, and D. Marr. 2016. Accelerating Deep Convolutional
Networks using low-precision and sparsity. arXiv: 1610.00324 (2016).

[90] Oreste Villa, Daniel R. Johnson, Mike Oconnor, Evgeny Bolotin, David Nellans,
Justin Luitjens, Nikolai Sakharnykh, Peng Wang, Paulius Micikevicius, Anthony
Scudiero, Stephen W. Keckler, and William J. Dally. 2014. Scaling the Power Wall:
A Path to Exascale. In International Conference for High Performance Computing,
Networking, Storage and Analysis (SC), IEEE, 830-841.

[91] Thomas Vogelsang. 2010. Understanding the Energy Consumption of Dynamic
Random Access Memories. In International Symposium on Microarchitecture (MICRO), IEEE, 363-374.

[92] Ren Wu, Shengen Yan, Yi Shan, Qingging Dang, and Gang Sun. 2015. Deep Image:
Scaling up Image Recognition. arXiv: 1501.02876 (2015).

[93] Mahmut Kandemir Mustafa Karakoy Xulong Tang, Orhan Kislal. 2018. Data
Movement Aware Computation Partitioning. In International Symposium on
Microarchitecture (MICRO).

[94] Yasuko Eckert Nuwan Jayasena and Gabriel Loh. 2014. Thermal Feasibility of
Die-Stacked Processing in Memory. In WoNDP: 2nd Workshop on Near-Data
Processing, International Symposium on Microarchitecture. IEEE.

[95] Dongping Zhang, Nuwan Jayasena, Alexander Lyashevsky, Joseph L Greathouse,
Lifan Xu, and Michael Ignatowski. 2014. TOP-PIM: Throughput-oriented
Programmable Processing in Memory. In International Symposium on Highperformance Parallel and Distributed Computing. ACM, 85-98.

[96] Tao Zhang, Ke Chen, Cong Xu, Guangyu Sun, Tao Wang, and Yuan Xie. 2014.
Half-DRAM: A high-bandwidth and low-power DRAM architecture from the
rethinking of fine-grained activation. In International Symposium on Computer
Architecture (ISCA). 349-360.

[97] Shuchang Zhou, Zekun Ni, Xinyu Zhou, He Wen, Yuxin Wu, and Yuheng Zou.
2016. DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with
Low Bitwidth Gradients. arXiv: 1606.06160 (2016).