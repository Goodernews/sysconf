[1] C. Clauss, T. Moschny, and N. Eicker, “Dynamic Process Management
with Allocation-internal Co-Scheduling towards Interactive Supercomputing,” in Proceedings of the 1st COSH Workshop on Co-Scheduling
of HPC Applications, C. Trinitis and J. Weidendorfer, Eds., Jan 2016,
p. 13.
[2] J. Breitbart, J. Weidendorfer, and C. Trinitis, “Case Study on Coscheduling for HPC Applications,” in 2015 44th International Conference on Parallel Processing Workshops, Sept 2015, pp. 277–285.
[3] D. Bailey, E. Barszcz, J. Barton, D. Browning, R. Carter, L. Dagum,
R. Fatoohi, P. Frederickson, T. Lasinski, R. Schreiber, H. Simon,
V. Venkatakrishnan, and S. Weeratunga, “The NAS Parallel Benchmarks,” The International Journal of Supercomputing Applications,
vol. 5, no. 3, pp. 63–73, 1991.
[4] S. Pickartz, J. Breitbart, and S. Lankes, “Co-scheduling on Upcoming
Many-Core Architectures,” in Proceedings of the Joined Workshops
COSH 2017 and VisorHPC 2017, Jan. 2017, pp. 27–32.
[5] G. T. Chetsa, L. Lefèvre, J.-M. Pierson, P. Stolf, and G. Da Costa,
“Exploiting performance counters to predict and improve energy performance of HPC systems,” Future Generation Computer Systems, vol. 36,
pp. 287–298, 2014.
[6] A. Kivity, Y. Kamay, D. Laor, and U. Lublin, “kvm: the Linux Virtual
Machine Monitor,” in Proceedings of the Linux Symposium, vol. 1,
Ottawa, Ontario, Canada, Jun 2007, pp. 225–230.
[7] P.
Menage,
Kernel
Documentation,
cgroups,
kernel.org,
https://www.kernel.org/doc/Documentation/cgroups/cgroups.txt.
[8] D. S. Milojičić, F. Douglis, Y. Paindaveine, R. Wheeler, and S. Zhou,
“Process Migration,” ACM Comput. Surv., vol. 32, no. 3, pp. 241–299,
Sep 2000.
[9] Y. M. Ma, C. R. Lee, and Y. C. Chung, “InﬁniBand virtualization
on KVM,” in 4th IEEE International Conference on Cloud Computing
Technology and Science Proceedings, Dec 2012, pp. 777–781.
[10] S. Pickartz, C. Clauss, S. Lankes, S. Krempel, T. Moschny, and A. Monti,
“Non-intrusive Migration of MPI Processes in OS-Bypass Networks,”
in 2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW), May 2016, pp. 1728–1735.
[11] S. Pickartz, S. Lankes, A. Monti, C. Clauss, and J. Breitbart, “Application migration in HPC—A driver of the exascale era?” in 2016
International Conference on High Performance Computing Simulation
(HPCS), Jul 2016, pp. 318–325.
[12] J. Breitbart, J. Weidendorfer, and C. Trinitis, “Automatic Co-Scheduling
Based on Main Memory Bandwidth Usage,” in Job Scheduling Strategies
for Parallel Processing: 19th and 20th International Workshops, 2017,
pp. 141–157.
[13] H. David, E. Gorbatov, U. R. Hanebutte, R. Khanna, and C. Le, “RAPL:
memory power estimation and capping,” in Low-Power Electronics and
Design (ISLPED), 2010 ACM/IEEE International Symposium on, 2010.
[14] D. Bailey, E. Barszcz, J. Barton, D. Browning, R. Carter, L. Dagum,
R. Fatoohi, S. Fineberg, P. Frederickson, T. Lasinski, R. Schreiber,
H. Simon, V. Venkatakrishnan, and S. Weeratunga, “The NAS Parallel
Benchmarks,” Tech. Rep., Mar 1994.
[15] A. Darling, L. Carey, and W.-c. Feng, “The design, implementation, and
evaluation of mpiBLAST,” proceedings of ClusterWorld, vol. 2003, pp.
13–15, 2003.
[16] J. Breitbart, S. Pickartz, J. Weidendorfer, and A. Monti, Viability of
Virtual Machines in HPC. Springer International Publishing, 2016, to
appear.
[17] R. Teyssier, “Cosmological hydrodynamics with adaptive mesh
reﬁnement-A new high resolution code called RAMSES,” Astronomy
& Astrophysics, vol. 385, no. 1, pp. 337–364, 2002.
[18] P.-F. Lavallée, G. C. de Verdière, P. Wautelet, D. Lecas, and J.M. Dupays, “Porting and optimizing HYDRO to new platforms and
programming paradigms lessons learnt,” 2012.
[19] S. Chakravorty, C. L. Mendes, and L. V. Kalé, Proactive Fault Tolerance
in MPI Applications Via Task Migration. Springer Berlin Heidelberg,
Dec 2006, pp. 485–496.
[20] C. Wang, F. Mueller, C. Engelmann, and S. L. Scott, “Proactive processlevel live migration and back migration in HPC environments,” Journal
of Parallel and Distributed Computing, vol. 72, no. 2, 2012.
[21] A. de Blanche and T. Lundqvist, “Addressing characterization methods
for memory contention aware co-scheduling,” The Journal of Supercomputing, vol. 71, no. 4, pp. 1451–1483, 2015.
[22] J. Mars, N. Vachharajani, R. Hundt, and M. L. Soffa, “Contention aware
execution: Online contention detection and response,” in Proceedings of
the 8th Annual IEEE/ACM International Symposium on Code Generation and Optimization. ACM, 2010, pp. 257–265.
[23] D. Eklov, N. Nikoleris, D. Black-Schaffer, and E. Hagersten, “Bandwidth bandit: Quantitative characterization of memory contention,” in
Proceedings of the 2013 IEEE/ACM International Symposium on Code
Generation and Optimization (CGO), Feb 2013, pp. 1–10.
[24] M. Schreiber, C. Riesinger, T. Neckel, H.-J. Bungartz, and A. Breuer,
“Invasive compute balancing for applications with shared and hybrid
parallelization,” International Journal of Parallel Programming, 2014.
[25] L. Wang, G. Von Laszewski, J. Dayal, and F. Wang, “Towards energy
aware scheduling for precedence constrained parallel tasks in a cluster
with dvfs,” in Proceedings of the 2010 10th IEEE/ACM International
Conference on Cluster, Cloud and Grid Computing. IEEE Computer
Society, 2010, pp. 368–377.
[26] A. Auweter, A. Bode, M. Brehm, H. Huber, and D. Kranzlmüller,
“Principles of energy efﬁciency in high performance computing,” in
Information and Communication on Technology for the Fight against
Global Warming. Springer, 2011, pp. 18–25.
[27] S. Lankes, S. Pickartz, and J. Breitbart, “HermitCore – A Unikernel
for Extreme Scale Computing,” in International Workshop on Runtime
and Operating Systems for Supercomputers (ROSS 2016), Kyoto, Japan,
2016.