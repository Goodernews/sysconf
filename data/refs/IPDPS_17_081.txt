[1] J. W. Choi, D. Bedard, R. Fowler, and R. Vuduc, “A roofline model of energy,” International Parallel & Distributed Pro- cessing Symposium (IPDPS), 2013.
[2] L. Page, S. Brin, R. Motwani, and T. Winograd, “The pager- ank citation ranking: Bringing order to the web.” Stanford InfoLab, Technical Report 1999-66, November 1999.
[3] S. Beamer, K. Asanovic ́, and D. A. Patterson, “The GAP benchmark suite,” arXiv:1508.03619, August 2015.
[4] P. Berkhin, “A survey on pagerank computing,” Internet Mathematics, vol. 2, no. 1, pp. 73–120, 2005.
[5] S. Beamer, K. Asanovic ́, and D. A. Patterson, “Locality exists in graph processing: Workload characterization on an Ivy Bridge server,” International Symposium on Workload Characterization (IISWC), 2015.
[6] J. Shun, “Shared-memory parallelism can be simple, fast, and scalable,” Ph.D. dissertation, Carnegie Mellon University, 2015.
[7] R. Nishtala, R. W. Vuduc, J. W. Demmel, and K. A. Yelick, “When cache blocking of sparse matrix vector multiply works and why,” Applicable Algebra in Engineering, Communica- tion and Computing, vol. 18, no. 3, pp. 297–311, 2007.
[8] P. Erdo ̋s and A. Re ́yni, “On random graphs. I,” Publicationes Mathematicae, vol. 6, pp. 290–297, 1959.
[9] “Graph500 benchmark.” www.graph500.org.
[10] J. Leskovec, D. Chakrabarti, J. Kleinberg, and C. Faloutsos, “Realistic, mathematically tractable graph generation and evolution, using Kronecker multiplication,” European Con- ference on Principles and Practice of Knowledge Discovery in Databases, 2005.
[11] H. Kwak, C. Lee, H. Park, and S. Moon, “What is Twitter, a social network or a news media?” International World Wide Web Conference (WWW), 2010.
[12] J. Yang and J. Leskovec, “Defining and evaluating net- work communities based on ground-truth,” CoRR, vol. abs/1205.6233, 2012.
[13] A. Sinha, Z. Shen, Y. Song, H. Ma, D. Eide, and K. Wang, “An overview of Microsoft academic service (MAS) and applications,” World Wide Web Consortium (W3C), 2015.
[14] T. Davis and Y. Hu, “The University of Florida sparse matrix collection,” ACM Transactions on Mathematical Software, vol. 38, pp. 1:1 – 1:25, 2011.
[15] “GAP benchmark suite reference code v0.8.” https://github.com/sbeamer/gapbs.
[16] “Intel performance counter www.intel.com/software/pcm.
monitor.”
[17] D. Nguyen, A. Lenharth, and K. Pingali, “A lightweight infrastructure for graph analytics,” Symposium on Operating Systems Principles (SOSP), 2013.
[18] N. Sundaram, N. R. Satish, M. M. A. Patwary, S. R. Dulloor, S. G. Vadlamudi, D. Das, and P. Dubey, “GraphMat: High performance graph analytics made productive,” Proceedings of the VLDB Endowment, 2015.
[19] J. Shun and G. E. Blelloch, “Ligra: a lightweight graph processing framework for shared memory,” Symposium on Principles and Practice of Parallel Programming (PPoPP), 2013.
[20] A. Buluc ̧, J. T. Fineman, M. Frigo, J. R. Gilbert, and C. E. Leiserson, “Parallel sparse matrix-vector and matrix- transpose-vector multiplication using compressed sparse blocks,” in Symposium on Parallelism in Algorithms and Architectures (SPAA), 2009, pp. 233–244.
[21] S.Beamer,K.Asanovic ́,andD.A.Patterson,“GAIL:The graph algorithm iron law,” Workshop on Irregular Applica- tions: Architectures and Algorithms (IA3), at the International Conference for High Performance Computing, Networking, Storage and Analysis (SC), 2015.
[22] O. Polychroniou and K. A. Ross, “A comprehensive study of main-memory partitioning and its application to large- scale comparison-and radix-sort,” in SIGMOD International Conference on Management of Data, 2014, pp. 755–766.
[23] F. M. Schuhknecht, P. Khanchandani, and J. Dittrich, “On the surprising difficulty of simple things: the case of radix partitioning,” Proceedings of the VLDB Endowment, vol. 8, no. 9, pp. 934–937, 2015.
[24] J. Wassenberg and P. Sanders, “Engineering a multi-core radix sort,” in Euro-Par Parallel Processing, 2011, pp. 160–169.
[25] “Intel 64 and IA-32 architectures optimization reference man- ual,” Intel Corporation, September 2015.
[26] S. Williams, L. Oliker, R. Vuduc, J. Shalf, K. Yelick, and J. Demmel, “Optimization of sparse matrix-vector multiplica- tion on emerging multicore platforms,” Parallel Computing, vol. 35, no. 3, pp. 178–194, 2009.
[27] M. A. Bender, G. S. Brodal, R. Fagerberg, R. Jacob, and E. Vicari, “Optimal sparse matrix dense vector multiplication in the I/O-model,” Symposium on Parallelism in Algorithms and Architectures (SPAA), 2007.
[28] E. Cuthill and J. McKee, “Reducing the bandwidth of sparse symmetric matrices,” in ACM National Conference, 1969.
[29] N. E. Gibbs, W. G. Poole, and P. K. Stockmeyer, “An algorithm for reducing the bandwidth and profile of a sparse matrix,” SIAM Journal on Numerical Analysis, vol. 13, no. 2, pp. 236–250, 1976.
[30] P. Boldi and S. Vigna, “The WebGraph framework I: Com- pression techniques,” in International World Wide Web Con- ference (WWW), 2004, pp. 595–601.
[31] A.-J. N. Yzelman and R. H. Bisseling, “A cache-oblivious sparse matrix–vector multiplication scheme based on the hilbert curve,” in Progress in Industrial Mathematics at ECMI. Springer, 2012, pp. 627–633.
[32] F. McSherry, M. Isard, and D. G. Murray, “Scalability! but at what COST?” Hot Topics in Operating Systems, 2015.
[33] J. Park, M. Penner, and V. Prasanna, “Optimizing graph algorithms for improved cache performance,” Transactions on Parallel and Distributed Systems, pp. 769–782, 2004.
[34] W. Xie, G. Wang, D. Bindel, A. Demers, and J. Gehrke, “Fast iterative graph computation with block updates,” Proceedings of the VLDB Endowment, vol. 6, no. 14, 2013.
[35] V. Kiriansky, Y. Zhang, and S. Amarasinghe, “Optimizing indirect memory references with milk,” International Confer- ence on Parallel Architectures and Compilation Techniques (PACT), pp. 299–312, 2016.
[36] Y. Zhang, V. Kiriansky, C. Mendis, M. Zaharia, and S. Ama- rasinghe, “Optimizing cache performance for graph analyt- ics,” arXiv:1608.01362, August 2016.
[37] D. Buono, F. Petrini, F. Checconi, X. Liu, X. Que, C. Long, and T.-C. Tuan, “Optimizing sparse matrix-vector multiplica- tion for large-scale data analytics,” International Conference on Supercomputing (ICS), 2016.
[38] A. Azad and A. Buluc ̧, “A work-efficient parallel sparse matrix-sparse vector multiplication algorithm,” International Parallel & Distributed Processing Symposium (IPDPS), 2017.
[39] S. Beamer, “Understanding and improving graph algorithm performance,” Ph.D. dissertation, University of California, Berkeley, 2016.
[40] S. Kamil, D. Coetzee, S. Beamer, H. Cook, E. Gonina, J. Harper, J. Morlan, and A. Fox, “Portable parallel per- formance from sequential, productive, embedded domain- specific languages,” Symposium on Principles and Practice of Parallel Programming (PPoPP), 2012.
