[1] P. Chi et al., “Prime: A novel processing-in-memory
architecture for neural network computation in reram-based
main memory,” in Proc. ISCA, 2016.
[2] A. Shafiee et al., “Isaac: A convolutional neural network
accelerator with in-situ analog arithmetic in crossbars,” in
Proc. ISCA, 2016.
[3] M. M. Waldrop, “The chips are down for moore’s law,”
Nature News, vol. 530, no. 7589, p. 144, 2016.
[4] K. Jarrett et al., “What is the best multi-stage architecture
for object recognition?,” in Proc. ICCV, 2009.
[5] H. Larochelle et al., “An empirical evaluation of deep
architectures on problems with many factors of variation,” in
Proc. ICML, 2007.
[6] Q. V. Le, “Building high-level features using large scale
unsupervised learning,” in Proc. ICASSP, 2013.
[7] P. Sermanet et al., “Overfeat: Integrated recognition,
localization and detection using convolutional networks,”
ArXiv Preprint arXiv:1312.6229, 2013.
[8] C. Farabet et al., “Learning hierarchical features for scene
labeling,” Pattern Analysis and Machine Intelligence, IEEE
Transactions on, vol. 35, no. 8, pp. 1915–1929, 2013.
[9] S. Antol et al., “Vqa: Visual question answering,” in Proc.
ICCV, 2015.
[10] K. Simonyan et al., “Very deep convolutional networks for
large-scale image recognition,” CoRR, vol. abs/1409.1556,
2014.
[11] V. Mnih et al., “Human-level control through deep
reinforcement learning,” Nature, vol. 518, no. 7540,
pp. 529–533, 2015.
[12] D. Silver et al., “Mastering the game of go with deep neural
networks and tree search,” Nature, vol. 529, no. 7587,
pp. 484–489, 2016.
[13] A. Krizhevsky et al., “Imagenet classification with deep
convolutional neural networks,” in Advances in NIPS, 2012.
[14] A. Farmahini-Farahani et al., “Nda: Near-dram acceleration
architecture leveraging commodity dram devices and
standard memory modules,” in Proc. HPCA, 2015.
[15] H.-S. P. Wong et al., “Metal–oxide rram,” Proceedings of the
IEEE, vol. 100, no. 6, pp. 1951–1970, 2012.
[16] X. Liu et al., “Reno: a high-eﬃcient reconfigurable
neuromorphic computing accelerator design,” in Proc. DAC,
2015.
[17] O. Russakovsky et al., “ImageNet Large Scale Visual
Recognition Challenge,” IJCV, vol. 115, no. 3, pp. 211–252,
2015.
[18] Y. LeCun et al., “The mnist database of handwritten digits,”
1998.
[19] X. Dong et al., “Nvsim: A circuit-level performance, energy,
and area model for emerging non-volatile memory,” in
Emerging Memory Technologies, pp. 15–50, Springer, 2014.
[20] H. Lee et al., “Convolutional deep belief networks for
scalable unsupervised learning of hierarchical
representations,” in Proc. ICML, 2009.
[21] D. Ciresan et al., “Multi-column deep neural networks for
image classification,” in Proc. CVPR, 2012.
[22] D. C. Ciresan et al., “Flexible, high performance
convolutional neural networks for image classification,” in
Proc. IJCAI, 2011.
[23] P. Sermanet et al., “Convolutional neural networks applied
to house numbers digit classification,” in Proc. ICPR, 2012.
[24] M. Oquab et al., “Learning and transferring mid-level image
representations using convolutional neural networks,” in
Proc. CVPR, 2014.
[25] Y. LeCun et al., “Backpropagation applied to handwritten
zip code recognition,” Neural computation, vol. 1, no. 4,
pp. 541–551, 1989.
[26] Y. Kim, “Convolutional neural networks for sentence
classification,” arXiv preprint arXiv:1408.5882, 2014.
[27] A. G. Howard, “Some improvements on deep convolutional
neural network based image classification,” arXiv preprint
arXiv:1312.5402, 2013.
[28] Y. Gong et al., “Deep convolutional ranking for multilabel
image annotation,” arXiv preprint arXiv:1312.4894, 2013.
[29] R. Collobert et al., “A unified architecture for natural
language processing: Deep neural networks with multitask
learning,” in Proc. ICML, 2008.
[30] O. Abdel-Hamid et al., “Applying convolutional neural
networks concepts to hybrid nn-hmm model for speech
recognition,” in Proc. ICASSP, 2012.
[31] N. Kalchbrenner et al., “A convolutional neural network for
modelling sentences,” arXiv preprint arXiv:1404.2188, 2014.
[32] L. Deng et al., “New types of deep neural network learning
for speech recognition and related applications: An
overview,” in Proc. ICASSP, 2013.
[33] A. Graves et al., “Speech recognition with deep recurrent
neural networks,” in Proc. ICASSP, 2013.
[34] Y. LeCun et al., “Gradient-based learning applied to
document recognition,” Proc. IEEE, 1998.
[35] M.-J. Lee et al., “A fast, high-endurance and scalable
non-volatile memory device made from asymmetric ta2o5x/tao2- x bilayer structures,” Nature materials, vol. 10,
no. 8, pp. 625–630, 2011.
[36] C. Hsu et al., “Self-rectifying bipolar taox/tio2 rram with
superior endurance over 1012 cycles for 3d high-density
storage-class memory vlsi tech,” in Proc. VLSIT, 2013.
[37] M. K. Qureshi et al., “Enhancing lifetime and security of
pcm-based main memory with start-gap wear leveling,” in
Proc. MICRO, 2009.
[38] S. Yu et al., “3d vertical rram-scaling limit analysis and
demonstration of 3d array operation,” in Proc. VLSIT, 2013.
[39] C. Xu et al., “Architecting 3d vertical resistive memory for
next-generation storage systems,” in Proc. ICCAD, 2014.
[40] S. Yu et al., “Investigating the switching dynamics and
multilevel capability of bipolar metal oxide resistive
switching memory,” Applied Physics Letters, vol. 98, no. 10,
p. 103514, 2011.
[41] M.-C. Wu et al., “A study on low-power, nanosecond
operation and multilevel bipolar resistance switching in
ti/zro2/pt nonvolatile memory with 1t1r architecture,”
Semiconductor Science and Technology, vol. 27, no. 6,
p. 065010, 2012.
[42] F. Alibart et al., “High precision tuning of state for
memristive devices by adaptable variation-tolerant
algorithm,” Nanotechnology, vol. 23, no. 7, p. 075201, 2012.
[43] M. Hu et al., “Dot-product engine for neuromorphic
computing: programming 1t1m crossbar to accelerate
matrix-vector multiplication,” in Proc. DAC, 2016.
[44] Y. Chen et al., “Dadiannao: A machine-learning
supercomputer,” in Proc. MICRO, 2014.
[45] Y. Jia et al., “Caﬀe: Convolutional architecture for fast
feature embedding,” in Proc. MM, 2014.
[46] D. Niu et al., “Design trade-oﬀs for high density cross-point
resistive memory,” in Proc. ISLPED, 2012.
[47] R. Fackenthal et al., “19.7 a 16gb reram with 200mb/s write
and 1gb/s read in 27nm technology,” in Proc. ISSCC, 2014.
[48] C. E. Kozyrakis et al., “Scalable processors in the
billion-transistor era: Iram,” Computer, vol. 30, no. 9,
pp. 75–78, 1997.
[49] J. Draper et al., “The architecture of the diva
processing-in-memory chip,” in Proc. ICS, 2002.
[50] M. Gokhale et al., “Processing in memory: The terasys
massively parallel pim array,” Computer, vol. 28, no. 4,
pp. 23–31, 1995.
[51] D. Elliott et al., “Computational ram: The case for simd
computing in memory,” in Workshop on Mixing Logic and
DRAM: Chips that Compute and Remember at ISCA, 1997.
[52]T. Yamauchi et al., “A single chip multiprocessor integrated
with dram,” in Workshop on Mixing Logic and DRAM at
ISCA, 1997.
[53] M. Oskin et al., Active pages: A computation model for
intelligent memory, vol. 26. IEEE Computer Society, 1998.
[54] R. Balasubramonian et al., “Near-data processing: Insights
from a micro-46 workshop,” Micro, IEEE, vol. 34, no. 4,
pp. 36–42, 2014.
[55] D. Zhang et al., “Top-pim: throughput-oriented
programmable processing in memory,” in Proc. HPDC, 2014.
[56] R. Nair et al., “Active memory cube: A processing-inmemory architecture for exascale systems,” IBM Journal of
Research and Development, vol. 59, no. 2/3, pp. 17–1, 2015.
[57] B. Akin et al., “Data reorganization in memory using
3d-stacked dram,” in ACM SIGARCH Computer
Architecture News, vol. 43, pp. 131–143, ACM, 2015.
[58] J. Ahn et al., “A scalable processing-in-memory accelerator
for parallel graph processing,” in Proc. ISCA, 2015.
[59] Q. Guo et al., “A resistive tcam accelerator for
data-intensive computing,” in Proc. ISCA, 2011.
[60] Q. Guo et al., “Ac-dimm: associative computing with
stt-mram,” in ACM SIGARCH Computer Architecture
News, vol. 41, pp. 189–200, ACM, 2013.
[61] Z. Chen et al., “Optimized learning scheme for grayscale
image recognition in a rram based analog neuromorphic
system,” in Proc. IEDM, 2015.
[62] B. Li et al., “Memristor-based approximated computation,”
in Proc. ISLPED, 2013.
[63] S. Chetlur et al., “cudnn: Eﬃcient primitives for deep
learning,” arXiv preprint arXiv:1410.0759, 2014.
[64] D. Mahajan et al., “Tabla: A unified template-based
framework for accelerating statistical machine learning,” in
Proc. HPCA, 2016.
[65] T. Chen et al., “Diannao: A small-footprint high-throughput
accelerator for ubiquitous machine-learning,” in ACM
Sigplan Notices, vol. 49, pp. 269–284, ACM, 2014.
[66] D. Liu et al., “Pudiannao: A polyvalent machine learning
accelerator,” in Proc. ASPLOS, 2015.
[67] Z. Du et al., “Shidiannao: shifting vision processing closer to
the sensor,” in Proc. ISCA, 2015.
[68] D. Kim et al., “Neurocube: A programmable digital
neuromorphic architecture with high-density 3d memory,” in
Proc. ISCA, 2016.