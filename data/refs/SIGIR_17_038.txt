[1] Ricardo Baeza-Yates, Berthier Ribeiro-Neto, and others. 1999. Modern information retrieval. Vol. 463.
[2] Sergey Bartunov, Dmitry Kondrashkin, Anton Osokin, and Dmitry Vetrov. 2015. Breaking Sticks and Ambiguities with Adaptive Skip-gram. arXiv preprint arXiv:1502.07257 (2015).
[3] Yoshua Bengio. 2009. Learning deep architectures for AI. Foundations and Trends® in Machine Learning 2, 1 (2009), 1–127.
[4] Yoshua Bengio, Re ́jean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research 3 (2003), 1137–1155.
[5] David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet alloca- tion. Journal of Machine Learning Research 3 (2003), 993–1022.
[6] Jonathan Chang, Sean Gerrish, Chong Wang, Jordan L Boyd-Graber, and David M Blei. 2009. Reading tea leaves: How humans interpret topic models. In Proceedings of NIPS. 288–296.
[7] Ronan Collobert and Jason Weston. 2008. A unified architecture for natural lan- guage processing: Deep neural networks with multitask learning. In Proceedings of ICML. 160–167.
[8] Ronan Collobert, Jason Weston, Le ́on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research 12 (2011), 2493–2537.
[9] Rajarshi Das, Manzil Zaheer, and Chris Dyer. 2015. Gaussian lda for topic models with word embeddings. In Proceedings of ACL. 795–804.
[10] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research 9 (2008), 1871–1874.
[11] Thomas Hofmann. 1999. Probabilistic latent semantic indexing. In Proceedings of SIGIR. 50–57.
[12] Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of ACL. 873–882.
[13] Ignacio Iacobacci, Mohammad Taher Pilehvar, and Roberto Navigli. 2016. Em- beddings for word sense disambiguation: An evaluation study. In Proceedings of ACL. 897–907.
[14] Jey Han Lau, David Newman, and Timothy Baldwin. 2014. Machine Reading Tea Leaves: Automatically Evaluating Topic Coherence and Topic Model Quality.. In 
[15] Quoc V Le and Tomas Mikolov. 2014. Distributed Representations of Sentences
and Documents.. In Proceedings of ICML. 1188–1196.
[16] Chenliang Li, Haoran Wang, Zhiqian Zhang, Aixin Sun, and Zongyang Ma. 2016.
Topic Modeling for Short Texts with Auxiliary Word Embeddings. In Proceedings
of SIGIR. 165–174.
[17] Shaohua Li, Tat-Seng Chua, Jun Zhu, and Chunyan Miao. 2016. Generative topic
embedding: a continuous representation of documents. In Proceedings of ACL.
666–675.
[18] Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2015. Learning context-sensitive
word embeddings with neural tensor skip-gram model. In Proceedings of IJCAI.
1284–1290.
[19] Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. 2015. Topical Word
Embeddings.. In Proceedings of AAAI. 2418–2424.
[20] Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.
Journal of Machine Learning Research 9 (2008), 2579–2605.
[21] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases and their compositionality. In
Advances in Neural Information Processing Systems. 3111–3119.
[22] Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic Regularities in Continuous Space Word Representations.. In Proceedings of NAACL. 746–751.
[23] Frederic Morin and Yoshua Bengio. 2005. Hierarchical Probabilistic Neural
Network Language Model. Proceedings of AISTATS (2005), 246.
[24] Arvind Neelakantan, Jeevan Shankar, Alexandre Passos, and Andrew McCallum. 2015. Efficient non-parametric estimation of multiple embeddings per word in
vector space. In Proceedings of EMNLP. 1059–1069.
[25] David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. 2010. Auto- matic evaluation of topic coherence. In Proceedings of NAACL. 100–108.
[26] Dat Quoc Nguyen, Richard Billingsley, Lan Du, and Mark Johnson. 2015. Im- proving Topic Models with Latent Feature Word Representations. TACL 3 (2015), 299–313.
[27] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global Vectors for Word Representation. In Proceedings of EMNLP. 1532–1543.
[28] Joseph Reisinger and Raymond J Mooney. 2010. Multi-prototype vector-space models of word meaning. In Proceedings of NAACL. 109–117.
[29] Navid Rekabsaz. 2016. Enhancing Information Retrieval with Adapted Word Embedding. In Proceedings of SIGIR. 1169–1169.
[30] Yafeng Ren, Yue Zhang, Meishan Zhang, and Donghong Ji. 2016. Improving Twitter Sentiment Classification Using Topic-Enriched Multi-Prototype Word Embeddings.. In Proceedings of AAAI. 3038–3044.
[31] Michael Roth and Mirella Lapata. 2016. Neural semantic role labeling with dependency path embeddings. (2016), 1192–1202.
[32] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1988. Learning representations by back-propagating errors. Cognitive modeling 5, 3 (1988), 1.
[33] Bahar Salehi, Paul Cook, and Timothy Baldwin. 2015. A Word Embedding Approach to Predicting the Compositionality of Multiword Expressions.. In Proceedings of NAACL. 977–983.
[34] Cyrus Shaoul. 2010. The westbury lab wikipedia corpus. Edmonton, AB: University of Alberta (2010).
[35] Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. 2011. Parsing nat- ural scenes and natural language with recursive neural networks. In Proceedings of ICML. 129–136.
[36] Keith Stevens, Philip Kegelmeyer, David Andrzejewski, and David Buttler. 2012. Exploring topic coherence over many models and many topics. In Proceedings of EMNLP. 952–961.
[37] Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang, Enhong Chen, and Tie-Yan Liu. 2014. A Probabilistic Model for Learning Multi-Prototype Word Embeddings.. In Proceedings of COLING. 151–160.
[38] Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of ACL. 384–394.
[39] Peter D Turney, Patrick Pantel, and others. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research 37, 1 (2010), 141–188.
[40] Hanna M Wallach. 2006. Topic modeling: beyond bag-of-words. In Proceedings of ICML. 977–984.
[41] Xuerui Wang, Andrew McCallum, and Xing Wei. 2007. Topical n-grams: Phrase and topic discovery, with an application to information retrieval. In Proceedings of ICDM. 697–702.
