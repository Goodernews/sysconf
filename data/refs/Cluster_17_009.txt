[1] J. Dean and S. Ghemawat, “Mapreduce: simplified data processing on
large clusters,” Communications of the ACM, vol. 51, no. 1, pp. 107113, 2008.

[2] H. Jin, S. Ibrahim, L. Qi, H. Cao, S. Wu, and X. Shi, “The MapReduce
Programming Model and Implementations,” Cloud Computing: Principles and Paradigms., pp. 373-390, 2011.

[3] “CMU OpenCloud Hadoop Cluster,” http://ftp.pdl.cmu.edu/pub/
datasets/hla/dataset.html, Accessed on Feb 2017.

[4] NICS, “Kraken Cray XT5 system, http://www.nics.tennessee.edu/
computing-resources/kraken.”

[5] NCSA, “BlueWaters project, http://www.ncsa.illinois.edu/Blue Waters/.”

[6] “Big Data and Extreme-scale Computing (BDEC) Workshop, http://
www.exascale.org/bdec/.”

[7] G. Fox, J. Qiu, 8. Jha, S. Ekanayake, and S. Kamburugamuve, “Big
Data, simulations and HPC convergence,” in Workshop on Big Data
Benchmarks. Springer, 2015, pp. 3-17.

[8] Y. Guo, W. Bland, P. Balaji, and X. Zhou, “Fault tolerant MapReduceMPI for HPC clusters,” in International Conference for High Performance Computing, Networking, Storage and Analysis. ACM, 2015,
p. 34.

[9] S. Venkataraman, A. Panda, G. Ananthanarayanan, M. J. Franklin, and
I. Stoica, “The power of choice in data-aware cluster scheduling,” in
International Conference on Operating Systems Design and Implementation. USENIX Association, 2014, pp. 301-316.

[10] G. Ananthanarayanan, S. Agarwal, S. Kandula, A. Greenberg, I. Stoica,
D. Harlan, and E. Harris, “Scarlett: coping with skewed content popularity in MapReduce clusters,” in European Conference on Computer
Systems. ACM, 2011, pp. 287-300.

[11] S. Ibrahim, H. Jin, L. Lu, B. He, G. Antoniu, and S. Wu, “Maestro:
Replica-aware map scheduling for MapReduce,” in International Symposium on Cluster, Cloud and Grid Computing. TEEE/ACM, 2012,
pp. 435-442.

[12] O. Yildiz, M. Dorier, 8. Ibrahim, R. Ross, and G. Antoniu, “On the root
causes of cross-application I/O interference in HPC storage systems,” in
International Parallel and Distributed Processing Symposium. TEEE,
2016, pp. 750-759.

[13] N. S. Islam, M. Wasi-ur Rahman, X. Lu, and D. K. Panda, “High
performance design for HDFS with byte-addressability of NVM and
RDMA,.” in International Conference on Supercomputing. ACM, 2016,
p. 8.

[14] N. Chaimov, A. Malony, S. Canon, C. Iancu, K. Z. Ibrahim, and J. Srinivasan, “Scaling Spark on HPC systems,” in International Symposium on
High-Performance Parallel and Distributed Computing. ACM, 2016,
pp. 97-110.

[15] K. Sato, K. Mohror, A. Moody, T. Gamblin, B. R. de Supinski,
N. Maruyama, and S. Matsuoka, “A user-level infiniband-based file
system and checkpoint strategy for burst buffers,” in International
Symposium on Cluster, Cloud and Grid Computing. MYEEE, 2014, pp.
21-30.

[16] Y. Wang, R. Goldstone, W. Yu, and T. Wang, “Characterization and
optimization of memory-resident MapReduce on HPC systems,” in
International Parallel and Distributed Processing Symposium. TEEE,
2014, pp. 799-808.

[17] “Hadoop Workload Analysis,” http://www.pdl.cmu.edu/HLA/index.
shtml, Accessed on Jan 2017.

[18] INRIA, “Grid’5000: http://www.grid5000.fr.”
R. B. Ross, R. Thakur et al., “PVFS: A parallel file system for Linux
clusters,” in Annual Linux Showcase and Conference, 2000, pp. 391430.

[19] “HiBench Big Data microbenchmark
intel-hadoop/HiBench.”

[20] H. Shan and J. Shalf, “Using IOR to Analyze the I/O Performance for
HPC Platforms,” in Cray User Group Conference, Seattle, WA, USA,
2007.

[21] M. Dorier, G. Antoniu, F. Cappello, M. Snir, R. Sisneros, O. Yildiz,
S. Ibrahim, T. Peterka, and L. Orf, “Damaris: Addressing performance
variability in data management for post-petascale simulations,” ACM
Transactions on Parallel Computing (TOPC), vol. 3, no. 3, p. 15, 2016.

[23] H. Abbasi, M. Wolf, G. Eisenhauer, $. Klasky, K. Schwan, and
F Zheng, “Datastager: scalable data staging services for petascale applications,” in International Symposium on High Performance Distributed
Computing. ACM, 2009, pp. 39-48.