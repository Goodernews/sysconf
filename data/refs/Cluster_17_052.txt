[1] S. Ashby, P. Beckman, J. Chen, P. Colella, B. Collins, D. Crawford,
J. Dongarra, D. Kothe, R. Lusk, P. Messina, and Others, “The opportunities and challenges of exascale computing.” summary report
of the advanced scientific computing advisory committee (ASCAC)
subcommittee at the US Department of Energy Office of Science, 2010.
[2] E. Deelman, T. Peterka, I. Altintas, C. Carothers, K. K. van
Dam, K. Moreland, M. Parashar, L. Ramakrishnan, M. Taufer,
and J. Vetter, “The Future of Scientific Workflows Report of the
DOE NGNS/CS Scientific Workflows Workshop,” Tech. Rep., April
2015. [Online]. Available: http://science.energy.gov/∼/media/ascr/pdf/
programdocuments/docs/workflows final report.pdf
[3] F. Zheng, H. Zou, G. Eisenhauer, K. Schwan, M. Wolf, J. Dayal,
T.-A. Nguyen, J. Cao, H. Abbasi, S. Klasky, N. Podhorszki, and
H. Yu, “FlexIO: I/O Middleware for Location-Flexible Scientific Data
Analytics,” in Parallel Distributed Processing (IPDPS), 2013 IEEE 27th
International Symposium on, May 2013, pp. 320–331.
[4] M. Dorier, G. Antoniu, F. Cappello, M. Snir, and L. Orf, “Damaris:
How to Efficiently Leverage Multicore Parallelism to Achieve Scalable,
Jitter-free I/O,” in CLUSTER - IEEE International Conference on Cluster
Computing. IEEE, Sep. 2012.
[5] M. Dreher and T. Peterka, “Decaf: Decoupled dataflows for in situ
high-performance workflows,” in Submitted to 2017 IEEE International
Conference on Cluster Computing (CLUSTER), Sept 2017.
[6] M. Dreher and B. Raffin, “A Flexible Framework for Asynchronous
In Situ and In Transit Analytics for Scientific Simulations,” in 14th
IEEE/ACM International Symposium on Cluster, Cloud and Grid
Computing, Chicago, United States, May 2014. [Online]. Available:
https://hal.inria.fr/hal-00941413
[7] B. Whitlock, J. M. Favre, and J. S. Meredith, “Parallel In Situ
Coupling of Simulation with a Fully Featured Visualization System,” in
Proceedings of the 11th Eurographics Conference on Parallel Graphics
and Visualization, ser. EGPGV ’11. Aire-la-Ville, Switzerland,
Switzerland: Eurographics Association, 2011, pp. 101–109. [Online].
Available: http://dx.doi.org/10.2312/EGPGV/EGPGV11/101-109
[8] N. Fabian, K. Moreland, D. Thompson, A. Bauer, P. Marion, B. Geveci,
M. Rasquin, and K. Jansen, “The Paraview Coprocessing Library: a
Scalable, General Purpose In Situ Visualization Library,” in Large Data
Analysis and Visualization (LDAV), 2011 IEEE Symposium on, Oct 2011,
pp. 89–96.
[9] M. Wilde, M. Hategan, J. M. Wozniak, B. Clifford, D. S. Katz, and
I. Foster, “Swift: A Language for Distributed Parallel Scripting,” Parallel
Computing, vol. 37, no. 9, 2011.
[10] S. Plimpton, “Fast parallel algorithms for short-range molecular
dynamics,” Journal of Computational Physics, 1995. [Online].
Available: http://lammps.sandia.gov
[11] S. Pronk, S. Pall, R. Schulz, P. Larsson, P. Bjelkmar, R. Apostolov,
M. R. Shirts, J. C. Smith, P. M. Kasson, D. van der Spoel, B. Hess, and
E. Lindahl, “Gromacs 4.5: a high-throughput and highly parallel open
source molecular simulation toolkit,” Bioinformatics, vol. 29, no. 7, pp.
845–854, 2013.
[12] M. Dorier, R. Sisneros, Roberto, T. Peterka, G. Antoniu, and
B. Semeraro, Dave, “Damaris/Viz: a Nonintrusive, Adaptable and UserFriendly In Situ Visualization Framework,” in LDAV - IEEE Symposium
on Large-Scale Data Analysis and Visualization, Atlanta, United States,
Oct. 2013. [Online]. Available: https://hal.inria.fr/hal-00859603
[13] S. Atchley, D. Dillow, G. Shipman, P. Geoffray, J. M. Squyres,
G. Bosilca, and R. Minnich, “The common communication interface
(cci),” in High Performance Interconnects (HOTI), 2011 IEEE 19th
Annual Symposium on. IEEE, 2011, pp. 51–60.
[14] “MPI: A Message-Passing Interface Standard Version 3.0,” Tech.
Rep., Sep 2012. [Online]. Available: http://mpi-forum.org/docs/mpi-3.
0/mpi30-report.pdf
[15] T. Peterka, R. Ross, W. Kendall, A. Gyulassy, V. Pascucci, H.-W.
Shen, T.-Y. Lee, and A. Chaudhuri, “Scalable parallel building blocks
for custom data analysis,” in Proceedings of Large Data Analysis and
Visualization Symposium LDAV’11, Providence, RI, 2011.
[16] J. Soumagne, D. Kimpe, J. A. Zounmevo, and M. Chaarawi, “Mercury:
Enabling remote procedure call for high-performance computing,” in
Cluster 2013, Indianapolis, IN, 2013.
[17] P. Hintjens, “Zeromq, messaging for many applications,” 2013.
[18] “Nanomsg,” http://nanomsg.org.
[19] M. Dreher and T. Peterka, “Bredala: Semantic Data Redistribution for
In Situ Applications,” in CLUSTER - IEEE International Conference on
Cluster Computing. IEEE, Sep. 2016.
[20] C. Docan, M. Parashar, and S. Klasky, “DataSpaces: an Interaction
and Coordination Framework for Coupled Simulation Workflows,” in
Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing (HPDC ’10). New York, NY, USA:
ACM, 2010, pp. 25–36.
[21] ——, “Enabling high-speed asynchronous data extraction and transfer
using dart,” Concurrency and Computation: Practice and Experience,
vol. 22, no. 9, pp. 1181–1204, 2010. [Online]. Available: http:
//dx.doi.org/10.1002/cpe.1567
[22] Q. Liu, J. Logan, Y. Tian, H. Abbasi, N. Podhorszki, J. Y.
Choi, S. Klasky, R. Tchoua, J. Lofstead, R. Oldfield, M. Parashar,
N. Samatova, K. Schwan, A. Shoshani, M. Wolf, K. Wu, and W. Yu,
“Hello ADIOS: the Challenges and Lessons of Developing Leadership
Class I/O Frameworks,” Concurrency and Computation: Practice and
Experience, vol. 26, no. 7, pp. 1453–1473, 2014. [Online]. Available:
http://dx.doi.org/10.1002/cpe.3125
[23] H. Abbasi, G. Eisenhauer, M. Wolf, K. Schwan, and S. Klasky,
“Just in time: Adding value to the io pipelines of high performance
applications with jitstaging,” in Proceedings of the 20th International
Symposium on High Performance Distributed Computing, ser. HPDC
’11. New York, NY, USA: ACM, 2011, pp. 27–36. [Online]. Available:
http://doi.acm.org/10.1145/1996130.1996137
[24] J. Dayal, D. Bratcher, G. Eisenhauer, K. Schwan, M. Wolf, X. Zhang,
H. Abbasi, S. Klasky, and N. Podhorszki, “Flexpath: Type-based publish/subscribe system for large-scale science analytics,” in Cluster, Cloud
and Grid Computing (CCGrid), 2014 14th IEEE/ACM International
Symposium on, May 2014, pp. 246–255.
[25] G. Eisenhauer, M. Wolf, H. Abbasi, and K. Schwan, “Event-based
systems: Opportunities and challenges at exascale,” in Proceedings of
the Third ACM International Conference on Distributed Event-Based
Systems, ser. DEBS ’09.
[26] P. Malakar, V. Vishwanath, T. Munson, C. Knight, M. Hereld, S. Leyffer,
and M. E. Papka, “Optimal scheduling of in-situ analysis for large-scale
scientific simulations,” in Proceedings of the International Conference
for High Performance Computing, Networking, Storage and Analysis,
ser. SC ’15.
[27] P. Malakar, V. Vishwanath, C. Knight, T. Munson, and M. E. Papka,
“Optimal execution of co-analysis for large-scale molecular dynamics
simulations,” in Proceedings of the International Conference for High
Performance Computing, Networking, Storage and Analysis, ser. SC ’16.
[28] M. Dorier, R. Sisneros, L. B. Gomez, T. Peterka, L. Orf, L. Rahmani,
G. Antoniu, and L. Boug, “Adaptive performance-constrained in situ
visualization of atmospheric simulations,” in 2016 IEEE International
Conference on Cluster Computing (CLUSTER), Sept 2016.
Https://debyer.readthedocs.io.
[29] M. Dreher, M. Piuzzi, T. Ahmed, C. Matthieu, M. Baaden, N. Férey,
S. Limet, B. Raffin, and S. Robert, “Interactive Molecular Dynamics:
Scaling up to Large Systems,” in International Conference on
Computational Science, ICCS 2013. Barcelone, Spain: Elsevier, Jun.
2013. [Online]. Available: https://hal.inria.fr/hal-00809024
