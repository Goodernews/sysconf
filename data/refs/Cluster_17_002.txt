[1] “US National Science Foundation,” https://www.nsf.gov/.

[2] J. Towns, T. Cockerill, M. Dahan, I. Foster, K. Gaither,
A. Grimshaw, V. Hazlewood, S. Lathrop, D. Lifka, G. D.
Peterson et al., “XSEDE: Accelerating Scientific Discovery,’ Computing in Science & Engineering, vol. 16, no. 5,
pp. 62-74, 2014.

[3] “XDMoD: Comprehensive HPC System Management
Tool,” https://xdmod.ccr.buffalo.edu/.

[4] “MPI-3 Standard Document,’ http://www.mpi-forum.
org/docs/mpi-3.0/mpi30-report.pdf.

[5] The Open MPI Development Team, “Open MPI : Open
Source High Performance Computing,” http://www.openmpi.org.

[6] Intel Coporation, “Intel MPI Library,”
http://software.intel.com/en-us/intel-mpi-library/.

[7] MVAPICH2: MPI over InfiniBand, 10GigEAWARP and
RoCE, https://mvapich.cse.ohio-state.edu/.

[8] InfiniBand Trade
http://www.infinibandta.org/.

[9] M. S. Birrittella) M. Debbage, R. Huggahalli, J. Kunz,
T. Lovett, T. Rimmer, K. D. Underwood, and R. C.
Zak, “Intel® Omni-path Architecture: Enabling Scalable, High Performance Fabrics,” in High-Performance
Interconnects (HOTI), 2015 IEEE 23rd Annual Symposium on. TEEE, 2015, pp. 1-9.

[10] T. Ma, G. Bosilca, A. Bouteiller, B. Goglin, J. M.
Squyres, and J. J. Dongarra, “Kernel Assisted Collective
Intra-node MPI Communication among Multi-core and
Many-core CPUs,” in Parallel Processing (ICPP), 2011
International Conference on. TEEE, 2011, pp. 532-541.

[11] A. R. Mamidala, R. Kumar, D. De, and D. K. Panda,
“MPI Collectives on Modern Multicore Clusters: Performance Optimizations and Communication Characteristics,” in Cluster Computing and the Grid, 2008. CCGRID’08. 8th IEEE International Symposium on. TEEE,
2008, pp. 130-137.

[12] R. L. Graham and G. Shipman, “MPI Support for Multicore Architectures: Optimized Shared Memory Collectives,” in European Parallel Virtual Machine/Message
Passing Interface Users’ Group Meeting. Springer,
2008, pp. 130-140.

[13] S. Sistare, R. Vaart, and E. Loh, “Optimization of MPI
Collectives on Clusters of Large-scale SMP’s,” in Supercomputing, ACM/IEEE 1999 Conference. EEE, 1999,
pp. 23-23.

[14] G. Alméasi, P. Heidelberger, C. J. Archer, X. Martorell,
C. C. Erway, J. E. Moreira, B. Steinmacher-Burow, and
Y. Zheng, “Optimization of MPI Collective Communication on BlueGene/L systems,” in Proceedings of the
19th annual international conference on Supercomputing. ACM, 2005, pp. 253-262.

[15] H. Zhu, D. Goodell, W. Gropp, and R. Thakur, “Hierarchical Collectives in MPICH2,” in European Parallel Virtual Machine/Message Passing Interface Users’ Group
Meeting. Springer, 2009, pp. 325-326.

[16] S. Li, T. Hoefler, and M. Snir, “NUMA-aware Sharedmemory Collective Communication for MPI,” in Proceedings of the 22nd international symposium on Highperformance parallel and distributed computing. ACM,
2013, pp. 85-96.

[17] H.-W. Jin, S. Sur, L. Chai, and D. K. Panda, “LiMIC:
Support for High-Performance MPI Intra-Node Communication on Linux Cluster,” in Parallel Processing, 2005.
ICPP 2005. International Conference on. TEEE, 2005,
pp. 184-191.

[18] B. Goglin and S. Moreaud, “KNEM: A Generic and
Scalable Kernel-Assisted Intra-Node MPI Communication Framework,” Journal of Parallel and Distributed
Computing, vol. 73, no. 2, pp. 176-188, 2013.

[19] J. Vienne, “Benefits of Cross Memory Attach for MPI
libraries on HPC Clusters,” in Proceedings of the 2014
Annual Conference on Extreme Science and Engineering
Discovery Environment. ACM, 2014, p. 33.

[20] T. Bird, “Measuring Function Duration with Ftrace,” in
Proceedings of the Linux Symposium, 2009, pp. 47-54.

[21] R. Thakur, R. Rabenseifner, and W. Gropp, “Optimization of Collective Communication Operations in
MPICH,” The International Journal of High Performance
Computing Applications, vol. 19, no. 1, pp. 49-66, 2005.

[22] D. W. Marquardt, “An Algorithm for Least-squares Estimation of Nonlinear Parameters,” Journal of the society
for Industrial and Applied Mathematics, vol. 11, no. 2,
pp. 431-441, 1963.

[23] J. Bruck, D. Dolev, C.-T. Ho, M.-C. Rosu, and R. Strong,
“Efficient Message Passing Interface (MPI) for Parallel
Computing on Clusters of Workstations,” in Proceedings
of the seventh annual ACM symposium on Parallel algorithms and architectures. ACM, 1995, pp. 64-73.

[24] E. W. Chan, M. F. Heimlich, A. Purkayastha, and R. A.
Van De Geijn, “On Optimizing Collective Communication,” in Cluster Computing, 2004 IEEE International
Conference on. TEEE, 2004, pp. 145-155.

[25] L. Chai, P. Lai, H.-W. Jin, and D. K. Panda, “Designing an Efficient Kernel-level and User-level Hybrid
Approach for MPI Intra-node Communication on Multicore Systems,” in Parallel Processing, 2008. ICPP’08.
37th International Conference on. TEEE, 2008, pp. 222229.

[26] S. Moreaud, B. Goglin, D. Goodell, and R. Namyst, “Optimizing MPI Communication within Large Multicore
Nodes with Kernel Assistance,” in Workshop on Communication Architecture for Clusters, held in conjunction
with IPDPS 2010, 2010, pp. 7-p.

[27] D. Buntinas, B. Goglin, D. Goodell, G. Mercier, and
S. Moreaud, “Cache-efficient, Intranode, Large-message
MPI Communication with MPICH2-Nemesis,” in Parallel Processing, 2009. ICPP’09. International Conference
on. TEEE, 2009, pp. 462-469.

[28] R. Brightwell and K. Pedretti, “Optimizing Multi-core
MPI Collectives with SMARTMAP,” in Parallel Processing Workshops, 2009. ICPPW’09. International Conference on. TEEE, 2009, pp. 370-377.

[29] M. Woodacre, D. Robb, D. Roe, and K. Feind, “The SGI
AltixTM 3000 Global Shared Memory Architecture,”
Silicon Graphics, Inc.(2003), 2005.

[30] C. Yeoh, “Cross Memory Attach,” https://lwn.net/
Articles/405284/.

[31] R. L. Graham and G. Shipman, “MPI Support for Multi
core Architectures: Optimized Shared Memory Collectives,” Lecture Notes in Computer Science, vol. 5205, p.
130, 2008.

[32] B. Tu, M. Zou, J. Zhan, X. Zhao, and J. Fan, “Multicore Aware Optimization for MPI Collectives,” in Cluster
Computing, 2008 IEEE International Conference on.
TEEE, 2008, pp. 322-325.

[33] T. Ma, G. Bosilca, A. Bouteiller, and J. Dongarra, “HierKNEM: an Adaptive Framework for Kernel-assisted
and Topology-aware Collective Communications on
Many-core Clusters,” in Parallel & Distributed Processing Symposium (IPDPS), 2012 IEEE 26th International.
TEEE, 2012, pp. 970-982.