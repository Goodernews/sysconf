[1] M. P. I. Forum, “MPI-2: Extensions to the Message-Passing Interface,”
July 1997, http://www.mpi-forum.org/docs/docs.html.
[2] R. Thakur, W. Gropp, and E. Lusk, “A case for using MPIs derived
datatypes to improve I/O performance,” in Proceedings of SC98: High
Performance Networking and Computing. ACM Press, November
1998. [Online]. Available: http://www.mcs.anl.gov/ thakur/dtype/
[3] H. Luu, M. Winslett, W. Gropp, R. Ross, P. Carns, K. Harms,
M. Prabhat, S. Byna, and Y. Yao, “A multiplatform study of I/O
behavior on petascale supercomputers,” in Proceedings of the 24th
International Symposium on High-Performance Parallel and Distributed
Computing, ser. HPDC ’15. New York, NY, USA: ACM, 2015, pp. 33–
44. [Online]. Available: http://doi.acm.org/10.1145/2749246.2749269
[4] S. Byna, R. Sisneros, K. Chadalavada, and Q. Koziol, “Tuning
parallel I/O on Blue Waters for writing 10 trillion particles,” in
”Cray User Group (CUG) meeting”, Apr 2015. [Online]. Available:
https://sdm.lbl.gov/ sbyna/research/papers/201504-CUG-VPICBW.pdf
[5] M. S. Breitenfeld, K. Chadalavada, R. Sisneros, S. Byna, Q. Koziol,
N. Fortner, Prabhat, and V. Vishwanath, “Recent progress in tuning
performance of large-scale I/O with parallel HDF5,” 11 2014.
[Online]. Available: http://www.pdsw.org/pdsw14/wips/breitenfeld-wippdsw14.pdf
[6] H. Bui, H. Finkel, V. Vishwanath, S. Habib, K. Heitmann, J. Leigh,
M. Papka, and K. Harms, “Scalable parallel I/O on a Blue Gene/Q
supercomputer using compression, topology-aware data aggregation,
and subﬁling,” in 2014 22nd Euromicro International Conference on
Parallel, Distributed, and Network-Based Processing, Feb 2014, pp.
107–111.
[7] F. Schmuck and R. Haskin, “GPFS: A shared-disk ﬁle system
for large computing clusters,” in Proceedings of the 1st USENIX
Conference on File and Storage Technologies, ser. FAST ’02.
Berkeley, CA, USA: USENIX Association, 2002. [Online]. Available:
http://dl.acm.org/citation.cfm?id=1083323.1083349
[8] “Lustre ﬁlesystem website,” http://lustre.org/.
[9] M. Chaarawi, S. Chandok, and E. Gabriel, Performance Evaluation of
Collective Write Algorithms in MPI I/O. Berlin, Heidelberg: Springer
Berlin Heidelberg, 2009, pp. 185–194.
[10] V. Venkatesan, R. Anand, J. Subhlok, and E. Gabriel, “Optimized
process placement for collective I/O operations,” in Proceedings of
the 20th European MPI Users’ Group Meeting, ser. EuroMPI ’13.
New York, NY, USA: ACM, 2013, pp. 31–36. [Online]. Available:
http://doi.acm.org/10.1145/2488551.2488567
[11] F. Isaila, P. Balaprakash, S. M. Wild, D. Kimpe, R. Latham, R. Ross,
and P. D. Hovland, “Collective I/O tuning using analytical and machinelearning models,” in IEEE Cluster 2015, IEEE. Chicago, IL: IEEE,
09/2015 2015.
[12] J. M. del Rosario, R. Bordawekar, and A. Choudhary, “Improved
parallel I/O via a two-phase run-time access strategy,” SIGARCH
Comput. Archit. News, vol. 21, no. 5, pp. 31–38, Dec. 1993. [Online].
Available: http://doi.acm.org/10.1145/165660.165667
[13] W. Gropp, “MPICH2: A new start for MPI implementations,” in
Proceedings of the 9th European PVM/MPI Users’ Group Meeting on
Recent Advances in Parallel Virtual Machine and Message Passing
Interface. London, UK, UK: Springer-Verlag, 2002, pp. 7–. [Online].
Available: http://dl.acm.org/citation.cfm?id=648139.749473
[14] R. Thakur, W. Gropp, and E. Lusk, “Data sieving and collective
I/O in ROMIO,” in Proceedings of the The 7th Symposium on the
Frontiers of Massively Parallel Computation, ser. FRONTIERS ’99.
Washington, DC, USA: IEEE Computer Society, 1999, pp. 182–.
[Online]. Available: http://dl.acm.org/citation.cfm?id=795668.796733
[15] ——, “On implementing MPI-IO portably and with high performance,”
in Proceedings of the Sixth Workshop on I/O in Parallel and Distributed
Systems, ser. IOPADS ’99. New York, NY, USA: ACM, 1999, pp.
23–32. [Online]. Available: http://doi.acm.org/10.1145/301816.301826
[16] ——, “Optimizing noncontiguous accesses in MPI I/O,” Parallel
Comput., vol. 28, no. 1, pp. 83–105, Jan. 2002. [Online]. Available:
http://dx.doi.org/10.1016/S0167-8191(01)00129-6
[17] Y. Tsujita, H. Muguruma, K. Yoshinaga, A. Hori, M. Namiki, and
Y. Ishikawa, “Improving collective I/O performance using pipelined
two-phase I/O,” in Proceedings of the 2012 Symposium on High
Performance Computing, ser. HPC ’12. San Diego, CA, USA: Society
for Computer Simulation International, 2012, pp. 7:1–7:8. [Online].
Available: http://dl.acm.org/citation.cfm?id=2338816.2338823
[18] Y. Tsujita, K. Yoshinaga, A. Hori, M. Sato, M. Namiki, and Y. Ishikawa,
“Multithreaded two-phase I/O: Improving collective MPI-IO performance on a Lustre ﬁle system,” in 2014 22nd Euromicro International
Conference on Parallel, Distributed, and Network-Based Processing,
Feb 2014, pp. 232–235.
[19] M. Chaarawi and E. Gabriel, “Automatically selecting the number of
aggregators for collective I/O operations,” in 2011 IEEE International
Conference on Cluster Computing (CLUSTER), Austin, TX, USA,
September 26-30, 2011, 2011, pp. 428–437. [Online]. Available:
http://dx.doi.org/10.1109/CLUSTER.2011.79
[20] R. Filgueira, D. E. Singh, J. C. Pichel, F. Isaila, and J. Carretero,
“Data locality aware strategy for two-phase collective I/O,” in High
Performance Computing for Computational Science - VECPAR 2008,
8th International Conference, Toulouse, France, June 24-27, 2008.
Revised Selected Papers, 2008, pp. 137–149. [Online]. Available:
http://dx.doi.org/10.1007/978-3-540-92859-1 14
[21] H. Bui, J. Leigh, E.-S. Jung, V. Vishwanath, and M. E. Papka,
“Improving data movement performance for sparse data patterns on the
Blue Gene/Q supercomputer.” in ICPP Workshops. IEEE Computer
Society, 2014, pp. 302–311. [Online]. Available: http://dblp.unitrier.de/db/conf/icppw/icppw2014.html#BuiLJVP14
[22] V. Vishwanath, M. Hereld, V. Morozov, and M. E. Papka, “Topologyaware data movement and staging for I/O acceleration on Blue
Gene/P supercomputing systems,” in Proceedings of 2011 International
Conference for High Performance Computing, Networking, Storage and
Analysis, ser. SC ’11. New York, NY, USA: ACM, 2011, pp. 19:1–
19:11. [Online]. Available: http://doi.acm.org/10.1145/2063384.2063409
[23] F. Tessier, P. Malakar, V. Vishwanath, E. Jeannot, and F. Isaila,
“Topology-aware data aggregation for intensive I/O on large-scale
supercomputers,” in Proceedings of the First Workshop on Optimization
of Communication in HPC, ser. COM-HPC ’16. Piscataway,
NJ, USA: IEEE Press, 2016, pp. 73–81. [Online]. Available:
https://doi.org/10.1109/COM-HPC.2016.13
[24] M. Gilge et al., IBM system blue gene solution - blue gene/Q application
development. IBM Redbooks, 2014.
[25] P. Schwan, “Lustre: Building a ﬁle system for 1,000-node clusters,” in
PROCEEDINGS OF THE LINUX SYMPOSIUM, 2003, p. 9.
[26] “IOR: Parallel ﬁlesystem I/O benchmark,” https://github.com/LLNL/ior.