[1] Apache Spark Survey 2015 Report. http://go.
databricks.com/2015-spark-survey/.
[2] Cori Supercomputer at NERSC.
http:
//www.nersc.gov/users/computationalsystems/cori/.
[3] B. Acun, A. Gupta, N. Jain, A. Langer, H. Menon,
E. Mikida, X. Ni, M. Robson, Y. Sun, E. Totoni,
L. Wesolowski, and L. Kale. Parallel Programming with Migratable Objects: Charm++ in Practice. SC’14, 2014.
[4] G. Almási, P. Heidelberger, C. J. Archer, X. Martorell, C. C. Erway, J. E. Moreira, B. SteinmacherBurow, and Y. Zheng. Optimization of MPI collective communication on BlueGene/L systems. ICS
’05, 2005.

[5] M. Anderson, S. Smith, N. Sundaram, M. Capota,
Z. Zhao, S. Dulloor, N. Satish, and T. L. Willke.
Bridging the gap between hpc and big data frameworks. Proceedings of the VLDB Endowment,
10(8), 2017.

[6] M. Barnett, R. Littlefield, D. G. Payne, and
R. van de Geijn. Global combine on mesh architectures with wormhole routing. In Proceedings Seventh International Parallel Processing Symposium,
1993.

[7] C. M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag New York, Inc., Secaucus,
NJ, USA, 2006.

[8] R. Choy and A. Edelman. Parallel matlab: Doing it
right. Proceedings of the IEEE, 93(2), 2005.
[9] A. Crotty, A. Galakatos, K. Dursun, T. Kraska,
C. Binnig, U. Cetintemel, and S. Zdonik. An architecture for compiling UDF-centric workflows.
Proc. VLDB Endow., 8(12), Aug. 2015.
[10] Databricks.
Project Tungsten:
Bringing Apache Spark closer to bare metal.
https://databricks.com/blog/2015/04/
28/project-tungsten-bringing-sparkcloser-to-bare-metal.html, 2015.
[11] Databricks.
GPU acceleration in Databricks:
Speeding up deep learning on Apache Spark.
https://databricks.com/blog/2016/10/
27/gpu-acceleration-in-databricks.html,
2016.
[12] J. Dongarra, T. Herault, and Y. Robert. Fault
tolerance techniques for high-performance computing. In Fault-Tolerance Techniques for HighPerformance Computing. Springer, 2015.
[13] G. M. Essertel, R. Y. Tahboub, J. M. Decker, K. J.
Brown, K. Olukotun, and T. Rompf. Flare: Native compilation for heterogeneous workloads in
Apache Spark. https://arxiv.org/abs/1703.
08219, 2017.
[14] A. Gupta, B. Acun, O. Sarood, and L. V. Kale. Towards realizing the potential of malleable parallel
jobs. HiPC ’14, 2014.
[15] H. Kaiser, T. Heller, B. Adelstein-Lelbach, A. Serio, and D. Fey. HPX: A task based programming
model in a global address space. PGAS ’14, 2014.


[16] F. McSherry, M. Isard, and D. G. Murray. Scalability! but at what cost? HOTOS’15, 2015.
[17] K. Ousterhout, A. Panda, J. Rosen, S. Venkataraman, R. Xin, S. Ratnasamy, S. Shenker, and I. Stoica. The case for tiny tasks in compute clusters.
In Proceedings of the 14th USENIX Conference on
Hot Topics in Operating Systems, HotOS’13, 2013.
[18] K. Ousterhout, R. Rasti, S. Ratnasamy, S. Shenker,
and B.-G. Chun. Making sense of performance in
data analytics frameworks. In Proceedings of the
12th USENIX Conference on Networked Systems
Design and Implementation, NSDI’15, 2015.
[19] K. Ousterhout, P. Wendell, M. Zaharia, and I. Stoica. Sparrow: Distributed, low latency scheduling.
In Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles, pages 69–
84. ACM, 2013.
[20] S. Palkar, J. J. Thomas, A. Shanbhag,
D. Narayanan, H. Pirk, M. Schwarzkopf, S. Amarasinghe, and M. Zaharia. Weld: A common
runtime for high performance data analytics. CIDR
2017, 8th Biennial Conference on Innovative Data
Systems Research, 2017.
[21] S. Prabhakaran, M. Neumann, S. Rinke, F. Wolf,
A. Gupta, and L. V. Kale. A batch system with efficient adaptive scheduling for malleable and evolving applications. IPDPS ’15, 2015.
[22] R. Thakur, R. Rabenseifner, and W. Gropp. Optimization of collective communication operations
in MPICH. The International Journal of High
Performance Computing Applications, 19(1):49–
66, 2005.
[23] E. Totoni, T. A. Anderson, and T. Shpeisman.
HPAT: High Performance Analytics with Scripting
Ease-of-Use. https://arxiv.org/abs/1611.
04934/.
[24] E. Totoni, T. A. Anderson, and T. Shpeisman.
HPAT: High Performance Analytics with Scripting
Ease-of-Use. ICS’17 (to appear), 2017.
[25] E. Totoni, A. Bhatele, E. J. Bohm, N. Jain, C. L.
Mendes, R. M. Mokos, G. Zheng, and L. V. Kale.
Simulation-based performance analysis and tuning
for a two-level directly connected system. In IEEE
17th International Conference on Parallel and Distributed Systems, 2011.
[26] E. Totoni, W. U. Hassan, T. A. Anderson, and
T. Shpeisman. HiFrames: High Performance Data
Frames in a Scripting Language. https://arxiv.
org/abs/1704.02341.

[27] S. Venkataraman, A. Panda, A. Ousterhout,
Kay Ghodsi, M. J. Franklin, B. Recht, and I. Stoica. Drizzle: Fast and adaptable stream processing at scale. http://shivaram.org/drafts/
drizzle.pdf, 2017.
[28] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma,
M. McCauley, M. J. Franklin, S. Shenker, and
I. Stoica. Resilient Distributed Datasets: A faulttolerant abstraction for in-memory cluster computing. In Proceedings of the 9th USENIX Conference
on Networked Systems Design and Implementation,
NSDI’12, 2012.
[29] M. Zaharia, T. Das, H. Li, T. Hunter, S. Shenker,
and I. Stoica. Discretized streams: Fault-tolerant
streaming computation at scale. In Proceedings of
the Twenty-Fourth ACM Symposium on Operating
Systems Principles, pages 423–438. ACM, 2013.
