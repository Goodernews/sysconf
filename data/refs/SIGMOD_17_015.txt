[1] Amazon ec2. https://aws.amazon.com/ec2/.
[2] Mahout project. http://mahout.apache.org/.
[3] Spark mllib. http://spark.apache.org/mllib/.
[4] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,

C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, et al.
Tensorﬂow: Large-scale machine learning on heterogeneous
distributed systems. arXiv preprint arXiv:1603.04467, 2016.
[5] A. Agarwal and J. C. Duchi. Distributed delayed stochastic

optimization. In NIPS, pages 873–881, 2011.

[6] S. Agrawal, V. Narasayya, and B. Yang. Integrating vertical
and horizontal partitioning into automated physical database
design. In SIGMOD, pages 359–370, 2004.

[7] A. Ahmed, M. Aly, J. Gonzalez, S. Narayanamurthy, and
A. Smola. Scalable inference in latent variable models. In
WSDM, pages 123–132, 2012.

[8] L. Bottou. Large-scale machine learning with stochastic
gradient descent. In COMPSTAT, pages 177–186. 2010.
[9] L. Bottou. Stochastic gradient descent tricks. In Neural

Networks: Tricks of the Trade, pages 421–436. 2012.

[10] Z. Cai, Z. Vagena, L. Perez, S. Arumugam, P. J. Haas, and
C. Jermaine. Simulation of database-valued markov chains
using simsql. In SIGMOD, pages 637–648, 2013.

[11] D. W. Cheung, S. D. Lee, and Y. Xiao. Effect of data

skewness and workload balance in parallel data mining.
TKDE, 14(3):498–514, 2002.

[12] T. Chilimbi, Y. Suzue, J. Apacible, and K. Kalyanaraman.

Project adam: Building an efﬁcient and scalable deep
learning training system. In OSDI, pages 571–582, 2014.

[13] B. Cui, H. Mei, and B. C. Ooi. Big data: the driver for

innovation in databases. National Science Review,
1(1):27–30, 2014.

[14] H. Cui, J. Cipar, Q. Ho, J. K. Kim, S. Lee, A. Kumar, J. Wei,

W. Dai, G. R. Ganger, P. B. Gibbons, et al. Exploiting
bounded staleness to speed up big data analytics. In USENIX
ATC, pages 37–48, 2014.

[15] C. Curino, E. Jones, Y. Zhang, and S. Madden. Schism: a

workload-driven approach to database replication and
partitioning. VLDB, 3(1-2):48–57, 2010.

[16] W. Dai, J. Wei, J. K. Kim, S. Lee, J. Yin, Q. Ho, and E. P.

Xing. Petuum: A framework for iterative-convergent
distributed ml. arXiv preprint arXiv:1312.7651, 2013.

[17] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao,

A. Senior, P. Tucker, K. Yang, Q. V. Le, et al. Large scale
distributed deep networks. In NIPS, pages 1223–1231, 2012.

[18] R. Gemulla, E. Nijkamp, P. J. Haas, and Y. Sismanis.

Large-scale matrix factorization with distributed stochastic
gradient descent. In SIGKDD, pages 69–77, 2011.

[19] S. Ghandeharizadeh and D. J. DeWitt. Hybrid-range
partitioning strategy: A new declustering strategy for
multiprocessor database machines. In VLDB, pages 481–492,
1990.

[20] A. Ghoting, R. Krishnamurthy, E. Pednault, B. Reinwald,
V. Sindhwani, S. Tatikonda, Y. Tian, and S. Vaithyanathan.
Systemml: Declarative machine learning on mapreduce. In
ICDE, pages 231–242, 2011.

[21] A. Harlap, H. Cui, W. Dai, J. Wei, G. R. Ganger, P. B.
Gibbons, G. A. Gibson, and E. P. Xing. Addressing the
straggler problem for iterative convergent parallel ml. In
SoCC, pages 98–111, 2016.

[22] J. M. Hellerstein, C. Ré, F. Schoppmann, D. Z. Wang,

E. Fratkin, A. Gorajek, K. S. Ng, C. Welton, X. Feng, K. Li,
et al. The madlib analytics library: or mad skills, the sql.
VLDB, 5(12):1700–1711, 2012.

[23] Q. Ho, J. Cipar, H. Cui, S. Lee, J. K. Kim, P. B. Gibbons,
G. A. Gibson, G. Ganger, and E. P. Xing. More effective
distributed ml via a stale synchronous parallel parameter
server. In NIPS, pages 1223–1231, 2013.

[24] Y. Huang, B. Cui, J. Jiang, K. Hong, W. Zhang, and Y. Xie.
Real-time video recommendation exploration. In SIGMOD,
pages 35–46, 2016.

[25] W. Kim and F. H. Lochovsky. Object-oriented concepts,

databases, and applications. 1989.

[26] A. Kumar, J. Naughton, and J. M. Patel. Learning
generalized linear models over normalized data. In
SIGMOD, pages 1969–1984, 2015.

[27] A. Lastovetsky, I.-H. Mkwawa, and M. O’Flynn. An accurate
communication model of a heterogeneous cluster based on a
switch-enabled ethernet network. In ICPADS, page 6, 2006.
[28] M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed,
V. Josifovski, J. Long, E. J. Shekita, and B.-Y. Su. Scaling
distributed machine learning with the parameter server. In
OSDI, pages 583–598, 2014.

[29] M. Li, T. Zhang, Y. Chen, and A. J. Smola. Efﬁcient

mini-batch training for stochastic optimization. In SIGKDD,
pages 661–670, 2014.

[30] Y. Low, D. Bickson, J. Gonzalez, C. Guestrin, A. Kyrola, and

J. M. Hellerstein. Distributed graphlab: a framework for
machine learning and data mining in the cloud. PVLDB,
5(8):716–727, 2012.

[31] J. Ma, L. K. Saul, S. Savage, and G. M. Voelker. Identifying
suspicious urls: an application of large-scale online learning.
In ICML, pages 681–688, 2009.

[32] G. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert,

I. Horn, N. Leiser, and G. Czajkowski. Pregel: a system for
large-scale graph processing. In SIGMOD, pages 135–146,
2010.

[33] S. Prasad, A. Fard, V. Gupta, J. Martinez, J. LeFevre, V. Xu,

M. Hsu, and I. Roy. Large-scale predictive analytics in
vertica: fast data transfer, distributed model creation, and
in-database prediction. In SIGMOD, pages 1657–1668, 2015.

[34] C. Qin and F. Rusu. Scalable i/o-bound parallel incremental
gradient descent for big data analytics in glade. In DanaC,
pages 16–20, 2013.

[35] C. Qin and F. Rusu. Speculative approximations for terascale

distributed gradient descent optimization. In DanaC, 2015.
[36] J. Rao, C. Zhang, N. Megiddo, and G. Lohman. Automating
physical database design in a parallel database. In SIGMOD,
pages 558–569, 2002.

[37] B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A lock-free

approach to parallelizing stochastic gradient descent. In
NIPS, pages 693–701, 2011.

[38] X. Shi, B. Cui, Y. Shao, and Y. Tong. Tornado: A system for
real-time iterative analysis over evolving data. In SIGMOD,
pages 417–430, 2016.

[39] A. Smola and S. Narayanamurthy. An architecture for
parallel topic models. PVLDB, 3(1-2):703–710, 2010.

[40] V. K. Vavilapalli, A. C. Murthy, C. Douglas, S. Agarwal,
M. Konar, R. Evans, T. Graves, J. Lowe, H. Shah, S. Seth,
et al. Apache hadoop yarn: Yet another resource negotiator.
In SOCC, page 5, 2013.

[41] J. Wei, W. Dai, A. Qiao, Q. Ho, H. Cui, G. R. Ganger, P. B.

Gibbons, G. A. Gibson, and E. P. Xing. Managed
communication and consistency for fast data-parallel
iterative analytics. In SOCC, pages 381–394, 2015.

[42] X. Wu, X. Zhu, G.-Q. Wu, and W. Ding. Data mining with

big data. TKDE, 26(1):97–107, 2014.

[43] R. S. Xin, J. E. Gonzalez, M. J. Franklin, and I. Stoica.

Graphx: A resilient distributed graph system on spark. In
GRADES, page 2, 2013.

[44] N. Xu, B. Cui, L. Chen, Z. Huang, and Y. Shao.

Heterogeneous environment aware streaming graph
partitioning. TKDE, 27(6):1560–1572, 2015.

[45] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma,

M. McCauley, M. J. Franklin, S. Shenker, and I. Stoica.
Resilient distributed datasets: A fault-tolerant abstraction for
in-memory cluster computing. In NSDI, pages 2–2, 2012.

[46] C. Zhang and C. Ré. Dimmwitted: A study of main-memory

statistical analytics. PVLDB, 7(12):1283–1294, 2014.

[47] S.-Y. Zhao and W.-J. Li. Fast asynchronous parallel

stochastic gradient descent: A lock-free approach with
convergence guarantee. In AAAI, 2016.

[48] M. Zinkevich, J. Langford, and A. J. Smola. Slow learners

are fast. In NIPS, pages 2331–2339, 2009.

[49] M. Zinkevich, M. Weimer, L. Li, and A. J. Smola.

Parallelized stochastic gradient descent. In NIPS, pages
2595–2603, 2010.

