[1] The opportunities and challenges of exascale computing, summary report of the

advanced scientiﬁc computing advisory committee (ASCAC) subcommittee at the

US DOE Oﬃce of Science. Technical report (2010)

[2] Klenk, B., Oden, L., Fröning, H.: Analyzing communication models for distributed

thread-collaborative processors in terms of energy and time. In: IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS),

Philadelphia, PA (2015)

[3] Flajslik, M., Dinan, J., Underwood, K.D.: Mitigating MPI message matching

misery. In: International Conference on High Performance Computing (ISC),

Frankfurt, Germany (2016)

[4] Bayatpour, M., Subramoni, H., Chakraborty, S., Panda, D.K.: Adaptive and

dynamic design for MPI tag matching. In: 2016 IEEE International Conference

on Cluster Computing (CLUSTER) (2016)

[5] U.S. DOE: Characterization of the DOE Mini-apps. https://portal.nersc.gov/

project/CAL/doe-miniapps.htm. Accessed 25 Oct 2016

[6] Faraj, A., Yuan, X.: Communication characteristics in the NAS parallel benchmarks. In: IASTED International Conference on Parallel and Distributed Computing Systems (PDCS), Cambridge, MA (2002)

[7] Riesen, R.: Communication patterns. In: Workshop on Communication Architecture for Clusters (CAC), Rhodes Island, Greece (2006)

[8] Vetter, J.S., Mueller, F.: Communication characteristics of large-scale scientiﬁc

applications for contemporary cluster architectures. J. Parallel Distrib. Comput.

63(9) (2003)

[9] Kamil, S., Oliker, L., Pinar, A., Shalf, J.: Communication requirements and interconnect optimization for high-end scientiﬁc applications. IEEE Trans. Parallel Distrib. Syst. 21(2) (2010)

[10]  Raponi, P.G., Petrini, F., Walkup, R., Checconi, F.: Characterization of the communication patterns of scientiﬁc applications on Blue Gene/P. In: IEEE International Symposium on Parallel and Distributed Processing Workshops (IPDPSW)

and Ph.D. Forum, DC, USA, Washington (2011)

[11]  Lammel, S., Zahn, F., Fröning, H.: SONAR: automated communication characterization for HPC applications. In: Taufer, M., Mohr, B., Kunkel, J.M. (eds.)

ISC High Performance 2016. LNCS, vol. 9945, pp. 98–114. Springer, Cham (2016).

doi:10.1007/978-3-319-46079-6 8

[12]  Brightwell, R., Underwood, K.D.: An analysis of NIC resource usage for oﬄoading MPI. In: IEEE International Parallel and Distributed Processing Symposium

(IPDPS), Santa Fe, NM (2004)

[13]  Underwood, K.D., Hemmert, K.S., Rodrigues, A., Murphy, R., Brightwell, R.: A

hardware acceleration unit for MPI queue processing. In: IEEE International Parallel and Distributed Processing Symposium (IPDPS), Denver, CO (2005)

[14]  Keller, R., Graham, R.L.: Characteristics of the unexpected message queue of MPI

applications. In: Keller, R., Gabriel, E., Resch, M., Dongarra, J. (eds.) EuroMPI

2010. LNCS, vol. 6305, pp. 179–188. Springer, Heidelberg (2010). doi:10.1007/

978-3-642-15646-5 19

[15]  Zounmevo, J.A., Afsahi, A.: An eﬃcient MPI message queue mechanism for largescale jobs. In: IEEE Conference on Parallel and Distributed Systems (ICPADS),

Singapore (2012)

[16]  Klenk, B., Fröning, H., Eberle, H., Dennison, L.: Relaxations for high-performance

message passing on massively parallel SIMT processors. In: IEEE International

Parallel and Distributed Processing Symposium (IPDPS), Orlando, FL (2017)

[17]  Hoeﬂer, T., Kambadur, P., Graham, R.L., Shipman, G., Lumsdaine, A.: A case

for standard non-blocking collective operations. In: Cappello, F., Herault, T.,

Dongarra, J. (eds.) EuroPVM/MPI 2007. LNCS, vol. 4757, pp. 125–134. Springer,

Heidelberg (2007). doi:10.1007/978-3-540-75416-9 22

[18]  Balaji, P., Chan, A., Thakur, R., Gropp, W., Lusk, E.: Toward message passing for

a million processes: characterizing MPI on a massive scale Blue Gene/P. Comput.

Sci. Res. Dev. (CSRD) 24(1), 11–19 (2009)

[19]  Koop, M.J., Huang, W., Gopalakrishnan, K., Panda, D.K.: Performance analysis

and evaluation of PCIe 2.0 and quad-data rate InﬁniBand. In: IEEE Symposium

on High Performance Interconnects (2008)
