[1] Jorge Albericio, Patrick Judd, Tayler Hetherington, Tor Aamodt, Natalie Enright
Jerger, and Andreas Moshovos. 2016. Cnvlutin: Ineffectual-Neuron-Free Deep
Convolutional Neural Network Computing. In Proceedings of the International
Symposium on Computer Architecture (ISCA). 1–13.
[2] Manoj Alwani, Han Chen, Michael Ferdman, and Peter Milder. 2016. FusedLayer CNN Accelerators. In Proceedings of the International Symposium on
Microarchitecture (MICRO).
[3] Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan
Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich
Elsen, Jesse Engel, Linxi Fan, Christopher Fougner, Tony Han, Awni Hannun,
Billy Jun, Patrick LeGresley, Libby Lin, Sharan Narang, Andrew Ng, Sherjil Ozair,
Ryan Prenger, Jonathan Raiman, Sanjeev Satheesh, David Seetapun, Shubho
Sengupta, Yi Wang, Zhiqian Wang, Chong Wang, Bo Xiao, Dani Yogatama, Jun
Zhan, and Zhenyao Zhu. 2015. Deep Speech 2: End-To-End Speech Recognition
in English and Mandarin. https://arxiv.org/abs/1512.02595. (2015).
[4] Caffe 2016. Caffe. http://caffe.berkeleyvision.org. (2016).
[5] Caffe 2017. Caffe Model Zoo. https://github.com/BVLC/caffe/wiki/Model-Zoo.
(2017).
[6] Tianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengyong Wu, Yunji Chen,
and Olivier Temam. 2014. DianNao: A Small-footprint High-throughput Accelerator for Ubiquitous Machine-learning. In Proceedings of the International
Conference on Architectural Support for Programming Languages and Operation
Systems (ASPLOS). 269–284.
[7] Yu-Hsin Chen, Joel Emer, and Vivienne Sze. 2016. Eyeriss: A Spatial Architecture
for Energy-Efficient Dataflow for Convolutional Neural Networks. In Proceedings
of the International Symposium on Computer Architecture (ISCA). 367–379.
[8] Yu-Hsin Chen, Tushar Krishna, Joel Emer, and Vivienne Sze. 2016. Eyeriss:
An Energy-efficient Reconfigurable Accelerator for Deep Convolutional Neural
Networks. In Proceedings of the International Solid State Circuits Conference
(ISSCC).
[9] Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray
Kavukcuoglu, and Pavel Kuksa. 2011. Natural Language Processing (Almost)
From Scratch. https://arxiv.org/abs/1103.0398. (2011).
[10] Jason Cong and Bingjun Xiao. 2014. Minimizing Computation in Convolutional
Neural Networks. In Proceedings of the International Conference on Artificial
Neural Networks (ICANN). 281–290.
[11] Gregory Diamos, Shubho Sengupta, Bryan Catanzaro, Mike Chrzanowski, Adam
Coates, Erich Elsen, Jesse Engel, Awni Hannun, and Sanjeev Satheesh. 2016.
Persistent RNNs: Stashing Recurrent Weights On-Chip. In Proceedings of the
International Conference on Machine Learning (ICML).
[12] Zidong Du, Robert Fasthuber, Tianshi Chen, Paolo Ienne, Ling Li, Tao Luo,
Xiaobing Feng, Yunji Chen, and Olivier Temam. 2015. ShiDianNao: Shifting
Vision Processing Closer to the Sensor. In Proceedings of the International
Symposium on Computer Architecture (ISCA). 92–104.
[13] Mingyu Gao, Jing Pu, Xuan Yang, Mark Horowitz, and Christos Kozyrakis. 2017.
TETRIS: Scalable and Efficient Neural Network Acceleration with 3D Memory.
In Proceedings of the International Conference on Architectural Support for
Programming Languages and Operation Systems (ASPLOS). 751–764.
[14] Alex Graves and Jurgen Schmidhuber. 2005. Framewise Phoneme Classification
With Bidirectional LSTM and Other Neural Network Architectures. In Neural
Networks.
[15] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark Horowitz,
and Bill Dally. 2016. EIE: Efficient Inference Engine on Compressed Deep
Neural Network. In Proceedings of the International Symposium on Computer

A. Parashar et al.
Architecture (ISCA). 243–254.
[16] Song Han, Huizi Mao, and William J. Dally. 2015. Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman
Coding. https://arxiv.org/abs/1510.00149. (2015).
[17] Song Han, Jeff Pool, John Tran, and William J. Dally. 2015. Learning Both
Weights and Connections for Efficient Neural Networks. In Proceedings of the
International Conference on Neural Information Processing Systems (NIPS).
1135–1143.
[18] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich
Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, and
Andrew Y. Ng. 2014. Deep Speech: Scaling Up End-To-End Speech Recognition.
https://arxiv.org/abs/1412.5567. (2014).
[19] Kaming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep Residual
Learning for Image Recognition. https://arxiv.org/abs/1512.03385. (2015).
[20] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Weinberger. 2016.
Deep Networks with Stochastic Depth. https://arxiv.org/abs/1603.09382. (2016).
[21] ImageNet. 2016. http://image-net.org. (2016).
[22] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012. ImageNet Classification with Deep Convolutional Neural Networks. In Proceedings of the International Conference on Neural Information Processing Systems (NIPS).
[23] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep Learning. Nature
521 (May 2015), 436–444.
[24] Jonathan Long, Evan Shelhamer, and Trevor Darrell. 2015. Fully Convolutional
Networks for Semantic Segmentation. In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR).
[25] Grant Martin and Gary Smith. 2009. High-Level Synthesis: Past, Present, and
Future. IEEE Design & Test of Computers 26, 4 (July/August 2009), 18–25.
[26] Mentor 2017. Catapult High-Level Synthesis. https://www.mentor.com/hls-lp/
catapult-high-level-synthesis. (2017).
[27] NVIDIA 2016. NVIDIA cuDNN. https://developer.nvidia.com/cudnn. (2016).
[28] Brandon Reagen, Paul Whatmough, Robert Adolf, Saketh Rama, Hyunkwang
Lee, Saekyu Lee, Jose Miguel Hernandez Lobato, Gu-Yeon Wei, and David
Brooks. 2016. Minerva: Enabling Low-Power, High-Accuracy Deep Neural Network Accelerators. In Proceedings of the International Symposium on Computer
Architecture (ISCA). 267–278.
[29] Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulfiqar, and Stephen W.
Keckler. 2016. vDNN: Virtualized Deep Neural Networks for Scalable, MemoryEfficient Neural Network Design. In Proceedings of the International Symposium
on Microarchitecture (MICRO).
[30] Karen Simonyan and Andrew Zisserman. 2015. Very Deep Convolutional Networks for Large-Scale Image Recognition. https://arxiv.org/abs/1409.1556. (May
2015).
[31] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015.
Going Deeper with Convolutions. In Proceedings of the Conference on Computer
Vision and Pattern Recognition (CVPR).
[32] Ganesh Venkatesh, Eriko Nurvitadhi, and Debbie Marr. 2016. Accelerating Deep
Convolutional Networks Using Low-precision and Sparsity. https://arxiv.org/abs/
1610.00324. (2016).
[33] Richard W. Vuduc. 2003. Automatic Performance Tuning of Sparse Matrix
Kernels. Ph.D. Dissertation. University of California, Berkeley.
[34] Shijin Zhang, Zidong Du, Lei Zhang, Huiying Lan, Shaoli Liu, Ling Li, Qi
Guo, Tianshi Chen, and Yunji Chen. 2016. Cambricon-X: An Accelerator for
Sparse Neural Networks. In Proceedings of the International Symposium on
Microarchitecture (MICRO).