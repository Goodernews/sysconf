[1] 2016. NIVDIA DIGITS DevBox. (2016). https://developer.nvidia.com/devbox.
[2] Jorge Albericio, Patrick Judd, Tayler Hetherington, Tor Aamodt, Natalie Enright
Jerger, and Andreas Moshovos. 2016. Cnvlutin: ineffectual-neuron-free deep
neural network computing. In Computer Architecture (ISCA), 2016 ACMAEEE
43rd Annual International Symposium on. TEEE, 1-13.

[3] Jimmy Ba and Rich Caruana. 2014. Do deep nets really need to be deep?. In
Advances in neural information processing systems. 2654-2662.

[4] Guoguo Chen, Carolina Parada, and Georg Heigold. 2014. Small-footprint keyword spotting using deep neural networks. In 2014 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP). TEEE, 4087-4091.
[5] Tianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengyong Wu, Yunji Chen,
and Olivier Temam, 2014. Diannao: A small-footprint high-throughput accelerator
for ubiquitous machine-learning. In ACM Sigplan Notices, Vol. 49. ACM, 269284.

[6] Tiangi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun
Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. Mxnet: A flexible and
efficient machine learning library for heterogeneous distributed systems. arXiv
preprint arXiv: 1512.01274 (2015).

[7] Yunji Chen, Tao Luo, Shaoli Liu, Shijin Zhang, Ligiang He, Jia Wang, Ling
Li, Tianshi Chen, Zhiwei Xu, Ninghui Sun, and others. 2014. Dadiannao: A
machine-learning supercomputer. In Proceedings of the 47th Annual IEEE/ACM
International Symposium on Microarchitecture. EEE Computer Society, 609622.

[8] Yu-Hsin Chen, Joel Emer, and Vivienne Sze. 2016. Eyeriss: A Spatial Architecture
for Energy-Efficient Dataflow for Convolutional Neural Networks. (2016).
[9] Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John
Tran, Bryan Catanzaro, and Evan Shelhamer. 2014. cudnn: Efficient primitives
for deep learning. arXiv preprint arXiv: 1410.0759 (2014).

[10] Ping Chi, Shuangchen Li, Z Qi, P Gu, C Xu, T Zhang, J Zhao, Y Liu, Y Wang, and
Y Xie. 2016. PRIME: A Novel Processing-In-Memory Architecture for Neural
Network Computation in RERAM-based Main Memory. In Proceedings of ISCA,
Vol. 43.

[11] Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings
of the 25th international conference on Machine learning. ACM, 160-167.
[12] Matthieu Courbariaux and Yoshua Bengio. 2016. Binarynet: Training deep neural
networks with weights and activations constrained to+ 1 or-1. arXiv preprint
arXiv: 1602.02830 (2016).

[13] Misha Denil, Babak Shakibi, Laurent Dinh, Nando de Freitas, and others. 2013.
Predicting parameters in deep learning. In Advances in Neural Information Processing Systems, 2148-2156.

[14] Zidong Du, Robert Fasthuber, Tianshi Chen, Paolo Ienne, Ling Li, Tao Luo,
Xiaobing Feng, Yunji Chen, and Olivier Temam. 2015. ShiDianNao: shifting
vision processing closer to the sensor. In ACM SIGARCH Computer Architecture
News, Vol. 43. ACM, 92-104.

[15] Ross Girshick. 2015. Fast R-CNN. In International Conference on Computer
Vision (ICCV).

[16] Yiwen Guo, Anbang Yao, and Yurong Chen. 2016. Dynamic Network Surgery
for Efficient DNNs. arXiv preprint arXiv:1608.04493 (2016).

[17] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.
2015. Deep learning with limited numerical precision. (2015).

[18] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A Horowitz,
and William J Dally. 2016. EIE: efficient inference engine on compressed deep
neural network. arXiv preprint arXiv: 1602.01528 (2016).

[19] Song Han, Huizi Mao, and William J Dally. 2015. A deep neural network
compression pipeline: Pruning, quantization, huffman encoding. arXiv preprint
arXiv:1510.00149 (2015).

[20] Song Han, Jeff Pool, John Tran, and William J Dally. 2015. Learning both Weights
and Connections for Efficient Neural Networks. arXiv preprint arXiv: 1506.02626
(2015).

[21] Babak Hassibi, David G Stork, and Gregory J Wolff. 1993. Optimal brain surgeon
and general network pruning. In Neural Networks, 1993., IEEE International
Conference on. IEEE, 293-299,

[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep Residual
Learning for Image Recognition. arXiv preprint arXiv:1512.03385 (2015).
[23] Tianxing He, Yuchen Fan, Yanmin Qian, Tian Tan, and Kai Yu. 2014. Reshaping
deep neural network for fast decoding by node-pruning. In 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). TEEE,
245-249,

[24] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in
a neural network. arXiv preprint arXiv: 1503.02531 (2015).

[25] Kevin Hsieh, Samira Khan, Nandita Vijaykumar, Kevin K Chang, Amirali
Boroumand, Saugata Ghose, and Onur Mutlu. 2016. Accelerating Pointer Chasing
in 3D-Stacked Memory: Challenges, Mechanisms, Evaluation. (2016).
[26] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long,
Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the 22nd ACM
international conference on Multimedia. ACM, 675-678.

[27] Patrick Judd, Jorge Albericio, and Andreas Moshovos. 2016. Stripes: Bit-serial
deep neural network computing. JEEE Computer Architecture Letters (2016).
Scalpel ISCA °17, June 24-28, 2017, Toronto, ON, Canada

[28] Alex Krizhevsky. 2012. cuda-convnet. (2012). https://code.google.com/p/
cuda-convnet/.

[29] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classification with deep convolutional neural networks. In Advances in neural information
processing systems. 1097-1105.

[30] Andrew Lavin. 2015. Fast algorithms for convolutional neural networks. arXiv
preprint arXiv: 1509.09308 (2015).

[31] Yann Le Cun, John S Denker, and Sara A Solla. 1989. Optimal brain damage.. In
NIPS, Vol. 89.

[32] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradientbased learning applied to document recognition. Proc. IEEE 86, 11 (1998),
2278-2324.

[33] Robert LiKamWa, Yunhui Hou, Julian Gao, Mia Polansky, and Lin Zhong. 2016.
RedEye: analog ConvNet image sensor architecture for continuous mobile vision.
In Proceedings of the 43rd International Symposium on Computer Architecture.
IEEE Press, 255-266.

[34] Min Lin, Qiang Chen, and Shuicheng Yan. 2013. Network in network. arXiv
preprint arXiv: 1312.4400 (2013).

[35] Thomas Miconi. 2016. Neural networks with differentiable structure. arXiv
preprint arXiv: 1606.06216 (2016).

[36] Brandon Reagen, Paul Whatmough, Robert Adolf, Saketh Rama, Hyunkwang
Lee, Sae Kyu Lee, José Miguel Herndndez-Lobato, Gu-Yeon Wei, and David
Brooks. 2016. Minerva: Enabling low-power, highly-accurate deep neural network
accelerators. In Proceedings of the 43rd International Symposium on Computer
Architecture. TEEE Press, 267-278.

[37] Ali Shafiee, Anirban Nag, Naveen Muralimanohar, Rajeev Balasubramonian,
John Paul Strachan, Miao Hu, R Stanley Williams, and Vivek Srikumar. 2016.
ISAAC: A Convolutional Neural Network Accelerator with In-Situ Analog Arithmetic in Crossbars. In Proc. ISCA.

[38] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks
for large-scale image recognition. arXiv preprint arXiv: 1409.1556 (2014).

[39] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan

Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from

overfitting. Journal of Machine Learning Research 15, 1 (2014), 1929-1958.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir

Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015.

Going deeper with convolutions. In Proceedings of the IEEE Conference on

Computer Vision and Pattern Recognition. 1-9.

[41] Vincent Vanhoucke, Matthieu Devin, and Georg Heigold. 2013. Multiframe deep
neural networks for acoustic modeling. In 2013 IEEE International Conference
on Acoustics, Speech and Signal Processing. IEEE, 7582-7585.

[42] Vincent Vanhoucke, Andrew Senior, and Mark Z Mao. 2011. Improving the speed
of neural networks on CPUs. (2011).

[43] Nicolas Vasilache, Jeff Johnson, Michael Mathieu, Soumith Chintala, Serkan
Piantino, and Yann LeCun. 2014. Fast convolutional nets with fbfft: A GPU
performance evaluation. arXiv preprint arXiv: 1412. 7580 (2014).