[1] “MPI: A Message-Passing Interface Standard,” http://www.mpi-forum.
org/docs/mpi-3.0/mpi30-report.pdf, Sep. 2012.
[2] J. Meng, B. Wang, Y. Wei, S. Feng, and P. Balaji, “SWAPAssembler: Scalable and Efficient Genome Assembly Towards
Thousands of Cores,” BMC Bioinformatics, Sep. 2014. [Online].
Available: http://dx.doi.org/10.1186/1471-2105-15-S9-S2
[3] B. Joó, D. D. Kalamkar, K. Vaidyanathan, M. Smelyanskiy, K. Pamnany,
V. W. Lee, P. Dubey, and W. Watson, Lattice QCD on Intel Xeon
PhiTM Coprocessors. Springer Berlin Heidelberg, 2013. [Online].
Available: http://dx.doi.org/10.1007/978-3-642-38750-0 4
[4] B. Rosa and L.-P. Wang, “Parallel Implementation of Particle
Tracking and Collision in a Turbulent Flow,” in Proceedings of
the 8th International Conference on Parallel Processing and Applied
Mathematics: Part II, ser. PPAM’09. Berlin, Heidelberg: SpringerVerlag, 2010, pp. 388–397. [Online]. Available: http://dl.acm.org/
citation.cfm?id=1893586.1893634
[5] S. Balay, W. D. Gropp, L. C. McInnes, and B. F. Smith, “Efficient
Management of Parallelism in Object Oriented Numerical Software
Libraries,” in Modern Software Tools in Scientific Computing, E. Arge,
A. M. Bruaset, and H. P. Langtangen, Eds. Birkhäuser Press, 1997,
pp. 163–202.
[6] A. Castro, H. Appel, M. Oliveira, C. A. Rozzi, X. Andrade, F. Lorenzen,
M. A. L. Marques, E. K. U. Gross, and A. Rubio, “Octopus: A Tool
for the Application of Time-Dependent Density Functional Theory,”
physica status solidi (b), vol. 243, no. 11, pp. 2465–2488, 2006.
[Online]. Available: http://dx.doi.org/10.1002/pssb.200642067
[7] T. Peterka, R. Ross, B. Nouanesengsy, T.-Y. Lee, H.-W. Shen,
W. Kendall, and J. Huang, “A Study of Parallel Particle Tracing for
Steady-State and Time-Varying Flow Fields,” in 25th IEEE International
Parallel & Distributed Processing Symposium (IPDPS). IEEE, 2011,
pp. 580–591.

[8] Argonne National Laboratory, “MPICH — High-Performance Portable
MPI,” http://www.mpich.org, 2014.
[9] The Ohio State University, “MVAPICH: MPI over InfiniBand, 10GigE/
iWARP and RoCE,” http://mvapich.cse.ohio-state.edu, 2014.
[10] Intel Corporation, “Intel MPI Library,” http://software.intel.com/en-us/
intel-mpi-library, 2014.
[11] Cray Inc., “Cray Message Passing Toolkit,” http://docs.cray.com/books/
S-3689-24, Cray Inc., Tech. Rep., 2004.
[12] W. Gropp and R. Thakur, “Thread-Safety in an MPI Implementation:
Requirements and Analysis,” Parallel Comput., vol. 33, no. 9, pp. 595–
604, 2007.
[13] S. Kumar, G. Dozsa, G. Almasi, P. Heidelberger, D. Chen, M. E.
Giampapa, M. Blocksome, A. Faraj, J. Parker, J. Ratterman, B. Smith,
and C. J. Archer, “The Deep Computing Messaging Framework: Generalized Scalable Message Passing on the Blue Gene/P Supercomputer,”
in Proceedings of the 22Nd Annual International Conference on Supercomputing, ser. ICS ’08, 2008, pp. 94–103.
[14] S. Kumar, Y. Sun, and L. V. Kale, “Acceleration of an Asynchronous
Message Driven Programming Paradigm on IBM Blue Gene/Q,” in
Proceedings of the 2013 IEEE 27th International Symposium on Parallel
and Distributed Processing, ser. IPDPS ’13, 2013, pp. 689–699.
[15] H. Pritchard, D. Roweth, D. Henseler, and P. Cassella, “Leveraging the
Cray Linux Environment Core Specialization Feature to Realize MPI
Asynchronous Progress on Cray XE Systems,” in Proceedings of the
Cray User’s Group Meeting (CUG), 2012.
[16] M. Si, A. J. Peña, J. Hammond, P. Balaji, M. Takagi, and Y. Ishikawa,
“Casper: An Asynchronous Progress Model for MPI RMA on ManyCore Architectures,” in Parallel and Distributed Processing, 2015.
IPDPS 2015.
[17] M. Si, A. J. Peña, J. Hammond, P. Balaji, and Y. Ishikawa, “Scaling
NWChem with Efficient and Portable Asynchronous Communication in
MPI RMA,” Cluster, Cloud and Grid Computing (CCGrid), 2015 15th
IEEE/ACM International Symposium, 2015.
[18] Y. Guo, C. J. Archer, M. Blocksome, S. Parker, W. Bland, K. Raffenetti,
and P. Balaji, “Memory Compression Techniques for Network Address
Management in MPI,” in Proceedings of the 2017 IEEE 31th International Symposium on Parallel and Distributed Processing, ser. IPDPS
’17, 2016.
[19] The
Ohio
State
University,
“OSU
Micro-Benchmarks,”
http://mvapich.cse.ohio-state.edu/benchmarks, 2013.
[20] K. Kandalla, H. Subramoni, K. Tomko, D. Pekurovsky, and D. K. Panda,
“A Novel Functional Partitioning Approach to Design High-Performance
MPI-3 Non-blocking Alltoallv Collective on Multi-core Systems,” in
2013 42nd International Conference on Parallel Processing, Oct. 2013,
pp. 611–620.
[21] F. Trahay and A. Denis, “A Scalable and Generic Task Scheduling
System for Communication Libraries,” in IEEE Cluster, Sep. 2009.
[22] F. Trahay, É. Brunet, and A. Denis, “An Analysis of the Impact of
Multi-Threading on Communication Performance,” in 9th Workshop on
Communication Architecture for Clusters (CAC), May 2009.
[23] S. Kumar, A. R. Mamidala, D. A. Faraj, B. Smith, M. Blocksome,
B. Cernohous, D. Miller, J. Parker, J. Ratterman, P. Heidelberger,
D. Chen, and B. Steinmacher-Burrow, “PAMI: A Parallel Active Message Interface for the Blue Gene/Q Supercomputer,” in 2012 IEEE
26th International Parallel and Distributed Processing Symposium, May
2012, pp. 763–773.
[24] K. Vaidyanathan, D. D. Kalamkar, K. Pamnany, J. R. Hammond, P. Balaji, D. Das, J. Park, and B. Joó, “Improving Concurrency and Asynchrony in Multithreaded MPI Applications Using Software Offloading,”
in Proceedings of the International Conference for High Performance
Computing, Networking, Storage and Analysis, ser. SC ’15, 2015.
[25] R. Brightwell and K. Underwood, Evaluation of an Eager Protocol
Optimization for MPI. Springer Berlin Heidelberg, 2003, pp. 327–334.