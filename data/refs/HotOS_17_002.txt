[1] AL-FARES, M., LOUKISSAS, A., AND VAHDAT, A. A scalable, commodity
data center network architecture. In Proceedings of the 2008 ACM SIGCOMM
Conference (Aug. 2008), pp. 63-74.

[2] ALSBERG, P. A., AND Day, J. D. A principle for resilient sharing of distributed
resources. In Proceedings of the 2nd International Conference on Software
Engineering (ICSE) (Oct. 1976), pp. 562-570.

[3] AMAZON. AWS service outage on October 22nd, 2012. https://aws.amazon.com/
message/680342.

[4] ANDREYEV, A. Introducing Data Center Fabric, The Next-generation Facebook
Data Center Network. https://code.facebook.com/posts/360346274145943/, Nov.
2014.

[5] BopIk, P., GOLDSZMIDT, M., Fox, A., WOODARD, D. B., AND ANDERSEN,
H. Fingerprinting the datacenter: Automated classification of performance crises.
In Proceedings of the 5th European Conference on Computer Systems (EuroSys)
(Apr. 2010), pp. 111-124.

[6] CASTRO, M., AND LISKOV, B. Practical Byzantine fault tolerance. In Proceedings
of the 3rd Symposium on Operating Systems Design and Implementation (OSDI)
(Feb. 1999), pp. 173-186.

[7] CHow, M., MEISNER, D., FLINN, J., PEEK, D., AND WENISCH, T. F. The

mystery machine: End-to-end performance analysis of large-scale Internet services.

In Proceedings of the 11th USENIX Conference on Operating Systems Design and

Implementation (OSDI) (Oct. 2014), pp. 217-231.

[8] CLEMENT, A., WONG, E., ALVISI, L., DAHLIN, M., AND MARCHETTI, M.

Making Byzantine fault tolerant systems tolerate Byzantine faults. In Proceedings

of the 6th USENIX Symposium on Networked Systems Design and Implementation

(NSDI) (Apr. 2009), pp. 153-168.

[9] COHEN, I., GOLDSZMIDT, M., KELLY, T., SYMONS, J., AND CHASE, J. S.

Correlating instrumentation data to system states: A building block for automated

diagnosis and control. In Proceedings of the 6th Conference on Symposium on
Operating Systems Design and Implementation (OSDI) (2004), pp. 16-16.
[10] CoHEN, I., ZHANG, S., GOLDSZMIDT, M., SYMONS, J., KELLY, T., AND Fox,
A. Capturing, indexing, clustering, and retrieving system history. In Proceedings
of the 20th ACM Symposium on Operating Systems Principles (SOSP) (Oct. 2005),
pp. 105-118.

[11] Gray, J. Why do computers stop and what can be done about it? In Proc.
Symposium on Reliability in Distributed Software and Database Systems (1986),
pp. 3-12.

[12] GREENBERG, A., HAMILTON, J. R., JAIN, N., KANDULA, S., KIM, C., LAHIRI,
P., MALTZ, D. A., PATEL, P., AND SENGUPTA, S. VL2: A scalable and flexible
data center network. In Proceedings of the 2009 ACM SIGCOMM Conference
(Aug. 2009), pp. 51-62.

[13] GUNAWI, H. S., HAo, M., SUMINTO, R. O., LAKSONO, A., SATRIA, A. D.,
ADITYATAMA, J., AND ELIAZAR, K. J. Why does the cloud stop computing?:
Lessons from hundreds of service outages. In Proceedings of the 7th ACM
Symposium on Cloud Computing (SoCC) (Oct. 2016), pp. 1-16.

[14] Guo, C., YUAN, L., XIANG, D., DANG, Y., HUANG, R., MALTz, D., Liu,
Z., WANG, V., PANG, B., CHEN, H., Lin, Z.-W., AND KURIEN, V. Pingmesh:
A large-scale system for data center network latency measurement and analysis.
In Proceedings of the 2015 ACM SIGCOMM Conference (New York, NY, USA,
2015), SIGCOMM ”15, ACM, pp. 139-152.

[15] HUANG, P., JIN, X., BOLOSKY, W. J., AND ZHOU, Y. Why does a cloud-scale
service fail despite fault-tolerance? Unpublished internal document (2014).

[16] LAMPoRT, L. The part-time parliament. ACM Transactions on Computer Systems
(TOCS) 16, 2 May 1998), 133-169.

[17] LeNgERs, J. B., GUPTA, T., AGUILERA, M. K., AND WALFISH, M. Improving
availability in distributed systems with failure informers. In Proceedings of the
10th USENIX Conference on Networked Systems Design and Implementation
(NSDI) (Apr. 2013), pp. 427-442.

[18] LENERS, J. B., Wu, H., HUNG, W.-L., AGUILERA, M. K., AND WALFISH,
M. Detecting failures in distributed systems with the Falcon spy network. In
Proceedings of the 23rd ACM Symposium on Operating Systems Principles (SOSP)
(Oct. 2011), pp. 279-294.

[19] MICROSOFT. Office 365 service incident on November 13th, 2013. https://blogs.
office.com/2012/11/13/update-on-recent-customer-issues/.

[20] OPPENHEIMER, D., GANAPATHI, A., AND PATTERSON, D. A. Why do Internet
services fail, and what can be done about it? In Proceedings of the 4th Conference
on USENIX Symposium on Internet Technologies and Systems (USITS) (Mar.
2003).

[21] PATTERSON, D. A., GIBSON, G., AND KATZ, R. H. A case for redundant
arrays of inexpensive disks (RAID). In Proceedings of the 1988 ACM SIGMOD

International Conference on Management of Data (1988), pp. 109-116.
[22] SINGH, A., ONG, J., AGARWAL, A., ANDERSON, G., ARMISTEAD, A., BAN
NON, R., BOVING, S., DESAI, G., FELDERMAN, B., GERMANO, P., KANAGALA, A., PROVOST, J., SIMMONS, J., TANDA, E., WANDERER, J., HOLZLE,
U., STUART, S., AND VAHDAT, A. Jupiter rising: A decade of Clos topologies
and centralized control in Google’s datacenter network. In Proceedings of the
2015 ACM SIGCOMM Conference (Aug. 2015), SIGCOMM ’15, pp. 183-197.

[23] VAN RENESSE, R., AND SCHNEIDER, F. B. Chain replication for supporting high
throughput and availability. In Proceedings of the 6th Conference on Symposium
on Operating Systems Design (OSDI) (Dec. 2004), pp. 91-104.