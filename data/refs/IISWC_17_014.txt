[1] R. Adolf, S. Rama, B. Reagen, G.-Y. Wei, and D. Brooks, “Fathom:
reference workloads for modern deep learning methods,” in Workload
Characterization (IISWC), 2016 IEEE International Symposium on.
IEEE, 2016, pp. 1–10.
[2] V. Sze, Y.-H. Chen, T.-J. Yang, and J. Emer, “Efﬁcient processing of deep neural networks: A tutorial and survey,” arXiv preprint
arXiv:1703.09039, 2017.
[3] F. N. Iandola, M. W. Moskewicz, K. Ashraf, S. Han, W. J. Dally,
and K. Keutzer, “Squeezenet: Alexnet-level accuracy with 50x fewer
parameters and¡ 1mb model size,” arXiv preprint arXiv:1602.07360,
2016.
[4] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,”
in Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2015, pp. 1–9.
[5] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications,” arXiv preprint
arXiv:1704.04861, 2017.
[6] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing
deep neural networks with pruning, trained quantization and huffman
coding,” arXiv preprint arXiv:1510.00149, 2015.
[7] T.-J. Yang, Y.-H. Chen, and V. Sze, “Designing energy-efﬁcient convolutional neural networks using energy-aware pruning,” arXiv preprint
arXiv:1611.05128, 2016.
[8] M. Courbariaux, Y. Bengio, and J.-P. David, “Training deep neural networks with low precision multiplications,” arXiv preprint
arXiv:1412.7024, 2014.
[9] N. D. Lane, S. Bhattacharya, P. Georgiev, C. Forlivesi, L. Jiao, L. Qendro, and F. Kawsar, “Deepx: A software accelerator for low-power
deep learning inference on mobile devices,” in 2016 15th ACM/IEEE
International Conference on Information Processing in Sensor Networks
(IPSN). IEEE, 2016, pp. 1–12.
[10] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B. Catanzaro,
and E. Shelhamer, “cudnn: Efﬁcient primitives for deep learning,” arXiv
preprint arXiv:1410.0759, 2014.
[11] 2017. [Online]. Available: https://developer.arm.com/technologies/
compute-library
[12] 2016. [Online]. Available: https://cloudplatform.googleblog.com/2016/
05/Google-supercharges-machine-learning-tasks-with-custom-chip.
html
[13] 2016.
[Online].
Available:
https://github.com/soumith/
convnet-benchmarks
[14] E. Garcia-Martin, N. Lavesson, and H. Grahn, “Identiﬁcation of energy
hotspots: A case study of the very fast decision tree,” in International
Conference on Green, Pervasive, and Cloud Computing. Springer, 2017,
pp. 267–281.
[15] N. D. Lane, S. Bhattacharya, P. Georgiev, C. Forlivesi, and F. Kawsar,
“An early resource characterization of deep learning on wearables,
smartphones and internet-of-things devices,” in Proceedings of the 2015
International Workshop on Internet of Things towards Applications.
ACM, 2015, pp. 7–12.
[16] 2017. [Online]. Available: http://www.ti.com/product/INA3221/
datasheet
[17] 2016.
[Online].
Available:
https://developer.arm.com/products/
software-development-tools/ds-5-development-studio/streamline
[18] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for
fast feature embedding,” in Proceedings of the ACM International
Conference on Multimedia. ACM, 2014, pp. 675–678.
[19] A. Berg, J. Deng, and L. Fei-Fei, “Large scale visual recognition challenge (ilsvrc), 2010,” URL http://www. image-net. org/challenges/LSVRC, 2010.
