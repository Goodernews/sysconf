
[1] Apache Mahout. mahout.apache.org.
[2] Apache Spark. https://en.wikipedia.org/wiki/Apache_Spark.
[3] Grid Search.

http://scikit-learn.org/stable/modules/grid_search.html.

[4] Hinge Loss and Smoothed Variants.

https://en.wikipedia.org/wiki/Hinge_loss.

[5] How many secrets do you have? https://github.com/

frankmcsherry/blog/blob/master/posts/2017-02-08.md.

[6] Preprocessing data in machine learning.

http://scikit-learn.org/stable/modules/preprocessing.html.

[7] Random Projection.

https://en.wikipedia.org/wiki/Random_projection.

[8] random unit vector in multi-dimensional space.

http://stackoverﬂow.com/questions/6283080/
random-unit-vector-in-multi-dimensional-space.

[9] M. Abadi, A. Chu, I. J. Goodfellow, H. B. McMahan,

I. Mironov, K. Talwar, and L. Zhang. Deep learning with
differential privacy. In Proceedings of the 2016 ACM
SIGSAC Conference on Computer and Communications
Security, Vienna, Austria, October 24-28, 2016, pages
308–318, 2016.

[10] R. Bassily, A. Smith, and A. Thakurta. Private empirical risk
minimization: Efﬁcient algorithms and tight error bounds. In
FOCS, 2014.

[11] S. Boyd and L. Vandenberghe. Convex Optimization.

Cambridge University Press, New York, NY, USA, 2004.

[12] S. Bubeck. Convex optimization: Algorithms and

complexity. Foundations and Trends in Machine Learning,
8(3-4):231–357, 2015.

[13] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate.

Differentially private empirical risk minimization. Journal of
Machine Learning Research, 12:1069–1109, 2011.
[14] K. Chaudhuri and S. A. Vinterbo. A stability-based

validation procedure for differentially private machine
learning. In NIPS, 2013.

[15] J. C. Duchi, M. I. Jordan, and M. J. Wainwright. Local
privacy and statistical minimax rates. In FOCS, 2013.

[16] C. Dwork, F. McSherry, K. Nissim, and A. Smith.

Calibrating noise to sensitivity in private data analysis. In
TCC, 2006.

[17] C. Dwork and A. Roth. The algorithmic foundations of

differential privacy. Foundations and Trends in Theoretical
Computer Science, 9(3-4):211–407, 2014.

[18] Ú. Erlingsson, V. Pihur, and A. Korolova. RAPPOR:
randomized aggregatable privacy-preserving ordinal
response. In Proceedings of the 2014 ACM SIGSAC
Conference on Computer and Communications Security,
Scottsdale, AZ, USA, November 3-7, 2014, pages 1054–1067,
2014.

[19] X. Feng, A. Kumar, B. Recht, and C. Ré. Towards a uniﬁed

architecture for in-rdbms analytics. In SIGMOD, 2012.

[20] J. Gray et al. Data Cube: A Relational Aggregation Operator

Generalizing Group-By, Cross-Tab, and Sub-Totals. Data
Min. Knowl. Discov., 1(1):29–53, Jan. 1997.

[21] M. Hardt, B. Recht, and Y. Singer. Train faster, generalize

better: Stability of stochastic gradient descent. ArXiv
e-prints, Sept. 2015.

[22] M. Hay, A. Machanavajjhala, G. Miklau, Y. Chen, and
D. Zhang. Principled evaluation of differentially private

algorithms using dpbench. CoRR, abs/1512.04817, 2015.

[23] J. Hellerstein et al. The MADlib Analytics Library or MAD

Skills, the SQL. In VLDB, 2012.

[24] P. Jain, P. Kothari, and A. Thakurta. Differentially private

online learning. In COLT, 2012.

[25] P. Jain and A. Thakurta. Differentially private learning with

kernels. In ICML, 2013.

[26] R. Johnson and T. Zhang. Accelerating stochastic gradient
descent using predictive variance reduction. In NIPS, 2013.

[27] D. Kifer, A. D. Smith, and A. Thakurta. Private convex

optimization for empirical risk minimization with
applications to high-dimensional regression. In COLT, 2012.

[28] A. Nemirovsky and D. Yudin. Problem complexity and

method efﬁciency in optimization. 1983.

[29] Y. Nesterov. Introductory lectures on convex optimization : a
basic course. Applied optimization. Kluwer Academic Publ.,
2004.

[30] N. Parikh and S. P. Boyd. Proximal algorithms. Foundations

and Trends in Optimization, 1(3):127–239, 2014.

[31] B. T. Polyak. Introduction to optimization. Optimization

Software, 1987.

[32] N. L. Roux, M. W. Schmidt, and F. R. Bach. A stochastic
gradient method with an exponential convergence rate for
ﬁnite training sets. In NIPS, 2012.

[33] B. I. P. Rubinstein, P. L. Bartlett, L. Huang, and N. Taft.

Learning in a large function space: Privacy-preserving
mechanisms for SVM learning. CoRR, abs/0911.5708, 2009.

[34] O. Shamir. Without-Replacement Sampling for Stochastic

Gradient Methods: Convergence Results and Application to
Distributed Optimization. ArXiv e-prints, Mar. 2016.
[35] S. Song, K. Chaudhuri, and A. D. Sarwate. Stochastic
gradient descent with differentially private updates. In
GlobalSIP, 2013.

[36] J. Zhang, X. Xiao, Y. Yang, Z. Zhang, and M. Winslett.

Privgene: differentially private model ﬁtting using genetic
algorithms. In SIGMOD, 2013.

[37] J. Zhang, Z. Zhang, X. Xiao, Y. Yang, and M. Winslett.

Functional mechanism: Regression analysis under
differential privacy. PVLDB, 2012.

[38] M. Zinkevich. Online convex programming and generalized

inﬁnitesimal gradient ascent. In ICML, 2003.

