[1] Roger Cavallo and Michael Pittarelli. 1987. The Theory of Probabilistic Databases. In Proceedings of the 13th International Conference on Very Large Data Bases
(VLDB), Brighton, UK. 71–81.
[2] Thomas M. Cover and Joy A. Thomas. 1991. Elements of Information Theory.
Wiley-Interscience, New York, NY, USA.
[3] Mehmet M. Dalkilic and Edward L. Roberston. 2000. Information Dependencies.
In Proceedings of the 19th ACM SIGACT-SIGMOD-SIGART Symposium on Principles
of Database Systems. ACM, 245–253.
[4] Alin Dobra and Johannes Gehrke. 2001. Bias Correction in Classification Tree
Construction. In Proceedings of the 18th International Conference on Machine
Learning (ICML), Williams College, MA. Morgan Kaufmann, 90–97.
[5] Usama M. Fayyad and Keki B. Irani. 1993. Multi-Interval Discretization of Continuous-Valued Attributes for Classification Learning. In Proceedings of the 13th International Joint Conference on Artificial Intelligence (IJCAI), Chambe ́ry,
France. 1022–1029.
[6] Luca M Ghiringhelli, Jan Vybiral, Sergey V Levchenko, Claudia Draxl, and
Matthias Scheffler. 2015. Big data of materials science: Critical role of the
descriptor. Physical review letters 114, 10 (2015), 105503.
[7] Chris Giannella and Edward L. Robertson. 2004. On approximation measures for
functional dependencies. Information Systems 29, 6 (2004), 483–507.
[8] Bryan R Goldsmith, Mario Boley, Jilles Vreeken, Matthias Scheffler, and Luca M Ghiringhelli. 2017. Uncovering structure-property relationships of materials by
subgroup discovery. New Journal of Physics 19, Article 013031 (2017), 14 pages.
[9] Lawrence Hubert and Phipps Arabie. 1985. Comparing partitions. Journal of
Classification 2, 1 (1985), 193–218.
[10] Yka ̈ Huhtala, Juha Ka ̈rkka ̈inen, Pasi Porkka, and Hannu Toivonen. 1999. TANE:
An efficient algorithm for discovering functional and approximate dependencies.
Comput. J. 42, 2 (1999), 100–111.
[11] Igor Kononenko. 1995. On Biases in Estimating Multi-valued Attributes. In
Proceedings of the 14th International Joint Conference on Artificial Intelligence
(IJCAI), San Francisco,CA,USA. 1034–1040.
[12] Andreas Krause and Carlos E Guestrin. 2012. Near-optimal nonmyopic value of
information in graphical models. Technical Report 1207.1394. arXiv.
[13] H.O. Lancaster. 1969. The chi-squared distribution. Wiley.
[14] Jixue Liu, Jiuyong Li, Chengfei Liu, and Yongfeng Chen. 2012. Discover dependen-
cies from data–a review. IEEE Transactions on Knowledge and Data Engineering
24, 2 (2012), 251–264.
[15] Christopher J Matheus and Larry A Rendell. 1989. Constructive Induction
On Decision Trees. In Proceedings of the 11th International Joint Conference on
Artificial Intelligence (IJCAI), Detroit, MI. 645.
[16] Kurt Mehlhorn and Peter Sanders. 2008. Algorithms and data structures: The basic
toolbox. Springer Science & Business Media.
[17] Hoang Vu Nguyen, Panagiotis Mandros, and Jilles Vreeken. 2016. Universal
Dependency Analysis. In Proceedings of the SIAM International Conference on
Data Mining (SDM), Miami, FL. SIAM, 792–800.
[18] Murali Rao, Yunmei Chen, Baba C. Vemuri, and Fei Wang. 2004. Cumulative
Residual Entropy: A New Measure of Information. IEEE Transactions on Infor-
mation Technology 50, 6 (2004), 1220–1228.
[19] Matthew Reimherr and Dan L Nicolae. 2013. On quantifying dependence: A
framework for developing interpretable measures. Statist. Sci. 28, 1 (2013), 116–
130.
[20] Simone Romano, James Bailey, Xuan Vinh Nguyen, and Karin Verspoor. 2014.
Standardized Mutual Information for Clustering Comparisons: One Step Further in Adjustment for Chance.. In Proceedings of the 31st International Conference on Machine Learning (ICML), Beijing, China. 1143–1151.
[21] Simone Romano, Nguyen Xuan Vinh, James Bailey, and Karin Verspoor. 2016. A Framework to Adjust Dependency Measure Estimates for Chance. In Proceedings of the SIAM International Conference on Data Mining (SDM), Miami, FL. SIAM.
[22] Mark S Roulston. 1999. Estimating the errors on measured entropy and mutual information. Physica D: Nonlinear Phenomena 125, 3 (1999), 285–294.
[23] Claude E. Shannon. 1948. A Mathematical Theory of Communication. The Bell System Technical Journal 27, 3 (1948), 379–423.
[24] James A Van Vechten. 1969. Quantum dielectric theory of electronegativity in covalent systems. I. Electronic dielectric constant. Physical Review 182, 3 (1969), 891.
[25] Nguyen Xuan Vinh, Julien Epps, and James Bailey. 2009. Information theoretic measures for clusterings comparison: is a correction for chance necessary?. In Proceedings of the 26th International Conference on Machine Learning (ICML), Montreal, Canada. ACM, 1073–1080.
[26] Nguyen Xuan Vinh, Julien Epps, and James Bailey. 2010. Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance. Journal of Machine Learning Research 11, Oct (2010), 2837–2854.
[27] Yisen Wang, Simone Romano, Vinh Nguyen, James Bailey, Xingjun Ma, and Shu-Tao Xia. 2017. Unbiased Multivariate Correlation Analysis. In Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI). AAAI.
[28] Allan P. White and Wei Zhong Liu. 1994. Technical Note: Bias in Information- Based Measures in Decision Tree Induction. Machine Learning 15, 3 (1994), 321–329.
