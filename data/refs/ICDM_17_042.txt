[1] K. Fukushima and S. Miyake, “Neocognitron: A self-organizing neural
network model for a mechanism of visual pattern recognition,” in
Competition and cooperation in neural nets. Springer, 1982, pp. 267–
285.
[2] Y. LeCun, J. Denker, D. Henderson, R. Howard, W. Hubbard, and
L. Jackel, “Handwritten digit recognition with a back-propagation network,” in Advances in Neural Information Processing Systems. Citeseer,
1990.
[3] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for
fast feature embedding,” in Proceedings of the 22nd ACM international
conference on Multimedia. ACM, 2014, pp. 675–678.
[4] R. Girshick, “Fast r-cnn,” in Proceedings of the IEEE International
Conference on Computer Vision, 2015, pp. 1440–1448.
[5] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks
for semantic segmentation,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2015, pp. 3431–3440.
[6] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
computation, vol. 9, no. 8, pp. 1735–1780, 1997.
[7] A. Graves, A.-r. Mohamed, and G. Hinton, “Speech recognition with
deep recurrent neural networks,” in Acoustics, speech and signal processing (icassp), 2013 ieee international conference on. IEEE, 2013,
pp. 6645–6649.
[8] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
with neural networks,” in Advances in neural information processing
systems, 2014, pp. 3104–3112.
[9] G. E. Hinton, “Training products of experts by minimizing contrastive
divergence,” Neural computation, vol. 14, no. 8, pp. 1771–1800, 2002.
[10] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv
preprint arXiv:1312.6114, 2013.
[11] D. J. Rezende, S. Mohamed, and D. Wierstra, “Stochastic backpropagation and approximate inference in deep generative models,” in
Proceedings of The 31st International Conference on Machine Learning,
2014, pp. 1278–1286.
[12] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in
Advances in neural information processing systems, 2014, pp. 2672–
2680.
[13] J. Donahue, P. Krähenbühl, and T. Darrell, “Adversarial feature learning,” 2016.
[14] J. T. Springenberg, “Unsupervised and semi-supervised learning
with categorical generative adversarial networks,” arXiv preprint
arXiv:1511.06390, 2015.
[15] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford,
X. Chen, and X. Chen, “Improved techniques for training
gans,” in Advances in Neural Information Processing Systems
29, D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and
R. Garnett, Eds.
Curran Associates, Inc., 2016, pp. 2234–
2242. [Online]. Available: http://papers.nips.cc/paper/6125-improvedtechniques-for-training-gans.pdf
[16] M. Noroozi and P. Favaro, “Unsupervised learning of visual representations by solving jigsaw puzzles,” in European Conference on Computer
Vision. Springer, 2016, pp. 69–84.
[17] C. Doersch, A. Gupta, and A. A. Efros, “Unsupervised visual representation learning by context prediction,” in Proceedings of the IEEE
International Conference on Computer Vision, 2015, pp. 1422–1430.
[18] X. Wang and A. Gupta, “Unsupervised learning of visual representations
using videos,” in Proceedings of the IEEE International Conference on
Computer Vision, 2015, pp. 2794–2802.
[19] J. Yang, D. Parikh, and D. Batra, “Joint unsupervised learning of
deep representations and image clusters,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2016, pp.
5147–5156.
[20] J. Xie, R. Girshick, and A. Farhadi, “Unsupervised deep embedding
for clustering analysis,” in Proceedings of The 33rd International
Conference on Machine Learning, 2016, pp. 478–487.
[21] N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P.
Tang, “On large-batch training for deep learning: Generalization gap and
sharp minima,” arXiv preprint arXiv:1609.04836, 2016.
[22] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
adversarial examples,” arXiv preprint arXiv:1412.6572, 2014.
[23] R. Salakhutdinov and G. Hinton, “Deep boltzmann machines,” in
Artiﬁcial Intelligence and Statistics, 2009, pp. 448–455.
[24] Y. Bengio, P. Lamblin, D. Popovici, H. Larochelle et al., “Greedy layerwise training of deep networks.”
[25] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel,
and Y. Bengio, “Show, attend and tell: Neural image caption generation
with visual attention,” in International Conference on Machine Learning,
2015, pp. 2048–2057.
[26] D. P. Kingma, S. Mohamed, D. J. Rezende, and M. Welling, “Semisupervised learning with deep generative models,” in Advances in Neural
Information Processing Systems, 2014, pp. 3581–3589.
[27] V. Dumoulin, I. Belghazi, B. Poole, A. Lamb, M. Arjovsky, O. Mastropietro, and A. Courville, “Adversarially learned inference,” arXiv preprint
arXiv:1606.00704, 2016.
[28] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey, “Adversarial autoencoders,” arXiv preprint arXiv:1511.05644, 2015.
[29] A. Dosovitskiy, J. T. Springenberg, M. Riedmiller, and T. Brox, “Discriminative unsupervised feature learning with convolutional neural
networks,” in Advances in Neural Information Processing Systems, 2014,
pp. 766–774.
[30] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE, vol. 86,
no. 11, pp. 2278–2324, 1998.
[31] T. K. Landauer, P. W. Foltz, and D. Laham, “An introduction to latent
semantic analysis,” Discourse processes, vol. 25, no. 2-3, pp. 259–284,
1998.
[32] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent dirichlet allocation,”
Journal of machine Learning research, vol. 3, no. Jan, pp. 993–1022,
2003.
[33] A. Rasmus, M. Berglund, M. Honkala, H. Valpola, and T. Raiko,
“Semi-supervised learning with ladder networks,” in Advances in Neural
Information Processing Systems, 2015, pp. 3546–3554.
[34] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng,
“Reading digits in natural images with unsupervised feature learning.”
[35] L. Maaløe, C. K. Sønderby, S. K. Sønderby, and O. Winther, “Auxiliary
deep generative models,” arXiv preprint arXiv:1602.05473, 2016.
[36] K. Swersky, J. Snoek, and R. P. Adams, “Multi-task bayesian
optimization,” in Advances in Neural Information Processing Systems
26, C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani,
and K. Q. Weinberger, Eds. Curran Associates, Inc., 2013, pp.
2004–2012. [Online]. Available: http://papers.nips.cc/paper/5086-multitask-bayesian-optimization.pdf
