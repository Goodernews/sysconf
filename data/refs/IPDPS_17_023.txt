[1] Nwhit - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/
wiki/File:Diagram of a marine seismic survey.png.
[2] D. Ristow and T. Rühl, “Fourier ﬁnite-difference migration,” Geophysics, vol. 59, no. 12, pp. 1882–1893, 1994.
[3] ——, “3-d implicit ﬁnite-difference migration by multiway splitting,”
Geophysics, vol. 62, no. 2, pp. 554–567, 1997.
[4] E. Baysal, D. D. Kosloff, and J. W. Sherwood, “Reverse time migration,”
Geophysics, vol. 48, no. 11, pp. 1514–1524, 1983.
[5] “Summit - the next peak in HPC,” https://www.olcf.ornl.gov/summit.
[6] “Sierra advanced technology system,” http://computation.llnl.gov/
computers/sierra-advanced-technology-system.
[7] “Aurora,” http://aurora.alcf.anl.gov.
[8] Khronos OpenCL Working Group et al., “The OpenCL speciﬁcation,”
version, vol. 1, no. 29, p. 8, 2008.
[9] “OpenACC standard,” www.openacc.org.
[10] “The OpenMP API speciﬁcation for parallel programming,” www.
openmp.org.

[11] “NVIDIA CUDA parallel computing platform,” http://www.nvidia.com/
cuda.
[12] S. Wienke, P. Springer, C. Terboven, and D. an Mey, “OpenACC–ﬁrst
experiences with real-world applications,” in Euro-Par 2012 Parallel
Processing. Springer, 2012, pp. 859–870.
[13] J. Herdman, W. Gaudin, S. McIntosh-Smith, M. Boulton, D. Beckingsale, A. Mallinson, and S. A. Jarvis, “Accelerating hydrocodes with
OpenACC, OpenCL and CUDA,” in High Performance Computing,
Networking, Storage and Analysis (SCC), 2012 SC Companion:. IEEE,
2012, pp. 465–471.
[14] T. Hoshino, N. Maruyama, S. Matsuoka, and R. Takaki, “CUDA vs
OpenACC: Performance case studies with kernel benchmarks and a
memory-bound CFD application,” in Cluster, Cloud and Grid Computing
(CCGrid), 2013 13th IEEE/ACM International Symposium on. IEEE,
2013, pp. 136–143.
[15] S. Christgau, J. Spazier, B. Schnor, M. Hammitzsch, A. Babeyko, and
J. Waechter, “A comparison of CUDA and OpenACC: accelerating the
tsunami simulation easywave,” in Architecture of Computing Systems
(ARCS), 2014 27th International Conference on. VDE, 2014, pp. 1–5.
[16] A. Hart, R. Ansaloni, and A. Gray, “Porting and scaling OpenACC
applications on massively-parallel, GPU-accelerated supercomputers,”
The European Physical Journal Special Topics, vol. 210, no. 1, pp. 5–16,
2012.
[17] F. Aminzadeh, B. Jean, and T. Kunz, 3-D salt and overthrust models.
Society of Exploration Geophysicists, 1997.
[18] “The OpenACC application programming interface,” http://openacc.org,
2013.
[19] “OpenACC Speciﬁcation 2.5,” http://www.openacc.org/sites/default/
ﬁles/OpenACC 2pt5.pdf.
[20] NVIDIA, “Cufft library,” http://docs.nvidia.com/cuda/cufft, 2010.
[21] M. Fehler and P. Keliher, SEAM Phase 1: Challenges of Subsalt
Imaging in Tertiary Basins, with Emphasis on Deepwater Gulf
of Mexico. Society of Exploration Geophysicists, 2011. [Online].
Available: http://library.seg.org/doi/abs/10.1190/1.9781560802945
[22] NVIDIA, “CuSparse Users guide,” http://docs.nvidia.com/cuda/cusparse,
2013.
[23] “Top 500,” http://www.top500.org/lists/2015/11/.
[24] “Titan supercomputer,” https://www.olcf.ornl.gov/titan.
[25] J. F. Lofstead, S. Klasky, K. Schwan, N. Podhorszki, and C. Jin,
“Flexible I/O and integration for scientiﬁc codes through the adaptable
I/O system (ADIOS),” in Proceedings of the 6th international workshop
on Challenges of large applications in distributed environments. ACM,
2008, pp. 15–24.
[26] P. Carns, R. Latham, R. Ross, K. Iskra, S. Lang, and K. Riley, “24/7
characterization of petascale I/O workloads,” in Cluster Computing and
Workshops, 2009. CLUSTER’09. IEEE International Conference on.
IEEE, 2009, pp. 1–10.

[27] M. Wolfe, “OpenACC 2.5 and Beyond,” http://on-demand.gputechconf.
com/gtc/2015/presentation/S5382-Michael-Wolfe.pdf.
[28] J. Beyer, D. Oehmke, and J. Sandoval, “Transferring user-deﬁned types
in OpenACC,” CUG2014 Final Proceedings, CUG2014, Lugano, May,
2014.
[29] R. Abdelkhalek, H. Calandra, O. Coulaud, J. Roman, and G. Latu, “Fast
seismic modeling and reverse time migration on a GPU cluster,” in High
Performance Computing & Simulation, 2009. HPCS’09. International
Conference on. IEEE, 2009, pp. 36–43.
[30] D. Foltinek, D. Eaton, J. Mahovsky, P. Moghaddam, R. McGarry et al.,
“Industrial-scale reverse time migration on GPU hardware,” in 2009 SEG
Annual Meeting. Society of Exploration Geophysicists, 2009.
[31] J. M. Levesque, R. Sankaran, and R. Grout, “Hybridizing S3D into an
exascale application using OpenACC: an approach for moving to multipetaﬂops and beyond,” in Proceedings of the International conference on
high performance computing, networking, storage and analysis. IEEE
Computer Society Press, 2012, p. 15.
[32] B. Cumming, C. Osuna, T. Gysi, M. Bianco, X. Lapillonne, O. Fuhrer,
and T. C. Schulthess, “A review of the challenges and results of
refactoring the community climate code cosmo for hybrid cray hpc
systems,” Proceedings of Cray User Group, 2013.
[33] O. Fuhrer, C. Osuna, X. Lapillonne, T. Gysi, B. Cumming, M. Bianco,
A. Arteaga, and T. Schulthess, “Towards a performance portable, architecture agnostic implementation strategy for weather and climate
models,” Supercomputing frontiers and innovations, vol. 1, no. 1, 2014.
[34] W. Sawyer, G. Zaengl, and L. Linardakis, “Towards a multi-node OpenACC implementation of the icon model,” in EGU General Assembly
Conference Abstracts, vol. 16, 2014, p. 15276.
[35] M. Norman, J. Larkin, A. Vose, and K. Evans, “A case study
of CUDA FORTRAN and OpenACC for an atmospheric climate
kernel,” Journal of Computational Science, vol. 9, pp. 1–6, 2015,
computational Science at the Gates of Nature. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S1877750315000605
[36] T. Hoshino, N. Maruyama, S. Matsuoka, and R. Takaki, “CUDA vs
OpenACC: Performance case studies with kernel benchmarks and a
memory-bound CFD application,” in Cluster, Cloud and Grid Computing
(CCGrid), 2013 13th IEEE/ACM International Symposium on, May
2013, pp. 136–143.
[37] A. Hart, OpenMP: Heterogenous Execution and Data Movements: 11th
International Workshop on OpenMP, IWOMP 2015, Aachen, Germany,
October 1-2, 2015, Proceedings. Cham: Springer International
Publishing, 2015, ch. First Experiences Porting a Parallel Application
to a Hybrid Supercomputer with OpenMP4.0 Device Constructs, pp. 73–
85. [Online]. Available: http://dx.doi.org/10.1007/978-3-319-24595-9 6

