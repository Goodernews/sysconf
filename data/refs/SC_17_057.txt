[1] 2017. Center for Exascale Simulation of Advanced Reactors. https://cesar.mcs.
anl.gov. (2017).
[2] 2017. Center for Exascale Simulation of Combustion in Turbulence. https:
//science.energy.gov/ascr/research/scidac/co-design/. (2017).
[3] 2017,. CORAL Benchmarks. https://asc.llnl.gov/CORAL-benchmarks. (2017,).
[4] 2017. Livermore Unstructured Lagrangian Explicit Shock Hydrodynamics
(LULESH). https://codesign.llnl.gov/lulesh.php. (2017).
[5] 2017. Monte Carlo Benchmark (MCB). https://codesign.llnl.gov/mcb.php. (2017).
[6] 2017. NAS Parallel Benchmarks. http://www.nas.nasa.gov/publications/npb.html.
(2017).
[7] 2017. Nekbone. https://cesar.mcs.anl.gov/content/software/thermal_hydraulics.
(2017).
[8] 2017. QMCPack. http://qmcpack.org. (2017).
[9] 2017. The Local-Self-Consistent Mutliple-Scattering (LSMSL) Code. https://www.
ccs.ornl.gov/mri/repository/LSMS/index.html. (2017).
[10] Adnan Agbaria, Dong-In Kang, and Karandeep Singh. 2006. LMPI: MPI for
heterogeneous embedded distributed systems. In Parallel and Distributed Systems,
2006. ICPADS 2006. 12th International Conference on, Vol. 1. IEEE, 8śpp.
[11] Abdelhalim Amer, Pavan Balaji, Wesley Bland, William Gropp, Rob Latham,
Huiwei Lu, Lena Oden, Antonio Pena, Ken Rafenetti, Sangmin Seo, et al. 2015.
MPICH User’s Guide. (2015).
[12] Pavan Balaji, Darius Buntinas, D. Goodell, W. D. Gropp, and Rajeev Thakur. 2010.
International Journal of High Performance Computing Applications (IJHPCA) 24
(2010), 49ś57.
[13] Brian W Barrett, Ron Brightwell, Ryan Grant, Simon D Hammond, and K Scott
Hemmert. 2014. An evaluation of MPI message rate on hybrid-core processors.
(2014).
[14] Surendra Byna, Xian-He Sun, Rajeev Thakur, and William Gropp. 2006. Automatic
memory optimizations for improving MPI derived datatype performance. In
European Parallel Virtual Machine/Message Passing Interface UsersâĂŹ Group
Meeting. Springer, 238ś246.
[15] James Dinan, Pavan Balaji, Dave Goodell, Doug Miller, Marc Snir, and Rajeev
Thakur. 2013. Enabling MPI Interoperability through Flexible Communication
Endpoints. In Proceedings of the 17th European MPI Users’ Group Meeting Conference on Recent Advances in the Message Passing Interface (EuroMPI’13). Madrid,
Spain, 13ś18.
[16] P. Fischer, K. Heisey, and M. Min. 2015. Scaling Limits for PDE-Based Simulation
(Invited). In 22nd AIAA Computational Fluid Dynamics Conference, AIAA Aviation.
AIAA 2015-3049.
[17] P. Fischer, J. Lottes, and S. Kerkemeier. 2008. Nek5000: Open
source spectral element CFD solver. http://nek5000.mcs.anl.gov and
https://github.com/Nek5000/nek5000. (2008).
[18] P. F. Fischer and A. T. Patera. 1991. Parallel Spectral Element Solution of the
Stokes Problem. J. Comput. Phys. 92 (1991), 380ś421.
[19] Mario Flajslik, James Dinan, and Keith D Underwood. 2016. Mitigating MPI
message matching misery. In International Conference on High Performance Computing. Springer, 281ś299.
[20] William Gropp, Torsten Hoeler, Rajeev Thakur, and Ewing Lusk. 2004. Using
Advanced MPI: Modern Features of the Message-Passing Interface. MIT Press.
[21] William Gropp, Ewing Lusk, and Rajeev Thakur. 1999. Using MPI-2: Advanced
Features of the Message-Passing Interface. MIT Press.
[22] Yanfei Guo, Charles Archer, Michael Blocksome, Scott Parker, Wesley Bland,
Kenneth J. Rafenetti, and Pavan Balaji. 2017. Memory Compression Techniques
for Network Address Management in MPI. In IEEE International Parallel and
Distributed Processing Symposium (IPDPS). Orlando, Florida.
[23] Salman Habib, Vitali Morozov, Nicholas Frontiere, Hal Finkel, Adrian Pope, and
Katrin Heitmann. 2013. HACC: Extreme Scaling and Performance Across Diverse
Architectures. In Proceedings of the International Conference on High Performance
Computing, Networking, Storage and Analysis (SC ’13). ACM, New York, NY, USA,
Article 6, 10 pages. https://doi.org/10.1145/2503210.2504566
[24] Michael A Heroux, Douglas W Doerler, Paul S Crozier, James M Willenbring,
H Carter Edwards, Alan Williams, Mahesh Rajan, Eric R Keiter, Heidi K Thornquist, and Robert W Numrich. 2009. Improving Performance via Mini-applications.
Technical Report SAND2009-5574. Sandia National Laboratories.
[25] Torsten Hoeler, James Dinan, Rajeev Thakur, Brian Barrett, Pavan Balaji, William
Gropp, and Keith Underwood. 2015. Remote Memory Access Programming in
MPI-3. TOPC’15 (2015).
[26] MPI Forum. 2015. MPI: A Message Passing Interface Standard. (2015). http:
//www.mpi-forum.org/docs/docs.html.
[27] M. Otten, J. Gong, A. Mametjanov, A. Vose, J. Levesque, P. Fischer, and M. Min.
2016. An MPI/OpenACC Implementation of a High Order Electromagnetics
Solver with GPUDirect Communication. Int. J. High Perf. Comput. Appl. (2016).
[28] Mohammad J Rashti and Ahmad Afsahi. 2008. Improving communication
progress and overlap in mpi rendezvous protocol over rdma-enabled interconnects. In High Performance Computing Systems and Applications, 2008. HPCS 2008.
22nd International Symposium on. IEEE, 95ś101.
[29] Xian-He Sun et al. 2003. Improving the performance of MPI derived datatypes
by optimizing memory-access cost. In Cluster Computing, 2003. Proceedings. 2003
IEEE International Conference on. IEEE, 412ś419.
[30] Rajeev Thakur and William D Gropp. 2003. Improving the performance of
collective operations in MPICH. In European Parallel Virtual Machine/Message
Passing Interface UsersâĂŹ Group Meeting. Springer, 257ś267.
[31] H. M. Tufo and P. F. Fischer. 1999. Terascale Spectral Element Algorithms and
Implementations. In Proc. of the ACM/IEEE SC99 Conf. on High Performance
Networking and Computing, Gordon Bell Prize. IEEE Computer Soc., CDROM.
[32] Isaías A Comprés Ureña, Michael Riepen, and Michael Konow. 2011. RCKMPIś
lightweight MPI implementation for IntelâĂŹs Single-chip Cloud Computer
(SCC). In European MPI Users’ Group Meeting. Springer, 208ś217.
[33] M. Valiev, E. J. Bylaska, N. Govind, K. Kowalski, T. P. Straatsma, H. J. J. Van Dam,
D. Wang, J. Nieplocha, E. Apra, T. L. Windus, and W. A. de Jong. 2010. NWChem:
A Comprehensive and Scalable Open-Source Solution for Large Scale Molecular
Simulations. Computer Physics Communications 181, 9 (2010), 1477ś1489.
