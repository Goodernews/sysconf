[1] J. Kepner, P. Aaltonen, D. Bader, A. Bulu, F. Franchetti,
J. Gilbert, D. Hutchison, M. Kumar, A. Lumsdaine, H. Meyerhenke, S. McMillan, C. Yang, J. D. Owens, M. Zalewski,
T. Mattson, and J. Moreira, “Mathematical foundations of
the graphblas,” in 2016 IEEE High Performance Extreme
Computing Conference (HPEC), Sept 2016.
[2] Graph
BLAS
Forum.
[Online].
Available:
http://www.graphblas.org
[3] NVIDIA. (2017) Nvidia cuSPARSE library. [Online].
Available: http://developer.nvidia.com/cuSPARSE
[4] J. Demouth, “Sparse matrix-matrix multiplication on the gpu,”
in Proceedings of the GPU Technology Conference, 2012.
[5] W. Liu and B. Vinter, “Csr5: An efﬁcient storage format
for cross-platform sparse matrix-vector multiplication,”
in Proceedings of the 29th ACM on International
Conference on Supercomputing.
New York, NY,
USA: ACM, 2015, pp. 339–350. [Online]. Available:
http://doi.acm.org/10.1145/2751205.2751209
[6] D. Merrill and M. Garland, “Merge-based parallel sparse
matrix-vector multiplication,” in Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. Piscataway, NJ, USA: IEEE
Press, 2016, pp. 58:1–58:12.
[7] M. Steinberger, R. Zayer, and H.-P. Seidel, “Globally homogeneous, locally adaptive sparse matrix-vector multiplication
on the gpu,” in Proceedings of the International Conference
on Supercomputing. New York, NY, USA: ACM, 2017, pp.
13:1–13:11.
[8] W. Liu and B. Vinter, “An efﬁcient gpu general sparse matrix-
matrix multiplication for irregular data,” in 2014 IEEE 28th
International Parallel and Distributed Processing Symposium,
2014, pp. 370–381.
[9] ——, “A framework for general sparse matrix–matrix mul-
tiplication on gpus and heterogeneous processors,” Journal
of Parallel and Distributed Computing, vol. 85, pp. 47–61,
2015.
[10] P. N. Q. Anh, R. Fan, and Y. Wen, “Balanced hashing and
efﬁcient gpu sparse general matrix-matrix multiplication,” in
2016 International Conference on Supercomputing. New
York, NY, USA: ACM, 2016, pp. 36:1–36:12.
[11] F. Gremse, A. Hofter, L. O. Schwen, F. Kiessling, and
U. Naumann, “Gpu-accelerated sparse matrix-matrix multipli-
cation by iterative row merging,” SIAM Journal on Scientiﬁc
Computing, vol. 37, no. 1, pp. C54–C71, 2015.
[12] S. Dalton, L. Olson, and N. Bell, “Optimizing sparse ma-
trixmatrix multiplication for the gpu,” ACM Transactions on
Mathematical Software (TOMS), vol. 41, no. 4, p. 25, 2015.
[13] R. Kunchum, A. Chaudhry, A. Sukumaran-Rajam, Q. Niu,
I. Nisa, and P. Sadayappan, “On improving performance of
sparse matrix-matrix multiplication on gpus,” in Proceedings
of the International Conference on Supercomputing. New
York, NY, USA: ACM, 2017, pp. 14:1–14:11.
[14] T. A. Davis and Y. Hu, “The university of ﬂorida sparse matrix
collection,” ACM Transactions on Mathematical Software
(TOMS), vol. 38, no. 1, p. 1, 2011.
[15] T. Davis. (2017) The suitesparse matrix collection. [Online].
Available: https://sparse.tamu.edu/about
[16] M. Deveci, C. Trott, and S. Rajamanickam, “Performance-
portable sparse matrix-matrix multiplication for many-core
architectures,” in 2017 IEEE International Parallel and Dis-
tributed Processing Symposium Workshops (IPDPSW), May
2017, pp. 693–702.
[17] S. Dalton, N. Bell, L. Olson, and M. Garland, “Cusp:
Generic parallel algorithms for sparse matrix and graph
computations,” 2014, version 0.5.0. [Online]. Available:
http://cusplibrary.github.io/
[18] F. G. Gustavson, “Two fast algorithms for sparse matri-
ces: Multiplication and permuted transposition,” ACM Trans.
Math. Softw., vol. 4, no. 3, pp. 250–269, Sep. 1978.
[19] S. Williams, A. Waterman, and D. Patterson, “Rooﬂine: an
insightful visual performance model for multicore architec-
tures,” Communications of the ACM, vol. 52, no. 4, pp. 65–76,
2009.
[20] J. Dongarra, J.-F. Pineau, Y. Robert, and F. Vivien, “Matrix
product on heterogeneous master-worker platforms,” in ACM
SIGPLAN Symposium on Principles and Practice of Parallel
Programming, 2008, pp. 53–62.
[21] Ü. Çatalyürek and C. Aykanat, “Patoh (partitioning tool for
hypergraphs),” pp. 1479–1487, 2011.
[22] G. Ballard, A. Druinsky, N. Knight, and O. Schwartz, “Hypergraph partitioning for sparse matrix-matrix multiplication,”
ACM Transactions on Parallel Computing (TOPC), vol. 3,
no. 3, p. 18, 2016.
[23] J.-W. Hong and H. T. Kung, “I/O complexity: The red-blue
pebble game,” in Proceedings of the thirteenth annual ACM
symposium on Theory of computing, 1981, pp. 326–333.
[24] D. Irony, S. Toledo, and A. Tiskin, “Communication lower
bounds for distributed-memory matrix multiplication,” J. Parallel Distrib. Comput., vol. 64, no. 9, pp. 1017–1026, 2004.
[25] N. Sedaghati, T. Mu, L.-N. Pouchet, S. Parthasarathy, and
P. Sadayappan, “Automatic selection of sparse matrix representation on gpus,” in Proceedings of the 29th ACM on
International Conference on Supercomputing. New York,
NY, USA: ACM, 2015, pp. 99–108.