[1] ]NVIDIA. (2016) GP100 Pascal Whitepaper. [Online]. Available: https://images.nvidia.com/content/pdf/tesla/whitepaper/
pascal-architecture- whitepaper.pdf

[2] T. Zheng, D. Nellans, A. Zulfiqar, M. Stephenson, and S. W. Keckler,
“Towards high performance paged memory for GPUs,” in IEEE International Symp. on High Performance Computer Architecture, 2016.

[3] I. Gelado, J. E. Stone, J. Cabezas, S. Patel, N. Navarro, and W. W. Hwu,
“An asymmetric distributed shared memory model for heterogeneous
parallel systems,” in ACM SIGARCH Computer Architecture News,
vol. 38, no. 1. ACM, 2010, pp. 347-358.

[4] AMD. (2012) Heterogeneous System Architecture: A Technical
Review. [Online]. Available: http://amd-dev.wpengine.netdna-cdn.com/
wordpress/media/2012/10/hsa10.pdf

[5] NVIDIA. (2011) New CUDA 4.0 release makes parallel
programming easier. [Online]. Available: http://www.nvidia.co.uk/
object/nvidia-cuda-4-0-press-20110228- uk html

[6] —. (2013) Unified memory in CUDA 6. [Online]. Available:
https://devblogs.nvidia.com/parallelforall/unified-memory-in-cuda-6/

[7] (2016) The future of Unified Memory. [Online].
Available: —_ http://on-demand.gputechconf.com/gtc/2016/presentation/
s6216-nikolay-sakharnykh-future-unified-memory.pdf

[8] R. Landaverde, T. Zhang, A. K. Coskun, and M. Herbordt, “An
investigation of unified memory access performance in CUDA,” in IEEE
High Performance Extreme Computing Conference (HPEC), 2014.

[9] W. Li, G. Jin, X. Cui, and S. See, “An evaluation of unified memory technology on NVIDIA GPUs,” in 15th IEEE/ACM International
Symposium on Cluster, Cloud and Grid Computing, 2015.

[10] NVIDIA. (2016) CUDA 8 features revealed. [Online]. Available:
https://devblogs.nvidia.com/parallelforall/cuda-8-features-revealed/

[11] D. J. Miller, P M. Watts, and A. W. Moore, “Motivating future
interconnects: A differential measurement analysis of PCI latency,” in
Proceedings of the 5th ACM/IEEE Symposium on Architectures for
Networking and Communications Systems (ANCS), 2009.

[12] J. Gémez-Luna, I. E. Hajj, L.-W. Chang, V. Garcia-Flores, S. G.
de Gonzalo, T. B. Jablin, A. J. Pefia, and W.-M. Hwu, “Chai:
Collaborative heterogeneous applications for integrated-architectures,”
in IEEE International Symposium on Performance Analysis of Systems
and Software (ISPASS), April 2017, pp. 1-10. [Online]. Available:
https://chai- benchmarks. github. io/

[13] W. J. Bolosky and M. L. Scott, “False sharing and its effect on shared
memory performance,” in USENIX Systems on USENIX Experiences
with Distributed and Multiprocessor Systems - Volume 4, 1993.

[14] J. Torrellas, H. S. Lam, and J. L. Hennessy, “False sharing and spatial
locality in multiprocessor caches,” IEEE Trans. on Computers, 1994.

[15] N. Agarwal, D. Nellans, M. O’Connor, S. W. Keckler, and T. F
Wenisch, “Unlocking bandwidth for GPUs in CC-NUMA systems,” in
Int. Symp. on High Performance Computer Architecture (HPCA), 2015.

[16] N. Agarwal, D. Nellans, M. Stephenson, M. O’Connor, and S. W.
Keckler, “Page placement strategies for GPUs within heterogeneous
memory systems,” in ACM SIGPLAN Notices, vol. 50, no. 4, 2015, pp.
607-618.

[17] J. Kehne, J. Metter, and F. Bellosa, “GPUswap: Enabling oversubscription of GPU memory through transparent swapping,” in ACM
SIGPLAN/SIGOPS Int. Conf. on Virtual Execution Environments, 2015.

[18] J. Zhao, G. Sun, G. H. Loh, and Y. Xie, “Optimizing GPU energy
efficiency with 3D die-stacking graphics memory and reconfigurable
memory interface,’ ACM Trans. Archit. Code Optim., 2013.

[19] B. Wang, B. Wu, D. Li, X. Shen, W. Yu, Y. Jiao, and J. S. Vetter,
“Exploring hybrid memory for GPU energy efficiency through softwarehardware co-design,” in Proc. of the 22nd International Conf. on
Parallel Architectures and Compilation Techniques (PACT), 2013.

[20] Y. Kim, J. Lee, J. E. Jo, and J. Kim, “GPUdmm: A high-performance
and memory-oblivious GPU architecture using dynamic memory management,” in IEEE 20th International Symposium on High Performance
Computer Architecture (HPCA), 2014.

[21] L. Zhao, R. Iyer, R. Dlikkal, and D. Newell, “Exploring DRAM
cache architectures for CMP server platforms,” in 25th International
Conference on Computer Design, 2007.

[22] X. Jiang, N. Madan, L. Zhao, M. Upton, R. Iyer, S. Makineni,
D. Newell, Y. Solihin, and R. Balasubramonian, “CHOP: Adaptive filterbased DRAM caching for CMP server platforms,” in International Symposium on High-Performance Computer Architecture (HPCA), 2010.

[23] M. K. Qureshi and G. H. Loh, “Fundamental latency trade-off in architecting DRAM caches: Outperforming impractical SRAM-tags with a
simple and practical design,” in 45th Annual IEEE/ACM International
Symposium on Microarchitecture (MICRO), 2012.

[24] J. Sim, G. H. Loh, H. Kim, M. O’Connor, and M. Thottethodi,
“A mostly-clean DRAM cache for effective hit speculation and selfbalancing dispatch,’ in Proceedings of the 45th Annual IEEE/ACM
International Symposium on Microarchitecture (MICRO), 2012.

[25] D. Jevdjic, S. Volos, and B. Falsafi, “Die-stacked DRAM caches for
servers: Hit ratio, latency, or bandwidth? have it all with footprint
cache,” in Proceedings of the 40th Annual International Symposium
on Computer Architecture (ISCA), 2013.

[26] C. Chou, A. Jaleel, and M. K. Qureshi, “CAMEO: A two-level memory
organization with capacity of main memory and flexibility of hardwaremanaged cache,” in Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), 2014.

[27] S. Shahar, S. Bergman, and M. Silberstein, “ActivePointers: A case
for software address translation on GPUs,” in Proceedings of the 43rd
International Symposium on Computer Architecture (ISCA), 2016.

[28] Altera. (2009) Scatter-gather DMA controller core. [Online].
Available: https://www.altera.co.jp/content/dam/altera- www/global/ja_
JP/pdfs/literature/hb/nios2/qts_qii55003.pdf

[29] ARM. (2005) PrimeCell DMA controller. [Online]. Available: http:
/finfocenter.arm.com/help/topic/com.arm.doc.ddi0196g/DDI0196.pdf
[30] M. Kistler, M. Perrone, and F. Petrini, “Cell multiprocessor communication network: Built for speed,” IEEE Micro, vol. 26, no. 3, 2006.

[31] J. Power, J. Hestness, M. S. Orr, M. D. Hill, and D. A. Wood,
“gem5-gpu: A heterogeneous CPU-GPU simulator,” IEEE Computer
Architecture Letters, vol. 14, no. 1, pp. 34-36, 2015.

[32] N. Binkert, B. Beckmann, G. Black, S. K. Reinhardt, A. Saidi, A. Basu,
J. Hestness, D. R. Hower, T. Krishna, S. Sardashti et al., “The gem5
simulator,” ACM SIGARCH Comp. Arch. News, vol. 39, no. 2, 2011.

[33] A. Bakhoda, G. L. Yuan, W. W. L. Fung, H. Wong, and T. M. Aamodt,
“Analyzing CUDA workloads using a detailed GPU simulator,” in IEEE
Int. Symp. on Performance Analysis of Systems and Software, 2009.
[34] NVIDIA. (2014) Coral white paper. [Online]. Available: http://info.nvidianews.com/rs/nvidia/images/Coral%20White%
20Paper%20Final-3-2.pdf

[35] S. Tandri and T. Abdelrahman, “Automatic partitioning of data and computations on scalable shared memory multiprocessors,” in International
Conference on Parallel Processing (ICPP), 1997.

[36] E. Rothberg, J. P. Singh, and A. Gupta, “Working sets, cache sizes, and
node granularity issues for large-scale multiprocessors,” in 20th Annual
International Symposium on Computer Architecture (ISCA), 1993.

[37] D. K. Lowenthal and M. James, “Run-time selection of block size in
pipelined parallel programs,” in 13th International and 10th Symposium
on Parallel and Distributed Processing (IPPS/SPDP), 1999, pp. 82-87.
[38] I. Chung and J. K. Hollingsworth, “A case study using automatic performance tuning for large-scale scientific programs,” in Int. Symposium on
High Performance Distributed Computing (HPDC), 2006, pp. 45-56.

[39] A. Rico, A. Ramirez, and M. Valero, “Available task-level parallelism
on the Cell BE,” Sci. Programming, vol. 17, no. 1-2, pp. 59-76, 2009.