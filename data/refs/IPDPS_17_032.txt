[1] S. Lee, J. S. Meredith, and J. S. Vetter, “COMPASS: A framework for automated performance modeling and prediction,” in Proc. of the 29th ACM Intl Conf on Supercomputing. New York, NY, USA: ACM, 2015, pp. 405–414.
[2] D.Unat,C.Chan,W.Zhang,S.Williams,J.Bachan,J.Bell,andJ.Shalf, “ExaSAT: An exascale co-design tool for performance modeling,” The International Journal of High Performance Computing Applications, vol. 29, no. 2, pp. 209–232, 2015.
[3] N. R. Tallent and A. Hoisie, “Palm: Easing the burden of analytical performance modeling,” in Proc. of the 28th ACM Intl. Conf. on Supercomputing. New York, NY, USA: ACM, 2014, pp. 221–230.
[4] J. Hammer, G. Hager, J. Eitzinger, and G. Wellein, “Automatic loop kernel analysis and performance modeling with kerncraft,” in Proc. of the 6th Intl Workshop on Performance Modeling, Benchmarking, and Simulation of High Performance Computing Systems. New York, NY, USA: ACM, 2015, pp. 1–11.
[5] A. Panyala, S. Krishnamoorthy, and D. Chavarria-Miranda, “Efficient approximation of the iterative PageRank algorithm,” Pacific Northwest National Laboratory, Tech. Rep., June 2016.
[6] H.Wasserman,“ASCISweep3Dbenchmarkcode,v.2.2b,”http://wwwc3. lanl.gov/pal/software/sweep3d/sweep3d_readme.html, December 1995.
[7] A. Vishnu, J. Narasimhan, L. Holder, D. Kerbyson, and A. Hoisie, “Fast and accurate support vector machines on large scale systems,” in Proc. of the 2015 IEEE Intl. Conf. on Cluster Computing, Sept 2015, pp. 110–119.
[8] G. E. Hammond, P. C. Lichtner, and R. T. Mills, “Evaluating the performance of parallel subsurface simulators: An illustrative example with PFLOTRAN,” Water Resources Research, vol. 50, no. 1, pp. 208– 228, 2014.
[9] Pacific Northwest National Laboratory, “Palm: Performance and Archi- tecture Lab Modeling tool,” http://hpc.pnnl.gov/palm/.
[10] C.-Q. Yang and B. P. Miller, “Critical path analysis for the execution of parallel and distributed programs,” in Proc. of the 8th Intl. Conf. on Distributed Computing Systems, Jun 1988, pp. 366–373.
[11] J. Hollingsworth, “Critical path profiling of message passing and shared- memory programs,” IEEE Trans. Parallel Distrib. Syst., vol. 9, no. 10, pp. 1029–1040, Oct 1998.
[12] D. Hensgen, R. Finkel, and U. Manber, “Two algorithms for barrier synchronization,” Intl. Journal of Parallel Programming, vol. 17, no. 1, pp. 1–17, 1988.
[13] M. P. I. Forum, MPI: A Message Passing Interface Standard, June 1999, http://www.mpi- forum.org/docs/mpi- 11.ps.
[14] M. Schulz, G. Bronevetsky, and B. R. Supinski, Recent Advances in Parallel Virtual Machine and Message Passing Interface: Proc. of the 15th European PVM/MPI Users’ Group Meeting. Berlin, Heidelberg: Springer Berlin Heidelberg, 2008, ch. On the Performance of Transparent MPI Piggyback Messages, pp. 194–201.
[15] B. Buck and J. K. Hollingsworth, “An API for runtime code patching,” The Intl. Journal of High Performance Computing Applications, vol. 14, no. 4, pp. 317–329, Winter 2000.
[16] Intel Corporation, “Intel Architecture Code Analyzer,” https://software. intel.com/en-us/articles/intel-architecture-code-analyzer, June 2012.
[17] Linux perf, “perf-mem,” http://man7.org/linux/man-pages/man1/perf- mem.1.html, April 2016.
[18] D. J. Kerbyson, M. Lang, and S. Pakin, “Adapting wave-front algorithms to efficiently utilize systems with deep communication hierarchies,” Parallel Computing, vol. 37, no. 9, pp. 550–561, 2011.
[19] S. Brin and L. Page, “The anatomy of a large-scale hypertextual web search engine,” Computer Networks and ISDN Systems: Proc. of the 7th Intl. World Wide Web Conference, vol. 30, no. 1, pp. 107–117, 1998.
[20] S.Balay,S.Abhyankar,M.F.Adams,J.Brown,P.Brune,K.Buschelman, L. Dalcin, V. Eijkhout, W. D. Gropp, D. Kaushik, M. G. Knepley, L. C. McInnes, K. Rupp, B. F. Smith, S. Zampini, and H. Zhang, “PETSc Web page,” http://www.mcs.anl.gov/petsc, 2016.
[21] X. Chen, G. E. Hammond, C. J. Murray, M. L. Rockhold, V. R. Vermeul, and J. M. Zachara, “Application of ensemble-based data assimilation techniques for aquifer characterization using tracer data at Hanford 300 area,” Water Resources Research, vol. 49, no. 10, pp. 7064–7076, 2013.
[22] H. Stengel, J. Treibig, G. Hager, and G. Wellein, “Quantifying perfor- mance bottlenecks of stencil computations using the execution-cache- memory model,” in Proc. of the 29th ACM Intl Conf on Supercomputing. New York, NY, USA: ACM, 2015, pp. 207–216.
[23] G. Marin, J. Dongarra, and D. Terpstra, “MIAMI: A framework for application performance diagnosis,” in Proc. of the 2014 IEEE Intl. Symp. on Performance Analysis of Systems and Software, Mar 2014, pp. 158–168.
[24] S. H. K. Narayanan, B. Norris, and P. D. Hovland, “Generating performance bounds from source code,” in Proc. of the 39th Intl. Conf. on Parallel Processing Workshops, Sept 2010, pp. 197–206.
[25] Y. J. Lo, S. Williams, B. Straalen, T. J. Ligocki, M. J. Cordery, N. J. Wright, M. W. Hall, and L. Oliker, High Performance Computing Systems. 5th Intl. Workshop on Performance Modeling, Benchmarking, and Simulation. Springer International Publishing, 2015, ch. Roofline Model Toolkit: A Practical Tool for Architectural and Program Analysis, pp. 129–148.
[26] J. Chen and R. Clapp, “Critical-path candidates: Scalable performance modeling for MPI workloads,” in Proc. of the 2015 IEEE Intl. Symp. on Performance Analysis of Systems and Software, March 2015, pp. 1–10.
[27] A. Snavely, L. Carrington, N. Wolter, J. Labarta, R. Badia, and A. Purkayastha, “A framework for performance modeling and prediction,” in Proc. of the 2002 ACM/IEEE Conf. on Supercomputing. Los Alamitos, CA, USA: IEEE Computer Society, 2002.
[28] D.Jeon,S.Garcia,C.Louie,andM.B.Taylor,“Kismet:Parallelspeedup estimates for serial programs,” SIGPLAN Not., vol. 46, no. 10, pp. 519– 536, Oct. 2011.
[29] G. Jin, L. Adhianto, J. Mellor-Crummey, W. N. Scherer III, and C. Yang, “Implementation and performance evaluation of the HPC challenge benchmarks in coarray fortran 2.0,” in Proc. of the 25th IEEE Intl.
Parallel and Distributed Processing Symp., May 2011.
[30] J. Meng, V. Morozov, K. Kumaran, V. Vishwanath, and T. Uram, “GROPHECY: GPU performance projection from CPU code skeletons,”
in Proc. of the 2011 ACM/IEEE Conf. on Supercomputing, 2011.
[31] J. Meng, V. A. Morozov, V. Vishwanath, and K. Kumaran, “Dataflow- driven GPU performance projection for multi-kernel transformations,” in Proc. of the 2012 ACM/IEEE Conf. on Supercomputing. Los Alamitos,
CA, USA: IEEE Computer Society Press, 2012.
[32] D.Bohme,F.Wolf,B.DeSupinski,M.Schulz,andM.Geimer,“Scalable
critical-path based performance analysis,” in Proc. of the 26th IEEE Intl.
Parallel and Distributed Processing Symp., May 2012, pp. 1330–1340.
[33] M. Schulz, “Extracting critical path graphs from mpi applications,” in Proc. of the 2005 IEEE Intl. Conf. on Cluster Computing, Sept 2005,
pp. 1–10.
