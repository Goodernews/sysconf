[1] J. Dean and S. Ghemawat, “Mapreduce: simplified data processing on large clusters,” Communications of the ACM, vol. 51,
no. 1, pp. 107-113, 2008.

[2] A. Bialecki, M. Cafarella, D. Cutting, and O. Omalley,
“Hadoop: a framework for running applications on large clusters
built of commodity hardware,” 2005.

[3] H.-c. Yang, A. Dasdan, R.-L. Hsiao, and D. S. Parker, “Mapreduce-merge: simplified relational data processing on large
clusters,” in SIGMOD, 2007.

[4] R. Kumar, B. Moseley, S. Vassilvitskii, and A. Vattani, “Fast
greedy algorithms in mapreduce and streaming,’ ACM Transactions on Parallel Computing, vol. 2, no. 3, p. 14, 2015.

[5] B. Bahmani, B. Moseley, A. Vattani, R. Kumar, and S. Vassilvitskii, “Scalable k-means++,” VLDB, 2012.

[6] S. Suri and S. Vassilvitskii, “Counting triangles and the curse
of the last reducer,’ in WWW, 2011.

[7] C.-T. Chu, S. K. Kim, Y.-A. Lin, Y. Yu, G. Bradski, A. Y.
Ng, and K. Olukotun, “Map-reduce for machine learning on
multicore,” in NIPS, 2006.

[8] M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, and
I. Stoica, “Spark: cluster computing with working sets.” in
HotCloud, 2010.

[9] K. L. Clarkson, “Coresets, sparse greedy approximation, and the
Frank-Wolfe algorithm,” ACM Trans. Algorithms, vol. 6, no. 4,
pp. 63:1-63:30, Sep. 2010.

[10] A. Bellet, Y. Liang, A. B. Garakani, M.-F. Balcan, and F. Sha,
“A Distributed Frank-Wolfe Algorithm for CommunicationEfficient Sparse Learning,” in SDM, 2015.

[11] S. Boyd and L. Vandenberghe, Convex Optimization.
York, NY, USA: Cambridge University Press, 2004.

[12] M. Frank and P. Wolfe, “An Algorithm for Quadratic Programming,” Naval Research Logistics Quarterly, vol. 3, no. 1-2, pp.
95-110, 1956.

[13] M. Jaggi, “Revisiting Frank-Wolfe: Projection-free sparse convex optimization.” in ICML, 2013.

[14] M. Dudik, Z. Harchaoui, and J. Malick, “Lifted coordinate descent for learning with trace-norm regularization,’ in AISTATS,
2012.

[15] E. Hazan and S. Kale, “Projection-free online learning,” in
ICML, 2012.

[16] Y. Ying and P. Li, “Distance metric learning with eigenvalue
optimization,” Journal of Machine Learning Research, vol. 13,
no. Jan, pp. 1-26, 2012.

[17] A. Joulin, K. Tang, and L. Fei-Fei, “Efficient image and video
co-localization with Frank-Wolfe algorithm,” in ECVV, 2014.

[18] J. Guélat and P. Marcotte, “Some comments on Wolfe’s away
step,” Mathematical Programming, vol. 35, no. 1, pp. 110-119,
1986.

[19] A. Beck and S. Shtern, “Linearly convergent away-step conditional gradient for non-strongly convex functions,’ Mathematical Programming, pp. 1-27, 2015.

[20] D. Garber and E. Hazan, “A linearly convergent variant of the
conditional gradient algorithm under strong convexity, with applications to online and stochastic optimization,” SIAM Journal
on Optimization, vol. 26, no. 3, pp. 1493-1528, 2016.

[21] S. Lacoste-Julien and M. Jaggi, “On the global linear convergence of Frank-Wolfe optimization variants,” in NIPS, 2015.

[22] Z. Harchaoui, M. Douze, M. Paulin, M. Dudik, and J. Malick,
“Large-scale image classification with trace-norm regularization,” in CVPR, 2012.

[23] B. Recht, C. Re, S. Wright, and F. Niu, “Hogwild: A lock-free
approach to parallelizing stochastic gradient descent,” in NIPS,
2011.

[24] M. Zinkevich, M. Weimer, L. Li, and A. J. Smola, “Parallelized
stochastic gradient descent,” in NIPS, 2010.

[25] M. Li, L. Zhou, Z. Yang, A. Li, F. Xia, D. G. Andersen, and
A. Smola, “Parameter server for distributed machine learning,”
in NIPS Workshop, 2013.

[26] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean,
M. Devin, S. Ghemawat, G. Irving, M. Isard et al., “Tensorflow:
A system for large-scale machine learning,” in OSDI, 2016.

[27] T. Yang, “Trading computation for communication: Distributed
stochastic dual coordinate ascent,” in MIPS, 2013.

[28] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Distributed optimization and statistical learning via the alternating
direction method of multipliers,’ Foundations and Trends in
Machine Learning, vol. 3, no. 1, pp. 1-122, 2011.

[29] G. Lan and Y. Zhou, “Conditional gradient sliding for convex
optimization,” STAM Journal on Optimization, vol. 26, no. 2,
pp. 1379-1409, 2016.

[30] E. Hazan and H. Luo, “Variance-reduced and projection-free
stochastic optimization,” in ICML, 2016.

[31] S. J. Reddi, S. Sra, B. Péczés, and A. Smola, “Stochastic FrankWolfe methods for non-convex optimization,” in Allerton, 2016.

[32] D. P. Bertsekas, Nonlinear programming. Athena scientific
Belmont, 1999.

[33] F. T. Leighton, Introduction to parallel algorithms and architectures: Trees Hypercubes. Elsevier, 2014.

[34] J. Sherman and W. J. Morrison, “Adjustment of an inverse
matrix corresponding to a change in one element of a given
matrix,” The Annals of Mathematical Statistics, vol. 21, no. 1,
pp. 124-127, 1950.

[35] A. Y. Ng, “Feature selection, 1 1 vs. 1 2 regularization, and
rotational invariance,” in ICML, 2004.

[36] R. Tibshirani, “Regression shrinkage and selection via the
lasso,” Journal of the Royal Statistical Society. Series B
(Methodological), pp. 267-288, 1996.

[37] P. Shah, B. N. Bhaskar, G. Tang, and B. Recht, “Linear system
identification via atomic norm regularization,” in CDC, 2012.

[38] S. Chen and A. Banerjee, “Structured estimation with atomic
norms: General bounds and applications,” in NIPS, 2015.

[39] V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky,
“The convex geometry of linear inverse problems,” Foundations
of Computational Mathematics, vol. 12, no. 6, pp. 805-849,
2012.

[40] A. Tewari, P. K. Ravikumar, and I. S. Dhillon, “Greedy algorithms for structurally constrained high dimensional problems,”
in NIPS, 2011.

[41] F. M. Harper and J. A. Konstan, “The movielens datasets:
History and context,’ ACM Trans. Interact. Intell. Syst., vol. 5,
no. 4, pp. 19:1-19:19, Dec. 2015.

[42] M. Lichman, “UCI machine learning repository,” 2013.

[43] G. Calinescu, C. Chekuri, M. Pal, and J. Vondrak, “Maximizing a monotone submodular function subject to a matroid
constraint,” SIAM Journal on Computing, vol. 40, no. 6, pp.
1740-1766, 2011.

[44] Y. Bian, B. Mirzasoleiman, J. M. Buhmann, and A. Krause,
“Guaranteed non-convex optimization: Submodular maximization over continuous domains,” in AISTATS, 2017.