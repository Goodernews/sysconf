[1] “NVIDIA CUDA.” [Online]. Available: http://docs.nvidia.com/cuda
[2] Message Passing Interface Forum, http://www.mpi-forum.org/.
[3] Open MPI, https://www.open-mpi.org/.
[4] MVAPICH, http://mvapich.cse.ohio-state.edu/.
[5] S.
Tariq,
“Lessons
learned
in
improving
scaling
of
applications on large gpu clusters,” in HPC Advisory
Council
Stanford
Conference,
February
2003.
[Online].
Available: http://www.hpcadvisorycouncil.com/events/2013/StanfordWorkshop/pdf/Presentations/Day%201/11 NVIDIA.pdf
[6] 2014 IEEE International Parallel & Distributed Processing
Symposium Workshops, Phoenix, AZ, USA, May 19-23,
2014.
IEEE Computer Society, 2014. [Online]. Available:
http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=6967893
[7] F. Daoud, A. Watad, and M. Silberstein, “Gpurdma: Gpu-side
library for high performance networking from gpu kernels,” in
Proceedings of the 6th International Workshop on Runtime and
Operating Systems for Supercomputers, ser. ROSS ’16. New
York, NY, USA: ACM, 2016, pp. 6:1–6:8. [Online]. Available:
http://doi.acm.org/10.1145/2931088.2931091

[8] NVIDIA, “Gpudirect,” https://developer.nvidia.com/gpudirect, 2015.
[9] ——, “Gpudirect rdma,” http://docs.nvidia.com/cuda/gpudirect-rdma,
2015.
[10] D. Rossetti, “Gpudirect: Integrating the gpu with a network interface,”
in GPU Technology Conference, April 2015. [Online]. Available: http://on-demand.gputechconf.com/gtc/2015/presentation/S5412Davide-Rossetti.pdf
[11] H. Wang, S. Potluri, M. Luo, A. K. Singh, S. Sur, and D. K. Panda,
“Mvapich2-gpu: Optimized gpu to gpu communication for inﬁniband
clusters,” Comput. Sci., vol. 26, no. 3-4, pp. 257–266, Jun. 2011.
[Online]. Available: http://dx.doi.org/10.1007/s00450-011-0171-3
[12] S. Potluri, K. Hamidouche, A. Venkatesh, D. Bureddy, and D. K.
Panda, “Efﬁcient inter-node mpi communication using gpudirect rdma
for inﬁniband clusters with nvidia gpus,” in Proceedings of the 2013
42Nd International Conference on Parallel Processing, ser. ICPP ’13.
Washington, DC, USA: IEEE Computer Society, 2013, pp. 80–89.
[Online]. Available: http://dx.doi.org/10.1109/ICPP.2013.17
[13] MVAPICH, http://mvapich.cse.ohio-state.edu/.
[14] A. M. Aji, J. Dinan, D. Buntinas, P. Balaji, W.-c. Feng, K. R. Bisset, and
R. Thakur, “MPI-ACC: An Integrated and Extensible Approach to Data
Movement in Accelerator-Based Systems,” in 14th IEEE International
Conference on High Performance Computing and Communications,
Liverpool, UK, June 2012.
[15] S. Potluri, D. Bureddy, H. Wang, H. Subramoni, and D. K.
Panda, “Extending openshmem for gpu computing,” in Proceedings
of the 2013 IEEE 27th International Symposium on Parallel and
Distributed Processing, ser. IPDPS ’13. Washington, DC, USA:
IEEE Computer Society, 2013, pp. 1001–1012. [Online]. Available:
http://dx.doi.org/10.1109/IPDPS.2013.104
[16] D. Cunningham, R. Bordawekar, and V. Saraswat, “Gpu programming
in a high level language: Compiling x10 to cuda,” in Proceedings
of the 2011 ACM SIGPLAN X10 Workshop, ser. X10 ’11. New
York, NY, USA: ACM, 2011, pp. 8:1–8:10. [Online]. Available:
http://doi.acm.org/10.1145/2212736.2212744
[17] T. Miyoshi, H. Irie, K. Shima, H. Honda, M. Kondo, and T. Yoshinaga,
“Flat: A gpu programming framework to provide embedded mpi,”
in Proceedings of the 5th Annual Workshop on General Purpose
Processing with Graphics Processing Units, ser. GPGPU-5. New
York, NY, USA: ACM, 2012, pp. 20–29. [Online]. Available:
http://doi.acm.org/10.1145/2159430.2159433
[18] T. Gysi, J. Baer, and T. Hoeﬂer, “dCUDA: Hardware Supported Overlap
of Computation and Communication,” Nov. 2016, accepted at The
International Conference for High Performance Computing, Networking,
Storage and Analysis (SC’16).
