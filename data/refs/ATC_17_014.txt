[1] A BADI , M., BARHAM , P., C HEN , J., C HEN , Z.,
DAVIS , A., D EAN , J., D EVIN , M., G HEMAWAT,
S., I RVING , G., I SARD , M., ET AL . Tensorflow:
A system for large-scale machine learning. arXiv
preprint arXiv:1605.08695 (2016).
[2] C HEN , J., M ONGA , R., B ENGIO , S., AND J OZE FOWICZ , R. Revisiting distributed synchronous
sgd. arXiv preprint arXiv:1604.00981 (2016).
[3] C HEN , T., L I , M., L I , Y., L IN , M., WANG , N.,
WANG , M., X IAO , T., X U , B., Z HANG , C., AND
Z HANG , Z. Mxnet: A flexible and efficient machine learning library for heterogeneous distributed
systems. arXiv preprint arXiv:1512.01274 (2015).
[4] C HILIMBI , T., A PACIBLE , Y. S. J., AND K ALYA NARAMAN , K. Project Adam: Building an Efficient and Scalable Deep Learning Training System.
In OSDI (2014).
[5] C OATES , A., H UVAL , B., WANG , T., W U , D. J.,
N G , A. Y., AND C ATANZARO , B. Deep Learning
with COTS HPC Systems. In ICML (2013).
[6] C OLLOBERT, R., K AVUKCUOGLU , K., AND
FARABET, C. Torch7: A Matlab-like Environment
for Machine Learning. In NIPSW (2011).
[7] C UI , H., Z HANG , H., G ANGER , G. R., G IB BONS , P. B., AND X ING , E. P. Geeps: Scalable deep learning on distributed gpus with a gpuspecialized parameter server. In Proceedings of the
Eleventh European Conference on Computer Systems (2016), ACM, p. 4.
[8] DAI , W., K UMAR , A., W EI , J., H O , Q., G IB SON , G., AND X ING , E. P. Analysis of highperformance distributed ml at scale through parameter server consistency models. In Proceedings of
the 29th AAAI Conference on Artificial Intelligence
(2015).
[9] D EAN , J., C ORRADO , G. S., M ONGA , R., C HEN ,
K., D EVIN , M., L E , Q. V., M AO , M. Z., R AN ZATO , M., S ENIOR , A., T UCKER , P., YANG , K.,
AND N G , A. Y. Large Scale Distributed Deep Networks. In NIPS (2012).
[10] D ENG , L., L I , J., H UANG , J.-T., YAO , K., Y U ,
D., S EIDE , F., S ELTZER , M. L., Z WEIG , G., H E ,
X., W ILLIAMS , J., G ONG , Y., AND ACERO , A.
Recent Advances in Deep Learning for Speech Research at Microsoft. In ICASSP (2013).
[11] H E , K., Z HANG , X., R EN , S., AND S UN , J.
Deep residual learning for image recognition. arXiv
preprint arXiv:1512.03385 (2015).
[12] H O , Q., C IPAR , J., C UI , H., K IM , J. K., L EE ,
S., G IBBONS , P. B., G IBSON , G. A., G ANGER ,
G. R., AND X ING , E. P. More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server. In NIPS (2013).
[13] I ANDOLA , F. N., M OSKEWICZ , M. W., A SHRAF,
K., AND K EUTZER , K. Firecaffe: near-linear acceleration of deep neural network training on compute clusters. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(2016), pp. 2592–2600.
[14] J IA , Y., S HELHAMER , E., D ONAHUE , J.,
K ARAYEV, S., L ONG , J., G IRSHICK , R.,
G UADARRAMA , S., AND DARRELL , T. Caffe:
Convolutional Architecture for Fast Feature Embedding. In MM (2014).
[15] K RIZHEVSKY, A. Learning Multiple Layers of
Features from Tiny Images. Master’s thesis, University of Toronto, 2009.
[16] K RIZHEVSKY, A., S UTSKEVER , I., AND H IN TON , G. E. ImageNet Classification with Deep
Convolutional Neural Networks. In NIPS (2012).
[17] L E , Q. V., M ONGA , R., D EVIN , M., C HEN , K.,
C ORRADO , G. S., D EAN , J., AND N G , A. Y.
Building High-level Features Using Large Scale
Unsupervised Learning. In ICML (2012).
[18] L I , H., K ADAV, A., K RUUS , E., AND U NGURE ANU , C. Malt: distributed data-parallelism for
existing ml applications. In Proceedings of the
Tenth European Conference on Computer Systems
(2015), ACM, p. 3.
[19] L I , M., A NDERSEN , D. G., PARK , J. W., S MOLA ,
A. J., A HMED , A., J OSIFOVSKI , V., L ONG ,
J., S HEKITA , E. J., AND S U , B.-Y. Scaling
distributed machine learning with the parameter
server. In 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14)
(2014), pp. 583–598.
[20] L IANG , X., H U , Z., Z HANG , H., G AN , C.,
AND X ING , E. P. Recurrent topic-transition gan
for visual paragraph generation. arXiv preprint
arXiv:1703.07022 (2017).
[21] M IKOLOV, T., C HEN , K., C ORRADO , G., AND
D EAN , J. Efficient Estimation of Word Representations in Vector Space. In ICLRW (2013).
[22] M ORITZ , P., N ISHIHARA , R., S TOICA , I., AND
J ORDAN , M. I. Sparknet: Training deep networks
in spark. arXiv preprint arXiv:1511.06051 (2015).
[23] RUSSAKOVSKY, O., D ENG , J., S U , H., K RAUSE ,
J., S ATHEESH , S., M A , S., H UANG , Z., K ARPA THY, A., K HOSLA , A., B ERNSTEIN , M., B ERG ,
A. C., AND F EI -F EI , L. ImageNet Large Scale
Visual Recognition Challenge. IJCV (2015), 1–42.
[24] S EIDE , F., F U , H., D ROPPO , J., L I , G., AND Y U ,
D. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech
dnns. In INTERSPEECH (2014), pp. 1058–1062.
[25] S EIDE , F., F U , H., D ROPPO , J., L I , G., AND Y U ,
D. On parallelizability of stochastic gradient descent for speech dnns. In 2014 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP) (2014), IEEE, pp. 235–239.
[26] S IMONYAN , K., AND Z ISSERMAN , A. Very
Deep Convolutional Networks for Large-Scale Image Recognition. In ICLR (2015).
[27] S ZEGEDY, C., L IU , W., J IA , Y., S ERMANET, P.,
R EED , S., A NGUELOV, D., E RHAN , D., VAN HOUCKE , V., AND R ABINOVICH , A. Going deeper
with convolutions. In CVPR (2015).
[28] S ZEGEDY, C., VANHOUCKE , V., I OFFE , S.,
S HLENS , J., AND W OJNA , Z. Rethinking the inception architecture for computer vision. arXiv
preprint arXiv:1512.00567 (2015).
[29] WANG , W., C HEN , G., D INH , T. T. A., G AO , J.,
O OI , B. C., TAN , K.-L., AND WANG , S. SINGA:
Putting Deep Learning in the Hands of Multimedia
Users. In MM (2015).
[30] WATCHARAPICHAT, P., M ORALES , V. L., F ER NANDEZ , R. C., AND P IETZUCH , P. Ako: Decentralised deep learning with partial gradient exchange. In Proceedings of the Seventh ACM Symposium on Cloud Computing (2016), ACM, pp. 84–
97.
[31] W EI , J., DAI , W., Q IAO , A., H O , Q., C UI ,
H., G ANGER , G. R., G IBBONS , P. B., G IBSON ,
G. A., AND X ING , E. P. Managed Communication and Consistency for Fast Data-parallel Iterative
Analytics. In SoCC (2015).
[32] X IE , P., K IM , J. K., Z HOU , Y., H O , Q., K U MAR , A., Y U , Y., AND X ING , E. Distributed Machine Learning via Sufficient Factor Broadcasting.
In arXiv (2015).

[33] YAHOO.
Caffe on spark.
http:
//yahoohadoop.tumblr.com/post/
129872361846/large-scaledistributed-deep-learning-onhadoop.
[34] YAN , Z., Z HANG , H., JAGADEESH , V., D E C OSTE , D., D I , W., AND P IRAMUTHU , R. Hdcnn: Hierarchical deep convolutional neural network for image classification. ICCV (2015).
[35] YAN , Z., Z HANG , H., WANG , B., PARIS , S., AND
Y U , Y. Automatic photo adjustment using deep
neural networks. ACM Transactions on Graphics
(TOG) 35, 2 (2016), 11.
[36] Y U , D., E VERSOLE , A., S ELTZER , M., YAO ,
K., H UANG , Z., G UENTER , B., K UCHAIEV, O.,
Z HANG , Y., S EIDE , F., WANG , H., ET AL . An introduction to computational networks and the computational network toolkit. Tech. rep.
[37] Z HANG , H., H U , Z., W EI , J., X IE , P., K IM ,
G., H O , Q., AND X ING , E. Poseidon: A
system architecture for efficient gpu-based deep
learning on multiple machines. arXiv preprint
arXiv:1512.06216 (2015).
[38] Z OU , Y., J IN , X., L I , Y., G UO , Z., WANG , E.,
AND X IAO , B. Mariana: Tencent Deep Learning
Platform and its Applications. In VLDB Endowment (2014).
