[1] 2016 november top500 list. https://www.top500.org/lists/2016/11/, 2016.
[2] A. V. Adinetz, P. F. Baumeister, H. Böttiger, T. Hater, T. Maurer, D. Pleiter,
W. Schenck, and S. F. Schifano. Performance Evaluation of Scientiﬁc Applications
on POWER8. In High Performance Computing Systems. Performance Modeling,
Benchmarking, and Simulation: 5th International Workshop, PMBS 2014., pages
24–45, 2015.
[3] A. Agrawal, P. Jain, A. Ansari, and J. Torrellas. Refrint: Intelligent Refresh
to Minimize Power in On-Chip Multiprocessor Cache Hierarchies. In 2013
IEEE 19th International Symposium on High Performance Computer Architecture
(HPCA), pages 400–411, 2013.
[4] E. Agullo, J. Demmel, J. Dongarra, B. Hadri, J. Kurzak, J. Langou, H. Ltaief,
P. Luszczek, and S. Tomov. Numerical Linear Algebra on Emerging Architectures:
The PLASMA and MAGMA Projects. Journal of Physics: Conference Series,
180(1):012037, 2009.
[5] A. Buttari, J. Langou, J. Kurzak, and J. Dongarra. A Class of Parallel Tiled
Linear Algebra Algorithms for Multicore Architectures. Parallel Computing,
35(1):38–53, 2009.
[6] M. Chang, P. Rosenfeld, S. Lu, and B. Jacob. Technology Comparison for
Large Last-level Caches (L3Cs): Low-leakage SRAM, Low Write-energy STTRAM, and Refresh-optimized eDRAM. In Proceedings of the 2013 IEEE 19th
International Symposium on High Performance Computer Architecture, HPCA
’13, pages 143–154, 2013.
[7] S. Che, M. Boyer, J. Meng, D. Tarjan, J. W. Sheaffer, and K. Skadron. A
Performance Study of General-Purpose Applications on Graphics Processors
Using CUDA. J. Parallel Distrib. Comput., 68(10):1370–1380, 2008.
[8] K. C. Chun, P. Jain, J. H. Lee, and C. H. Kim. A sub-0.9V logic-compatible
embedded DRAM with boosted 3T gain cell, regulated bit-line write scheme and
PVT-tracking read reference bias. In 2009 Symposium on VLSI Circuits, pages
134–135, 2009.
[9] K. C. Chun, P. Jain, J. H. Lee, and C. H. Kim. A 3T Gain Cell Embedded DRAM
Utilizing Preferential Boosting for High Density and Low Power On-Die Caches.
IEEE Journal of Solid-State Circuits, 46(6):1495–1505, 2011.
[10] H. David, E. Gorbatov, U. R. Hanebutte, R. Khanna, and C. Le. RAPL: Memory
Power Estimation and Capping. In Proceedings of the 16th ACM/IEEE International Symposium on Low Power Electronics and Design, pages 189–194,
2010.
[11] T. A. Davis and Y. Hu. The University of Florida Sparse Matrix Collection. ACM
Trans. Math. Softw., 38(1):1:1–1:25, 2011.
[12] D. Doerﬂer, J. Deslippe, S. Williams, L. Oliker, B. Cook, T. Kurth, M. Lobet,
T. M. Malas, J. Vay, and H. Vincenti. Applying the Rooﬂine Performance Model
to the Intel Xeon Phi Knights Landing Processor. In ISC Workshops, 2016.
[13] J. Dongarra et al. The International Exascale Software Project Roadmap. Int. J.
High Perform. Comput. Appl., 25(1):3–60, 2011.
[14] E. J. Fluhr et al. POWER8: A 12-core Server-Class Processor in 22nm SOI with
7.6Tb/s Off-Chip Bandwidth. In 2014 IEEE International Solid-State Circuits
Conference Digest of Technical Papers, pages 96–97, 2014.
[15] J. Barth et al. A 500 MHz Random Cycle, 1.5 ns Latency, SOI Embedded DRAM
Macro Featuring a Three-Transistor Micro Sense Ampliﬁer. IEEE Journal of
Solid-State Circuits, 43(1):86–95, 2008.
[16] P. Hammarlund et al. Haswell: The Fourth-Generation Intel Core Processor. IEEE
Micro, 34(2):6–20, 2014.
[17] M. Frigo and S. G Johnson. The Design and Implementation of FFTW3. Proceedings of the IEEE, 93(2):216–231, 2005.
[18] R. Ge, X. Feng, and K. W. Cameron. Performance-Constrained Distributed DVS
Scheduling for Scientiﬁc Applications on Power-aware Clusters. In Supercomputing, 2005. Proceedings of the ACM/IEEE SC 2005 Conference, pages 34–45,
2005.
[19] Z. Guz, E. Bolotin, I. Keidar, A. Kolodny, A. Mendelson, and U. C. Weiser. ManyCore vs. Many-Thread Machines: Stay Away From the Valley. IEEE Computer
Architecture Letters, 8(1):25–28, 2009.
[20] A. Hartstein,
V. Srinivasan, T. R. Puzak, and P. G. Emma. Cache Miss Behavior:
√
Is It 2. In Proceedings of the 3rd Conference on Computing Frontiers, pages
313–320, 2006.
[21] A. Heinecke, A. Breuer, M. Bader, and P. Dubey. High Order Seismic Simulations
on the Intel Xeon Phi Processor (Knights Landing). In High Performance Computing: 31st International Conference, ISC High Performance 2016, Proceedings,
pages 343–362, 2016.
[22] K. Hou, W. Liu, H. Wang, and W. Feng. Fast Segmented Sort on GPUs. In
Proceedings of the International Conference on Supercomputing, ICS ’17, pages
12:1–12:10, 2017.
[23] K. Hou, H. Wang, and W. Feng. GPU-UniCache: Automatic Code Generation
of Spatial Blocking for Stencils on GPUs. In Proceedings of the Computing
Frontiers Conference, CF’17, pages 107–116, 2017.
[24] A. Iosup, S. Ostermann, M. N. Yigitbasi, R. Prodan, T. Fahringer, and D. Epema.
Performance Analysis of Cloud Computing Services for Many-Tasks Scientiﬁc
Computing. IEEE Transactions on Parallel and Distributed Systems, 22(6):931–
945, 2011.
[25] S. S. Iyer, J. E. Barth, P. C. Parries, J. P. Norum, J. P. Rice, L. R. Logan, and
D. Hoyniak. Embedded DRAM: Technology platform for the Blue Gene/L chip.
IBM Journal of Research and Development, 49(2.3):333–350, 2005.
[26] J. Jeffers and J. Reinders. Intel Xeon Phi coprocessor high-performance programming. MK, 1st edition, 2013.
[27] J. Jeffers, J. Reinders, and A. Sodani. Intel Xeon Phi Processor High Performance
Programming. MK, 2nd edition, 2016.
[28] S. Junkins. The Compute Architecture of Intel Processor Graphics Gen8. Technical report, Intel Corp., 2015.
[29] A. Li, S. L. Song, E. Brugel, A. Kumar, D. Chavarrı́a-Miranda, and H. Corporaal.
X: A Comprehensive Analytic Model for Parallel Machines. In 2016 IEEE
International Parallel and Distributed Processing Symposium (IPDPS), pages
242–252, 2016.
[30] W. Liu. Parallel and Scalable Sparse Basic Linear Algebra Subprograms. PhD
thesis, University of Copenhagen, 2015.
[31] W. Liu, A. Li, J. Hogg, I. S. Duff, and B. Vinter. A Synchronization-Free
Algorithm for Parallel Sparse Triangular Solves. In Euro-Par 2016: Parallel Processing: 22nd International Conference on Parallel and Distributed Computing,
Proceedings, pages 617–630, 2016.
[32] W. Liu and B. Vinter. A Framework for General Sparse Matrix-matrix Multiplication on GPUs and Heterogeneous Processors. Journal of Parallel and Distributed
Computing, 85:47–61, 2015.
[33] W. Liu and B. Vinter. CSR5: An Efﬁcient Storage Format for Cross-Platform
Sparse Matrix-Vector Multiplication. In Proceedings of the 29th ACM International Conference on Supercomputing, ICS ’15, pages 339–350, 2015.
[34] W. Liu and B. Vinter. Speculative Segmented Sum for Sparse Matrix-Vector
Multiplication on Heterogeneous Processors. Parallel Computing, 2015.
[35] W. Luk, J. Cai, R. Dennard, M. Immediato, and S. Kosonocky. A 3-Transistor
DRAM Cell with Gated Diode for Enhanced Speed and Retention Time. In 2006
Symposium on VLSI Circuits, 2006. Digest of Technical Papers., pages 184–185,
2006.
[36] J. D. McCalpin. Memory Bandwidth and Machine Balance in Current High Performance Computers. IEEE Computer Society Technical Committee on Computer
Architecture Newsletter, 1995.
[37] S. Mittal and J. S. Vetter. A Survey Of Techniques for Architecting DRAM Caches.
IEEE Transactions on Parallel and Distributed Systems, 27(6):1852–1863, 2016.
[38] S. Mittal, J. S. Vetter, and D. Li. A Survey Of Architectural Approaches for Managing Embedded DRAM and Non-Volatile On-Chip Caches. IEEE Transactions
on Parallel and Distributed Systems, 26(6):1524–1537, 2015.
[39] J. Park, M. Smelyanskiy, N. Sundaram, and P. Dubey. Sparsifying Synchronization for High-Performance Shared-Memory Sparse Triangular Solver. In
Supercomputing: 29th International Conference, Proceedings, pages 124–140,
2014.
[40] A. J. Smith. Cache Memories. ACM Comput. Surv., 14(3):473–530, 1982.
[41] S. Smith, J. Park, and G. Karypis. Sparse Tensor Factorization on Many-Core
Processors with High-Bandwidth Memory. In 2017 IEEE International Parallel
and Distributed Processing Symposium, IPDPS ’17, 2017.
[42] A. Sodani, R. Gramunt, J. Corbal, H. Kim, K. Vinod, S. Chinthamani, S. Hutsell,
R. Agarwal, and Y. Liu. Knights Landing: Second-Generation Intel Xeon Phi
Product. IEEE Micro, 36(2):34–46, 2016.
[43] D. Somasekhar, Y. Ye, P. Aseron, S. L. Lu, M. Khellah, J. Howard, G. Ruhl,
T. Karnik, S. Y. Borkar, V. De, and A. Keshavarzi. 2GHz 2Mb 2T Gain-Cell
Memory Macro with 128GB/s Bandwidth in a 65nm Logic Process. In 2008 IEEE
International Solid-State Circuits Conference - Digest of Technical Papers, pages
274–613, 2008.
[44] H. Wang, W. Liu, K. Hou, and W. Feng. Parallel Transposition of Sparse Data
Structures. In Proceedings of the 2016 International Conference on Supercomputing, ICS ’16, pages 33:1–33:13, 2016.
[45] V. M. Weaver, M. Johnson, K. Kasichayanula, J. Ralph, P. Luszczek, D. Terpstra,
and S. Moore. Measuring Energy and Power with PAPI. In Proceedings of the
2012 41st International Conference on Parallel Processing Workshops, pages
262–268, 2012.
[46] C. Wilkerson, A. R. Alameldeen, Z. Chishti, W. Wu, D. Somasekhar, and S. Lu.
Reducing Cache Power with Low-cost, Multi-bit Error-correcting Codes. In Proceedings of the 37th Annual International Symposium on Computer Architecture,
ISCA ’10, pages 83–93, 2010.
[47] S. Williams, J. Shalf, L. Oliker, S. Kamil, P. Husbands, and K. Yelick. The
Potential of the Cell Processor for Scientiﬁc Computing. In Proceedings of the
3rd Conference on Computing Frontiers, CF ’06, pages 9–20, 2006.
[48] S. Williams, A. Waterman, and D. Patterson. Rooﬂine: An Insightful Visual
Performance Model for Multicore Architectures. Communications of the ACM,
52(4):65–76, 2009.
[49] C. Yount, J. Tobin, A. Breuer, and A. Duran. YASK – Yet Another Stencil
Kernel: A Framework for HPC Stencil Code-Generation and Tuning. In 2016
Sixth International Workshop on Domain-Speciﬁc Languages and High-Level
Frameworks for High Performance Computing, pages 30–39, 2016.
