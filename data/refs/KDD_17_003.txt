[1] O. Alter, P.O. Brown, and D. Botstein. 2000. Singular value decomposition
for genome-wide expression data processing and modeling. Proceedings of the
National Academy of Sciences 97, 18 (2000), 10101–10106.
[2] R. Bhatia. 2015. Positive Definite Matrices. Princeton University Press.
[3] C. Boutsidis, P. Drineas, and M. Magdon-Ismail. 2004. Near-Optimal ColumnBased Matrix Reconstruction. SIAM J. Comput. 43, 2 (2004), 687–717.
[4] C. Boutsidis and A. Gittens. 2013. Improved matrix algorithms via the Subsampled Randomized Hadamard Transform. SIAM Journal of MAtrix Analysis and
Applications 34, 3 (2013), 1301–1340.
[5] C. Boutsidis and D.P. Woodruff. 2014. Optimal CUR matrix decompositions. In
Proceedings of the forty-sixth annual ACM symposium on Theory of computing.
353–362.
[6] E.J. Candes and B. Recht. 2009. Exact Matrix Completion via Matrix Completion.
Foundations of Computational Mathematics 9 (2009), 717–772.
[7] K.L. Clarkson, P. Drineas, M. Magdon-Ismail, M.W. Mahoney, X. Meng, and D.P.
Woodruff. 2013. The Fast Cauchy Transform and Faster Robust Linear Regression.
In Annual ACM-SIAM Symposium on Discrete Algorithms. 466–477.
[8] K.L. Clarkson and D.P. Woodruff. 2013. Low rank approximation and regression
in input sparsity time. In Annual ACM Symposium on theory of computing.
[9] S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer, and R. Harshman. 1990.
Indexing by latent semantic analysis. Journal of the American Society for Information Science 41, 6 (1990), 391–407.
[10] P. Drineas, R. Kannan, and M.W. Mahoney. 2006. Fast Monte Carlo algorithms
for matrices II: computing a low-rank approximation to a matrix. SIAM Journal
of Computing 36, 1 (2006), 158–183.
[11] P. Drineas, M. Magdon-Ismail, M.W. Mahoney, and D.P. Woodruff. 2012. Fast
approximation of matrix coherence and statistical leverage. Journal of Machine
Learning Research 13, 1 (2012), 3475–3506.
[12] P. Drineas and M.W. Mahoney. 2005. On the Nyström Method for Approximating
a Gram Matrix for Improved Kernel-Based Learning. Journal of Machine Learning
Research 6 (2005), 2153–2175.
[13] C. Fowlkes, S. Belongie, F. Chung, and J. Malik. 2004. Spectral grouping using the
Nyström method. IEEE Transactions on Pattern Analysis and Machine Intelligence
26, 2 (2004), 214–225.
[14] A. Frieze, R. Kannan, and S. Vempala. 2004. Fast monte-carlo algorithms for
finding low-rank approximations. J. ACM 51, 6 (2004), 1025–1041.
[15] G. H. Golub and C. F. Van Loan. 1996. Matrix Computation. Johns Hopkins
University Press.
[16] S.A. Goreinov, E.E. Tyrtyshnikov, and N.L. Zamarashki. 1997. A theory of
pseudoskeleton approximations. Linear Algebra Appl. 261, 1 (1997), 1–21.
[17] N. Halko, P.G. Martinsson, and J.A. Tropp. 2011. Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions. SIAM Rev. 53, 2 (2011), 217–288.
[18] S. Kumar, M. Mohri, and A. Talwalkar. 2012. Sampling methods for the Nyström
method. Journal of Machine Learning Research 13, 1 (2012), 981–1006.
[19] L. Lan, K. Zhang, H. Ge, W. Cheng, J. Liu, A. Rauber, X. Li, J. Wang, and H. Zha.
2017. Low-rank Decomposition Meets Kernel Learning: A Generalized Nyström
Method. Artificial Intelligence 250 (2017), 1–15.
[20] D.D. Lee and H.S. Seung. 1999. Learning the parts of objects by non-negative
matrix factorization. Nature 401, 6755 (1999), 788 – 791.
[21] E. Liberty. 2013. Simple and deterministic matrix sketching. In ACM SIGKDD
international conference on Knowledge discovery and data mining. 581–588.
[22] E. Liberty, F. Woolfe, P. Martinsson, V. Rokhlin, and M. Tygert. 2007. Randomized
algorithms for the low-rank approximation of matrices. Proceedings of National
Academy of Sciences 104, 51 (2007), 20167–20172.
[23] Y. Linde, A. Buzo, and R.M. Gray. 1980. An Algorithm for Vector Quantizer
Design. IEEE Transactions on Communications 28, 1 (1980), 84–95.
[24] M.W. Mahoney. 2011. Randomized Algorithms for Matrices and Data. Foundations
and Trends in Machine Learning 3, 2 (2011), 123 – 224.
[25] M.W. Mahoney and P. Drineas. 2009. CUR matrix decompositions for improved
data analysis. Proceedings of National Academy of Sciences 106 (2009), 697–702.
[26] I. Mitliagkas, C. Caramanis, and P. Jain. 2013. Memory limited, streaming PCA.
In Advances in Neural Information Processing Systems. 2886–2894.
[27] J. Nelson and H.L. Nguyên. 2015. OSNAP: Faster Numerical Linear Algebra
Algorithms via Sparser Subspace Embeddings. In IEEE Annual Symposium on
Foundations of Computer Science (FOCS). 135–143.
[28] S. Wang and Z. Zhang. 2012. A Scalable CUR Matrix Decomposition Algorithm:
Lower Time Complexity and Tighter Bound. In Advances in Neural Information
Processing Systems 25. 647–655.
[29] S. Wang, Z. Zhang, and T. Zhang. 2016. Towards More Efficient Nystrom Approximation and CUR Matrix Decomposition. Journal of Machine Learning Research
17 (2016), 1–49.
[30] C.K. I. Williams and M. Seeger. 2001. Using the Nystrom Method to Speed
Up Kernel Machines. In Advances in Neural Information Processing Systems 13.
682–688.
[31] S. Yun, M. lelarge, and A. Proutiere. 2015. Fast and Memory Optimal Low-Rank
Matrix Approximation. In Advances in Neural Information Processing Systems 28.
3177–3185.
[32] H. Zha, C. Ding, M. Gu, X. He, and H. Simon. 2002. Spectral relaxation for
K-means clustering.. In Neural Information Processing Systems 14.
[33] K. Zhang, L. Lan, Z. Wang, and F. Moerchen. 2012. Scaling up Kernel SVM on
Limited Resources: A Low-rank Linearization Approach. In Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics (Proceedings
of Machine Learning Research), Vol. 22. 1425–1434.
[34] K. Zhang, I. Tsang, and J. Kwok. 2008. Improved Nyström low-rank approximation and error analysis. In International conference on Machine learning. 1232–
1239.
[35] T. Zhou and D. Tao. 2011. Godec: Randomized low-rank & sparse matrix decomposition in noisy case (2011). In International Conference on Machine Learning.
