[1] NERSC, “Cori,” https://www.nersc.gov/users/computationalsystems/cori/.
[2] ——, “Trinity and NERSC-8 Computing Platforms: Draft
Technical Requirements,” https://www.lanl.gov/business/
vendors/ assets/docs/Trinity-NERSC-8-DRAFT-technicalrequirements.pdf.
[3] A. Moody, G. Bronevetsky, K. Mohror, and B. R. De Supinski, “Design, modeling, and evaluation of a scalable multilevel checkpointing system,” in International Conference
for High Performance Computing, Networking, Storage and
Analysis (SC). IEEE, 2010, pp. 1–11.
[4] J. Kim, W. J. Dally, S. Scott, and D. Abts, “Technologydriven, highly-scalable dragonﬂy topology,” ACM SIGARCH
Comput. Architecture News, vol. 36, no. 3, pp. 77–88, Jun.
2008.
[5] M. Mubarak, C. D. Carothers, R. B. Ross, and P. Carns,
“Modeling a million-node dragonﬂy network using massively parallel discrete-event simulation,” in High Performance Comput., Networking, Storage and Anal. (SCC) SC
Companion, 2012, pp. 366–376.
[6] B. Prisacari, G. Rodriguez, P. Heidelberger, D. Chen,
C. Minkenberg, and T. Hoeﬂer, “Efﬁcient Task Placement
and Routing of Nearest Neighbor Exchanges in Dragonﬂy
Networks,” in HPDC 2014. New York, NY, USA: ACM,
2014, pp. 129–140.
[7] N. Jain, A. Bhatele, X. Ni, N. J. Wright, and L. V. Kale,
“Maximizing throughput on a dragonﬂy network,” in Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE
Press, 2014, pp. 336–347.
[8] M. Mubarak, C. D. Carothers, R. B. Ross, and P. Carns,
“Enabling parallel simulation of large-scale HPC network
systems,” in IEEE Transactions on Parallel and Distributed
Systems. IEEE, 2016.
[9] X. Yang, J. Jenkins, M. Mubarak, R. Ross, and Z. Lan,
“Watch out for the bully! job interference study on dragonﬂy
networks,” in International Conference for High Performance
Computing, Networking, Storage and Analysis (SC), 2016.
[10] N. Jain, A. Bhatele, S. T. White, T. Gamblin, and L. V.
Kale, “Evaluating HPC setworks via simulation of parallel
workloads,” in Proceedings of the ACM/IEEE International
Conference for High Performance Computing, Networking,
Storage and Analysis, ser. SC ’16. IEEE Computer Society,
Nov. 2016, lLNL-CONF-690662.
[11] Argonne Leadership Computing Facility (ALCF), “Theta,
Argonne’s Cray XC System.” [Online]. Available: https:
//www.alcf.anl.gov/theta
[12] Mubarak, Misbah and Ross, Robert B. and Carothers,
Christopher D., “Validation of CODES dragonﬂy model
with Theta Cray XC System,” 2017. [Online]. Available: http://www.mcs.anl.gov/publication/validation-studycodes-dragonﬂy-network-model-theta-cray-xc-system
[13] S. Lang, P. Carns, R. Latham, R. Ross, K. Harms, and W. Allcock, “I/O Performance Challenges at Leadership Scale,” in
Supercomputing (SC) 2009, ser. SC ’09. New York, NY,
USA: ACM, 2009, pp. 40:1–40:12.
[14] D. Zhao, D. Zhang, K. Wang, and I. Raicu, “Exploring
Reliability of Exascale Systems Through Simulations,” in
Proceedings of the High Performance Computing Symposium,
ser. HPC ’13. San Diego, CA, USA: Society for Computer
Simulation International, 2013, pp. 1:1–1:9.
[15] D. Bard, “Accelerating science with the NERSC burst buffer
early user program.” [Online]. Available: http://salishan.ahscnm.org/uploads/4/9/7/0/49704495/2016-bard.pdf
[16] N. Liu, J. Cope, P. Carns, C. Carothers, R. Ross, G. Grider,
A. Crume, and C. Maltzahn, “On the Role of Burst Buffers in
Leadership-class Storage Systems,” in 2012 IEEE 28th Symposium on Mass Storage Systems and Technologies (MSST),
Apr. 2012, pp. 1–11.
[17] D. Kimpe, K. Mohror, A. Moody, B. Van Essen, M. Gokhale,
R. Ross, and B. R. de Supinski, “Integrated In-system
Storage Architecture for High Performance Computing,” in
Proceedings of the 2Nd International Workshop on Runtime
and Operating Systems for Supercomputers, ser. ROSS ’12.
ACM, 2012, pp. 4:1–4:6.
[18] K. Sato, K. Mohror, A. Moody, T. Gamblin, B. R. de Supinski,
N. Maruyama, and S. Matsuoka, “A User-Level InﬁniBandBased File System and Checkpoint Strategy for Burst
Buffers,” in CCGrid 2014, May 2014, pp. 21–30.
[19] T. Wang, S. Oral, Y. Wang, B. Settlemyer, S. Atchley,
and W. Yu, “BurstMem: A high-performance burst buffer
system for scientiﬁc applications,” in 2014 IEEE International
Conference on Big Data, Oct. 2014, pp. 71–79.
[20] S. Herbein, D. H. Ahn, D. Lipari, T. R. Scogland,
M. Stearman, M. Grondona, J. Garlick, B. Springmeyer, and
M. Taufer, “Scalable I/O-aware job scheduling for burst buffer
enabled HPC clusters,” in HPDC 2016, ser. HPDC ’16. New
York, NY, USA: ACM, 2016, pp. 69–80.
[21] DDN, “DDN inﬁnite memory engine.” [Online]. Available:
http://www.ddn.com/press-releases/ddn-inﬁnite-memoryengine-burst-buffer-exceeds-1-tb-per-second-ﬁle-systemperformance-for-japans-fastest-supercomputer/
[22] H. B. Project, “Juelich Supercomputing Center.” [Online].
Available: https://top500.org/news/juelich-supercomputingcentre-deploys-cray-and-ibm-supercomputers-for-humanbrain-project/
[23] N. R. Adiga et al., “Blue Gene/L torus interconnection
network,” IBM J. of Res. and Develop., vol. 49, no. 2.3, pp.
265–276, Mar. 2005.
[24] “NERSC
Hopper
retired
supercomputer,”
Online:
http://www.nersc.gov/users/computational-systems/retired-

systems/hopper/, last visited Sept. 30, 2015.
[25] Argonne Leadership Computing Facility (ALCF), “Aurora,
Argonne’s next generation supercomputer.” [Online]. Available: http://aurora.alcf.anl.gov(Accessedon:Apr.27,2015)
[26] “NERSC
Edison
supercomputer,”
Online:
http:
//www.nersc.gov/users/computational-systems/edison/,
last visited Sept. 30, 2015.
[27] C. D. Carothers, D. Bauer, and S. Pearce, “ROSS: A highperformance, low-memory, modular Time Warp system,” J. of
Parallel and Distributed Comput., vol. 62, no. 11, pp. 1648–
1669, Nov. 2002.
[28] P. D. Barnes, C. D. Carothers, D. R. Jefferson, and J. M.
LaPre, “Warp speed: Executing Time Warp on 1,966,080
cores,” in Proc. of the 2013 ACM SIGSIM Conf. on Principles
of Advanced Discrete Simulation (PADS), May 2013, pp. 327–
336.
[29] N. Wolfe, C. D. Carothers, M. Mubarak, R. Ross, and
P. Carns, “Modeling a Million-Node Slim Fly Network Using
Parallel Discrete-Event Simulation,” in SIGSIM PADS 2016.
New York, NY, USA: ACM, 2016, pp. 189–199.
[30] M. Mubarak, C. D. Carothers, R. B. Ross, and P. Carns, “A
case study in using massively parallel simulation for extremescale torus network codesign,” in Proc. of the 2nd ACM
SIGSIM/PADS Conf. on Principles of Advanced Discrete
Simulation, 2014, pp. 27–38.
[31] C. Ruemmler and J. Wilkes, “An Introduction to Disk Drive
Modeling,” Computer, vol. 27, no. 3, pp. 17–28, Mar. 1994.
[32] S. Snyder, P. Carns, R. Latham, M. Mubarak, R. Ross,
C. Carothers, B. Behzad, H. V. T. Luu, S. Byna, and Prabhat,
“Techniques for Modeling Large-scale HPC I/O Workloads,”
in PMBS workshop at SC’15. ACM, 2015, pp. 5:1–5:11.
[33] G. Faanes et al., “Cray cascade: a scalable HPC system based
on a dragonﬂy network,” in Proc. of the Int. Conf. on High
Performance Comput., Networking, Storage and Anal. (SC),
2012, pp. 103–113.
[34] J. Won, G. Kim, J. Kim, T. Jiang, M. Parker, and S. Scott,
“Overcoming far-end congestion in large-scale networks,”
in High Performance Computer Architecture (HPCA), 2015
IEEE 21st International Symposium on. IEEE, 2015, pp.
415–427.
[35] R. Hockney, “Performance parameters and benchmarking of
supercomputers,” Parallel computing, vol. 17, no. 10-11, pp.
1111–1130, 1991.
[36] W. Gropp and E. Lusk, “Reproducible measurements of
MPI performance characteristics,” in Proc. of the 6th Eur.
PVM/MPI Users’ Group Meeting on Recent Advances in
Parallel Virtual Mach. and Message Passing Interface, 1999,
pp. 11–18.
[37] J. T. Daly, “A higher order estimate of the optimum checkpoint interval for restart dumps,” Future Generation Computer
Systems, vol. 22, no. 3, pp. 303–312, 2006.
[38] NERSC, “HPCG benchmark,” http://www.nersc.gov/
research-and-development/apex/apex-benchmarks/hpcg/.
[39] M. Besta and T. Hoeﬂer, “Slim Fly: A cost effective lowdiameter network topology,” in Proc. of the Int. Conf. for
High Performance Comput., Networking, Storage and Anal.
(SC), 2014, pp. 348–359.
[40] X. Yuan, S. Mahapatra, W. Nienaber, S. Pakin, and M. Lang,
“A new routing scheme for Jellyﬁsh and its performance
with HPC workloads,” in Proceedings of the International
Conference on High Performance Computing, Networking,
Storage and Analysis. ACM, 2013, p. 36.
[41] M. Gilge, “Cray Data Warp user’s guide.” [Online]. Available:
http://docs.cray.com/books/S-2558-5204/S-2558-5204.pdf
[42] F. Petrini, D. J. Kerbyson, and S. Pakin, “The case of
the missing supercomputer performance: Achieving optimal
performance on the 8,192 processors of ASCI Q,” in Supercomputing, 2003 ACM/IEEE Conference. IEEE, 2003, pp.
55–55.