[1] “Apache Mahout.” http://mahout.apache.org/
[2] “Apache Spark MLlib.” http://spark.apache.org/
mllib/
[3] “Linux Traffic Control.” http://tldp.org/HOWTO/
Traffic-Control-HOWTO/intro.html
[4] “Deep neural networks for acoustic modeling in
speech recognition: The shared views of four research groups,” IEEE Signal Process. Mag., 2012.
[5] M. Abadi, A. Agarwal, P. Barham, E. Brevdo,
Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean,
M. Devin, S. Ghemawat, I. Goodfellow, A. Harp,
G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser,
M. Kudlur, J. Levenberg, D. Mané, R. Monga,
S. Moore, D. Murray, C. Olah, M. Schuster,
J. Shlens, B. Steiner, I. Sutskever, K. Talwar,
P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viégas,
O. Vinyals, P. Warden, M. Wattenberg, M. Wicke,
Y. Yu, and X. Zheng, “TensorFlow: Largescale machine learning on heterogeneous systems,”
2015, software available from tensorflow.org.
http://tensorflow.org/
[6] A. Ahmed, M. Aly, J. Gonzalez, S. M. Narayanamurthy, and A. J. Smola, “Scalable inference in
latent variable models,” in WSDM, 2012.
[7] Amazon, “AWS global infrastructure.” https://aws.
amazon.com/about-aws/global-infrastructure/
[8] Amazon, “Amazon EC2 pricing,” Janurary 2017.
https://aws.amazon.com/ec2/pricing/
[9] A. Auradkar, C. Botev, S. Das, D. D. Maagd,
A. Feinberg, P. Ganti, L. Gao, B. Ghosh,
K. Gopalakrishna, B. Harris, J. Koshy, K. Krawez,
J. Kreps, S. Lu, S. Nagaraj, N. Narkhede, S. Pachev,
I. Perisic, L. Qiao, T. Quiggle, J. Rao, B. Schulman, A. Sebastian, O. Seeliger, A. Silberstein,
B. Shkolnik, C. Soman, R. Sumbaly, K. Surlaker,
S. Topiwala, C. Tran, B. Varadarajan, J. Westerman,
Z. White, D. Zhang, and J. Zhang, “Data infrastructure at LinkedIn,” in ICDE, 2012.
[10] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent
Dirichlet allocation,” JMLR, 2003.
[11] J. K. Bradley, A. Kyrola, D. Bickson, and
C. Guestrin, “Parallel coordinate descent for L1-
regularized loss minimization,” in ICML, 2011.
[12] I. Cano, M. Weimer, D. Mahajan, C. Curino, and
G. M. Fumarola, “Towards geo-distributed machine
learning,” CoRR, 2016.
[13] T. M. Chilimbi, Y. Suzue, J. Apacible, and K. Kalyanaraman, “Project Adam: Building an efficient and
scalable deep learning training system,” in OSDI,
2014.
[14] C. Chu, S. K. Kim, Y. Lin, Y. Yu, G. R. Bradski, A. Y.
Ng, and K. Olukotun, “Map-Reduce for machine
learning on multicore,” in NIPS, 2006.
[15] G. Cormode, M. N. Garofalakis, S. Muthukrishnan,
and R. Rastogi, “Holistic aggregates in a networked
world: Distributed tracking of approximate quantiles,” in SIGMOD, 2005.
[16] H. Cui, J. Cipar, Q. Ho, J. K. Kim, S. Lee, A. Kumar, J. Wei, W. Dai, G. R. Ganger, P. B. Gibbons,
G. A. Gibson, and E. P. Xing, “Exploiting bounded
staleness to speed up big data analytics,” in USENIX
ATC, 2014.
[17] H. Cui, A. Tumanov, J. Wei, L. Xu, W. Dai,
J. Haber-Kucharsky, Q. Ho, G. R. Ganger, P. B.
Gibbons, G. A. Gibson, and E. P. Xing, “Exploiting iterative-ness for parallel ML computations,” in SoCC, 2014, software available at
https://github.com/cuihenggang/iterstore.
[18] H. Cui, H. Zhang, G. R. Ganger, P. B. Gibbons,
and E. P. Xing, “GeePS: Scalable deep learning on
distributed GPUs with a GPU-specialized parameter server,” in EuroSys, 2016, software available at
https://github.com/cuihenggang/geeps.
[19] W. Dai, A. Kumar, J. Wei, Q. Ho, G. Gibson,
and E. P. Xing, “Analysis of high-performance distributed ML at scale through parameter server consistency models,” in AAAI, 2015.
[20] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin,
Q. V. Le, M. Z. Mao, M. Ranzato, A. W. Senior,
P. A. Tucker, K. Yang, and A. Y. Ng, “Large scale
distributed deep networks,” in NIPS, 2012.
[21] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani,
“Least angle regression,” in The Annals of Statistics,
2004.
[22] G. Elidan, I. McGraw, and D. Koller, “Residual
belief propagation: Informed scheduling for asynchronous message passing,” in UAI, 2006.
[23] ESnet and Lawrence Berkeley National Laboratory,
“iperf3.” http://software.es.net/iperf/
[24] A. Frome, G. S. Corrado, J. Shlens, S. Bengio,
J. Dean, M. Ranzato, and T. Mikolov, “DeViSE:
A deep visual-semantic embedding model,” in NIPS,
2013.
[25] R. Gemulla, E. Nijkamp, P. J. Haas, and Y. Sismanis,
“Large-scale matrix factorization with distributed
stochastic gradient descent,” in SIGKDD, 2011.
[26] J. E. Gonzalez, Y. Low, H. Gu, D. Bickson, and
C. Guestrin, “PowerGraph: Distributed graphparallel computation on natural graphs,” in OSDI,
2012.
[27] J. E. Gonzalez, R. S. Xin, A. Dave, D. Crankshaw,
M. J. Franklin, and I. Stoica, “GraphX: Graph processing in a distributed dataflow framework,” in
OSDI, 2014.
[28] Google, “Google data center locations.” https://www.google.com/about/datacenters/
inside/locations/index.html
[29] A. G. Greenberg, J. R. Hamilton, D. A. Maltz, and
P. Patel, “The cost of a cloud: research problems in
data center networks,” Computer Communication
Review, 2009.
[30] T. L. Griffiths and M. Steyvers, “Finding scientific
topics,” Proceedings of the National Academy of
Sciences of the United States of America, 2004.
[31] A. Gupta, F. Yang, J. Govig, A. Kirsch, K. Chan,
K. Lai, S. Wu, S. G. Dhoot, A. R. Kumar, A. Agiwal, S. Bhansali, M. Hong, J. Cameron, M. Siddiqi,
D. Jones, J. Shute, A. Gubarev, S. Venkataraman,
and D. Agrawal, “Mesa: Geo-replicated, near realtime, scalable data warehousing,” PVLDB, 2014.
[32] A. Harlap, H. Cui, W. Dai, J. Wei, G. R. Ganger, P. B.
Gibbons, G. A. Gibson, and E. P. Xing, “Addressing the straggler problem for iterative convergent
parallel ML,” in SoCC, 2016.
[33] B. Heintz, A. Chandra, and R. K. Sitaraman, “Optimizing grouped aggregation in geo-distributed
streaming analytics,” in HPDC, 2015.
[34] Q. Ho, J. Cipar, H. Cui, S. Lee, J. K. Kim, P. B.
Gibbons, G. A. Gibson, G. R. Ganger, and E. P.
Xing, “More effective distributed ML via a stale
synchronous parallel parameter server,” in NIPS,
2013.
[35] C. Hong, S. Kandula, R. Mahajan, M. Zhang,
V. Gill, M. Nanduri, and R. Wattenhofer, “Achieving high utilization with software-driven WAN,” in
SIGCOMM, 2013.
[36] C. Hung, L. Golubchik, and M. Yu, “Scheduling
jobs across geo-distributed datacenters,” in SoCC,
2015.
[37] M. Jaggi, V. Smith, M. Takác, J. Terhorst,
S. Krishnan, T. Hofmann, and M. I. Jordan,
“Communication-efficient distributed dual coordinate ascent,” in NIPS, 2014.
[38] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev,
J. Long, R. B. Girshick, S. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for fast feature embedding,” CoRR, 2014.
[39] A. Karpathy, G. Toderici, S. Shetty, T. Leung,
R. Sukthankar, and F. F. Li, “Large-scale video classification with convolutional neural networks,” in
CVPR, 2014.
[40] J. K. Kim, Q. Ho, S. Lee, X. Zheng, W. Dai, G. A.
Gibson, and E. P. Xing, “STRADS: a distributed
framework for scheduled model parallel machine
learning,” in EuroSys, 2016.
[41] K. Kloudas, R. Rodrigues, N. M. Preguiça, and
M. Mamede, “PIXIDA: optimizing data parallel jobs
in wide-area data analytics,” PVLDB, 2015.
[42] N. Laoutaris, M. Sirivianos, X. Yang, and P. Rodriguez, “Inter-datacenter bulk transfers with netstitcher,” in SIGCOMM, 2011.
[43] Y. LeCun, B. E. Boser, J. S. Denker, D. Henderson,
R. E. Howard, W. E. Hubbard, and L. D. Jackel,
“Backpropagation applied to handwritten zip code
recognition,” Neural Computation, 1989.
[44] G. Lee, J. J. Lin, C. Liu, A. Lorek, and D. V. Ryaboy,
“The unified logging infrastructure for data analytics
at Twitter,” PVLDB, 2012.
[45] M. Li, D. G. Andersen, J. W. Park, A. J. Smola,
A. Ahmed, V. Josifovski, J. Long, E. J. Shekita, and
B. Su, “Scaling distributed machine learning with
the parameter server,” in OSDI, 2014.
[46] M. Li, D. G. Andersen, A. J. Smola, and K. Yu,
“Communication efficient distributed machine learning with the parameter server,” in NIPS, 2014.
[47] Y. Low, J. Gonzalez, A. Kyrola, D. Bickson,
C. Guestrin, and J. M. Hellerstein, “Distributed
GraphLab: A framework for machine learning in
the cloud,” VLDB, 2012.
[48] E. K. Lua, J. Crowcroft, M. Pias, R. Sharma, and
S. Lim, “A survey and comparison of peer-to-peer
overlay network schemes,” IEEE Communications
Surveys and Tutorials, 2005.
[49] D. Mahajan, S. S. Keerthi, S. Sundararajan, and
L. Bottou, “A functional approximation based distributed learning algorithm,” CoRR, 2013.
[50] X. Meng, J. K. Bradley, B. Yavuz, E. R. Sparks,
S. Venkataraman, D. Liu, J. Freeman, D. B. Tsai,
M. Amde, S. Owen, D. Xin, R. Xin, M. J. Franklin,
R. Zadeh, M. Zaharia, and A. Talwalkar, “MLlib:
Machine learning in Apache Spark,” CoRR, 2015.
[51] Microsoft, “Azure regions.” https://azure.microsoft.
com/en-us/region
[52] W. Neiswanger, C. Wang, and E. P. Xing, “Asymptotically exact, embarrassingly parallel MCMC,” in
UAI, 2014.
[53] “New York Times dataset,” http://www.ldc.upenn.
edu/.
[54] D. Newman, A. U. Asuncion, P. Smyth, and
M. Welling, “Distributed algorithms for topic models,” JMLR, 2009.
[55] C. Olston, J. Jiang, and J. Widom, “Adaptive filters
for continuous queries over distributed data streams,”
in SIGMOD, 2003.
[56] C. Olston and J. Widom, “Offering a precisionperformance tradeoff for aggregation queries over
replicated data,” in VLDB, 2000.
[57] Q. Pu, G. Ananthanarayanan, P. Bodík, S. Kandula,
A. Akella, P. Bahl, and I. Stoica, “Low latency geodistributed data analytics,” in SIGCOMM, 2015.
[58] A. Rabkin, M. Arye, S. Sen, V. S. Pai, and M. J.
Freedman, “Aggregation and degradation in JetStream: Streaming analytics in the wide area,” in
NSDI, 2014.
[59] B. Recht, C. Ré, S. J. Wright, and F. Niu, “Hogwild:
A lock-free approach to parallelizing stochastic gradient descent,” in NIPS, 2011.
[60] P. Richtárik and M. Takác, “Distributed coordinate
descent method for learning with big data,” CoRR,
2013.
[61] D. E. Rumelhart, G. E. Hinton, and R. J. Williams,
“Learning representations by back-propagating errors,” Cognitive modeling, 1988.
[62] O. Russakovsky, J. Deng, H. Su, J. Krause,
S. Satheesh, S. Ma, Z. Huang, A. Karpathy,
A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei,
“ImageNet large scale visual recognition challenge,”
IJCV, 2015.
[63] O. Shamir, N. Srebro, and T. Zhang,
“Communication-efficient distributed optimization
using an approximate Newton-type method,” in
ICML, 2014.
[64] A. J. Smola and S. M. Narayanamurthy, “An architecture for parallel topic models,” VLDB, 2010.
[65] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in CVPR,
2015.
[66] M. Takác, A. S. Bijral, P. Richtárik, and N. Srebro,
“Mini-batch primal and dual methods for SVMs,” in
ICML, 2013.
[67] TeleGeography, “Global Internet geography.”
https://www.telegeography.com/research-services/
global-internet-geography/
[68] A. Thusoo, Z. Shao, S. Anthony, D. Borthakur,
N. Jain, J. S. Sarma, R. Murthy, and H. Liu, “Data
warehousing and analytics infrastructure at Facebook,” in SIGMOD, 2010.
[69] K. I. Tsianos, S. F. Lawlor, and M. G. Rabbat, “Communication/computation tradeoffs in consensusbased distributed optimization,” in NIPS, 2012.
[70] L. G. Valiant, “A bridging model for parallel computation,” Commun. ACM, 1990.
[71] R. Viswanathan, A. Akella, and G. Ananthanarayanan, “Clarinet: WAN-aware optimization for
analytics queries,” in OSDI, 2016.
[72] A. Vulimiri, C. Curino, B. Godfrey, K. Karanasos,
and G. Varghese, “WANalytics: Analytics for a geodistributed data-intensive world,” in CIDR, 2015.
[73] A. Vulimiri, C. Curino, P. B. Godfrey, T. Jungblut,
J. Padhye, and G. Varghese, “Global analytics in the
face of bandwidth and regulatory constraints,” in
NSDI, 2015.
[74] J. Wei, W. Dai, A. Qiao, Q. Ho, H. Cui, G. R.
Ganger, P. B. Gibbons, G. A. Gibson, and E. P. Xing,
“Managed communication and consistency for fast
data-parallel iterative analytics,” in SoCC, 2015.
[75] O. Wolfson, B. Xu, S. Chamberlain, and L. Jiang,
“Moving objects databases: Issues and solutions,” in
SSDBM, 1998.
[76] Z. Wu, M. Butkiewicz, D. Perkins, E. Katz-Bassett,
and H. V. Madhyastha, “SPANStore: Cost-effective
geo-replicated storage spanning multiple cloud services,” in SOSP, 2013.
[77] E. P. Xing, Q. Ho, W. Dai, J. K. Kim, J. Wei, S. Lee,
X. Zheng, P. Xie, A. Kumar, and Y. Yu, “Petuum:
A new platform for distributed machine learning on
big data,” in SIGKDD, 2015.
[78] E. P. Xing, Q. Ho, P. Xie, and W. Dai, “Strategies
and principles of distributed machine learning on
big data,” CoRR, 2015.
[79] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma,
M. McCauly, M. J. Franklin, S. Shenker, and I. Stoica, “Resilient distributed datasets: A fault-tolerant
abstraction for in-memory cluster computing,” in
NSDI, 2012.
[80] Y. Zhang, Q. Gao, L. Gao, and C. Wang, “PrIter: A
distributed framework for prioritized iterative computations,” in SoCC, 2011.
[81] Y. Zhang, J. C. Duchi, and M. J. Wainwright,
“Communication-efficient algorithms for statistical
optimization,” JMLR, 2013.
[82] Y. Zhang and X. Lin, “DiSCO: Distributed optimization for self-concordant empirical loss,” in ICML,
2015.
[83] M. Zinkevich, A. J. Smola, and J. Langford, “Slow
learners are fast,” in NIPS, 2009.
[84] M. Zinkevich, M. Weimer, A. J. Smola, and L. Li,
“Parallelized stochastic gradient descent,” in NIPS,
2010.

