[1] The Green500 List. [Online]. http://www.green500.org/
[2] Amazon Elastic Compute Cloud User Guide for Linux Instances. [Online]. http://docs.aws.amazon.com/AWSEC2/ latest/UserGuide/using_cluster_computing.html.
[3] Microsoft Azure, N Series, GPU enabled Virtual Machines. [Online]. https://azure.microsoft.com/en-us/pricing/details/virtual- machines/series/#n-series.
[4] Google Cloud Platform: GRAPHICS PROCESSING UNIT (GPU), Leverage GPUs on Google Cloud for machine learning and scientific computing. [Online]. https://cloud.google.com/gpu/.
[5] T. Geller. 2011. Supercomputing’s Exaflop Target. In Commun. Of the ACM.
[6] M. Abadi et al. 2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems. Google preliminary whitepaper.
[7] D. Yu, K. Yao, and Y. Zhang. 2015. The Computational Network Toolkit [Best of the Web]. In IEEE Signal Processing Magazine.
[8] R. Collobert, K. Kavukcuoglu, and C. Farabet. 2011. Torch7: A Matlab-Like Environment for Machine Learning. In BigLearn NIPS Workshop.
[9] Install GraphLab Create with GPU Acceleration. [Online]. https://dato.com/download/install-graphlab-create-gpu.html/.
[11] J. Nelson, B. Holt, B. Myers, P. Briggs, L. Ceze, S. Kahan, and M. Oskin. 2015. Latency-tolerant Software Distributed Shared Memory. In Proc. of the USENIX Annual Technical Conference (ATC).
[12] J. Gonzalez, Y. Low, H. Gu, D. Bickson, and C. Guestrin. 2012. PowerGraph: Distributed Graph-Parallel Computation on Natural Graphs. In Proc. of the USENIX Conference on Operating Systems Design and Implementation (OSDI).
[13] S. Potluri, K. Hamidouche, A. Venkatesh, D. Bureddy, and D. Panda. 2013. Efficient Inter-Node MPI Communication Using GPUDirect RDMA for InfiniBand Clusters with NVIDIA GPUs. In Proc. of the International Conference on Parallel Processing (ICPP).
[14] H. Wang, S. Potluri, M. Luo, A. Singh, S. Sur, and D. Panda. 2011. MVAPICH2-GPU: Optimized GPU to GPU Communication for InfiniBand Clusters. In Proc. of the International Supercomputing Conference (ISC).
[15] L. Oden and H. Fröning. 2013. GGAS: Global GPU Address Spaces for Efficient Communication in Heterogeneous Clusters. In Proc. of the IEEE International Conference Cluster Computing (Cluster).
[16] J. Stuart and J. Owens. 2009. Message Passing on Data-Parallel Architectures. In Proc. of the IEEE International Symposium on Parallel Distributed Processing.
[17] S. Potluri, N. Luehr, and N. Sakharnykh. 2016. Simplifying Multi- GPU Communication with NVSHMEM. [Online]. http://on- demand.gputechconf.com/gtc/2016/presentation/s6378-nathan-luehr- simplyfing-multi-gpu-communication-nvshmem.pdf.
[18] S. Kim, S. Huh, Y. Hu, X. Zhang, E. Witchel, A. Wated, and M. Silberstein. 2014. GPUnet: Networking Abstractions for GPU Programs. In Proc. of the USENIX Symp. on Operating Systems Design and Implementation (OSDI).
[19] F. Daoud, A. Watad, and M. Silberstein. 2016. GPUrdma: GPU-Side Library for High Performance Networking from GPU Kernels. In Workshop on Runtime and OS Support for Supercomputers.
[20] OpenCL 2.0 Reference Pages. [Online]. http://www.khronos.org/registry/cl/sdk/2.0/docs/man/xhtml/.
[21] CUDA C Programming Guide. [Online]. http://docs.nvidia.com/cuda/cuda-c-programming-guide/.
[22] HSA Foundation. 2015. HSA Programmer’s Reference Manual: HSAIL Virtual ISA and Programming Model, Compiler Writer’s Guide, and Object Format (BRIG) Version 1.0.1.
[23] S. Junkins. 2016. The Compute Architecture of Intel® Processor Graphics Gen9. Intel whitepaper, v1.0.
[24] HPC Challenge Benchmark: RandomAccess. [Online]. http://icl.cs.utk.edu/projectsfiles/hpcc/RandomAccess/.
[25] Wikipedia. Counting Sort. [Online]. https://en.wikipedia.org/wiki/Counting_sort.
[26] M. Orr, B. Beckmann, S. Reinhardt, and D. Wood. 2014. Fine-Grain Task Aggregation and Coordination on GPUs. In Proc. of the International Symp. on Computer Architecture (ISCA).
[27] H. Levy. 2003. Single Producer Consumer on a Bounded Array Problem. Course notes. [Online]. https://courses.cs.washington.edu/courses/cse451/03wi/section/prodc ons.htm.
[28] W. Fung and T. Aamodt. 2011. Thread Block Compaction for Efficient SIMT Control Flow. In Proc. of the International Symp. on High Performance Computer Architecture (HPCA).
[29] University of Florida Sparse Matrix Collection. [Online]. http://www.cise.ufl.edu/research/sparse/matrices/.
[30] NERSC. Meraculous Data. [Online]. http://portal.nersc.gov/project/m888/apex/Meraculous_data/.
[31] OpenMPI FAQ. [Online]. https://www.open- mpi.org/faq/?category=supported-systems#thread-support.
[32] S. Che. 2014. GasCL: A Vertex-Centric Graph Model for GPUs. In Proc. of the IEEE High Performance Extreme Computing Conference (HPEC).
[33] E. Georganas, A. Buluç, J. Chapman, L. Oliker, D. Rokhsar, and Katherine Yelick. 2014. Parallel De Bruijn Graph Construction and Traversal for De Novo Genome Assembly. In Proc. of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC).
[34] M. Silberstein, B. Ford, I. Keidar, and E. Witchel. 2013. GPUfs: Integrating File Systems with GPUs. In Proc. of the International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS).
[35] J. Wang, N. Rubin, A. Sidelnik, and S. Yalamanchili. 2015. Dynamic Thread Block Launch: a Lightweight Execution Mechanism to Support Irregular Applications on GPUs. In Proc. of the International Symp. on Computer Architecture (ISCA).
[36] W. Hillis and G. Steele. 1986. Data Parallel Algorithms. In Comm. of the ACM.
[37] A. Morari, A. Tumeo, D. Chavarria-Miranda, O. Villa, and M. Valero. 2014. Scaling Irregular Applications through Data Aggregation and Software Multithreading. In Proc. of the International Parallel and Distributed Processing Symp. (IPDPS).
[38] CCIX Consortium. Cache Coherent Interconnect for Accelerators (CCIX). [Online]. http://www.ccixconsortium.com
