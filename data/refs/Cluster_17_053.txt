[1] V. Eijkhout, Introduction to High Performance Scientiﬁc
Computing.
lulu.com, 2011.
[Online].
Available:
http://www.tacc.utexas.edu/ eijkhout/istc/istc.html
[2] A. Grama, A. Gupta, G. Karypis, and V. Kumar, Introduction
to Parallel Computing, Second Edition, 2003.

[3] C. Andrieu, N. De Freitas, A. Doucet, and M. I. Jordan,
“An introduction to MCMC for machine learning,” Machine
Learning, vol. 50, no. 1-2, pp. 5–43, 2003.

[4] G. Casella and E. I. George, “Explaining the Gibbs Sampler,”
The American Statistician, vol. 46, no. 3, pp. 167–174, 1992.
[5] L. Murray, “Distributed Markov chain Monte Carlo,”
...
Processing Systems Workshop on Learning
on . . . , pp.
1–4, 2010.
[Online].
Available:
http://lccc.eecs.berkeley.edu/Papers/dmcmc short.pdf
[6] J. Weare and J. Goodman, “Ensemble Samplers With Afﬁne
Invariance,” vol. 5, no. 2, 2010.
[7] M. B. Thompson, “A Comparison of Methods for Computing
Autocorrelation Time,” ArXiv e-prints, no. 1007, p. 8, 2010.
[Online]. Available: http://arxiv.org/abs/1011.0175
[8] D. Foreman-Mackey, D. W. Hogg, D. Lang, and
J. Goodman, “emcee : The MCMC Hammer,” Publications
of the Astronomical Society of the Paciﬁc, vol. 125,
no. 925, pp. 306–312, 2013. [Online]. Available:
http://arxiv.org/abs/1202.3665
[9] E. Angelino, E. Kohler, A. Waterland, M. Seltzer,
and R. P. Adams, “Accelerating MCMC via Parallel
Predictive Prefetching,” Uncertainty in Artiﬁcial Intelligence,
Proceedings of the Thirtieth Conference, pp. 1–14, 2014.
[Online]. Available: http://arxiv.org/abs/1403.7265
[10] W. Neiswanger, C. Wang, and E. P. Xing, “Asymptotically
exact, embarrassingly parallel mcmc,” in Proceedings of
the Thirtieth Conference on Uncertainty in Artiﬁcial
Intelligence, ser. UAI’14. Arlington, Virginia, United States:
AUAI Press, 2914, pp. 623–632. [Online]. Available:
http://dl.acm.org/citation.cfm?id=3020751.3020816
[11] S. L. Scott, A. W. Blocker, F. V. Bonassi, H. A. Chipman,
E. I. George, and R. E. McCulloch, “Bayes and big data: The
consensus monte carlo algorithm,” International Journal of
Management Science and Engineering Management, vol. 11,
pp. 78–88, 2016.
[12] J. A. Christen and C. Foxy, “A general purpose sampling
algorithm for continuous distributions (the t-walk),” Bayesian
Analysis, vol. 5, no. 2, pp. 263–282, 2010.
[13] D. Tsafrir, Y. Etsion, D. G. Feitelson, and S. Kirkpatrick,
“System noise, os clock ticks, and ﬁne-grained parallel
applications,” in Proceedings of the 19th Annual International
Conference on Supercomputing, ser. ICS ’05. New York,
NY, USA: ACM, 2005, pp. 303–312. [Online]. Available:
http://doi.acm.org/10.1145/1088149.1088190

[14] H. Bauke and S. Mertens, “Random numbers for large-scale
distributed Monte Carlo simulations,” Physical Review E Statistical, Nonlinear, and Soft Matter Physics, vol. 75, no. 6,
pp. 1–14, 2007.
[15] U. Drepper, “What every programmer should know about
memory,” Red Hat, pp. 1–114, 2007. [Online]. Available:
https://people.freebsd.org/ lstewart/articles/cpumemory.pdf
[16] M. Jamil and X.-S. Yang, “A literature survey of benchmark functions for global optimization problems.” CoRR, vol.
abs/1308.4008, 2013.