[1] Mantevo Applications . http://mantevo.org/packages/
[2] 2015. The High Performance Conjugate Gradients Benchmark. (2015). http:
//hpcg-benchmark.org/
[3] Albert Alexandrov, Mihai F. Ionescu, Klaus E. Schauser, and Chris Scheiman.
1995. LogGP: Incorporating Long Messages into the LogP Model&Mdash;One
Step Closer Towards a Realistic Model for Parallel Computation. In Proceedings
of the Seventh Annual ACM Symposium on Parallel Algorithms and Architectures
(SPAA ’95). ACM, New York, NY, USA, 95–105. DOI:http://dx.doi.org/10.1145/
215399.215427
[4] Ammar Ahmad Awan, K Hamidouche, A Venkatesh, and DK Panda. 2016. Efficient Large Message Broadcast using NCCL and CUDA-Aware MPI for Deep
Learning. In Proceedings of the 23rd European MPI Users’ Group Meeting. ACM,
15–22.
[5] Mark S Birrittella, Mark Debbage, Ram Huggahalli, James Kunz, Tom Lovett,
Todd Rimmer, Keith D Underwood, and Robert C Zak. 2015. Intel® Omni-path
Architecture: Enabling Scalable, High Performance Fabrics. In High-Performance
Interconnects (HOTI), 2015 IEEE 23rd Annual Symposium on. IEEE, 1–9.
[6] Open MPI : Open Source High Performance Computing. 2017. http://www.openmpi.org. (2017).
[7] D. E. Culler, R. M. Karp, D. A. Patterson, A. Sahay, K. E. Schauser, E. Santos,
R. Subramonian, and T. von Eicken. 1993. LogP: Towards a Realistic Model of
Parallel Computation. In Fourth ACM SIGPLAN Symposium on Principles and
Practice of Parallel Programming. 262–273.
[8] Richard L. Graham, Devendar Bureddy, Pak Lui, Hal Rosenstock, Gilad Shainer,
Gil Bloch, Dror Goldenerg, Mike Dubman, Sasha Kotchubievsky, Vladimir Koushnir, Lion Levi, Alex Margolin, Tamir Ronen, Alexander Shpiner, Oded Wertheim,
and Eitan Zahavi. 2016. Scalable Hierarchical Aggregation Protocol (SHArP): A
Hardware Architecture for Efficient Data Reduction. In Proceedings of the First
Workshop on Optimization of Communication in HPC (COM-HPC ’16). IEEE Press,
Piscataway, NJ, USA, 1–10. DOI:http://dx.doi.org/10.1109/COM-HPC.2016.6
[9] Roger W. Hockney. 1994. The Communication Challenge for MPP: Intel Paragon
and Meiko CS-2. Parallel Comput. 20, 3 (March 1994), 389–398. DOI:http://dx.
doi.org/10.1016/S0167-8191(06)80021-9
[10] IB 2017. InfiniBand Trade Association. http://www.infinibandta.com. (2017).
[11] J. Liu and W. Jiang and P. Wyckoff and D. K. Panda and D. Ashton and D. Buntinas
and B. Gropp and B. Tooney. 2004. High Performance Implementation of MPICH2
over InfiniBand with RDMA Support. In IPDPS.
[12] Krishna Kandalla, Hari Subramoni, Karen Tomko, Dmitry Pekurovsky, Sayantan
Sur, and Dhabaleswar K. Panda. 2011. High-Performance and Scalable NonBlocking All-to-All with Collective Offload on InfiniBand Clusters: A Study
with Parallel 3D FFT. Comput. Sci. 26 (June 2011), 237–246. Issue 3-4. DOI:
http://dx.doi.org/10.1007/s00450-011-0170-4
[13] K. Kandalla, U. Yang, J. Keasler, T. Kolev, A. Moody, H. Subramoni, K. Tomko, J.
Vienne, and D. K. Panda. 2012. Designing Non-blocking Allreduce with Collective
Offload on InfiniBand Clusters: A Case Study with Conjugate Gradient Solvers.
In IEEE International Symposium on Parallel and Distributed Processing (IPDPS).
[14] Thilo Kielmann, Henri E. Bal, and Kees Verstoep. 2000. Fast Measurement of
LogP Parameters for Message Passing Platforms. In Proceedings of the 15 IPDPS
2000 Workshops on Parallel and Distributed Processing (IPDPS ’00). Springer-Verlag,
London, UK, UK, 1176–1183.
[15] Shigang Li, Torsten Hoefler, Chungjin Hu, and Marc Snir. 2014. Improved MPI
Collectives for MPI Processes in Shared Address Spaces. Cluster Computing 17, 4
(Dec. 2014), 1139–1155. DOI:http://dx.doi.org/10.1007/s10586-014-0361-4
[16] Shigang Li, Torsten Hoefler, and Marc Snir. 2013. NUMA-aware Shared-memory
Collective Communication for MPI. In Proceedings of the 22Nd International
Symposium on High-performance Parallel and Distributed Computing (HPDC
’13). ACM, New York, NY, USA, 85–96. DOI:http://dx.doi.org/10.1145/2462902.
2462903
[17] J. Liu, W. Jiang, P. Wyckoff, D. K. Panda, D. Ashton, D. Buntinas, W. Gropp,
and B. Toonen. 2004. Design and Implementation of MPICH2 over InfiniBand
with RDMA Support. In Proceedings of Int’l Parallel and Distributed Processing
Symposium (IPDPS ’04).
[18] M. Venkata and R. Graham and J. Ladd and P. Shamis and I. Rabinovitz and F.
Vasily and G. Shainer. 2011. ConnectX-2 CORE-Direct Enabled Asynchronous
Broadcast Collective Communications. In Proceedings of the 25th IEEE International Parallel and Distributed Processing Symposium, Workshops.
[19] Message Passing Interface Forum 1994. MPI: A Message-Passing Interface Standard.
Message Passing Interface Forum.
[20] OSU Micro-Benchmarks. 2017. http://mvapich.cse.ohio-state.edu/benchmarks.
(2017).
[21] MPI3 2012. MPI-3 Standard Document. http://www.mpi-forum.org/docs/mpi-3.
0/mpi30-report.pdf. (2012).
[22] MVAPICH2 2017. MVAPICH: MPI over InfiniBand, Omni-Path, Ethernet/iWARP,
and RoCE. http://mvapich.cse.ohio-state.edu/. (2017).
[23] J. Pjesivac-Grbovic, T. Angskun, G. Bosilca, G. E. Fagg, E. Gabriel, and J. J.
Dongarra. 2005. Performance Analysis of MPI Collective Operations. In 19th
IEEE International Parallel and Distributed Processing Symposium. 8 pp.–. DOI:
http://dx.doi.org/10.1109/IPDPS.2005.335
[24] Rolf Rabenseifner. 1999. Automatic MPI Counter Profiling of all Users: First
Results on a CRAY T3E 900-512. In Proceedings of the message passing interface
developerfis and userfis conference, Vol. 1999. 77–85.
[25] Rolf Rabenseifner. 2004. Optimization of Collective Reduction Operations. In
International Conference on Computational Science. Springer, 1–9.
[26] Rajeev Thakur, Rolf Rabenseifner, and William Gropp. 2005. Optimization of
Collective Communication Operations in MPICH. Int. J. High Perform. Comput.
Appl. 19, 1 (Feb. 2005), 49–66. DOI:http://dx.doi.org/10.1177/1094342005051521
[27] J. Zhang, X. Lu, J. Jose, M. Li, R. Shi, and D. K. D. K. Panda. 2014. High Performance
MPI Library over SR-IOV enabled InfiniBand Clusters. In 2014 21st International
Conference on High Performance Computing (HiPC). 1–10. DOI:http://dx.doi.org/
10.1109/HiPC.2014.7116876
[28] Jie Zhang, Xiaoyi Lu, Jithin Jose, Rong Shi, and Dhabaleswar K. (DK) Panda.
2014. Can Inter-VM Shmem Benefit MPI Applications on SR-IOV Based Virtualized
Infiniband Clusters? Springer International Publishing, Cham, 342–353. DOI:
http://dx.doi.org/10.1007/978-3-319-09873-9 29
