[1] “http://www.top500.org/.”
[2] P. Micikevicius, “3D finite difference computation on GPUs using
CUDA,” in GPGPU-2: Proceedings of 2nd Workshop on General
Purpose Processing on Graphics Processing Units, pp. 79–84, ACM,
2009.
[3] H. S. Kim, D. Unat, S. B. Baden, and J. P. Schulze, “A new approach
to interactive viewpoint selection for volume data sets,” Information
Visualization, vol. 12, no. 3-4, pp. 240–256, 2013.
[4] J. Zhou, D. Unat, D. J. Choi, C. C. Guest, and Y. Cui, “Hands-on
performance tuning of 3d finite difference earthquake simulation on gpu
fermi chipset,” Procedia Computer Science, vol. 9, pp. 976 – 985, 2012.

[5] S. Lee and R. Eigenmann, “OpenMPC: Extended OpenMP Programming and Tuning for GPUs,” in Proceedings of the 2010 ACM/IEEE
International Conference for High Performance Computing, Networking,
Storage and Analysis, SC ’10, pp. 1–11, IEEE Computer Society, 2010.
[6] M. Wolfe, “Implementing the PGI Accelerator model,” in Proceedings
of the 3rd Workshop on General-Purpose Computation on Graphics
Processing Units, GPGPU ’10, pp. 43–50, 2010.
[7] J. Kim, Y.-J. Lee, J. Park, and J. Lee, “Translating openmp device
constructs to opencl using unnecessary data transfer elimination,” in
Proceedings of the International Conference for High Performance
Computing, Networking, Storage and Analysis, SC ’16, (Piscataway, NJ,
USA), pp. 51:1–51:12, IEEE Press, 2016.
[8] T. Hoshino, N. Maruyama, S. Matsuoka, and R. Takaki, “Cuda vs
openacc: Performance case studies with kernel benchmarks and a
memory-bound cfd application,” in 2013 13th IEEE/ACM International
Symposium on Cluster, Cloud, and Grid Computing, pp. 136–143, May
2013.
[9] NVDIA, “NVDIA NVLINK High-Speed Interconnect: Application Performance,” tech. rep., 2014.
[10] G. Rivera and C.-W. Tseng, “Tiling optimizations for 3D scientific
computations,” in Proceedings of the 2000 ACM/IEEE conference on
Supercomputing (CDROM), SC ’00, IEEE Computer Society, 2000.
[11] D. Unat, A. Dubey, T. Hoefler, J. Shalf, M. Abraham, M. Bianco,
B. L. Chamberlain, R. Cledat, H. C. Edwards, H. Finkel, K. Fuerlinger,
F. Hannig, E. Jeannot, A. Kamil, J. Keasler, P. H. J. Kelly, V. Leung,
H. Ltaief, N. Maruyama, C. J. Newburn, and M. Pericas, “Trends in data
locality abstractions for hpc systems,” IEEE Transactions on Parallel
and Distributed Systems, vol. PP, no. 99, pp. 1–1, 2017.
[12] D. Unat, T. Nguyen, W. Zhang, M. N. Farooqi, B. Bastem, G. Michelogiannakis, A. Almgren, and J. Shalf, “TiDA: High-Level Programming
Abstractions for Data Locality Management,” in Proceedings of 31st
International Conference on High Performance Computing, ISC High
Performance 2016, pp. 116–135, Springer International Publishing,
2016.
[13] T. D. Han and T. S. Abdelrahman, “Reducing branch divergence in gpu
programs,” in Proceedings of the Fourth Workshop on General Purpose
Processing on Graphics Processing Units, GPGPU-4, (New York, NY,
USA), pp. 3:1–3:8, ACM, 2011.
[14] D. Unat, C. Chan, W. Zhang, S. Williams, J. Bachan, J. Bell, and
J. Shalf, “Exasat: An exascale co-design tool for performance modeling,”
The International Journal of High Performance Computing Applications,
vol. 29, no. 2, pp. 209–232, 2015.
[15] K. Datta, M. Murphy, V. Volkov, S. Williams, J. Carter, L. Oliker,
D. Patterson, J. Shalf, and K. Yelick, “Stencil computation optimization
and auto-tuning on state-of-the-art multicore architectures,” in Proceedings of the 2008 ACM/IEEE Conference on Supercomputing, SC ’08,
(Piscataway, NJ, USA), pp. 4:1–4:12, IEEE Press, 2008.
[16] M. Harris, “How to overlap data transfers in cuda c/c++.” https://
devblogs.nvidia.com/parallelforall/how-overlap-data-transfers-cuda-cc/,
December 2012.
[17] D. Unat, X. Cai, and S. B. Baden, “Mint: Realizing cuda performance
in 3d stencil methods with annotated c,” in Proceedings of the International Conference on Supercomputing, ICS ’11, (New York, NY, USA),
pp. 214–224, ACM, 2011.
[18] S. Lee and R. Eigenmann, “Openmpc: Extended openmp programming and tuning for gpus,” in Proceedings of the 2010 ACM/IEEE
International Conference for High Performance Computing, Networking,
Storage and Analysis, SC ’10, (Washington, DC, USA), pp. 1–11, IEEE
Computer Society, 2010.
[19] M. Khan, P. Basu, G. Rudy, M. Hall, C. Chen, and J. Chame, “A scriptbased autotuning compiler system to generate high-performance cuda
code,” ACM Trans. Archit. Code Optim., vol. 9, pp. 31:1–31:25, Jan.
2013.
[20] J. K. Rich Hornung, “The RAJA Portability Layer: Overview and
Status,” in Lawrence Livermore National Laboratory, Technical Report
LLNL-TR-661403, ACM, 2014.
[21] R. Keryell, R. Reyes, and L. Howes, “Khronos sycl for opencl: a
tutorial,” in Proceedings of the 3rd International Workshop on OpenCL,
p. 24, ACM, 2015.
[22] M. Nakao, H. Murai, T. Shimosaka, A. Tabuchi, T. Hanawa, Y. Kodama,
T. Bokut, and M. Sato, “Xcalableacc: Extension of xcalablemp pgas
language using openacc for accelerator clusters,” in Proceedings of
the First Workshop on Accelerator Programming Using Directives,
WACCPD ’14, (Piscataway, NJ, USA), pp. 27–36, IEEE Press, 2014.

[23] H. C. Edwards and C. R. Trott, “Kokkos: Enabling performance portability across manycore architectures,” in 2013 Extreme Scaling Workshop
(xsw 2013), pp. 18–24, Aug 2013.
[24] M. Bianco and B. Cumming, “A generic strategy for multi-stage
stencils,” in Euro-Par 2014 Parallel Processing (F. Silva, I. Dutra, and
V. Santos Costa, eds.), vol. 8632 of Lecture Notes in Computer Science,
pp. 584–595, Springer International Publishing, 2014.
[25] N. Bell and J. Hoberock, “Thrust: A productivity-oriented library for
cuda,” GPU computing gems Jade edition, vol. 2, pp. 359–371, 2011.
[26] A. C. Jacob, S. F. Antao, H. Sung, A. E. Eichenberger, C. Bertolli, G.-T.
Bercea, T. Chen, Z. Sura, G. Rokos, and K. OBrien, “Towards performance portable gpu programming with raja,” Workshop on Portability
Among HPC Architectures for Scientific Applications, 2015.
[27] K. Gregory and A. Miller, “C++ amp: accelerated massive parallelism
with microsoft visual c++,” 2014.
[28] M. Haidl, B. Hagedorn, and S. Gorlatch, “Programming gpus with
C++14 and just-in-time compilation,” in Parallel Computing: On the
Road to Exascale, Proceedings of the International Conference on
Parallel Computing, ParCo 2015, 1-4 September 2015, Edinburgh,
Scotland, UK, pp. 247–256, 2015.
[29] M. E. Belviranli, F. Khorasani, L. N. Bhuyan, and R. Gupta, “Cumas:
Data transfer aware multi-application scheduling for shared gpus,” in
Proceedings of the 2016 International Conference on Supercomputing,
ICS 2016, Istanbul, Turkey, June 1-3, 2016, pp. 31:1–31:12, 2016.
[30] T. Gysi, J. Bär, and T. Hoefler, “dcuda: hardware supported overlap of
computation and communication,” in Proceedings of the International
Conference for High Performance Computing, Networking, Storage and
Analysis, SC 2016, Salt Lake City, UT, USA, November 13-18, 2016,
pp. 52:1–52:12, 2016.
[31] M. Wahib, N. Maruyama, and T. Aoki, “Daino: A high-level framework for parallel and efficient amr on gpus,” in Proceedings of the
International Conference for High Performance Computing, Networking,
Storage and Analysis, SC ’16, (Piscataway, NJ, USA), pp. 53:1–53:12,
IEEE Press, 2016.
[32] G. Bikshandi, J. Guo, D. Hoeflinger, G. Almasi, B. B. Fraguela, M. J.
Garzarán, D. Padua, and C. von Praun, “Programming for parallelism
and locality with hierarchically tiled arrays,” in Proceedings of the
eleventh ACM SIGPLAN symposium on Principles and practice of
parallel programming, PPoPP ’06, (New York, NY, USA), pp. 48–57,
ACM, 2006.
[33] W. Zhang, A. Almgren, M. Day, T. Nguyen, J. Shalf, and D. Unat,
“Boxlib with tiling: An adaptive mesh refinement software framework,”
SIAM Journal on Scientific Computing, vol. 38, no. 5, pp. S156–S172,
2016.
[34] T. Grosser, A. Cohen, P. H. J. Kelly, J. Ramanujam, P. Sadayappan,
and S. Verdoolaege, “Split tiling for gpus: Automatic parallelization
using trapezoidal tiles,” in Proceedings of the 6th Workshop on General
Purpose Processor Using Graphics Processing Units, GPGPU-6, (New
York, NY, USA), pp. 24–31, ACM, 2013.