[1] Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan
Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, and
others. 2015. Deep speech 2: End-to-end speech recognition in english and
mandarin. arXiv preprint arXiv:1512.02595 (2015).
[2] Jianmin Chen, Rajat Monga, Samy Bengio, and Rafal Jozefowicz. 2016. Revisiting
distributed synchronous SGD. arXiv preprint arXiv:1604.00981 (2016).
[3] Adam Coates, Brody Huval, Tao Wang, David Wu, Bryan Catanzaro, and Andrew
Ng. 2013. Deep learning with COTS HPC systems. In International Conference
on Machine Learning. 1337–1345.
[4] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. 2014. Training deep neural networks with low precision multiplications. arXiv preprint
arXiv:1412.7024 (2014).
[5] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
Andrew Senior, Paul Tucker, Ke Yang, Quoc V Le, and others. 2012. Large scale
distributed deep networks. In Advances in neural information processing systems.
1223–1231.
[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In IEEE Conference on Computer
Vision and Pattern Recognition. IEEE, 248–255.
[7] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep learning. MIT
Press.
[8] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.
2015. Deep learning with limited numerical precision. In Proceedings of the 32nd
International Conference on Machine Learning (ICML-15). 1737–1746.
[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. 770–778.
[10] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua
Bengio. 2016. Quantized neural networks: Training neural networks with low
precision weights and activations. arXiv preprint arXiv:1609.07061 (2016).
[11] Forrest N Iandola, Matthew W Moskewicz, Khalid Ashraf, and Kurt Keutzer.
2016. FireCaffe: near-linear acceleration of deep neural network training on
compute clusters. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. 2592–2600.
[12] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
and Ping Tak Peter Tang. 2016. On large-batch training for deep learning:
Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836 (2016).
[13] Alex Krizhevsky. 2009. Learning multiple layers of features from tiny images.
Masterfis thesis, Department of Computer Science, University of Toronto (2009).
[14] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classification with deep convolutional neural networks. In Advances in neural information
processing systems. 1097–1105.
[15] Quoc V Le. 2013. Building high-level features using large scale unsupervised
learning. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 8595–8598.
[16] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradientbased learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278–
2324.
[17] Chao Li, Yi Yang, Min Feng, Srimat Chakradhar, and Huiyang Zhou. 2016. Optimizing memory efficiency for deep convolutional neural networks on GPUs.
In Proceedings of the International Conference for High Performance Computing,
Networking, Storage and Analysis. IEEE Press, 54.
[18] Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed,
Vanja Josifovski, James Long, Eugene J Shekita, and Bor-Yiing Su. 2014. Scaling
Distributed Machine Learning with the Parameter Server.. In OSDI, Vol. 14.
583–598.
[19] Mu Li, Tong Zhang, Yuqiang Chen, and Alexander J Smola. 2014. Efficient
mini-batch training for stochastic optimization. In Proceedings of the 20th ACM
SIGKDD international conference on Knowledge discovery and data mining. ACM,
661–670.
[20] Maurice Peemen, Bart Mesman, and Henk Corporaal. 2011. Efficiency optimization of trainable feature extractors for a consumer platform. In International
Conference on Advanced Concepts for Intelligent Vision Systems. Springer, 293–304.
[21] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. 2011. Hogwild:
A lock-free approach to parallelizing stochastic gradient descent. In Advances in
Neural Information Processing Systems. 693–701.
[22] Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 2014. 1-bit stochastic
gradient descent and its application to data-parallel distributed training of speech
DNNs.. In Interspeech. 1058–1062.
[23] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).
[24] Ilya Sutskever, James Martens, George E Dahl, and Geoffrey E Hinton. 2013. On
the importance of initialization and momentum in deep learning. (2013).
[25] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015.
Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 1–9.
[26] Yang You, James Demmel, Kenneth Czechowski, Le Song, and Richard Vuduc.
2015. CA-SVM: Communication-avoiding support vector machines on distributed
systems. In Parallel and Distributed Processing Symposium (IPDPS), 2015 IEEE
International. IEEE, 847–859.
[27] Yang You, Xiangru Lian, Ji Liu, Hsiang-Fu Yu, Inderjit S Dhillon, James Demmel,
and Cho-Jui Hsieh. 2016. Asynchronous parallel greedy coordinate descent. In
Advances in Neural Information Processing Systems. 4682–4690.
[28] Sixin Zhang, Anna E Choromanska, and Yann LeCun. 2015. Deep learning with
elastic averaging SGD. In Advances in Neural Information Processing Systems.
685–693.
