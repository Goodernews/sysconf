[1] P. Micikevicius, “3D Finite Difference Computation on GPUs
Using CUDA,” in Proceedings of 2Nd Workshop on Gen-
eral Purpose Processing on Graphics Processing Units, ser.
GPGPU-2. New York, NY, USA: ACM, 2009, pp. 79–84.
[2] W. F. Ames, Numerical Methods for Partial Differential

Equations. Academic press, 2014.

[3] T. Shimokawabe, T. Aoki, T. Takaki, T. Endo, A. Ya-
manaka, N. Maruyama, A. Nukada, and S. Matsuoka, “Peta-
scale phase-ﬁeld simulation for dendritic solidiﬁcation on
the TSUBAME 2.0 supercomputer,” in Proc. of 2011 In-
ternational Conference for High Performance Computing,
Networking, Storage and Analysis, ser. SC ’11. New York,
NY, USA: ACM, 2011, pp. 3:1–3:11.

[4] D. Rossinelli, B. Hejazialhosseini, P. Hadjidoukas, C. Bekas,
A. Curioni, A. Bertsch, S. Futral, S. J. Schmidt, N. A.
Adams, and P. Koumoutsakos, “11 PFLOP/s simulations of
cloud cavitation collapse,” in Proc. of 2013 International
Conference for High Performance Computing, Networking,
Storage and Analysis, ser. SC ’13.
IEEE, 2013, pp. 1–13.
[5] M. F. Adams, J. Brown, J. Shalf, B. V. Straalen, E. Strohmaier,
and S. Williams, “HPGMG 1.0: A Benchmark for Ranking
High Performance Computing Systems,” LBNL, LBNL Tech-
nical Report LBNL 6630E, 2014.

[6] W. K. Anderson, W. D. Gropp, D. K. Kaushik, D. E. Keyes,
and B. F. Smith, “Achieving high sustained performance
in an unstructured mesh CFD application,” in Proc. of the
1999 ACM/IEEE conference on Supercomputing, ser. SC ’99.
ACM, 1999, p. 69.

[7] D. Knoll and D. E. Keyes, “Jacobian-free Newton-Krylov
methods: A survey of approaches and applications,” J. Com-
put. Phys., vol. 193, pp. 357–397, 2004.

[8] K. Asanovic, R. Bodik, J. Demmel, T. Keaveny, K. Keutzer,
J. Kubiatowicz, N. Morgan, D. Patterson, K. Sen,
J. Wawrzynek, D. Wessel, and K. Yelick, “A View of the
Parallel Computing Landscape,” Commun. ACM, vol. 52, pp.
56–67, Oct. 2009.

[9] V. W. Lee, C. Kim, J. Chhugani, M. Deisher, D. Kim,
A. D. Nguyen, N. Satish, M. Smelyanskiy, S. Chennupaty,
P. Hammarlund, R. Singhal, and P. Dubey, “Debunking the
100x GPU vs. CPU Myth: An Evaluation of Throughput
Computing on CPU and GPU,” in Proc. of the 37th Annual In-
ternational Symposium on Computer Architecture, ser. ISCA
’10. New York, NY, USA: ACM, 2010, pp. 451–460.

[10] H. Fu, J. Liao, J. Yang, L. Wang, Z. Song, X. Huang, C. Yang,
W. Xue, F. Liu, F. Qiao, W. Zhao, X. Yin, C. Hou, C. Zhang,
W. Ge, J. Zhang, Y. Wang, C. Zhou, and G. Yang, “The
Sunway TaihuLight supercomputer: System and applications,”
Sci. China Inf. Sci., pp. 1–16, Jun. 2016.

[11] P.

Johnsen, M. Straka, M. Shapiro, A. Norton, and
T. Galarneau, “Petascale wrf simulation of hurricane sandy
deployment of ncsa’s cray xe6 blue waters,” in Proc. of the
International Conference on High Performance Computing,

Networking, Storage and Analysis, ser. SC ’13. New York,
NY, USA: ACM, 2013, pp. 63:1–63:7.

[12] H. Yashiro, M. Terai, R. Yoshida, S.-i. Iga, K. Minami,
and H. Tomita, “Performance Analysis and Optimization of
Nonhydrostatic ICosahedral Atmospheric Model (NICAM)
on the K Computer and TSUBAME2.5,” in Proc. of the
Platform for Advanced Scientiﬁc Computing Conference, ser.
PASC ’16. New York, NY, USA: ACM, 2016, pp. 3:1–3:8.
[13] T. Shimokawabe, T. Aoki, C. Muroi, J. Ishida, K. Kawano,
T. Endo, A. Nukada, N. Maruyama, and S. Matsuoka, “An
80-Fold Speedup, 15.0 TFlops Full GPU Acceleration of
Non-Hydrostatic Weather Model ASUCA Production Code,”
in Proc. of the 2010 ACM/IEEE International Conference
for High Performance Computing, Networking, Storage and
Analysis, ser. SC ’10. Washington, DC, USA: IEEE Com-
puter Society, 2010, pp. 1–11.

[14] I. Carpenter, R. Archibald, K. Evans, J. Larkin, P. Micikevi-
cius, M. Norman, J. Rosinski, J. Schwarzmeier, and M. Tay-
lor, “Progress Towards Accelerating HOMME on Hybrid
Multi-core Systems,” Int. J. High Perform. Comput. Appl.,
vol. 27, pp. 335–347, Aug. 2013.

[15] C. Yang, W. Xue, H. Fu, L. Gan, L. Li, Y. Xu, Y. Lu,
J. Sun, G. Yang, and W. Zheng, “A Peta-scalable CPU-GPU
algorithm for global atmospheric simulations,” in Proc. of the
18th ACM SIGPLAN Symposium on Principles and Practice
of Parallel Programming, ser. PPoPP ’13. New York, NY,
USA: ACM, 2013, pp. 1–12.

[16] W. Xue, C. Yang, H. Fu, X. Wang, Y. Xu, J. Liao, L. Gan,
Y. Lu, R. Ranjan, and L. Wang, “Ultra-scalable CPU-MIC
acceleration of mesoscale atmospheric modeling on Tianhe-
2,” IEEE Transactions on Computers, vol. 64, pp. 2382–2393,
Aug. 2015.

[17] C. Yang, W. Xue, H. Fu, H. You, X. Wang, Y. Ao, F. Liu,
L. Gan, P. Xu, L. Wang, G. Yang, and W. Zheng, “10M-
core Scalable Fully-implicit Solver for Nonhydrostatic At-
mospheric Dynamics,” in Proceedings of the International
Conference for High Performance Computing, Networking,
Storage and Analysis, ser. SC ’16.
Piscataway, NJ, USA:
IEEE Press, 2016, pp. 6:1–6:12.

[18] P. H. Lauritzen, C. Jablonowski, M. A. Taylor, and R. D.
Nair, Numerical techniques for global atmospheric models.
Springer Science & Business Media, 2011, vol. 80.

[19] S. Gottlieb, C.-W. Shu, and E. Tadmore, “Strong stability pre-
serving high-order time integration methods,” SIAM Review,
vol. 43, pp. 89–112, 2001.

[20] S. Balay, S. Abhyankar, M. F. Adams,

J. Brown,
P. Brune, K. Buschelman, L. Dalcin, V. Eijkhout, W. D.
Gropp, D. Kaushik, M. G. Knepley, L. C. McInnes,
K. Rupp, B. F. Smith, S. Zampini, H. Zhang, and
H. Zhang, “PETSc Web page,” 2016. [Online]. Available:
http://www.mcs.anl.gov/petsc

[21] P.

C.

and

Ullrich

“Operator-Split
Runge–Kutta–Rosenbrock Methods
for Nonhydrostatic
Atmospheric Models,” Mon. Wea. Rev., vol. 140, pp.
1257–1284, Sep. 2011.

Jablonowski,

[22] S. Williams, A. Waterman, and D. Patterson, “Rooﬂine: An
Insightful Visual Performance Model for Multicore Architec-
tures,” Commun. ACM, vol. 52, pp. 65–76, Apr. 2009.

[23] J. Michalakes and M. Vachharajani, “Gpu acceleration of
numerical weather prediction,” Parallel Process. Lett., vol. 18,
pp. 531–548, Dec. 2008.

[24] M. W. Govett, J. Middlecoff, and T. Henderson, “Running
the NIM Next-Generation Weather Model on GPUs,” in 2010
10th IEEE/ACM International Conference on Cluster, Cloud

and Grid Computing (CCGrid), May 2010, pp. 792–796.

[25] T. Shimokawabe, T. Aoki, J.

Ishida, K. Kawano, and
C. Muroi, “145 TFlops Performance on 3990 GPUs of
TSUBAME 2.0 Supercomputer for an Operational Weather
Prediction,” Procedia Computer Science, vol. 4, pp. 1535–
1544, Jan. 2011.

[26] G. Rivera and C.-W. Tseng, “Tiling optimizations for 3D
scientiﬁc computations,” in Proc. of the 2000 ACM/IEEE
Conference on Supercomputing, ser. SC ’00. Washington,
DC, USA: IEEE Computer Society, 2000.

[27] D. Wonnacott, “Using time skewing to eliminate idle time
due to memory bandwidth and network limitations,” in Proc.
of the 14th International Parallel and Distributed Processing
Symposium, ser. IPDPS ’00, 2000, pp. 171–180.

[28] K. Datta, S. Kamil, S. Williams, L. Oliker, J. Shalf, and
K. Yelick, “Optimization and Performance Modeling of Sten-
cil Computations on Modern Microprocessors,” SIAM Rev.,
vol. 51, pp. 129–159, Feb. 2009.

[29] M. Frigo and V. Strumpen, “Cache Oblivious Stencil Compu-
tations,” in Proc. of the 19th Annual International Conference
on Supercomputing, ser. ICS ’05. New York, NY, USA:
ACM, 2005, pp. 361–366.

[30] S. Kamil, K. Datta, S. Williams, L. Oliker, J. Shalf, and
K. Yelick, “Implicit and Explicit Optimizations for Stencil
Computations,” in Proc. of the 2006 Workshop on Memory
System Performance and Correctness, ser. MSPC ’06. New
York, NY, USA: ACM, 2006, pp. 51–60.

[31] G. Wellein, G. Hager, T. Zeiser, M. Wittmann, and H. Fehske,
“Efﬁcient Temporal Blocking for Stencil Computations by
Multicore-Aware Wavefront Parallelization,” in Proc. Inter-
national Computer Software and Applications Conference,
vol. 1, Jul. 2009, pp. 579–586.

[32] J. Treibig, G. Wellein, and G. Hager, “Efﬁcient multicore-
aware parallelization strategies for iterative stencil computa-
tions,” Journal of Computational Science, vol. 2, pp. 130–137,
May 2011.

[33] A. Nguyen, N. Satish, J. Chhugani, C. Kim, and P. Dubey,
“3.5-D Blocking Optimization for Stencil Computations on
Modern CPUs and GPUs,” in Proc. of 2010 International
Conference for High Performance Computing, Networking,
Storage and Analysis, ser. SC ’11, Nov. 2010, pp. 1–13.

[34] L. Szustak, K. Rojek, T. Olas, L. Kuczynski, K. Halbiniak,
and P. Gepner, “Adaptation of MPDATA Heterogeneous Sten-
cil Computation to Intel Xeon Phi Coprocessor,” Scientiﬁc
Programming, vol. 2015, p. e642705, May 2015.

[35] S. Williams, J. Shalf, L. Oliker, S. Kamil, P. Husbands, and
K. Yelick, “The Potential of the Cell Processor for Scientiﬁc
Computing,” in Proc. of the 3rd Conference on Computing
Frontiers, ser. CF ’06. New York, NY, USA: ACM, 2006,
pp. 9–20.

[36] K. Datta, M. Murphy, V. Volkov, S. Williams, J. Carter,
L. Oliker, D. Patterson, J. Shalf, and K. Yelick, “Stencil
Computation Optimization and Auto-tuning on State-of-the-
art Multicore Architectures,” in Proc. of the 2008 ACM/IEEE
Conference on Supercomputing, ser. SC ’08. Piscataway, NJ,
USA: IEEE Press, 2008, pp. 4:1–4:12.

[37] T. Henretty, K. Stock, L.-N. Pouchet, F. Franchetti, J. Ra-
manujam, and P. Sadayappan, “Data Layout Transformation
for Stencil Computations on Short-vector SIMD Architec-
tures,” in Proc. of
the 20th International Conference on
Compiler Construction, ser. CC’11/ETAPS’11. Berlin, Hei-
delberg: Springer-Verlag, 2011, pp. 225–245.

