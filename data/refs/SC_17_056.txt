[1] Agarwal, R. C., Balle, S. M., Gustavson, F. G., Joshi, M., and
Palkar, P. A three-dimensional approach to parallel matrix
multiplication.
[2] Bergstra, J., Pinto, N., and Cox, D. Machine learning for predictive auto-tuning with boosted regression trees. In Innovative
Parallel Computing (InPar), 2012 (May 2012).
[3] Ding, Y., Ansel, J., Veeramachaneni, K., Shen, X., O’Reilly, U.M., and Amarasinghe, S. Autotuning algorithmic choice for input
sensitivity. In Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation
(2015), PLDI 2015.
[4] Frigo, M., and Johnson, S. The design and implementation of
↵tw3. Proceedings of the IEEE (2005).
[5] Fursin, G. G., O’Boyle, M. F. P., and Knijnenburg, P. M. W.
Evaluating iterative compilation. In Proceedings of the 15th International Conference on Languages and Compilers for Parallel
Computing, LCPC’02.
[6] Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and
Bengio, Y. Binarized neural networks. In NIPS (2016), pp. 4107–
4115.
[7] Keskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and
Tang, P. T. P. On large-batch training for deep learning: Generalization gap and sharp minima.
[8] Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. In Advances
in Neural Information Processing Systems 25.
[9] Lahabar, S., and Narayanan, P. Singular value decomposition
on gpu using cuda. In Parallel Distributed Processing, 2009.
IPDPS 2009. IEEE International Symposium on (2009).
[10] Liu, Y., Zhang, E., and Shen, X. A cross-input adaptive framework
for gpu program optimizations. In Parallel Distributed Processing,
2009. IPDPS 2009. IEEE International Symposium on (2009).
[11] Magni, A., Grewe, D., and Johnson, N. Input-aware auto-tuning
for directive-based gpu programming. In Proceedings of the
6th Workshop on General Purpose Processor Using Graphics
Processing Units (2013), GPGPU-6.
[12] Muralidharan, S., Shantharam, M., Hall, M., Garland, M., and
Catanzaro, B. Nitro: A framework for adaptive code variant
tuning. In Proceedings of the 2014 IEEE 28th International
Parallel and Distributed Processing Symposium, IPDPS ’14.
[13] Pschel, M., Moura, J. M. F., Singer, B., Xiong, J., Johnson, J.,
Padua, D., Veloso, M., Johnson, R. W., Pschel, M., Moura, J.
M. F., Singer, B., Xiong, J., Johnson, J., Padua, D., Veloso, M.,
and Johnson, R. W. Spiral: A generator for platform-adapted
libraries of signal processing algorithms. Journal of High Performance Computing and Applications 18 (2004).
[14] Samadi, M., Hormati, A., Mehrara, M., Lee, J., and Mahlke,
S. Adaptive input-aware compilation for graphics engines. In
Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation (2012), PLDI
’12.
[15] Tillet, P., Rupp, K., Selberherr, S., and Lin, C. T. Towards
performance-portable, scalable, and convenient linear algebra.
In 4th USENIX Workshop on Hot Topics in Parallelism, HotPar’12, Berkeley, CA, USA, June 7-8, 2012.
[16] Volkov, V. Understanding latency hiding on gpus., 2016.
[17] Vuduc, R., Demmel, J. W., and Yelick, K. A. OSKI: A library of
automatically tuned sparse matrix kernels. Journal of Physics:
Conference Series 16, 1 (2005).
[18] Wang, Q., Zhang, X., Zhang, Y., and Yi, Q. Augem: Automatically
generate high performance dense linear algebra kernels on x86
cpus. In Proceedings of the International Conference on High
Performance Computing, Networking, Storage and Analysis
(2013), SC ’13.
[19] Whaley, R. C., and Dongarra, J. J. Automatically tuned linear algebra software. In Proceedings of the 1998 ACM/IEEE
Conference on Supercomputing (1998), SC ’98.
[20] Xiong, J., Johnson, J., Johnson, R., and Padua, D. Spl: A language
and compiler for dsp algorithms. In Proceedings of the ACM
SIGPLAN 2001 Conference on Programming Language Design
and Implementation (2001), PLDI ’01.
[21] Yotov, K., Li, X., Ren, G., Cibulskis, M., DeJong, G., Garzaran,
M., Padua, D., Pingali, K., Stodghill, P., and Wu, P. A comparison of empirical and model-driven optimization. SIGPLAN
Not..
