[1] Cuda c programming guide. Technical Report PG-02829-001_v8.0,
NVIDIA, June 2017.

[2] M. Abadi, P. Barham, and other. TensorFlow: A System for Large-Scale
Machine Learning. In 12th USENIX Symposium on Operating Systems
Design and Implementation, pages 265-283, 2016.

[3] D. Abrahams and A. Gurtovoy. C++ Template Metaprogramming:
Concepts, Tools, and Techniques from Boost and Beyond. Pearson
Education, 2004.

[4] M. Aldinucci, S. Campa, M. Danelutto, P. Kilpatrick, and M. Torquati.
Design patterns percolating to parallel programming framework implementation. International Journal of Parallel Programming, 42(6):1012—
1031, 2014.

[5] K. Asanovic, R. Bodik, et al. A view of the parallel computing
landscape. Communications of ACM, 52:56-67, October 2009.

[6] N. Bell and J. Hoberock. GPU Computing Gems Jade Edition, chapter
Thrust: A Productivity-Oriented Library for CUDA, pages 359-371.
Morgan Kaufmann Publishers Inc., 1st edition, 2011.

[7] S. Che, M. Boyer, J. Meng, D. Tarjan, J. W. Sheaffer, S.-H. Lee, and
K. Skadron. Rodinia: A benchmark suite for heterogeneous computing.
In Proceedings of the 2009 IEEE International Symposium on Workload
Characterization (SWC), pages 44-54, IEEE Computer Society, 2009.

[8] M. Cole. Bringing skeletons out of the closet: a pragmatic manifesto
for skeletal parallel programming. Parallel computing, 30(3):389-406,
2004.

[9] R. Collobert, S. Bengio, and J. Mariéthoz. Torch: a modular machine
learning software library. Technical report, Idiap, 2002.

[10] A. V. George, S. Manoj, S. R. Gupte, and S. Sarkar. An Empirical Evaluation of Design Abstraction and Performance of Thrust Framework.
In 46th International Conference on Parallel Processing Workshops
(ICPPW). TEEE Computer Society, 2017.

[11] H. Gonzdlez-Vélez and M. Leyton. A survey of algorithmic skeleton frameworks: High-level structured parallel programming enablers.
Softw. Pract. Exper., 40(12):1135-1160, 2010.

[12] B. Kuhn, P. Petersen, and E. O’Toole. Openmp versus threading in
e/e++. Concurrency: Pract. Exper, 12:1165-1176, 2000.

[13] E. Rubin, E. Levy, A. Barak, and T. Ben-Nun. Maps: Optimizing
massively parallel applications using device-level memory abstraction.
ACM Trans. Archit. Code Optim., 11(4):44:1-44:22, Dec. 2014.

[14] S. Ryoo, C. I. Rodriguesy, et al. Optimization Principles and Application Performance Evaluation of a Multithreaded GPU Using CUDA.
In ACM SIGPLAN Symp. on Principles and Practice of Parallel
Programming (PPoPP). ACM, 2008.

[15] S. Sarkar, S. Mitra, and A. Srinivasan. Reuse and refactoring of gpu
kernels to design complex applications. In Intl. Symp. on Parallel and
Distributed Processing with Applications, pages 134-141, 2012.

[16] D. Schmidl, T. Cramer, S. Wienke, C. Terboven, and M. S. Miiller.
Assessing the Performance of OpenMP Programs on the Intel Xeon
Phi, pages 547-558. Springer Berlin Heidelberg, 2013.

[17] Y.-P. You, H.-J. Wu, Y.-N. Tsai, and Y.-T. Chao. Virtcl: A framework
for opencl device abstraction and management. In Proceedings of the
20th ACM SIGPLAN Symposium on Principles and Practice of Parallel
Programming (PPoPP 2015), pages 161-172. ACM, 2015.