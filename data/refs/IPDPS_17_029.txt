[1] C. Olschanowsky, M. M. Strout, S. Guzik, J. Loffeld, and J. Hittinger,
“A study on balancing parallelism, data locality, and recomputation in
existing PDE solvers,” in Int. Conf. for High Performance Computing,
Networking, Storage and Analysis (SC), 2014, pp. 793–804.
[2] J. Ragan-Kelley, C. Barnes, A. Adams, S. Paris, F. Durand, and
S. Amarasinghe, “Halide: A language and compiler for optimizing
parallelism, locality, and recomputation in image processing pipelines,”
in ACM SIGPLAN Conference on Programming Language Design and
Implementation (PLDI), 2013, pp. 519–530.
[3] S. Kronawitter and C. Lengauer, “Optimization of two Jacobi smoother
kernels by domain-specific program transformation,” in Int. Workshop
on High-Performance Stencil Computations, 2014, pp. 75–80.
[4] I. J. Bertolacci, C. Olschanowsky, B. Harshbarger, B. L. Chamberlain,
D. G. Wonnacott, and M. M. Strout, “Parameterized diamond tiling
for stencil computations with chapel parallel iterators,” in ACM Int.
Conference on Supercomputing (ICS), 2015, pp. 197–206.

[5] T. Malas, J. Hornich, G. Hager, H. Ltaief, C. Pflaum, and D. Keyes,
“Optimization of an electromagnetics code with multicore wavefront
diamond blocking and multi-dimensional intra-tile parallelization,” in
IEEE International Parallel and Distributed Processing Symposium
(IPDPS), 2016, pp. 142–151.
[6] R. Strzodka, M. Shaheen, D. Paja, and H.-P. Seidel, “Cache accurate time
skewing in iterative stencil computations,” in International Conference
on Parallel Processing (ICPP), 2011, pp. 571–581.
[7] T. Henretty, K. Stock, L.-N. Pouchet, F. Franchetti, J. Ramanujam, and
P. Sadayappan, “Data layout transformation for stencil computations
on short-vector SIMD architectures,” in International Conference on
Compiler Construction (CC), 2011, pp. 225–245.
[8] S. Kamil, C. Chan, L. Oliker, J. Shalf, and S. Williams, “An auto-tuning
framework for parallel multicore stencil computations,” in IEEE Int.
Symp. on Parallel & Distributed Processing (IPDPS), 2010, pp. 1–12.
[9] Y. Luo, G. Tan, Z. Mo, and N. Sun, “Fast: A fast stencil autotuning
framework based on an optimal-solution space model,” in ACM Int.
Conference on Supercomputing (ICS), 2015, pp. 187–196.
[10] J. Ansel, S. Kamil, K. Veeramachaneni, J. Ragan-Kelley, J. Bosboom,
U.-M. O’Reilly, and S. Amarasinghe, “Opentuner: An extensible framework for program autotuning,” in Int. Conference on Parallel Architectures and Compilation Techniques (PACT), 2014.
[11] H. Leather, E. Bonilla, and M. O’Boyle, “Automatic feature generation
for machine learning based optimizing compilation,” in Int. Symp. on
Code Generation and Optimization (CGO), 2009, pp. 81–91.
[12] Y. Tang, R. A. Chowdhury, B. C. Kuszmaul, C.-K. Luk, and C. E.
Leiserson, “The Pochoir stencil compiler,” in Proc. of ACM Symposium
on Parallelism in Algorithms and Architectures, 2011, pp. 117–128.
[13] V. Bandishti, I. Pananilath, and U. Bondhugula, “Tiling stencil
computations to maximize parallelism,” in Int. Conference on High
Performance Computing, Networking, Storage and Analysis (SC), 2012,
pp. 40:1–40:11.
[14] J. Shirako, K. Sharma, N. Fauzia, L.-N. Pouchet, J. Ramanujam,
P. Sadayappan, and V. Sarkar, “Analytical bounds for optimal tile size
selection,” in International Conference on Compiler Construction (CC),
2012, pp. 101–121.
[15] X. Zhou, J.-P. Giacalone, M. J. Garzarán, R. H. Kuhn, Y. Ni, and
D. Padua, “Hierarchical overlapped tiling,” in Int. Symp. on Code
Generation and Optimization (CGO). ACM, 2012, pp. 207–218.
[16] M. Wahib and N. Maruyama, “Scalable kernel fusion for memory-bound
GPU applications,” in International Conference for High Performance
Computing, Networking, Storage and Analysis, (SC), 2014, pp. 191–202.
[17] H. Stengel, J. Treibig, G. Hager, and G. Wellein, “Quantifying performance bottlenecks of stencil computations using the execution-cachememory model,” in ACM International Conference on Supercomputing
(ICS), 2015, pp. 207–216.
[18] M. G. Lagoudakis and M. L. Littman, “Algorithm selection using
reinforcement learning,” in Proc. of the Int. Conference on Machine
Learning (ICML), 2000, pp. 511–518.
[19] C. Ţăpuş, I.-H. Chung, and J. K. Hollingsworth, “Active harmony:
Towards automated performance tuning,” in ACM/IEEE Conference on
Supercomputing (SC), 2002, pp. 1–11.
[20] S. Long and M. F. P. O’Boyle, “Adaptive Java optimisation
using instance-based learning,” in International Conference on
Supercomputing, ICS, 2004, pp. 237–246.
[21] R. Vuduc, J. W. Demmel, and K. A. Yelick, “OSKI: A library of
automatically tuned sparse matrix kernels,” in Proc. SciDAC, J. Physics:
Conf. Ser., vol. 16, 2005, pp. 521–530.
[22] N. Thomas, G. Tanase, O. Tkachyshyn, J. Perdue, N. M. Amato, and
L. Rauchwerger, “A framework for adaptive algorithm selection in
stapl,” in ACM SIGPLAN Symposium on Principles and Practice of
Parallel Programming (PPoPP), 2005, pp. 277–288.
[23] F. V. Agakov, E. V. Bonilla, J. Cavazos, B. Franke, G. Fursin, M. F. P.
O’Boyle, J. Thomson, M. Toussaint, and C. K. I. Williams, “Using
machine learning to focus iterative optimization,” in Int. Symposium on
Code Generation and Optimization (CGO), 2006, pp. 295–305.
[24] J. Cavazos and M. F. P. O’Boyle, “Method-specific dynamic
compilation using logistic regression,” in ACM SIGPLAN Conference on
Object-Oriented Programming, Systems, Languages, and Applications
(OOPSLA), 2006, pp. 229–240.
[25] J. Pjesivac-Grbovic, G. Bosilca, G. E. Fagg, T. Angskun, and
J. Dongarra, “Decision trees and MPI collective algorithm selection
problem,” in Euro-Par: International European Conference on Parallel
and Distributed Computing, 2007, pp. 107–117.

[26] J. Ansel, C. Chan, Y. L. Wong, M. Olszewski, Q. Zhao, A. Edelman, and
S. Amarasinghe, “Petabricks: A language and compiler for algorithmic
choice,” in ACM SIGPLAN Conf. on Programming Language Design
and Implementation (PLDI), 2009, pp. 38–49.
[27] D. Grewe and M. F. P. O’Boyle, “A static task partitioning approach for
heterogeneous systems using opencl,” in Proc. of the 20th International
Conference on Compiler Construction (CC), 2011, pp. 286–305.
[28] K. Kofler, I. Grasso, B. Cosenza, and T. Fahringer, “An automatic
input-sensitive approach for heterogeneous task partitioning,” in ACM
International Conference on Supercomputing (ICS), 2013, pp. 149–160.
[29] H. Jordan, S. Pellegrini, P. Thoman, K. Kofler, and T. Fahringer,
“INSPIRE: The Insieme parallel intermediate representation,” in Int.
Conference on Parallel Architectures and Compilation Techniques
(PACT), 2013, pp. 7–18.
[30] Z. Wang, G. Tournavitis, B. Franke, and M. F. P. O’Boyle,
“Integrating profile-driven parallelism detection and machine-learningbased mapping,” TACO, vol. 11, no. 1, p. 2, 2014.
[31] K. Kofler, B. Cosenza, and T. Fahringer, “Automatic data layout
optimizations for GPUs,” in Euro-Par: International Conference on
Parallel and Distributed Computing, 2015, pp. 263–274.
[32] K. Datta, M. Murphy, V. Volkov, S. Williams, J. Carter, L. Oliker,
D. Patterson, J. Shalf, and K. Yelick, “Stencil computation optimization
and auto-tuning on state-of-the-art multicore architectures,” in
ACM/IEEE Conf. on Supercomputing (SC), 2008, pp. 4:1–4:12.
[33] Y. Zhang and F. Mueller, “Autogeneration and autotuning of 3d stencil
codes on homogeneous and heterogeneous GPU clusters,” IEEE Trans.
Parallel Distributed System, vol. 24, no. 3, pp. 417–427, Mar. 2013.
[34] T. Gysi, T. Grosser, and T. Hoefler, “Modesto: Data-centric
analytic optimization of complex stencil programs on heterogeneous
architectures,” in ACM International Conference on Supercomputing
(ICS), 2015, pp. 177–186.
[35] M. Christen, O. Schenk, and H. Burkhart, “Patus: A code generation
and autotuning framework for parallel iterative stencil computations
on modern microarchitectures,” in IEEE International Parallel &
Distributed Processing Symposium (IPDPS), 2011, pp. 676–687.
[36] C. Yount, J. Tobin, A. Breuer, and A. Duran, “Yask-yet another stencil
kernel: A framework for hpc stencil code-generation and tuning,” in
Workshop on Domain-Specific Languages and High-Level Frameworks
for HPC (WOLFHPC), 2016, pp. 30–39.
[37] M. Christen, O. Schenk, and Y. Cui, “Patus for convenient
high-performance stencils: evaluation in earthquake simulations,” in
Conference on High Performance Computing Networking, Storage and
Analysis (SC), 2012, p. 11.
[38] D. Guerrera, H. Burkhart, and A. Maffia, “Reproducible experiments in
parallel computing: Concepts and stencil compiler benchmark study,” in
Euro-Par 2014: Parallel Processing Workshops, 2014, pp. 464–474.
[39] P. Auer, N. Cesa-Bianchi, and P. Fischer, “Finite-time analysis of the
multiarmed bandit problem,” Machine Learning, vol. 47, no. 2, pp.
235–256.
[40] S. Muralidharan, M. Shantharam, M. Hall, M. Garland, and B. Catanzaro, “Nitro: A framework for adaptive code variant tuning,” in IEEE
International Parallel and Distributed Processing Symposium (IPDPS),
2014, pp. 501–512.
[41] M. Stephenson and S. P. Amarasinghe, “Predicting unroll factors using
supervised classification,” in IEEE / ACM International Symposium on
Code Generation and Optimization (CGO), 2005, pp. 123–134.
[42] K. Stock, L.-N. Pouchet, and P. Sadayappan, “Using machine learning
to improve automatic vectorization,” ACM Trans. Archit. Code Optim.,
vol. 8, no. 4, pp. 50:1–50:23, Jan. 2012.
[43] T. Joachims, “Training linear SVMs in linear time,” in ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining
(KDD), 2006, pp. 217–226.
[44] ——, “Optimizing search engines using clickthrough data,” in ACM
SIGKDD International Conference on Knowledge Discovery and Data
Mining (KDD), 2002, pp. 133–142.
[45] G. H. Bakir, T. Hofmann, B. Schölkopf, A. J. Smola, B. Taskar, and
S. V. N. Vishwanathan, Predicting Structured Data (Neural Information
Processing). The MIT Press, 2007.
[46] D. H. Wolpert and W. G. Macready, “No free lunch theorems for
optimization,” Trans. Evol. Comp, vol. 1, no. 1, pp. 67–82, Apr. 1997.
[47] M. Gendreau and J.-Y. Potvin, Handbook of Metaheuristics, 2nd ed.
Springer Publishing Company, Incorporated, 2010.
[48] M. Kendall, Rank Correlation Methods, 4th ed. Hodder Arnold, 1976.

