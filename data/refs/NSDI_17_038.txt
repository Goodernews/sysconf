[1] Apache foundation. mahout project. http://mahout.
apache.org.
[2] ParameterServer. https://github.com/dmlc/
parameter server.
[3] Petuum v0.93. https://github.com/petuum/bosen/
tree/release 0.93.
[4] PowerGraph v2.2. https://github.com/dato-code/
PowerGraph.
[5] AHMED, A., ALY, M., GONZALEZ, J., NARAYANAMURTHY,
S., AND SMOLA, A. J. Scalable inference in latent variable models. In Proceedings of the Fifth ACM International Conference on
Web Search and Data Mining (2012), WSDM’12, ACM.
[6] BENNETT, J., AND LANNING, S. The Netflix Prize. In Proceedings of KDD cup and workshop (2007), vol. 2007.
[7] CHEN, R., SHI, J., CHEN, Y., AND CHEN, H. PowerLyra: Differentiated graph computation and partitioning on skewed graphs.
In Proceedings of the Tenth European Conference on Computer
Systems (2015), EuroSys’15, ACM.
[8] CHEN, R., SHI, J., ZANG, B., AND GUAN, H. Bipartiteoriented distributed graph partitioning for big learning. In Proceedings of 5th Asia-Pacific Workshop on Systems (2014), APSys’14, ACM.
[9] CHILIMBI, T., SUZUE, Y., APACIBLE, J., AND KALYANARAMAN, K. Project Adam: Building an efficient and scalable deep
learning training system. In 11th USENIX Symposium on Operating Systems Design and Implementation (2014), OSDI’14,
USENIX.
[10] CHING, A., EDUNOV, S., KABILJO, M., LOGOTHETIS, D.,
AND MUTHUKRISHNAN, S. One trillion edges: Graph processing at Facebook-scale. Proc. VLDB Endow. 8, 12 (Aug. 2015).
[11] CUI, H., CIPAR, J., HO, Q., KIM, J. K., LEE, S., KUMAR,
A., WEI, J., DAI, W., GANGER, G. R., GIBBONS, P. B., GIBSON, G. A., AND XING, E. P. Exploiting bounded staleness to
speed up big data analytics. In 2014 USENIX Annual Technical
Conference (2014), USENIX ATC’14, USENIX.
[12] CUI, H., ZHANG, H., GANGER, G. R., GIBBONS, P. B., AND
XING, E. P. GeePS: Scalable deep learning on distributed GPUs
with a GPU-specialized parameter server. In Proceedings of the
Eleventh European Conference on Computer Systems (2016), EuroSys’16, ACM.
[13] DAI, W., KUMAR, A., WEI, J., HO, Q., GIBSON, G., AND
XING, E. P. High-performance distributed ML at scale through
parameter server consistency models. In Proceedings of the
Twenty-Ninth AAAI Conference on Artificial Intelligence (2015),
AAAI’15, AAAI Press.
[14] DE SA, C. M., ZHANG, C., OLUKOTUN, K., RE´, C., AND RE´,
C. Taming the wild: A unified analysis of hogwild-style algorithms. In Advances in Neural Information Processing Systems
28 (2015), NIPS’15, Curran Associates, Inc.
[15] DEAN, J., CORRADO, G., MONGA, R., CHEN, K., DEVIN, M.,
MAO, M., AURELIO RANZATO, M., SENIOR, A., TUCKER, P.,
YANG, K., LE, Q. V., AND NG, A. Y. Large scale distributed
deep networks. In Advances in Neural Information Processing
Systems 25, NIPS’12. Curran Associates, Inc., 2012.
[16] GEMULLA, R., NIJKAMP, E., HAAS, P. J., AND SISMANIS, Y.
Large-scale matrix factorization with distributed stochastic gradient descent. In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
(2011), KDD’11, ACM.
[17] GONZALEZ, J. E., LOW, Y., GU, H., BICKSON, D., AND
GUESTRIN, C. PowerGraph: Distributed graph-parallel computation on natural graphs. In Presented as part of the 10th USENIX
Symposium on Operating Systems Design and Implementation
(2012), OSDI’12, USENIX.
[18] GONZALEZ, J. E., XIN, R. S., DAVE, A., CRANKSHAW, D.,
FRANKLIN, M. J., AND STOICA, I. GraphX: Graph processing
in a distributed dataflow framework. In 11th USENIX Symposium on Operating Systems Design and Implementation (2014),
OSDI’14, USENIX.
[19] GRIFFITHS, T. L., AND STEYVERS, M. Finding scientific topics.
Proceedings of the National Academy of Sciences 101, suppl 1
(2004).
[20] HO, Q., CIPAR, J., CUI, H., LEE, S., KIM, J. K., GIBBONS,
P. B., GIBSON, G. A., GANGER, G., AND XING, E. P. More
effective distributed ML via a stale synchronous parallel parameter server. In Advances in Neural Information Processing Systems
26, NIPS’13. Curran Associates, Inc., 2013.
[21] JOHNSON, M., SAUNDERSON, J., AND WILLSKY, A. Analyzing Hogwild parallel Gaussian Gibbs sampling. In Advances in
Neural Information Processing Systems 26, NIPS’13. Curran Associates, Inc., 2013.
[22] KHAYYAT, Z., AWARA, K., ALONAZI, A., JAMJOOM, H.,
WILLIAMS, D., AND KALNIS, P. Mizan: A system for dynamic
load balancing in large-scale graph processing. In Proceedings of
the 8th ACM European Conference on Computer Systems (2013),
EuroSys’13, ACM.
[23] KIM, J. K., HO, Q., LEE, S., ZHENG, X., DAI, W., GIBSON,
G. A., AND XING, E. P. STRADS: A distributed framework for
scheduled model parallel machine learning. In Proceedings of
the Eleventh European Conference on Computer Systems (2016),
EuroSys’16, ACM.
[24] KYROLA, A., BLELLOCH, G., AND GUESTRIN, C. GraphChi:
Large-scale graph computation on just a PC. In Presented as part
of the 10th USENIX Symposium on Operating Systems Design
and Implementation (2012), OSDI’12, USENIX.
[25] LEE, S., KIM, J. K., ZHENG, X., HO, Q., GIBSON, G. A., AND
XING, E. P. On model parallelization and scheduling strategies
for distributed machine learning. In Advances in Neural Information Processing Systems 27 (2014), NIPS’14, Curran Associates,
Inc.
[26] LI, M., ANDERSEN, D. G., PARK, J. W., SMOLA, A. J.,
AHMED, A., JOSIFOVSKI, V., LONG, J., SHEKITA, E. J., AND
SU, B.-Y. Scaling distributed machine learning with the parameter server. In 11th USENIX Symposium on Operating Systems
Design and Implementation (2014), OSDI’14, USENIX.
[27] LI, M., ANDERSEN, D. G., AND SMOLA, A. J. Distributed
delayed proximal gradient methods. In NIPS Workshop on Optimization for Machine Learning (2013).
[28] LI, M., ANDERSEN, D. G., SMOLA, A. J., AND YU, K. Communication efficient distributed machine learning with the parameter server. In Advances in Neural Information Processing Systems 27 (2014), NIPS’14, Curran Associates, Inc.
[29] LOW, Y., BICKSON, D., GONZALEZ, J., GUESTRIN, C., KYROLA, A., AND HELLERSTEIN, J. M. Distributed GraphLab:
A framework for machine learning and data mining in the cloud.
Proc. VLDB Endow. 5, 8 (Apr. 2012).
[30] MALEWICZ, G., AUSTERN, M. H., BIK, A. J., DEHNERT,
J. C., HORN, I., LEISER, N., AND CZAJKOWSKI, G. Pregel:
A system for large-scale graph processing. In Proceedings of the
2010 ACM SIGMOD International Conference on Management
of Data (2010), SIGMOD’10, ACM.
[31] MURRAY, D. G., MCSHERRY, F., ISAACS, R., ISARD, M.,
BARHAM, P., AND ABADI, M. Naiad: A timely dataflow system. In Proceedings of the Twenty-Fourth ACM Symposium on
Operating Systems Principles (2013), SOSP’13, ACM.
[32] NELSON, J., HOLT, B., MYERS, B., BRIGGS, P., CEZE, L.,
KAHAN, S., AND OSKIN, M. Latency-tolerant software distributed shared memory. In 2015 USENIX Annual Technical Conference (2015), USENIX ATC’15, USENIX.
[33] NGUYEN, D., LENHARTH, A., AND PINGALI, K. A lightweight
infrastructure for graph analytics. In Proceedings of the TwentyFourth ACM Symposium on Operating Systems Principles(2013),
SOSP’13, ACM.
[34] PARIKH, N., AND BOYD, S. Proximal algorithms. Found. Trends
Optim. 1, 3 (Jan. 2014).
[35] POWER, R., AND LI, J. Piccolo: Building fast, distributed programs with partitioned tables. In Proceedings of the 9th USENIX
Conference on Operating Systems Design and Implementation
(2010), OSDI’10, USENIX.
[36] PRABHAKARAN, V., WU, M., WENG, X., MCSHERRY, F.,
ZHOU, L., AND HARADASAN, M. Managing large graphs on
multi-cores with graph awareness. In 2012 USENIX Annual Technical Conference (2012), USENIX ATC’12, USENIX.
[37] RECHT, B., RE, C., WRIGHT, S., AND NIU, F. Hogwild:
A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems
24, NIPS’11. Curran Associates, Inc., 2011.
[38] ROY, A., BINDSCHAEDLER, L., MALICEVIC, J., AND
ZWAENEPOEL, W. Chaos: Scale-out graph processing from secondary storage. In Proceedings of the 25th Symposium on Operating Systems Principles (2015), SOSP’15, ACM.
[39] ROY, A., MIHAILOVIC, I., AND ZWAENEPOEL, W. X-Stream:
Edge-centric graph processing using streaming partitions. In Proceedings of the Twenty-Fourth ACM Symposium on Operating
Systems Principles (2013), SOSP’13, ACM.
[40] SMOLA, A., AND NARAYANAMURTHY, S. An architecture for
parallel topic models. Proc. VLDB Endow. 3, 1-2 (Sept. 2010).
[41] SPARKS, E. R., TALWALKAR, A., SMITH, V., KOTTALAM, J.,
PAN, X., GONZALEZ, J., FRANKLIN, M. J., JORDAN, M. I.,
AND KRASKA, T. MLI: An API for Distributed Machine Learning. In 2013 IEEE 13th International Conference on Data Mining
(2013), ICDM’13, IEEE.
[42] WEI, J., DAI, W., QIAO, A., HO, Q., CUI, H., GANGER, G. R.,
GIBBONS, P. B., GIBSON, G. A., AND XING, E. P. Managed
communication and consistency for fast data-parallel iterative analytics. In Proceedings of the Sixth ACM Symposium on Cloud
Computing (2015), SoCC’15, ACM.
[43] WU, M., YANG, F., XUE, J., XIAO, W., MIAO, Y., WEI, L.,
LIN, H., DAI, Y., AND ZHOU, L. GraM: Scaling graph computation to the trillions. In Proceedings of the Sixth ACM Symposium
on Cloud Computing (2015), SoCC’15, ACM.
[44] XING, E. P., HO, Q., DAI, W., KIM, J.-K., WEI, J., LEE, S.,
ZHENG, X., XIE, P., KUMAR, A., AND YU, Y. Petuum: A new
platform for distributed machine learning on big data. In Proceedings of the 21th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (2015), KDD’15, ACM.
[45] YAO, L., MIMNO, D., AND MCCALLUM, A. Efficient methods for topic model inference on streaming document collections. In Proceedings of the 15th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (2009),
KDD’09, ACM.
[46] ZHANG, K., CHEN, R., AND CHEN, H. NUMA-aware graphstructured analytics. In Proceedings of the 20th ACM SIGPLAN
Symposium on Principles and Practice of Parallel Programming
(2015), PPoPP’15, ACM.
[47] ZHENG, D., MHEMBERE, D., BURNS, R., VOGELSTEIN, J.,
PRIEBE, C. E., AND SZALAY, A. S. FlashGraph: Processing
billion-node graphs on an array of commodity SSDs. In 13th
USENIX Conference on File and Storage Technologies (2015),
FAST’15, USENIX.

