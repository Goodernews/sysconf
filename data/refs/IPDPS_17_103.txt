[1] A. Novikov, D. Podoprikhin, A. Osokin, and D. Vetrov, “Tensorizing

neural networks,” CoRR, vol. abs/1509.06569, 2015.

[2] I. Perros, R. Chen, R. Vuduc, and J. Sun, “Sparse hierarchical tucker
factorization and its application to healthcare,” IEEE International
Conference on Data Mining (ICDM), 2015.

[3] T. G. Kolda and B. W. Bader, “Tensor decompositions and applications,”

SIAM Review, vol. 51, no. 3, pp. 455–500, 2009.

[4] A. Cichocki, “Era of big data processing: A new approach via tensor
networks and tensor decompositions,” CoRR, vol. abs/1403.2048, 2014.
[5] L. De Lathauwer and D. Nion, “Decompositions of a higher-order tensor
in block terms—Part III: Alternating least squares algorithms,” SIAM
Journal on Matrix Analysis and Applications, vol. 30, no. 3, pp. 1067–
1083, 2008.

[6] J. Li, Y. Ma, C. Yan, and R. Vuduc, “Optimizing sparse tensor times
matrix on multi-core and many-core architectures,” in ACM/IEEE the
Sixth Workshop on Irregular Applications: Architectures and Algorithms.
Salt Lake City, Utah, USA: IEEE, 2016.

[7] J. Li, C. Battaglino, I. Perros, J. Sun, and R. Vuduc, “An input-
adaptive and in-place approach to dense tensor-times-matrix multiply,”
in ACM/IEEE Supercomputing (SC ’15). New York, NY, USA: ACM,
2015.

[8] S. Smith, N. Ravindran, N. Sidiropoulos, and G. Karypis, “Splatt:
Efﬁcient and parallel sparse tensor-matrix multiplication,” in Proceed-
ings of the 29th IEEE International Parallel & Distributed Processing
Symposium, ser. IPDPS, 2015.

[9] J. C. Ho, J. Ghosh, and J. Sun, “Marble: High-throughput pheno-
typing from electronic health records via sparse nonnegative tensor
factorization,” in Proceedings of the 20th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, ser. KDD ’14.
New York, NY, USA: ACM, 2014, pp. 115–124.

[10] Y. Wang, R. Chen, J. Ghosh, J. C. Denny, A. Kho, Y. Chen, B. A.
Malin, and J. Sun, “Rubik: Knowledge guided tensor factorization and
completion for health data analytics,” in Proceedings of the 21th ACM
SIGKDD International Conference on Knowledge Discovery and Data
Mining, ser. KDD ’15. New York, NY, USA: ACM, 2015, pp. 1265–
1274.

[11] U. Kang, E. Papalexakis, A. Harpale, and C. Faloutsos, “GigaTensor:
Scaling tensor analysis up by 100 times—algorithms and discoveries,”
in Proceedings of the 18th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, ser. KDD ’12. New York, NY,
USA: ACM, 2012, pp. 316–324.

[12] M. Abadi et al., “TensorFlow: Large-scale machine learning on hetero-

geneous systems,” 2015, software available from tensorﬂow.org.

[13] A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky,
“Tensor decompositions for learning latent variable models,” J. Mach.
Learn. Res., vol. 15, no. 1, pp. 2773–2832, Jan. 2014.

[14] E. E. Papalexakis, C. Faloutsos, and N. D. Sidiropoulos, “ParCube:
Sparse parallelizable tensor decompositions,” in Proceedings of the 2012

European Conference on Machine Learning and Knowledge Discovery
in Databases - Volume Part I, ser. ECML PKDD’12. Berlin, Heidelberg:
Springer-Verlag, 2012, pp. 521–536.

[15] L. Grasedyck, D. Kressner, and C. Tobler, “A literature survey of low-
rank tensor approximation techniques,” GAMM-Mitteilungen, vol. 36,
no. 1, pp. 53–78, 2013.

[16] A. Cichocki, N. Lee, I. V. Oseledets, A. Phan, Q. Zhao, and D. Mandic,
“Low-rank tensor networks for dimensionality reduction and large-scale
optimization problems: Perspectives and challenges part 1,” ArXiv e-
prints, Sep. 2016.

[17] A. H. Phan, P. Tichavsk, and A. Cichocki, “Fast alternating LS algo-
rithms for high order CANDECOMP/PARAFAC tensor factorizations,”
IEEE Transactions on Signal Processing, vol. 61, no. 19, pp. 4834–4846,
Oct 2013.

[18] F. Huang, N. Niranjan U., I. Perros, R. Chen, J. Sun, and A. Anand-
kumar, “Scalable Latent Tree Model and its Application to Health
Analytics,” ArXiv e-prints, Jun. 2014.

[19] O. Kaya and B. Uc¸ar, “Scalable sparse tensor decompositions in dis-
tributed memory systems,” in Proceedings of the International Con-
ference for High Performance Computing, Networking, Storage and
Analysis, ser. SC ’15. New York, NY, USA: ACM, 2015, pp. 77:1–
77:11.

[20] J. H. Choi and S. Vishwanathan, “Dfacto: Distributed factorization of
tensors,” in Advances in Neural Information Processing Systems 27,
Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger,
Eds. Curran Associates, Inc., 2014, pp. 1296–1304.

[21] B. W. Bader and T. G. Kolda, “Efﬁcient MATLAB computations with
sparse and factored tensors,” SIAM Journal on Scientiﬁc Computing,
vol. 30, no. 1, pp. 205–231, December 2007.

[22] B. W. Bader, T. G. Kolda et al., “MATLAB Tensor Toolbox (Version

2.6),” Available online, February 2015.

[23] S. Smith and G. Karypis, “Tensor-matrix products with a compressed
sparse tensor,” in Proceedings of the 5th Workshop on Irregular Appli-
cations: Architectures and Algorithms. ACM, 2015, p. 7.

[24] J. Carroll and J.-J. Chang, “Analysis of individual differences in
multidimensional scaling via an n-way generalization of eckart-young
decomposition,” Psychometrika, vol. 35, no. 3, pp. 283–319, 1970.

[25] S. Zhou, X. V. Nguyen, J. Bailey, Y. Jia, and I. Davidson, “Accelerating
online CP decompositions for higher order tensors,” 22th ACM SIGKDD
2016 Conference on Knowledge Discovery & Data Mining (submitted),
Aug 2016.

[26] A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. Hruschka, and
T. Mitchell, “Toward an architecture for never-ending language learn-
ing,” 2010.

[27] O. G¨orlitz, S. Sizov, and S. Staab, “PINTS: Peer-to-peer infrastructure
for tagging systems,” in Proceedings of the 7th International Conference
on Peer-to-peer Systems, ser. IPTPS’08. Berkeley, CA, USA: USENIX
Association, 2008, pp. 19–19.

[28] S. Smith, J. W. Choi, J. Li, R. Vuduc, J. Park, X. Liu, and G. Karypis.
(2017) FROSTT: The formidable repository of open sparse tensors and
tools. [Online]. Available: http://frostt.io/

[29] S. Smith, N. Ravindran, N. Sidiropoulos, and G. Karypis, “SPLATT: The
Surprisingly ParalleL spArse Tensor Toolkit (Version 1.1.1),” Available
online, 2016.

[30] E. C. Chi and T. G. Kolda, “On tensors, sparsity, and nonnegative
factorizations,” SIAM Journal on Matrix Analysis and Applications,
vol. 33, no. 4, pp. 1272–1299, 2012.

[31] L. Sorber, M. Van Barel, and L. De Lathauwer, “Tensorlab (Version

v3.0),” Available online, 2014.

[32] M. Baskaran, B. Meister, N. Vasilache, and R. Lethin, “Efﬁcient and
scalable computations with sparse tensors,” in High Performance Ex-
treme Computing (HPEC), 2012 IEEE Conference on, Sept 2012, pp.
1–6.

[33] O. Kaya and B. Uar, “High-performance parallel algorithms for the
Tucker decomposition of higher order sparse tensors.” Inria - Research
Centre Grenoble Rhone-Alpes, RR-8801, 2015.

[34] ——, “Parallel cp decomposition of sparse tensors using dimension
trees.” Inria - Research Centre Grenoble Rhone-Alpes, RR-8976, 2016.

