[1] R. Babbar and B. Scho ̈lkopf. 2017. DiSMEC: Distributed Sparse Machines for Extreme Multi-label Classification. In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining (WSDM 2017).
[2] Kush Bhatia, Himanshu Jain, Purushottam Kar, Manik Varma, and Prateek Jain. 2015. Sparse local embeddings for extreme multi-label classification. In Advances in Neural Information Processing Systems. 730–738.
[3] Koby C. and Yoram S. 2002. On the algorithmic implementation of multiclass kernel-based vector machines. The Journal of Machine Learning Research (2002).
[4] Jie Chen and others. 2016. Efficient one-vs-one kernel ridge regression for speech recognition. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2454–2458.
[5] Y.N. Chen and H.T. Lin. 2012. Feature-aware label space dimension reduction for multi-label classification. In Advances in Neural Information Processing Systems.
[6] Anna Choromanska, Alekh Agarwal, and John Langford. 2013. Extreme multi class classification. In NIPS Workshop: eXtreme Classification, submitted.
[7] Anna E Choromanska and John Langford. 2015. Logarithmic time online multi- class prediction. In Advances in Neural Information Processing Systems. 55–63.
[8] K. Crammer and Y. Singer. 2001. On the algorithmic implementation of multiclass kernel-based vector machines. JMLR (2001).
[9] Koby Crammer and Yoram Singer. 2003. A family of additive online algorithms for category ranking. Journal of Machine Learning Research 3, Feb (2003), 1025–1058.
[10] Wei Dai, Abhimanu Kumar, Jinliang Wei, Qirong Ho, Garth Gibson, and Eric P. Xing. 2015. Analysis of High-Performance Distributed ML at Scale through Parameter Server Consistency Models. In Proceedings of the 29th AAAI Conference on Artificial Intelligence.
[11] J. Deng, A.C. Berg, K. Li, and Li F-F. What does classifying more than 10,000 image categories tell us? In ECCV 2010.
[12] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. The Journal of Machine Learning Research 9 (2008), 1871–1874.
[13] Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S Sathiya Keerthi, and Sellaman- ickam Sundararajan. 2008. A dual coordinate descent method for large-scale linear SVM. In Proceedings of 25th international conference on Machine learning.
[14] Himanshu Jain, Yashoteja Prabhu, and Manik Varma. 2016. Extreme Multi-label Loss Functions for Recommendation, Tagging, Ranking & Other Missing Label Applications. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM.
[15] Kalina Jasinska, Krzysztof Dembczynski, Ro ́bert Busa-Fekete, Karlson Pfannschmidt, Timo Klerx, and E Hu ̈llermeier. 2016. Extreme f-measure maxi- mization using sparse probability estimates. In Proceedings of the 33nd Interna- tional Conference on Machine Learning. 1435–1444.
[16] Sham Kakade, Shai Shalev-Shwartz, and Ambuj Tewari. 2009. On the du- ality of strong convexity and strong smoothness: Learning applications and matrix regularization. Unpublished Manuscript, http://ttic. uchicago. edu/shai/papers/KakadeShalevTewari09. pdf (2009).
[17] A. Kapoor, R. Viswanathan, and P. Jain. Multilabel classification using bayesian compressed sensing. In NIPS 2012.
[18] S Sathiya Keerthi and others. 2008. A sequential dual method for large scale multi-class linear SVMs. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 408–416.
[19] Jin Kyu Kim and others. 2016. STRADS: a distributed framework for sched- uled model parallel machine learning. In Proceedings of the Eleventh European Conference on Computer Systems. ACM, 5.
[20] Simon Lacoste-Julien, Martin Jaggi, Mark Schmidt, and Patrick Pletscher. 2013. Block-Coordinate Frank-Wolfe Optimization for Structural SVMs. In ICML 2013 International Conference on Machine Learning. 53–61.
[21] P. Langley. 2000. Crafting Papers on Machine Learning. In Proceedings of the 17th International Conference on Machine Learning (ICML 2000), Pat Langley (Ed.). Morgan Kaufmann, Stanford, CA, 1207–1216.
[22] O. Meshi, M. Mahdavi, and A. Schwing. Smooth and Strong: MAP Inference with Linear Convergence. In NIPS 2015.
[23] Ioannis Partalas and others. 2015. Lshtc: A benchmark for large-scale text classification. arXiv preprint arXiv:1503.08581 (2015).
[24] Yashoteja Prabhu and Manik Varma. 2014. Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining.
[25] Ali Rahimi and Benjamin Recht. 2007. Random features for large-scale kernel machines. In Advances in neural information processing systems. 1177–1184.
[26] P. Richta ́rik and M. Taka ́cˇ. 2014. Iteration complexity of randomized block- coordinate descent methods for minimizing a composite function. Mathematical Programming (2014).
[27] Shai Shalev-Shwartz and Tong Zhang. 2013. Stochastic dual coordinate ascent methods for regularized loss. The Journal of Machine Learning Research (2013).
[28] Ryan J Tibshirani and others. 2013. The lasso problem and uniqueness. Electronic Journal of Statistics 7 (2013), 1456–1490.
[29] Jinliang Wei, Wei Dai, Aurick Qiao, Qirong Ho, Henggang Cui, Gregory R Ganger, Phillip B Gibbons, Garth A Gibson, and Eric P Xing. 2015. Managed communica- tion and consistency for fast data-parallel iterative analytics. In Proceedings of the Sixth ACM Symposium on Cloud Computing. ACM, 381–394.
[30] Jason Weston, Ameesh Makadia, and Hector Yee. 2013. Label Partitioning For Sublinear Ranking.. In ICML (2). 181–189.
[31] Lingfei Wu and Andreas Stathopoulos. 2015. A preconditioned hybrid svd method for accurately computing singular triplets of large matrices. SIAM Journal on Scientific Computing 37, 5 (2015), S365–S388.
[32] Lingfei Wu, Ian EH Yen, Jie Chen, and Rui Yan. 2016. Revisiting random binning features: Fast convergence and strong parallelizability. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.
[33] Chang Xu, Dacheng Tao, and Chao Xu. 2016. Robust extreme multi-label learning. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA August. 13–17.
[34] I.E.H. Yen, X. Huang, K. Zhong, P. Ravikumar, and I.S Dhillon. 2016. Pd-sparse: A primal and dual sparse approach to extreme multiclass and multilabel classifi- cation. In Proceedings of the 33nd International Conference on Machine Learning.
[35] I.E.H. Yen, T.W. Lin, S.D. Lin, P. Ravikumar, and I.S. Dhillon. Sparse random feature algorithm as coordinate descent in Hilbert space. In NIPS 2014.
[36] H.F. Yu, P. Jain, P. Kar, and I. S Dhillon. 2013. Large-scale multi-label learning with missing labels. arXiv:1307.5101 (2013).
[37] Hui Zou and Trevor Hastie. 2005. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (2005).
