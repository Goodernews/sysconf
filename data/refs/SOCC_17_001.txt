[1] 2017. Apache Giraph. (2017). http://giraph.apache.org/.
[2] 2017. Apache Hadoop. (2017). http://hadoop.apache.org/.
[3] 2017. Apache Mahout. (2017). http://mahout.apache.org/.
[4] 2017. Apache Spark. (2017). http://spark.apache.org/.
[5] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al.
2016. TensorFlow: A system for large-scale machine learning. In Proc. of OSDI.
265–283.

[6] Umut Acar, Arthur Charguéraud, and Mike Rainey. 2013. Scheduling Parallel
Programs by Work Stealing with Private Deques. In Proc. of PPoPP. ACM, 219–
228.

[7] Muzaffer Can Altinigneli, Claudia Plant, and Christian Böhm. 2013. Massively
parallel expectation maximization using graphics processing units. In Proc. of
KDD. ACM, 838–846.

[8] Narayanaswamy Balakrishnan and Suvra Pal. 2016. Expectation maximization-
based likelihood inference for flexible cure rate models with Weibull lifetimes.
Statistical methods in medical research 25, 4 (2016), 1535–1563.

[9] M Emre Celebi, Hassan A Kingravi, and Patricio A Vela. 2013. A comparative
study of efficient initialization methods for the k-means clustering algorithm.
Expert Systems with Applications 40, 1 (2013), 200–210.

[10] Trishul M Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman.
2014. Project Adam: Building an Efficient and Scalable Deep Learning Training
System.. In Proc. of OSDI, Vol. 14. 571–582.

[11] Corinna Cortes and Vladimir Vapnik. 1995. Support-vector networks. Machine

learning 20, 3 (1995), 273–297.

[12] Wei Dai, Abhimanu Kumar, Jinliang Wei, Qirong Ho, Garth Gibson, and Eric P
Xing. 2015. High-Performance Distributed ML at Scale through Parameter Server
Consistency Models. In Proc. Of AAAI.

[13] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
Andrew Senior, Paul Tucker, Ke Yang, Quoc V Le, et al. 2012. Large scale
distributed deep networks. In Advances in neural information processing systems.
1223–1231.

[14] Arthur P Dempster, Nan M Laird, and Donald B Rubin. 1977. Maximum likeli-
hood from incomplete data via the EM algorithm. Journal of the royal statistical
society. Series B (methodological) (1977), 1–38.

[15] James Dinan, D Brian Larkins, Ponnuswamy Sadayappan, Sriram Krishnamoorthy,
and Jarek Nieplocha. 2009. Scalable work stealing. In Proc. of the Conference on
High Performance Computing Networking, Storage and Analysis. ACM, 53.

[16] James Dinan, Stephen Olivier, Gerald Sabin, Jan Prins, P Sadayappan, and Chau-
Wen Tseng. 2007. Dynamic load balancing of unbalanced computations using
message passing. In Proc. of IPDPS. IEEE, 1–8.

[17] J. C. Dunn. 1973. A fuzzy relative of the ISODATA Process and Its Use in
Detecting Compact Well-Separated Clusters. Journal of Cybernetics 3 (1973),
32–57.

[18] Aaron Harlap, Henggang Cui, Wei Dai, Jinliang Wei, Gregory R Ganger, Phillip B
Gibbons, Garth A Gibson, and Eric P Xing. 2016. Addressing the straggler
problem for iterative convergent parallel ML.. In Proc. of SoCC. ACM, 98–111.
[19] Qirong Ho, James Cipar, Henggang Cui, Seunghak Lee, Jin Kyu Kim, Phillip B
Gibbons, Garth A Gibson, Greg Ganger, and Eric P Xing. 2013. More effective
distributed ml via a stale synchronous parallel parameter server. In Proc. of NIPS.
1223–1231.

[20] Asim Kadav and Erik Kruus. 2016. ASAP: Asynchronous Approximate Data-

Parallel Computation. arXiv preprint arXiv:1612.08608 (2016).

[21] Wojtek Kowalczyk, Nikos A Vlassis, et al. 2004. Newscast EM.. In Proc. of NIPS.

713–720.

[22] Stuart Lloyd. 1982. Least squares quantization in PCM. IEEE transactions on

information theory 28, 2 (1982), 129–137.

[23] James MacQueen et al. 1967. Some methods for classification and analysis of mul-
tivariate observations. In Proc. of the fifth Berkeley symposium on mathematical
statistics and probability, Vol. 1. 281–297.

[24] Grzegorz Malewicz, Matthew H Austern, Aart JC Bik, James C Dehnert, Ilan Horn,
Naty Leiser, and Grzegorz Czajkowski. 2010. Pregel: a system for large-scale
graph processing. In Proc. of SIGMOD. 135–146.

[25] Xiao-Li Meng and David Van Dyk. 1997. The EM Algorithm–an Old Folk-song
Sung to a Fast New Tune. Journal of the Royal Statistical Society: Series B
(Statistical Methodology) 59, 3 (1997), 511–567.

[26] Derek G Murray, Frank McSherry, Rebecca Isaacs, Michael Isard, Paul Barham,
and Martín Abadi. 2013. Naiad: a timely dataflow system. In Proc. of SOSP.
ACM, 439–455.

[27] Radford M Neal and Geoffrey E Hinton. 1998. A view of the EM algorithm that
justifies incremental, sparse, and other variants. In Learning in graphical models.
Springer, 355–368.

[28] Thanh Minh Nguyen and QM Jonathan Wu. 2013. Fast and robust spatially
constrained gaussian mixture model for image segmentation. IEEE transactions
on circuits and systems for video technology 23, 4 (2013), 621–635.

[29] Xinghao Pan, Jianmin Chen, Rajat Monga, Samy Bengio, and Rafal Jozefowicz.
2017. Revisiting Distributed Synchronous SGD. arXiv preprint arXiv:1702.05800
(2017).

[30] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. 2011. Hogwild:
A lock-free approach to parallelizing stochastic gradient descent. In Proc. Of NIPS.
693–701.

[31] David Sculley. 2010. Web-scale k-means clustering. In Proc. of WWW. ACM,

1177–1178.

[32] Supreeth Subramanya, Tian Guo, Prateek Sharma, David Irwin, and Prashant
Shenoy. 2015. Spoton: A batch computing service for the spot market. In Proc. of
SoCC. ACM, 329–341.

[33] Bo Thiesson, Christopher Meek, and David Heckerman. 2001. Accelerating EM

for large databases. Machine Learning 45, 3 (2001), 279–299.

[34] Zhigang Wang, Yu Gu, Yubin Bao, Ge Yu, and Jeffrey Xu Yu. 2016. Hybrid
Pulling/Pushing for I/O-Efficient Distributed and Iterative Graph Computing. In
Proc. of SIGMOD. ACM, 479–494.

[35] Jinliang Wei, Wei Dai, Aurick Qiao, Qirong Ho, Henggang Cui, Gregory R
Ganger, Phillip B Gibbons, Garth A Gibson, and Eric P Xing. 2015. Managed
communication and consistency for fast data-parallel iterative analytics. In Proc.
of SoCC. ACM, 381–394.

[36] Jason Wolfe, Aria Haghighi, and Dan Klein. 2008. Fully distributed EM for very

large datasets. In Proc. of ICML. ACM, 1184–1191.

[37] Xindong Wu, Vipin Kumar, J Ross Quinlan, Joydeep Ghosh, Qiang Yang, Hiroshi
Motoda, Geoffrey J McLachlan, Angus Ng, Bing Liu, S Yu Philip, et al. 2008. Top
10 algorithms in data mining. Knowledge and information systems 14, 1 (2008),
1–37.

[38] Yu-Wei Wu, Blake A Simmons, and Steven W Singer. 2015. MaxBin 2.0: an
automated binning algorithm to recover genomes from multiple metagenomic
datasets. Bioinformatics 32, 4 (2015), 605–607.

[39] Jiangtao Yin and Lixin Gao. 2016. Asynchronous distributed incremental compu-

tation on evolving graphs. In Proc. of ECML/PKDD. Springer, 722–738.

[40] Jiangtao Yin, Lixin Gao, and Zhongfei Mark Zhang. 2014. Scalable nonnegative
matrix factorization with block-wise updates. In Proc. of ECML/PKDD. Springer,
337–352.

[41] Jiangtao Yin, Yanfeng Zhang, and Lixin Gao. 2012. Accelerating expectation-
maximization algorithms with frequent updates. In Cluster Computing, 2012 IEEE
International Conference on. IEEE, 275–283.

[42] Hyokun Yun, Hsiang-Fu Yu, Cho-Jui Hsieh, SVN Vishwanathan, and Inderjit
Dhillon. 2014. NOMAD: Non-locking, stOchastic Multi-machine algorithm
for Asynchronous and Decentralized matrix completion. PVLDB 7, 11 (2014),
975–986.
