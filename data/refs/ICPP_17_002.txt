[1] CIFAR10 model.
https://github.com/eladhoffer/ConvNet-torch/blob/
master/Models/Model.lua, accessed January 11, 2016.
[2] O. Abdel-Hamid, A-R Mohamed, H. Jiang, and et al. Convolutional
neural networks for speech recognition. IEEE/ACM Transactions on
audio, speech, and language processing, 22(10):1533–1545, 2014.
[3] L. Bottou. Online learning and stochastic approximations, 1998.
[4] T. Chilimbi, Y. Suzue, J. Apacible, and K. Kalyanaraman. Project adam:
Building an efﬁcient and scalable deep learning training system. In 11th
USENIX Symposium on Operating Systems Design and Implementation
(OSDI 14), pages 571–582, 2014.
[5] J. Dean, G. Corrado, R. Monga, and et al. Large scale distributed deep
networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger,
editors, Advances in Neural Information Processing Systems 25, pages
1223–1231. Curran Associates, Inc., 2012.
[6] O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. Optimal
distributed online prediction using mini-batches. Journal of Machine
Learning Research, 13(Jan):165–202, 2012.
[7] S. Ghadimi and G. Lan. Stochastic ﬁrst-and zeroth-order methods for
nonconvex stochastic programming. SIAM Journal on Optimization,
23(4):2341–2368, 2013.
[8] NVIDIA GPUDirect, https://developer.nvidia.com/gpudirect.
[9] A. Krizhevsky. Learning multiple layers of features from tiny images.
Master’s thesis, 2009.
[10] A. Krizhevsky, I. Sutskever, and G.E. Hinton. Imagenet classiﬁcation
with deep convolutional neural networks. In Advances in neural
information processing systems, pages 1097–1105, 2012.
[11] M. Li, D. G. Andersen, J. W. Park, and et al. Scaling distributed machine
learning with the parameter server. In 11th USENIX Symposium on
Operating Systems Design and Implementation (OSDI 14), pages 583–
598, Broomﬁeld, CO, October 2014. USENIX Association.
[12] M. Li, T. Zhang, Y. Chen, and A. J. Smola. Efﬁcient mini-batch training
for stochastic optimization. In Proceedings of the 20th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining,
KDD ’14, pages 661–670, New York, NY, USA, 2014. ACM.
[13] X. Lian, Y. Huang, Y. Li, and J. Liu. Asynchronous parallel stochastic
gradient for nonconvex optimization. In Advances in Neural Information
Processing Systems, pages 2737–2745, 2015.
[14] Q. Lin, Z. Lu, and L. Xiao. An accelerated proximal coordinate gradient
method. In Advances in Neural Information Processing Systems, pages
3059–3067, 2014.
[15] mpiT – MPI for Torch, https://github.com/sixin-zh/mpiT.
[16] Multi-process
service,
https://docs.nvidia.com/deploy/pdf/CUDA
Multi Process Service Overview.pdf.
[17] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic
approximation approach to stochastic programming. SIAM Journal on
optimization, 19(4):1574–1609, 2009.
[18] One
Stop
System
High
Density
Compute
Accelerator,
http://www.onestopsystems.com/blog-post/
one-stop-systems-shows-its-16-gpu-monster-machine-gtc-2015.
[19] T. Paine, H. Jin, J. Yang, and et al. Gpu asynchronous stochastic
gradient descent to speed up neural network training. arXiv preprint
arXiv:1312.6186, 2013.
[20] B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A lock-free approach
to parallelizing stochastic gradient descent. In Advances in Neural
Information Processing Systems, pages 693–701, 2011.
[21] H. Robbins and D. Siegmund. A convergence theorem for non negative
almost supermartingales and some applications. In Herbert Robbins
Selected Papers, pages 111–135. Springer, 1985.
[22] O. Shamir and T. Zhang. Stochastic gradient descent for non-smooth
optimization: Convergence results and optimal averaging schemes. In
ICML (1), pages 71–79, 2013.
[23] N. Srivastava, G.E. Hinton E, A. Krizhevsky, and et al. Dropout: a
simple way to prevent neural networks from overﬁtting. Journal of
Machine Learning Research, 15(1):1929–1958, 2014.
[24] C. Szegedy, W. Liu, Y. Jia, and et al. Going deeper with convolutions.
In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 1–9, 2015.
[25] Torch – A scientiﬁc computing framework for Luajit, http://torch.ch.
[26] D.R. Wilson and T.R. Martinez. The general inefﬁciency of batch
training for gradient descent learning. Neural Network, 16(10):1429–
1451, December 2003.
[27] L. Xiao and T. Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM Journal on Optimization, 24(4):2057–
2075, 2014.
[28] R. Zhang and J.T. Kwok. Asynchronous distributed admm for consensus
optimization. In Proceedings of the 31st International Conference on
Machine Learning (ICML 2014), pages 1701–1709, 2014.
[29] S. Zhang, A. Choromanska, and Y. LeCun. Deep learning with elastic
averaging SGD. In Advances in Neural Information Processing Systems
28: Annual Conference on Neural Information Processing Systems 2015,
December 7-12, 2015, Montreal, Quebec, Canada, pages 685–693,
2015.
[30] M. Zinkevich, M. Weimer, L. Li, and A.J. Smola. Parallelized stochastic
gradient descent. In Advances in neural information processing systems,
pages 2595–2603, 2010.