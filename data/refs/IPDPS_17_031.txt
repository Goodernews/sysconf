[1] R. D. Hornung and J. A. Keasler, “The RAJA Poratability Layer: Overview and Status,” Lawrence Livermore National Laboratory, Tech. Rep. LLNL-TR-661403, Sep. 2014.
[2] H. C. Edwards, C. R. Trott, and D. Sunderland, “Kokkos: Enabling manycore performance portability through polymorphic memory access patterns,” Journal of Parallel and Distributed Computing, vol. 74, pp. 3202–3216, Dec. 2014.
[3] R. Clint Whaley, A. Petitet, and J. J. Dongarra, “Automated empirical
optimizations of software and the ATLAS project,” Parallel Computing,
vol. 27, no. 1-2, pp. 3–35, Jan. 2001.
[4] M. Frigo and S. G. Johnson, “The Design and Implementation of
FFTW3,” Proceedings of the IEEE, vol. 93, no. 2, pp. 216–231, 2005.
[5] M. Püschel et al., “Spiral: A Generator for Platform-Adapted Libraries
of Signal Processing Alogorithms,” International Journal of High Performance Computing Applications, vol. 18, no. 1, pp. 21–45, Feb. 2004.
[6] C. Tapus, I.-H. Chung, and J. K. Hollingsworth, “Active Harmony:
Towards Automated Performance Tuning,” in Supercomputing 2002
(SC’02), Nov. 2002, p. 44.
[7] I.-H. Chung and J. K. Hollingsworth, “Using Information from Prior
Runs to Improve Automated Tuning Systems,” in Supercomputing 2004
(SC’04), Nov. 2004, pp. 30–30.
[8] ——, “A Case Study Using Automatic Performance Tuning for
Large-Scale Scientific Programs,” Proceedings of the 15th IEEE
International Symposium on High Performance Distributed Computing,
pp. 45–56, Jun. 2006.
[9] S. Muralidharan, M. Shantharam, M. Hall, M. Garland, and B. Catanzaro,
“Nitro: A Framework for Adaptive Code Variant Tuning,” in Proceedings
of the IEEE International Symposium on Parallel & Distributed
Processing, May 2014, pp. 501–512.
[10] Y. Ding, J. Ansel, K. Veeramachaneni, X. Shen, U.-M. O’Reilly, and
S. Amarasinghe, “Autotuning algorithmic choice for input sensitivity,”
in Proceedings of the 36th ACM SIGPLAN Conference on Programming
Language Design and Implementation (PLDI’15), Jun. 2015, pp. 379–390.
[11] I. Karlin et al., “Exploring Traditional and Emerging Parallel
Programming Models Using a Proxy Application,” in Proceedings of
the 27th IEEE International Symposium on Parallel & Distributed
Processing, May 2013, pp. 919–932.
[12] D. A. Beckingsale, W. Gaudin, A. Herdman, and S. Jarvis, “Resident
Block-Structured Adaptive Mesh Refinement on Thousands of Graphics
Processing Units,” in Proceedings of the 44th International Conference
on Parallel Processing, Aug. 2015, pp. 61–70.
[13] R. Darlington, T. McAbee, and G. Rodrigue, “A Study of ALE
Simulations of Rayleigh-Taylor Instability,” Computer Physics
Communications, vol. 135, pp. 58–73, 2001.
[14] B. E. Morgan and J. A. Greenough, “Large-Eddy and Unsteady RANS
Simulations of a Shock-Accelerated Heavy Gas Cylinder,” Shock Waves,
Apr. 2015.
[15] Dyninst. [Online]. Available: http://www.dyninst.org
[16] Caliper. [Online]. Available: https://github.com/LLNL/Caliper
[17] J. Quinlan, “Simplifying decision trees,” International Journal of
Man-Machine Studies, vol. 27, no. 3, pp. 221 – 234, 1987.
[18] P. E. Utgoff, “Incremental induction of decision trees,” Machine
Learning, vol. 4, no. 2, pp. 161–186, 1989.
[19] pandas: Python Data Analysis Library. [Online]. Available:
http://pandas.pydata.org
[20] NumPy. [Online]. Available: http://www.numpy.org
[21] F. Pedregosa et al., “Scikit-learn: Machine Learning in Python,” The
Journal of Machine Learning Research, vol. 12, Feb. 2011.
[22] L. I. Sedov, “Propagation of strong shock waves,” Journal of Applied
Mathematics and Mechanics, vol. 10, pp. 241–250, 1946.
[23] G. A. Sod, “A Survey of Several Finite Difference Methods for Systems
of Nonlinear Hyperbolic Conservation Laws,” Journal of Computational
Physics, vol. 27, no. 1, pp. 1–31, Apr. 1978.

[24] S. Galera, P.-H. Maire, and J. Breil, “A two-dimensional unstructured
cell-centered multi-material ALE scheme using VOF interface
reconstruction,” Journal of Computational Physics, vol. 229, no. 16, pp.
5755–5787, Aug. 2010.
[25] J. K. Hollingsworth and P. J. Keleher, “Prediction and adaptation in
Active Harmony,” in Proceedings of the 7th International Symposium
on High Performance Distributed Computing, Jul. 1998, pp. 180–188.
[26] J. Bergstra, N. Pinto, and D. Cox, “Machine learning for predictive
auto-tuning with boosted regression trees,” in Proceedings of Innovative
Parallel Computing, May 2012, pp. 1–9.
[27] A. Calotoiu, T. Hoefler, M. Poke, and F. Wolf, “Using automated
performance modeling to find scalability bugs in complex codes,” in
Supercomputing 2013 (SC’13), Nov. 2013, pp. 1–12.
[28] T. Hoefler, W. Gropp, W. Kramer, and M. Snir, “Performance modeling
for systematic performance tuning,” in Supercomputing 2011 (SC’11),
2011, pp. 1–12.
[29] A. Hartono, B. Norris, and P. Sadayappan, “Annotation-based empirical
performance tuning using Orio,” in IEEE International Symposium on
Parallel & Distributed Processing, May 2009, pp. 1–11.
[30] J. Ansel et al., “OpenTuner,” in Proceedings of the 23rd International
Conference on Parallel Architectures and Compilation Techniques, New
York, New York, USA, 2014, pp. 303–316.
[31] R. Vuduc, J. W. Demmel, and K. A. Yelick, “OSKI: A library
of automatically tuned sparse matrix kernels,” Journal of Physics:
Conference Series, vol. 16, no. 1, pp. 521–530, Aug. 2005.
[32] A. Bhattacharyya and T. Hoefler, “PEMOGEN: Automatic Adaptive
Performance Modeling during Program Runtime,” in Proceedings of the
23rd International Conference on Parallel Architectures and Compilation
Techniques, Aug. 2014, pp. 393–404.
[33] E. Park, J. Cavazos, L. N. Pouchet, and C. Bastoul, “Predictive
modeling in a polyhedral optimization space,” in Proceedings of the
9th Annual IEEE/ACM International Symposium on Code Generation
and Optimization, Apr. 2011, pp. 119–129.
[34] F. Agakov et al., “Using Machine Learning to Focus Iterative
Optimization,” in Proceedings of the International Symposium on Code
Generation and Optimization, Mar. 2006, pp. 295–305.
[35] G. Wu, J. L. Greathouse, A. Lyashevsky, N. Jayasena, and D. Chiou,
“GPGPU performance and power estimation using machine learning,”
in Proceedings of the 21st IEEE International Symposium on High
Performance Computer Architecture, Feb. 2015, pp. 564–576.
[36] S. Song, C. Su, and B. Rountree, “A simplified and accurate model
of power-performance efficiency on emergent GPU architectures,” in
Proceedings of the 27th IEEE International Symposium on Parallel &
Distributed Processing, May 2013, pp. 673–686.
[37] M. A. Curtis-Maury et al., “Identifying energy-efficient concurrency
levels using machine learning,” in Proceedings of the IEEE Conference
on Cluster Computing, Sep. 2007, pp. 488–495.
[38] C. Su, D. Li, D. S. Nikolopoulos, K. W. Cameron, B. R. de Supinski,
and E. A. Leon, “Model-based, memory-centric performance and power
optimization on NUMA multiprocessors,” in Proceedings of the IEEE
International Symposium on Workload Characterization, Nov. 2012, pp.
164–173.
[39] N. Jain, A. Bhatele, M. P. Robson, T. Gamblin, and L. V. Kale, “Predicting
application performance using supervised learning on communication
features,” in Supercomputing 2013 (SC’13), Nov. 2013, pp. 1–12.
