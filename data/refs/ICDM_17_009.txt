[1] J. A. Hanley and B. J. McNeil, “The meaning and use of the area under
of receiver operating characteristic (roc) curve.” in Radiology, 1982.
[2] C. Cortes and M. Mohri, “Auc optimization vs. error rate minimization,”
in NIPS, 2003.
[3] T. Joachims, “A support vector method for multivariate performance
measures,” in ICML, 2005, pp. 377–384.
[4] P. Zhao, S. C. H. Hoi, R. Jin, and T. Yang, “Online AUC maximization,” in Proceedings of the 28th International Conference on Machine
Learning, ICML 2011, Bellevue, Washington, USA, June 28 - July 2,
2011, pp. 233–240.
[5] W. Gao, R. Jin, S. Zhu, and Z.-H. Zhou, “One-pass auc optimization,”
in ICML (3), 2013, pp. 906–914.
[6] Y. Ding, P. Zhao, S. C. H. Hoi, and Y. Ong, “An adaptive gradient
method for online AUC maximization,” in AAAI 2015, January 25-30,
Austin, Texas, USA., 2015, pp. 2568–2574.
[7] J. Hu, H. Yang, I. King, M. R. Lyu, and A. M. So, “Kernelized online
imbalanced learning with ﬁxed budgets,” in AAAI 2015, January 25-30,
Austin, Texas, USA., 2015, pp. 2666–2672.
[8] P. Kar, B. K. Sriperumbudur, P. Jain, and H. Karnick, “On the generalization ability of online learning algorithms for pairwise loss functions,”
in ICML 2013, Atlanta, GA, USA, 16-21 June, 2013, pp. 441–449.

[9] S. C. H. Hoi, J. Wang, and P. Zhao, “LIBOL: a library for online learning
algorithms,” Journal of Machine Learning Research, vol. 15, no. 1, pp.
495–499, 2014.
[10] N. Cesa-Bianchi and G. Lugosi, Prediction, learning, and games.
Cambridge University Press, 2006.
[11] S. Shalev-Shwartz, “Online learning and online convex optimization,”
Foundations and Trends in Machine Learning, vol. 4, no. 2, pp. 107–
194, 2012.
[12] F. Rosenblatt, “The perceptron: a probabilistic model for information
storage and organization in the brain.” Psychological review, vol. 65,
no. 6, p. 386, 1958.
[13] K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer,
“Online passive-aggressive algorithms,” Journal of Machine Learning
Research, vol. 7, pp. 551–585, 2006.
[14] M. Dredze, K. Crammer, and F. Pereira, “Conﬁdence-weighted linear
classiﬁcation,” in ICML, Helsinki, 2008, pp. 264–271.
[15] K. Crammer, A. Kulesza, and M. Dredze, “Adaptive regularization
of weight vectors,” in NIPS 2009, 7-10 December, Vancouver, British
Columbia, Canada., 2009, pp. 414–422.
[16] J. C. Duchi and Y. Singer, “Efﬁcient online and batch learning using
(8) forward backward splitting,” Journal of Machine Learning Research,
vol. 10, pp. 2899–2934, 2009.
[17] S. Shalev-Shwartz and T. Zhang, “Stochastic dual coordinate ascent
methods for regularized loss,” Journal of Machine Learning Research,
vol. 14, no. 1, pp. 567–599, 2013.
[18] B. Scholkopf and A. J. Smola, Learning with Kernels: Support Vector
Machines, Regularization, Optimization, and Beyond. Cambridge, MA,
USA: MIT Press, 2001.
[19] J. Kivinen, A. J. Smola, and R. C. Williamson, “Online learning with
kernels,” in NIPS, 2001, pp. 785–792.
[20] K. Crammer, J. S. Kandola, and Y. Singer, “Online classiﬁcation on a
budget,” in NIPS 2003, December 8-13,Vancouver and Whistler, British
Columbia, Canada, 2003, pp. 225–232.
[21] F. Orabona, J. Keshet, and B. Caputo, “Bounded kernel-based online
learning,” Journal of Machine Learning Research, vol. 10, pp. 2643–
2666, 2009.
[22] A. Rahimi and B. Recht, “Random features for large-scale kernel machines,” in NIPS 2007, Vancouver, British Columbia, Canada, December
3-6, 2007, pp. 1177–1184.
[23] C. K. I. Williams and M. W. Seeger, “Using the nyström method to
speed up kernel machines,” in NIPS, 2000, pp. 682–688.
[24] T. Yang, Y. Li, M. Mahdavi, R. Jin, and Z. Zhou, “Nyström method vs
random fourier features: A theoretical and empirical comparison”,” in
NIPS2012, 2012, pp. 485–493.
[25] J. S. Vitter, “Random sampling with a reservoir,” ACM Trans. Math.
Softw., vol. 11, no. 1, pp. 37–57, 1985.
[26] M. Zinkevich, “Online convex programming and generalized inﬁnitesimal gradient ascent,” in ICML 2003, August 21-24, 2003, Washington,
DC, USA, 2003, pp. 928–936.
[27] C. Wang, X. Chen, A. J. Smola, and E. P. Xing, “Variance reduction for
stochastic gradient optimization,” in NIPS 2013, December 5-8, Lake
Tahoe, Nevada, United States., 2013, pp. 181–189.
[28] W. Rudin, Fourier Analysis on Groups, ser. A Wiley-interscience publication. Wiley, 1990.
[29] L. Bo and C. Sminchisescu, “Efﬁcient match kernel between sets
of features for visual recognition,” in NIPS 2009, Vancouver, British
Columbia, Canada, December 7-10, 2009, pp. 135–143.
[30] S. C. H. Hoi, J. Wang, P. Zhao, J. Zhuang, and Z. Liu, “Large scale
online kernel classiﬁcation,” in IJCAI 2013, Beijing, China, August 3-9,
2013.
[31] J. Lu, S. C. Hoi, J. Wang, P. Zhao, and Z.-Y. Liu, “Large scale online
kernel learning,” Journal of Machine Learning Research, vol. 17, no. 47,
pp. 1–43, 2016.
[32] R. Chitta, R. Jin, and A. K. Jain, “Efﬁcient kernel clustering using
random fourier features,” in ICDM 2012, Brussels, Belgium, December
10-13, 2012, pp. 161–170.
[33] M. Raginsky and S. Lazebnik, “Locality-sensitive binary codes from
shift-invariant kernels,” in NIPS 2009, Vancouver, British Columbia,
Canada, December 7-10, 2009, pp. 1509–1517.
[34] B. Schölkopf, R. Herbrich, and A. J. Smola, “A generalized representer
theorem,” in COLT, 2001, pp. 416–426.
[35] C. Cortes, M. Mohri, and A. Talwalkar, “On the impact of kernel
approximation on learning accuracy,” in AISTATS 2010, Chia Laguna
Resort, Sardinia, Italy, May 13-15, 2010, pp. 113–120.