[1] E. Osuna, R. Freund, and F Girosi, “An improved training
algorithm for support vector machines,” in Neural Networks
for Signal Processing [1997] VII. Proceedings of the 1997
IEEE Workshop. YEEE, 1997, pp. 276-285.

[2] T. Joachims, “Making large scale svm learning practical,”
1999.

[3] J. C. Platt, “12 fast training of support vector machines
using sequential minimal optimization,” Advances in kernel
methods, pp. 185-208, 1999.

[4] T.-K. Lin and S.-Y. Chien, “Support vector machines on
gpu with sparse matrix format,” in Machine Learning and
Applications (ICMLA), 2010 Ninth International Conference
on. TEEE, 2010, pp. 313-318.

[5] B. Catanzaro, N. Sundaram, and K. Keutzer, “Fast support
vector machine training and classification on graphics processors,” in Proceedings of the 25th international conference
on Machine learning. ACM, 2008, pp. 104-111.

[6] A. Krizhevsky and G. Hinton, “Learning multiple layers of
features from tiny images,” 2009.

[7] Y. You, J. Demmel, K. Czechowski, L. Song, and R. Vuduc,
“Ca-svm: Communication-avoiding support vector machines
on distributed systems,” in Parallel and Distributed Processing Symposium (IPDPS), 2015 IEEE International. YEEE,
2015, pp. 847-859.

[8] E. Y. Chang, “Psvm: Parallelizing support vector machines on
distributed computers,” in Foundations of Large-Scale Multimedia Information Management and Retrieval. Springer,
2011, pp. 213-230.

[9] Y. You, S. L. Song, H. Fu, A. Marquez, M. M. Dehnavi,
K. Barker, K. W. Cameron, A. P. Randles, and G. Yang, ““Micsvm: Designing a highly efficient support vector machine for
advanced modern multi-core and many-core architectures,” in
Parallel and Distributed Processing Symposium, 2014 IEEE
28th International. YEEE, 2014, pp. 809-818.

[10] Y. You, H. Fu, S. L. Song, A. Randles, D. Kerbyson,
A. Marquez, G. Yang, and A. Hoisie, “Scaling support vector
machines on modern hpc platforms,” Journal of Parallel and
Distributed Computing, vol. 76, pp. 16-31, 2015.

[11] Y. You, X. Lian, J. Liu, H.-E Yu, I. S. Dhillon, J. Demmel,
and C.-J. Hsieh, “Asynchronous parallel greedy coordinate
descent,” in Advances In Neural Information Processing Systems, 2016, pp. 4682-4690.

[12] Y. You, J. Demmel, K. Czechowski, L. Song, and R. Vuduc,
“Design and implementation of a communication-optimal
classifier for distributed kernel support vector machines,”
IEEE Transactions on Parallel and Distributed Systems,
vol. 28, no. 4, pp. 974-988, 2017.

[13] T. Bertin-Mahieux, D. P. Ellis, B. Whitman, and P. Lamere,
“The million song dataset,” in ISMIR 2011: Proceedings of
the 12th International Society for Music Information Retrieval
Conference, October 24-28, 2011, Miami, Florida. University of Miami, 2011, pp. 591-596.

[14] L. group. (2015) Compressed row storage (crs). [Online].
Available: http://netlib.org/linalg/html_templates/node91.html

[15] R. G. Grimes, D. R. Kincaid, W. I. MacGregor, and D. M.
Young, “Itpack report: adaptive iterative algorithms using
symmetric sparse storage,” CNA-139, Center for Numerical
Analysis, University of Texas, Austin, Texas, vol. 78712, 1978.

[16] Y. Saad, SPARSKIT: A basic toolkit for sparse matrix computations. Research Institute for Advanced Computer Science,
NASA Ames Research Center Moffet Field, California, 1990.

[17] Iterative methods for sparse linear systems. Siam, 2003.

[18] A. Rocha and S. Goldenstein, “Multiclass from binary: Expanding one-vs-all, one-vs-one and ecoc-based approaches.”

[19] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradientbased learning applied to document recognition,” Proceedings
of the IEEE, vol. 86, no. 11, pp. 2278-2324, 1998.

[20] I. Guyon, S. Gunn, A. Ben-Hur, and G. Dror, “Result analysis
of the nips 2003 feature selection challenge,” in Advances in
Neural Information Processing Systems, 2004, pp. 545-552.

[21] T. Davis and Y. Hu, “The university of florida sparse matrix
collection,” 2014. [Online]. Available: http://www.cise.ufl.
edu/research/sparse/matrices/

[22] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell, “Caffe: Convolutional
architecture for fast feature embedding,” in Proceedings of the
22nd ACM international conference on Multimedia. ACM,
2014, pp. 675-678.

[23] N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy,
and P. T. P. Tang, “On large-batch training for deep learning: Generalization gap and sharp minima,” arXiv preprint
arXiv: 1609.04836, 2016.

[24] I. Sutskever, J. Martens, G. E. Dahl, and G. E. Hinton, “On the
importance of initialization and momentum in deep learning.”
ICML (3), vol. 28, pp. 1139-1147, 2013.

[25] R. D. Blumofe, C. F. Joerg, B. C. Kuszmaul, C. E. Leiserson,
K. H. Randall, and Y. Zhou, Cilk: An efficient multithreaded
runtime system. ACM, 1995, vol. 30, no. 8.

[26] C.-C. Chang and C.-J. Lin, “Libsvm: a library for support
vector machines,” ACM Transactions on Intelligent Systems
and Technology (TIST), vol. 2, no. 3, p. 27, 2011.

[27] R. Vuduc, J. W. Demmel, and K. A. Yelick, “Oski: A library
of automatically tuned sparse matrix kernels,” in Journal of
Physics: Conference Series, vol. 16, no. 1. IOP Publishing,
2005, p. 521.

[28] S. Sonnenburg, V. Franc, E. Yom-Tov, and M. Sebag, “Pascal large scale learning challenge,’ in 25th International
Conference on Machine Learning (ICML2008) Workshop.
http:/Nargescale. first. fraunhofer. de. J. Mach. Learn. Res,
vol. 10, 2008, pp. 1937-1953.

[29] R.-E. Fan, P.-H. Chen, and C.-J. Lin, “Working set selection
using second order information for training support vector machines,” The Journal of Machine Learning Research,
vol. 6, pp. 1889-1918, 2005.

[30] S. Herrero-Lopez, J. R. Williams, and A. Sanchez, “Parallel
multiclass classification using svms on gpus,” in Proceedings
of the 3rd Workshop on General-Purpose Computation on
Graphics Processing Units. ACM, 2010, pp. 2-11.

[31] Y. You, S. Song, H. Fu, A. Marquez, M. Dehnavi, K. Barker,
K. W. Cameron, A. Randles, and G. Yang, “Mic-svm: Designing a highly efficient support vector machine for advanced
modern multi-core and many-core architectures,” in Parallel
& Distributed Processing (IPDPS), 2014 IEEE 28th International Symposium on. YEEE, 2014.

[32] Y. You, H. Fu, S. L. Song, M. M. Dehnavi, L. Gan,
X. Huang, and G. Yang, “Evaluating multi-core and manycore architectures through accelerating the three-dimensional
lax—wendroff correction stencil,’ The International Journal
of High Performance Computing Applications, vol. 28, no. 3,
pp. 301-318, 2014.

[33] Y. You, H. Fu, X. Huang, G. Song, L. Gan, W. Yu, and
G. Yang, “Accelerating the 3d elastic wave forward modeling
on gpu and mic,” in Parallel and Distributed Processing
Symposium Workshops & PhD Forum (IPDPSW), 2013 IEEE
27th International. YEEE, 2013, pp. 1088-1096.

[34] Y. You, D. Bader, and M. M. Dehnavi, “Designing a heuristic
cross-architecture combination for breadth-first search,’ in
Parallel Processing (ICPP), 2014 43rd International Conference on. JEEE, 2014, pp. 70-79.

[35] Y. You, H. Fu, D. Bader, and G. Yang, “Designing and implementing a heuristic cross-architecture combination for graph
traversal,” Journal of Parallel and Distributed Computing,
2016.