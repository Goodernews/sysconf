[1] Dimitris Achlioptas. 2003. Database-friendly Random Projections: Johnson- Lindenstrauss with Binary Coins. J. Comput. Syst. Sci. 66, 4 (June 2003), 671–687. https://doi.org/10.1016/S0022- 0000(03)00025- 4
[2] Jimmy Ba and Brendan Frey. 2013. Adaptive dropout for training deep neural networks. In Advances in Neural Information Processing Systems 26. 3084–3092.
[3] Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin Chen. 2015. Compressing Neural Networks with the Hashing Trick. In Proceedings of the 32nd International Conference on Machine Learning. 2285–2294.
[4] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. 2016. Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or-1. arXiv preprint arXiv:1602.02830 (2016).
[5] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior, Paul Tucker, Ke Yang, Quoc V Le, et al. 2012. Large scale
[6] Jinyang Gao, Hosagrahar Visvesvaraya Jagadish, Wei Lu, and Beng Chin Ooi. 2014.
DSH: data sensitive hashing for high-dimensional KNN search. In Proceedings of
the 2014 ACM SIGMOD. ACM, 1127–1138.
[7] Aristides Gionis, Piotr Indyk, Rajeev Motwani, et al. 1999. Similarity search in
high dimensions via hashing. VLDB 99, 6 (1999), 518–529.
[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual
Learning for Image Recognition. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR).
[9] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed,
Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N
Sainath, et al. 2012. Deep neural networks for acoustic modeling in speech
recognition: The shared views of four research groups. Signal Processing Magazine,
IEEE 29, 6 (2012), 82–97.
[10] Qiang Huang, Jianlin Feng, Yikai Zhang, Qiong Fang, and Wilfred Ng. 2015.
Query-aware locality-sensitive hashing for approximate nearest neighbor search.
Proceedings of the VLDB Endowment 9, 1 (2015), 1–12.
Removing the Curse of Dimensionality. In STOC. Dallas, TX, 604–613.
[12] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classifica-
tion with deep convolutional neural networks. In Advances in neural information
processing systems. 1097–1105.
[13] Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua
Bengio. 2007. An empirical evaluation of deep architectures on problems with
many factors of variation. In Proceedings of the 24th international conference on
Machine learning. ACM, 473–480.
[14] Yann LeCun, Fu Jie Huang, and Leon Bottou. 2004. Learning methods for generic
object recognition with invariance to pose and lighting. In Computer Vision
and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer
Society Conference on, Vol. 2. IEEE, II–97.
[15] Jure Leskovec, Anand Rajaraman, and Jeffrey David Ullman. 2014. Mining of
massive datasets. Cambridge University Press.
[16] Ping Li, Trevor J Hastie, and Kenneth W Church. 2006. Very sparse random
projections. In Proceedings of the 12th ACM SIGKDD international conference on
Knowledge discovery and data mining. ACM, 287–296.
[17] Zhouhan Lin, Matthieu Courbariaux, Roland Memisevic, and Yoshua Bengio.
2015. Neural Networks with Few Multiplications. arXiv preprint arXiv:1510.03009
(2015).
[18] Gaëlle Loosli, Stéphane Canu, and Léon Bottou. 2007. Training Invariant Support
Vector Machines using Selective Sampling. In Large Scale Kernel Machines, Léon
Bottou, Olivier Chapelle, Dennis DeCoste, and Jason Weston (Eds.). MIT Press,
Cambridge, MA., 301–320. http://leon.bottou.org/papers/loosli-canu-bottou-2006
[19] Qin Lv, William Josephson, Zhe Wang, Moses Charikar, and Kai Li. 2007. Multi-
probe LSH: efficient indexing for high-dimensional similarity search. In Pro-
ceedings of the 33rd international conference on Very large data bases. VLDB
Endowment, 950–961.
[20] Alireza Makhzani and Brendan Frey. 2013. k-Sparse Autoencoders. arXiv preprint
[21] Alireza Makhzani and Brendan J Frey. 2015. Winner-Take-All Autoencoders. In

arXiv:1312.5663 (2013).
Advances in Neural Information Processing Systems 28. 2791–2799.
[22] H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. 2016. Communication-Efficient Learning of Deep Net-
works from Decentralized Data. In Proceedings of the 20th International Conference
on Artificial Intelligence and Statistics (AISTATS). http://arxiv.org/abs/1602.05629
[23] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. 2011. Hogwild:
A lock-free approach to parallelizing stochastic gradient descent. In Advances in
Neural Information Processing Systems. 693–701.
[24] Anshumali Shrivastava. 2016. Simple and Efficient Weighted Minwise Hashing.
In Advances in Neural Information Processing Systems 29. 1498–1506.
[25] Anshumali Shrivastava and Ping Li. 2014. Asymmetric LSH (ALSH) for sublinear
time maximum inner product search (MIPS). In Advances in Neural Information
Processing Systems. 2321–2329.
[26] Anshumali Shrivastava and Ping Li. 2014. Improved Densification of One Permu-
tation Hashing. In UAI. Quebec, CA.
[27] Anshumali Shrivastava and Ping Li. 2015. Improved Asymmetric Locality Sensi-
tive Hashing (ALSH) for Maximum Inner Product Search (MIPS). In Conference
on Uncertainty in Artificial Intelligence (UAI).
[28] Vikas Sindhwani, Tara Sainath, and Sanjiv Kumar. 2015. Structured transforms
for small-footprint deep learning. In Advances in Neural Information Processing
Systems. 3070–3078.
[29] Ryan Spring and Anshumali Shrivastava. 2017. A New Unbiased and Efficient
Class of LSH-Based Samplers and Estimators for Partition Function Computation
in Log-Linear Models. arXiv preprint arXiv:1703.05160 (2017).
[30] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from
overfitting. The Journal of Machine Learning Research 15, 1 (2014), 1929–1958.

