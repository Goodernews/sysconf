[1] Caffe 2. https://caffe2.ai. Accessed: 2017-04-21.

[2] Cuda multi-process service. https:
//docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf.
Accessed: 2017-04-27.

[3] PyTorch. http://pytorch.org. Accessed: 2017-04-21.

[4] Tensorflowonspark. https://github.com/yahoo/TensorFlowOnSpark. Accessed:
2017-04-21.

[5] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat,

G. Irving, M. Isard, M. Kudluy, J. Levenberg, R. Monga, S. Moore, D. G. Murray,

B. Steiner, P. Tucker, V. Vasudevan, P. Warden, M. Wicke, Y. Yu, and X. Zheng,

TensorFlow: A system for large-scale machine learning. In OSDI, 2016.

[6] O. Abdel-Hamid, A.-R. Mohamed, H. Jiang, and G. Penn. Applying

convolutional neural networks concepts to hybrid NN-HMM model for speech

recognition. In ICASSP, 2012.

[7] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici, B. Varadarajan, and
S. Vijayanarasimhan. Youtube-8m: A large-scale video classification benchmark.
arXiv preprint arXiv: 1609.08675, 2016.

[8] A. A. Awan, K. Hamidouche, J. M. Hashmi, and D. K. Panda. S-Caffe:
Co-designing MPI runtimes and Caffe for scalable deep learning on modern
GPU clusters. In PPoPP, 2017.

[9] R. Bekkerman, M. Bilenko, and J. Langford. Scaling up machine learning: Parallel
and distributed approaches. Cambridge University Press, 2011.

[10] J. Bergstra, F. Bastien, O. Breuleux, P. Lamblin, R. Pascanu, O. Delalleau,

G. Desjardins, D. Warde-Farley, I. Goodfellow, A. Bergeron, and Y. Bengio.
Theano: Deep learning on GPUs with Python. In BigLearn, NIPS Workshop, 2011.

[11] K.J. Brown, H. Lee, T. Rompf, A. K. Sujeeth, C. De Sa, C. Aberger, and
K. Olukotun. Have abstraction and eat performance, too: Optimized
heterogeneous computing with parallel patterns. In CGO, 2016.

[12] S. Chaudhuri. An overview of query optimization in relational systems. In
SIGMOD/PODS, 1998.

[13] S. Chaudhuri and V. R. Narasayya. An efficient, cost-driven index selection tool
for Microsoft SQL server. In VLDB, 1997.

[14] K. Chellapilla, S. Puri, and P. Simard. High performance convolutional neural
networks for document processing. In ICFHR, 2006.

[15] T. Chen, Z. Du, N. Sun, J. Wang, C. Wu, Y. Chen, and O. Temam. DianNao: A
small-footprint high-throughput accelerator for ubiquitous machine-learning. In
ACM SIGPLAN Notices, 2014.

[16] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang, and
Z. Zhang, MXNet: A flexible and efficient machine learning library for
heterogeneous distributed systems. arXiv preprint arXiv:1512.01274, 2015.

[17] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A Matlab-like
environment for machine learning. In BigLearn, NIPS Workshop, 2011.

[18] J. Currey, A. Eversole, and C. Rossbach. Scheduling dataflow execution across
multiple accelerators. In SEMA Workshop, 2014.

[19] G.E. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent pre-trained deep
neural networks for large-vocabulary speech recognition. TASLP, 20(1):30-42,
2012.

[20] J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin, Q. V. Le, M. Z. Mao,

M. Ranzato, A. Senior, P. Tucker, K. Yang, and A. Y. Ng. Large scale distributed
deep networks. In NIPS, 2012.

[21] L. Deng and D. Yu. Deep learning: Methods and applications. Foundations and

Trends in Signal Processing, 7(3—4):197—-387, 2014.

[22] A. Griewank and A. Walther. Evaluating derivatives: Principles and techniques of
algorithmic differentiation. SLAM, 2008.

[23] T. Grust. Monad comprehensions: A versatile representation for queries. In The
Functional Approach to Data Management, 2004.

[24] B. Guenter. Efficient symbolic differentiation for graphics applications. TOG,
26(3):108, 2007.

[25] J. Hauswald, Y. Kang, M. A. Laurenzano, Q. Chen, C. Li, T. Mudge, RG.
Dreslinski, J. Mars, and L. Tang. DjiNN and Tonic: DNN as a service and its
implications for future warehouse scale computers. In ISCA, 2015.

[26] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image
recognition. arXiv preprint arXiv:1512.03385, 2015.

[27] B. Hindman, A. Konwinski, M. Zaharia, A. Ghodsi, A. D. Joseph, R. Katz,

S. Shenker, and I. Stoica. Mesos: A platform for fine-grained resource sharing in
the data center. In NSDF, 2011.

[28] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-R. Mohamed, N. Jaitly, A. Senior,

V. Vanhoucke, P. Nguyen, T. N. Sainath, and B. Kingsbury. Deep neural
networks for acoustic modeling in speech recognition: The shared views of four
research groups. IEEE Signal Processing Magazine, 29(6):82-97, 2012.

[29] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural
computation, 9(8):1735-1780, 1997.

[30] K. Irick, M. DeBole, V. Narayanan, and A. Gayasen. A hardware efficient support
vector machine architecture for FPGA. In FCCM, 2008.

[31] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, $. Guadarrama,
and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In
ACMMM, 2014.

[32] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, S. Bates,

S. Bhatia, N. Boden, A. Borchers, et al. In-datacenter performance analysis of a
Tensor Processing Unit. In ISCA, 2017.

[33] K. Kang, H. Li, J. Yan, X. Zeng, B. Yang, T. Xiao, C. Zhang, Z. Wang, R. Wang,
X. Wang, and W. Ouyang. T-CNN: Tubelets with convolutional neural networks
for object detection from videos. arXiv preprint arXiv:1604.02532, 2016.

[34] K. Kang, W. Ouyang, H. Li, and X. Wang. Object detection from video tubelets
with convolutional neural networks. In CVPR, 2016.

[35] C. Kim and J.-N. Hwang. Fast and automatic video object segmentation and
tracking for content-based applications. IEEE Transactions on Circuits and
Systems for Video Technology, 12(2):122-129, 2002.

[36] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification with deep
convolutional neural networks. In NIPS, 2012.

[37] Y. LeCun and Y. Bengio. Convolutional networks for images, speech, and time
series. The handbook of brain theory and neural networks, 3361(10):1995, 1995.

[38] T. Lengauer and R. E. Tarjan. A fast algorithm for finding dominators in a
flowgraph. ACM TOPLAS, 1(1):121-141, Jan. 1979.

[39] N. P. Lopes, D. Menendez, S. Nagarakatte, and J. Regehr. Provably correct
peephole optimizations with alive. ACM SIGPLAN Notices, 50(6):22-32, 2015.

[40] P. Moritz, R. Nishihara, L Stoica, and M. L Jordan. SparkNet: Training deep
networks in Spark. arXiv preprint arXiv:1511.06051, 2015.

[41] S. Palkar, J.J. Thomas, A. Shanbhag, D. Narayanan, H. Pirk, M. Schwarzkopf,

S. Amarasinghe, and M. Zaharia. Weld: A common runtime for high
performance data analytics. In CIDR, 2017.

[42] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards real-time object
detection with region proposal networks. arXiv preprint arXiv:1506.01497, 2015.

[43] T. Rompf, A. K. Sujeeth, N. Amin, K. J. Brown, V. Jovanovic, H. Lee,

M. Jonnalagedda, K. Olukotun, and M. Odersky. Optimizing data structures in
high-level programs: New directions for extensible compilers based on staging.
ACM SIGPLAN Notices, 48(1):497-510, 2013.

[44] R. Sochey, C. C. Lin, C. Manning, and A. Y. Ng. Parsing natural scenes and
natural language with recursive neural networks. In ICML, 2011.

[45] Y. Sui and J. Xue. SVF: Interprocedural static value-flow analysis in llvm. In CC,
2016.

[46] I. Sutskever, J. Martens, and G. E. Hinton. Generating text with recurrent neural
networks. In ICML, 2011.

[47] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural
networks. In NIPS, 2014.

[48] L. Truong, R. Barik, E. Totoni, H. Liu, C. Markley, A. Fox, and T. Shpeisman.
Latte: A language, compiler, and runtime for elegant and efficient deep neural
networks. In PLDE, 2016.

[49] V.K. Vavilapalli, A.C. Murthy, C. Douglas, S. Agarwal, M. Konar, R. Evans,

T. Graves, J. Lowe, H. Shah, S. Seth, B. Saha, C. Curino, O. O’Malley, S. Radia,
B. Reed, and E. Baldeschwieler. Apache Hadoop YARN: Yet another resource
negotiator. In SOCC, 2013.

[50] P. Viola and M. Jones. Rapid object detection using a boosted cascade of simple
features. In CVPR, 2001.

[51] W. Wang, G. Chen, T. Dinh, J. Gao, B. Ooi, and K. Tan. SINGA: A distributed
system for deep learning. Technical report, NUS Tech Report, 2015.

[52] M. N. Wegman and F. K. Zadeck. Constant propagation with conditional
branches. ACM TOPLAS, 13(2):181-210, Apr. 1991.

[53] D. Yu, A. Eversole, M. Seltzer, K. Yao, O. Kuchaiev, Y. Zhang, F. Seide, Z. Huang,
B. Guenter, H. Wang, J. Droppo, G. Zweig, C. Rossbach, J. Gao, A. Stolcke,

J. Currey, M. Slaney, G. Chen, A. Agarwal, C. Basoglu, M. Padmilac, A. Kamenev,
V. Ivanov, S. Cypher, H. Parthasarathi, B. Mitra, B. Peng, and X. Huang. An
introduction to computational networks and the computational network toolkit.
Technical report, Microsoft Research, October 2014.

[54] D. Yu and M. L. Seltzer. Improved bottleneck features using pretrained deep
neural networks. In Interspeech, pages 237-240, 2011.

[55] C. Zhang, P. Li, G. Sun, Y. Guan, B. Xiao, and J. Cong. Optimizing FPGA-based
accelerator design for deep convolutional neural networks. In FPGA, 2015.

[56] M. Zhu, L. Liu, C. Wang, and Y. Xie. CNNLab: a novel parallel framework for
neural networks using GPU and FPGA-a practical study with trade-off analysis.
arXiv preprint arXiv: 1606.06234, 2016.