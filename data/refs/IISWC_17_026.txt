[1] . Lindholm, J. Nickolls, S. Oberman, and J. Montrym, “NVIDIA Tesla:
A unified graphics and computing architecture,” IEEE micro, vol. 28,
no. 2, 2008.

[2] “AMD FirePro Accelerators for High Performance Compute,” http://
www.amd.com/en-us/solutions/professional/hpc, 2017.

[3] J. Nickolls and W. J. Dally, “The GPU Computing Era,” Micro, IEEE,
vol. 30, no. 2, 2010, pp. 56-69.

[4] L. Wang, M. Huang, and T. El-Ghazawi, “Exploiting concurrent kernel
execution on graphic processing units,” in High Performance Computing
and Simulation (HPCS), 2011 International Conference on. IEEE,
2011, pp. 24-32.

[5] C. Gregg, J. Dorn, K. M. Hazelwood, and K. Skadron, “Fine-grained
resource sharing for concurrent gpgpu kernels.” in HotPar, 2012.

[6] NVIDIA, “Nvidia’s Next Generation CUDA™ Compute Architecture,
Kepler™ GK110,” 2012.

[7] S. Che, M. Boyer, J. Meng, D. Tarjan, J. W. Sheaffer, S.-H. Lee,
and K. Skadron, “Rodinia: A benchmark suite for heterogeneous
computing,” in Workload Characterization, 2009. ISWC 2009. IEEE
International Symposium on. IEEE, 2009, pp. 44-54.

[8] K. Datta, M. Murphy, V. Volkov, S. Williams, J. Carter, L. Oliker,
D. Patterson, J. Shalf, and K. Yelick, “Stencil computation optimization
and auto-tuning on state-of-the-art multicore architectures,” in Proceedings of the 2008 ACM/IEEE conference on Supercomputing. IEEE
Press, 2008, p. 4.

[9] 8S. Kamil, C. Chan, L. Oliker, J. Shalf, and S. Williams, “An auto-tuning
framework for parallel multicore stencil computations,” in Parallel &
Distributed Processing (IPDPS), 2010 IEEE International Symposium
on. IEEE, 2010, pp. 1-12.

[10] Y. Torres, A. Gonzalez-Escribano, and D. R. Llanos, “Understanding
the impact of cuda tuning techniques for fermi,” in High Performance
Computing and Simulation (HPCS), 2011 International Conference on.
TEEE, 2011, pp. 631-639.

[11] NVIDIA, “CUDA Profiler Users Guide (Version 8.0),” 2016.

[12]  Y. Jiao, H. Lin, P. Balaji, and W.-c. Feng, “Power and performance characterization of computational kernels on the gpu,” in Green Computing
and Communications (GreenCom), 2010 IEEE/ACM Int’! Conference
on & Int'l Conference on Cyber, Physical and Social Computing
(CPSCom). IEEE, 2010, pp. 221-228.

[13] A. Bakhoda, G. L. Yuan, W. W. Fung, H. Wong, and T. M. Aamodt,
“Analyzing cuda workloads using a detailed gpu simulator,” in Performance Analysis of Systems and Software, 2009. ISPASS 2009. IEEE
International Symposium on. TEEE, 2009, pp. 163-174.

[14] R. Ubal, B. Jang, P. Mistry, D. Schaa, and D. Kaeli, “Multi2sim: a
simulation framework for cpu-gpu computing,” in Proceedings of the
21st international conference on Parallel architectures and compilation
techniques. ACM, 2012, pp. 335-344.
[15] A. Kerr, G. Diamos, and S. Yalamanchili, “A characterization and
analysis of ptx kernels,” in Workload Characterization, 2009. IISWC
2009. IEEE International Symposium on. IEEE, 2009, pp. 3–12.
[16] M. Stephenson, S. K. Sastry Hari, Y. Lee, E. Ebrahimi, D. R. Johnson,
D. Nellans, M. O’Connor, and S. W. Keckler, “Flexible software
profiling of gpu architectures,” in Proceedings of the 42nd Annual
International Symposium on Computer Architecture. ACM, 2015, pp.
185–197.
[17] M. Stephenson and D. R. Johnson, “SASSI,” https://github.com/NVlabs/
SASSI, 2015.
[18] NVIDIA, “CUDA Binary Utilities,” 2017.
[19] NVIDIA, “CUDA Toolkit,” 2017.
[20] X. Mei and X. Chu, “Dissecting GPU memory hierarchy through
microbenchmarking,” IEEE Transactions on Parallel and Distributed
Systems, vol. 28, no. 1, 2017, pp. 72–86.
[21] L. v. d. Maaten and G. Hinton, “Visualizing data using t-sne,” Journal
of Machine Learning Research, vol. 9, no. Nov, 2008, pp. 2579–2605.
[22] B. Van Werkhoven, J. Maassen, F. J. Seinstra, and H. E. Bal, “Performance models for cpu-gpu data transfers,” in Cluster, Cloud and Grid
Computing (CCGrid), 2014 14th IEEE/ACM International Symposium
on. IEEE, 2014, pp. 11–20.
[23] Q. Jiao, M. Lu, H. P. Huynh, and T. Mitra, “Improving gpgpu energyefficiency through concurrent kernel execution and dvfs,” in Code
Generation and Optimization (CGO), 2015 IEEE/ACM International
Symposium on. IEEE, 2015, pp. 1–11.
[24] S. Pai, M. J. Thazhuthaveetil, and R. Govindarajan, “Improving GPGPU
concurrency with elastic kernels,” in ACM SIGPLAN Notices, vol. 48,
no. 4. ACM, 2013, pp. 407–418.
[25] Y. Ukidave, X. Li, and D. Kaeli, “Mystic: Predictive scheduling for gpu
based cloud servers using machine learning,” in Parallel and Distributed
Processing Symposium, 2016 IEEE International. IEEE, 2016, pp.
353–362.
[26] X. Gong, Z. Chen, A. K. Ziabari, R. Ubal, and D. Kaeli, “Twinkernels:
an execution model to improve gpu hardware scheduling at compile
time,” in Proceedings of the 2017 International Symposium on Code
Generation and Optimization. IEEE Press, 2017, pp. 39–49.
[27] D. Tarjan, K. Skadron, and P. Micikevicius, “The art of performance
tuning for cuda and manycore architectures,” Birds-of-a-feather session
at SC, vol. 9, 2009.
[28] Q. Fang and D. A. Boas, “Monte carlo simulation of photon migration
in 3d turbid media accelerated by graphics processing units,” Optics
express, vol. 17, no. 22, 2009, pp. 20 178–20 190.
[29] L. Yu, Y. Ukidave, and D. Kaeli, “Gpu-accelerated hmm for speech
recognition,” in Parallel Processing Workshops (ICCPW), 2014 43rd
International Conference on. IEEE, 2014, pp. 395–402.
[30] S. Hong and H. Kim, “An analytical model for a gpu architecture
with memory-level and thread-level parallelism awareness,” in ACM
SIGARCH Computer Architecture News, vol. 37, no. 3. ACM, 2009,
pp. 152–163.
[31] F. Wende, F. Cordes, and T. Steinke, “On improving the performance of
multi-threaded cuda applications with concurrent kernel execution by
kernel reordering,” in Application Accelerators in High Performance
Computing (SAAHPC), 2012 Symposium on, July 2012, pp. 74–83.
[32] F. Wende, T. Steinke, and F. Cordes, “Multi-threaded kernel offloading
to GPGPU using Hyper-Q on kepler architecture,” ZIB-Rep. June 2014.
[33] Y. Liang, H. Huynh, K. Rupnow, R. Goh, and D. Chen, “Efficient gpu
spatial-temporal multitasking,” 2014.
[34] A. Jog, E. Bolotin, Z. Guz, M. Parker, S. W. Keckler, M. T. Kandemir,
and C. R. Das, “Application-aware memory system for fair and efficient execution of concurrent gpgpu applications,” in Proceedings of
Workshop on General Purpose Processing Using GPUs. ACM, 2014.
[35] X. Shen, H. Zhou, and G. Chen, “A software framework for efficient
preemptive scheduling on gpu,” North Carolina State University. Dept.
of Computer Science, Tech. Rep., 2016.
[36] J. J. K. Park, Y. Park, and S. Mahlke, “Chimera: Collaborative preemption for multitasking on a shared gpu,” ACM SIGARCH Computer
Architecture News, vol. 43, no. 1, 2015, pp. 593–606.
