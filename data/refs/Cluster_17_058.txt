[1] S. Kanev, J. P. Darago, K. Hazelwood, P. Ranganathan,
T. Moseley, G.-Y. Wei, and D. Brooks, “Proﬁling a
warehouse-scale computer,” SIGARCH Comput. Archit. News,
vol. 43, no. 3, pp. 158–169, Jun. 2015.

[2] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauley, M. J. Franklin, S. Shenker, and I. Stoica, “Resilient distributed datasets: A fault-tolerant abstraction for
in-memory cluster computing,” in Proceedings of the 9th
USENIX Conference on Networked Systems Design and Implementation, ser. NSDI’12. Berkeley, CA, USA: USENIX
Association, 2012, pp. 2–2.

[3] J. Dean and S. Ghemawat, “Mapreduce: Simpliﬁed data
processing on large clusters,” in Proceedings of the 6th
Conference on Symposium on Opearting Systems Design &
Implementation - Volume 6, ser. OSDI’04. Berkeley, CA,
USA: USENIX Association, 2004, pp. 10–10.


[4] M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly, “Dryad:
Distributed data-parallel programs from sequential building
blocks,” in Proceedings of the 2Nd ACM SIGOPS/EuroSys
European Conference on Computer Systems 2007, ser. EuroSys ’07. New York, NY, USA: ACM, 2007, pp. 59–72.


[5] H. Kim, S. Wang, and R. Rajkumar, “Responsive and enforced interrupt handling for real-time system virtualization,” in 21st IEEE International Conference on Embedded
and Real-Time Computing Systems and Applications, RTCSA
2015, Hong Kong, China, August 19-21, 2015, 2015, pp. 90–
99.

[6] I. Ahmad, A. Gulati, and A. Mashtizadeh, “vic: Interrupt
coalescing for virtual machine storage device io,” in Proceedings of the 2011 USENIX Conference on USENIX Annual
Technical Conference, ser. USENIXATC’11. Berkeley, CA,
USA: USENIX Association, 2011, pp. 4–4.


[7] X. Chang, J. K. Muppala, Z. Han, and J. Liu, “Analysis
of interrupt coalescing schemes for receive-livelock problem
in gigabit ethernet network hosts,” in Proceedings of IEEE
International Conference on Communications, ICC 2008,
Beijing, China, 19-23 May 2008, 2008, pp. 1835–1839.
[8] T. Hirofuchi and R. Takano, “Raminate: Hypervisor-based
virtualization for hybrid main memory systems,” in SoCC.
ACM, 2016, pp. 112–125.

[9] N. Li, H. Jiang, D. Feng, and Z. Shi, “Pslo: Enforcing the
xth percentile latency and throughput slos for consolidated
vm storage,” in EuroSys. ACM, 2016, pp. 28:1–28:14.
[10] K. Rajan, D. Kakadia, C. Curino, and S. Krishnan, “Perforator: Eloquent performance models for resource optimization,”
in SoCC. ACM, 2016, pp. 415–427.

[11] R. R. Sambasivan, I. Shafer, J. Mace, B. H. Sigelman,
R. Fonseca, and G. R. Ganger, “Principled workﬂow-centric
tracing of distributed systems,” in SoCC. ACM, 2016, pp.
401–414.
[12] P. Wang, H. Xu, Z. Niu, D. Han, and Y. Xiong, “Expeditus: Congestion-aware load balancing in clos data center
networks,” in SoCC. ACM, 2016, pp. 442–455.

[13] J. Zhi, N. Bila, and E. de Lara, “Oasis: Energy proportionality
with hybrid server consolidation,” in EuroSys. ACM, 2016,
pp. 10:1–10:13.
[14] T. Zhu, D. S. Berger, and M. Harchol-Balter, “Snc-meister:
Admitting more tenants with tail latency slos,” in SoCC, 2016,
pp. 374–387.
[15] J. Duggan, Y. Chi, H. Hacigümüs, S. Zhu, and U. Çetintemel,
“Packing light: Portable workload performance prediction for
the cloud,” in ICDE, 2013, pp. 258–265.
[16] J. Duggan, U. Cetintemel, O. Papaemmanouil, and E. Upfal,
“Performance prediction for concurrent database workloads,”
in SIGMOD. ACM, 2011, pp. 337–348.
[17] H. Hacigumus, Y. Chi, W. Wu, S. Zhu, J. Tatemura, and J. F.
Naughton, “Predicting query execution time: Are optimizer
cost models really unusable?” in ICDE. IEEE CS, 2013, pp.
1081–1092.
[18] G. W. Dunlap, S. T. King, S. Cinar, M. A. Basrai, and
P. M. Chen, “Revirt: Enabling intrusion analysis through
virtual-machine logging and replay,” SIGOPS Oper. Syst. Rev.,
vol. 36, no. SI, pp. 211–224, Dec. 2002.
[19] S. T. Jones, A. C. Arpaci-Dusseau, and R. H. Arpaci-Dusseau,
“Geiger: Monitoring the buffer cache in a virtual machine
environment,” SIGARCH Comput. Archit. News, vol. 34,
no. 5, pp. 14–24, Oct. 2006.
[20] A. Melekhova and L. Markeeva, “Estimating working set
size by guest os performance counters means,” in CLOUD
COMPUTING 2015, The Sixth International Conference on
Cloud Computing, GRIDs, and Virtualization, 2015, pp. 33–
38.
[21] “HI
Data
Archive
at
Berkeley,”
https://purcell.ssl.berkeley.edu/, accessed: 7-May-2017.
[22] J. Eli Goldston Peek, B. L. Babler, K. A. Douglas, Y. Zheng,
S. Clark, M. E. Putman, S. Stanimirovic, C. E. Heiles, S. J.
Gibson, and E. J. Korpela, “The Galactic Arecibo L-Band
Feed Array Survey Data Release 2,” in American Astronomical Society Meeting Abstracts, ser. American Astronomical
Society Meeting Abstracts, vol. 227, Jan. 2016, p. 347.08.
[23] P. A. Henning, C. M. Springob, R. F. Minchin, E. Momjian,
B. Catinella, T. McIntyre, F. Day, E. Muller, B. Koribalski,
J. L. Rosenberg, S. Schneider, L. Staveley-Smith, and W. van
Driel, “The arecibo l-band feed array zone of avoidance
survey. i. precursor observations through the inner and outer
galaxy,” The Astronomical Journal, vol. 139, no. 6, p. 2130,
2010.

[24] L. Cheng, J. Rao, and F. C. M. Lau, “vscale: Automatic
and efﬁcient processor scaling for smp virtual machines,”
in Proceedings of the Eleventh European Conference on
Computer Systems, ser. EuroSys ’16. New York, NY, USA:
ACM, 2016, pp. 2:1–2:14.

[25] P. Lu and K. Shen, “Virtual machine memory access tracing
with hypervisor exclusive cache,” in 2007 USENIX Annual
Technical Conference on Proceedings of the USENIX Annual
Technical Conference, ser. ATC’07. Berkeley, CA, USA:
USENIX Association, 2007, pp. 3:1–3:15.

[26] Q. Huang, J. Xia, M. Sun, K. Liu, J. Li, Z. Gui, C. Xu,
Q. Yang, Chaowei, J. Xia, M. Sun, K. Liu, J. Li, Z. Gui,
C. Xu, and C. Yang, “How to test the readiness of open-source
cloud computing solutions,” in Spatial Cloud Computing,
C. Yang and Q. Huang, Eds. Oxford: CRC Press, 2013,
ch. 14, pp. 241–260.
[27] C. C. Eglantine, NBench.
9786136257211.

TypPRESS, 2012, iSBN:

[28] A.
Kopytov,
“SysBench
http://sysbench.sourceforge.net/.
[Online].
http://sysbench.sourceforge.net/

software,”
Available:

[29] J. D. McCalpin, “Memory Bandwidth and Machine Balance
in Current High Performance Computers,” IEEE CS TCCA
Newsletter, pp. 19–25, Dec. 1995.
[30] ——, “STREAM: Sustainable Memory Bandwidth in High
Performance Computers,” University of Virginia, Charlottesville, Virginia, Tech. Rep., 2007.
[31] L. Bergstrom, “Measuring numa effects with the stream
benchmark,” CoRR, vol. abs/1103.3225, 2011.
[32] P.
J.
Mucci,
K.
London,
and
P.
J.
Mucci,
“The
CacheBench
Report,”
URL:
www.earth.lsa.umich.edu/ keken/benchmarks/
cachebench.pdf, Retrieved: February, 2016.
[33] W. Norcott and D. Capps, “IOzone Filesystem Benchmark,”
URL: www.iozone.org, Retrieved: February, 2016.
[34] M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed,
V. Josifovski, J. Long, E. J. Shekita, and B.-Y. Su, “Scaling
distributed machine learning with the parameter server,” in
Proceedings of the 11th USENIX Conference on Operating
Systems Design and Implementation, ser. OSDI’14. Berkeley,
CA, USA: USENIX Association, 2014, pp. 583–598.
[35] P. S. Bradley and O. L. Mangasarian, “Feature selection
via concave minimization and support vector machines,” in
Proceedings of the Fifteenth International Conference on
Machine Learning, ser. ICML ’98. San Francisco, CA, USA:
Morgan Kaufmann Publishers Inc., 1998, pp. 82–90.
[36] V. Bolón-Canedo, D. Peteiro-Barral, A. Alonso-Betanzos,
B. Guijarro-Berdiñas, and N. Sánchez-Maroño, Scalability
Analysis of ANN Training Algorithms with Feature Selection.
Berlin, Heidelberg: Springer Berlin Heidelberg, 2011, pp. 84–
93.