[1] “BigDL,” https://github.com/intel-analytics/BigDL.

[2] “CaffeOnSpark,” https://github.com/yahoo/CaffeOnS park.

[3] “CIFAR-10 Quick,” https://github.com/yahoo/CaffeOnSpark/blob/maste
1/data/cifar 10_quick_train_test.prototxt.

[4] “CIFAR Job With 4 Spark Executors And Above Crashes,”
https://github.com/yahoo/TensorFlowOnS park/issues/8 1#issuecomment302464649.

[5] “cuda-convnet,” https://code.google.com/archive/p/cuda-convnet/.
[6] “Deeplearning4j,” http://deeplearning4j.org.

[7] “IMAGENET,” http://www.image-net.org/.

[8] “LMDB,” https://symas.com/lightning-memory-mapped-database/.

[9] “TensorFlow,” http://tensorflow.org/.

[10] “TensorFrames,” https://github.com/databricks/tensorframes.
[11] “TesnorFlowOnSpark,” https://github.com/yahoo/TensorFlowOnSpark.
[12] “The CIFAR-10 Dataset,” https://www.cs.toronto.edu/ kriz/cifar.html.
[13] “THE MNIST DATABASE,” http://yann.lecun.com/exdb/mnist/.
[14] “Yahoo Flickr Creative Commons 100M,”
https://webscope.sandbox. yahoo.com/catalog.php?dataty pe=i&did=67.
[15] “CNTK,” Feb. 2017. [Online]. Available: http://www.cntk.ai/

[16] A. Awan, K. Hamidouche, J. Hashmi, and D. K. Panda, “S-Caffe:
Co-designing MPI Runtimes and Caffe for Scalable Deep Learning
on Modern GPU Clusters,” in 22nd ACM SIGPLAN Symposium on
Principles and Practice of Parallel Programming, February 2017.
[17] X.-W. Chen and X. Lin, “Big Data Deep Learning: Challenges and
Perspectives,” vol. 2. IEEE, 2014, pp. 514-525.

[18] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B. Catanzaro,
and E. Shelhamer, “cuDNN: Efficient Primitives for Deep Learning,”
2014.

[19] W. Chong, D. Blei, and F.-F. Li, “Simultaneous Image Classification and
Annotation,” in Computer Vision and Pattern Recognition, 2009. CVPR
2009. IEEE Conference on. YEEE, 2009, pp. 1903-1910.

[20] N. S. Islam, X. Lu, M. Wasi-ur Rahman, D. Shankar, and D. K. Panda,
“Triple-H: A Hybrid Approach to Accelerate HDFS on HPC Clusters
with Heterogeneous Storage Architecture,” in Cluster, Cloud and Grid
Computing (CCGrid), 2015 15th IEEE/ACM International Symposium
on. TYEEE, 2015, pp. 101-110.

[21] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for
fast feature embedding,” in Proceedings of the 22nd ACM international
conference on Multimedia. ACM, 2014, pp. 675-678.

[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet Classification
with Deep Convolutional Neural Networks,” in Advances in neural
information processing systems, 2012, pp. 1097-1105.

[23] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based
Learning Applied to Document Recognition,’ Proceedings of the IEEE,
vol. 86, no. 11, pp. 2278-2324, 1998.

[24] X. Lu, D. Shankar, S. Gugnani, and D. K. D. K. Panda, “HighPerformance Design of Apache Spark with RDMA and Its Benefits
on Various Workloads,” in 2016 IEEE International Conference on Big
Data (Big Data), Dec 2016, pp. 253-262.

[25] X. Lu, N. S. Islam, M. W. Rahman, J. Jose, H. Subramoni, H. Wang,
and D. K. Panda, “High-Performance Design of Hadoop RPC with
RDMA over InfiniBand,” in The Proceedings of IEEE 42nd International
Conference on Parallel Processing (ICPP), France, October 2013.

[26] X. Lu, M. W. U. Rahman, N. Islam, D. Shankar, and D. K. Panda,
“Accelerating Spark with RDMA for Big Data Processing: Early Experiences,” in High-performance interconnects (HOTI), 2014 IEEE 22nd
annual symposium on. JEEE, 2014, pp. 9-16.

[27] R. L. Moore, C. Baru, D. Baxter, G. C. Fox, A. Majumdar, P. Papadopoulos, W. Pfeiffer, R. S. Sinkovits, S. Strande, M. Tatineni et al., “Gateways
to Discovery: Cyberinfrastructure for the Long Tail of Science,” in
Proceedings of the 2014 Annual Conference on Extreme Science and
Engineering Discovery Environment. ACM, 2014, p. 39.

[28] P. Moritz, R. Nishihara, I. Stoica, and M. I. Jordan, “Sparknet: Training
Deep Networks in Spark,” 2015.

[29] M. M. Najafabadi, F. Villanustre, T. M. Khoshgoftaar, N. Seliya,
R. Wald, and E. Muharemagic, “Deep Learning Applications and
Challenges in Big Data Analytics,” vol. 2, no. 1. Springer International
Publishing, 2015, p. 1.

[30] K. Simonyan and A. Zisserman, “Very Deep Convolutional Networks
for Large-scale Image Recognition,” 2014.

[31] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V. Vanhoucke, and A. Rabinovich, “Going Deeper with Convolutions,”
in Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2015, pp. 1-9.

[32] A. Vishnu, C. Siegel, and J. Daily, “Distributed TensorFlow with MPI,”
CoRR, vol. abs/1603.02339, 2016.

[33] E. Wang, Q. Zhang, B. Shen, G. Zhang, X. Lu, Q. Wu, and Y. Wang,
“Intel Math Kernel Library,” in High-Performance Computing on the
Intel® Xeon Phi. Springer, 2014, pp. 167-188.

[34] M. Wasi-ur Rahman, X. Lu, N. S. Islam, R. Rajachandrasekar, and D. K.
Panda, “High-performance Design of YARN MapReduce on Modern
HPC Clusters with Lustre and RDMA,” in Parallel and Distributed
Processing Symposium (IPDPS), 2015 IEEE International. YEEE, 2015,
pp. 291-300.