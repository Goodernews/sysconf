[1] “How to Quantize Neural Networks with TensorFlow.” [Online]. Available:
https://www.tensor�ow.org/performance/quantization
[2] J. Albericio, P. Judd, T. Hetherington, T. Aamodt, N. E. Jerger, and A. Moshovos,
“Cnvlutin: Ine�ectual-neuron-free deep neural network computing,” in 2016
IEEE/ACM International Conference on Computer Architecture (ISCA), 2016.
[3] H. Alemdar, N. Caldwell, V. Leroy, A. Prost-Boucle, and F. Pétrot, “Ternary neural
networks for resource-e�cient AI applications,” CoRR, vol. abs/1609.00222, 2016.
[Online]. Available: http://arxiv.org/abs/1609.00222
[4] A. D. Booth, “A signed binary multiplication technique,” The Quarterly Journal of
Mechanics and Applied Mathematics, vol. 4, no. 2, pp. 236–240, 1951.
[5] Y. Chen, T. Luo, S. Liu, S. Zhang, L. He, J. Wang, L. Li, T. Chen, Z. Xu, N. Sun, and
O. Temam, “Dadiannao: A machine-learning supercomputer,” in Microarchitecture
(MICRO), 2014 47th Annual IEEE/ACM International Symposium on, Dec 2014, pp.
609–622.
[6] Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne, “Eyeriss:
An Energy-E�cient Recon�gurable Accelerator for Deep Convolutional Neural
Networks,” in IEEE International Solid-State Circuits Conference, ISSCC 2016, Digest
of Technical Papers, 2016, pp. 262–263.
[7] M. Courbariaux, Y. Bengio, and J.-P. David, “BinaryConnect: Training Deep
Neural Networks with binary weights during propagations,” ArXiv e-prints, Nov.
2015.
[8] H. Esmaeilzadeh, E. Blem, R. St. Amant, K. Sankaralingam, and D. Burger, “Dark
silicon and the end of multicore scaling,” in Proceedings of the 38th Annual International Symposium on Computer Architecture, ser. ISCA ’11. New York, NY,
USA: ACM, 2011, pp. 365–376.
[9] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies for
accurate object detection and semantic segmentation,” CoRR, vol. abs/1311.2524,
2013.
[10] R. Gonzalez and M. Horowitz, “Energy dissipation in general purpose microprocessors,” Solid-State Circuits, IEEE Journal of, vol. 31, no. 9, pp. 1277–1284, Sep
1996.
[11] Google, “Low-precision matrix multiplication,” https://github.com/google/
gemmlowp, 2016.
[12] S. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz, and W. J.
Dally, “EIE: E�cient Inference Engine on Compressed Deep Neural Network,”
arXiv:1602.01528 [cs], Feb. 2016, arXiv: 1602.01528. [Online]. Available:
http://arxiv.org/abs/1602.01528
[13] S. Han, H. Mao, and W. J. Dally, “Deep Compression: Compressing Deep
Neural Networks with Pruning, Trained Quantization and Hu�man Coding,”
arXiv:1510.00149 [cs], Oct. 2015, arXiv: 1510.00149. [Online]. Available:
http://arxiv.org/abs/1510.00149
[14] A. Y. Hannun, C. Case, J. Casper, B. C. Catanzaro, G. Diamos, E. Elsen, R. Prenger,
S. Satheesh, S. Sengupta, A. Coates, and A. Y. Ng, “Deep speech: Scaling up
end-to-end speech recognition,” CoRR, vol. abs/1412.5567, 2014.
[15] F. N. Iandola, M. W. Moskewicz, K. Ashraf, S. Han, W. J. Dally, and
K. Keutzer, “Squeezenet: Alexnet-level accuracy with 50x fewer parameters
and <1mb model size,” CoRR, vol. abs/1602.07360, 2016. [Online]. Available:
http://arxiv.org/abs/1602.07360
[16] P. Judd, J. Albericio, T. Hetherington, T. Aamodt, N. Enright Jerger, and
A. Moshovos, “Proteus: Exploiting numerical precision variability in deep neural
networks,” in Workshop On Approximate Computing (WAPCO), 2016.
[17] P. Judd, J. Albericio, T. Hetherington, T. Aamodt, N. E. Jerger, R. Urtasun, and
A. Moshovos, “Reduced-Precision Strategies for Bounded Memory in Deep Neural
Nets, arXiv:1511.05236v4 [cs.LG] ,” arXiv.org, 2015.
[18] P. Judd, J. Albericio, T. Hetherington, T. Aamodt, and A. Moshovos, “Stripes:
Bit-serial Deep Neural Network Computing ,” in Proceedings of the 49th Annual
IEEE/ACM International Symposium on Microarchitecture, ser. MICRO-49, 2016.
[19] P. Judd, J. Albericio, and A. Moshovos, “Stripes: Bit-serial Deep Neural Network
Computing ,” Computer Architecture Letters, 2016.

[20] J. Kim, K. Hwang, and W. Sung, “X1000 real-time phoneme recognition VLSI
using feed-forward deep neural networks,” in 2014 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP), May 2014, pp. 7510–7514.
[21] A. J. Martin, M. Nyström, and P. I. Pénzes, “Et2: A metric for time and energy
e�ciency of computation,” in Power aware computing. Springer, 2002, pp. 293–
315.
[22] N. Muralimanohar and R. Balasubramonian, “Cacti 6.0: A tool to understand
large caches.”
[23] V. Nair and G. E. Hinton, “Recti�ed linear units improve restricted boltzmann
machines,” in Proceedings of the 27th International Conference on Machine Learning
(ICML-10), 2010, pp. 807–814.
[24] A. Parashar, M. Rhu, A. Mukkara, A. Puglielli, R. Venkatesan, B. Khailany, J. Emer,
S. W. Keckler, and W. J. Dally, “Scnn: An accelerator for compressed-sparse
convolutional neural networks,” in Proceedings of the 44th Annual International
Symposium on Computer Architecture, ser. ISCA ’17. New York, NY, USA: ACM,
2017, pp. 27–40. [Online]. Available: http://doi.acm.org/10.1145/3079856.3080254
[25] M. Poremba, S. Mittal, D. Li, J. Vetter, and Y. Xie, “Destiny: A tool for modeling emerging 3d nvm and edram caches,” in Design, Automation Test in Europe
Conference Exhibition (DATE), 2015, March 2015, pp. 1543–1546.
[26] B. Reagen, P. Whatmough, R. Adolf, S. Rama, H. Lee, S. K. Lee, J. M. HernándezLobato, G.-Y. Wei, and D. Brooks, “Minerva: Enabling low-power, highly-accurate
deep neural network accelerators,” in Proceedings of the 43rd International Symposium on Computer Architecture. IEEE Press, 2016, pp. 267–278.
[27] Synopsys, “Design Compiler,” http://www.synopsys.com/Tools/
Implementation/RTLSynthesis/DesignCompiler/Pages.
[28] C. S. Wallace, “A suggestion for a fast multiplier,” IEEE Trans. Electronic
Computers, vol. 13, no. 1, pp. 14–17, 1964. [Online]. Available: http:
//dx.doi.org/10.1109/PGEC.1964.263830
[29] P. Warden, “Low-precision matrix multiplication,” https://petewarden.com, 2016.
[30] H. H. Yao and E. E. Swartzlander, “Serial-parallel multipliers,” in Proceedings
of 27th Asilomar Conference on Signals, Systems and Computers, Nov. 1993, pp.
359–363 vol.1.