[1] M. Bauer, S. Treichler, E. Slaughter, and A. Aiken, “Legion:
Expressing Locality and Independence with Logical Regions,”
in Proceedings of
the International Conference on High
Performance Computing, Networking, Storage and Analysis.
IEEE Computer Society Press, 2012, p. 66.

[2] L. Kal´e and S. Krishnan, “CHARM++: A Portable Concurrent
Object Oriented System Based on C++,” in Proceedings of
OOPSLA’93, A. Paepcke, Ed. ACM Press, September 1993,
pp. 91–108.

[3] Message Passing Interface Forum, “MPI: A Message-Passing
[Online].

Interface Standard Version 3.1,” June 2015.
Available: http://www.mpi-forum.org/

[4] S. Habib, V. Morozov, H. Finkel, A. Pope, K. Heitmann,
K. Kumaran, T. Peterka, J. Insley, D. Daniel, P. Fasel et al.,
“The Universe at Extreme Scale: Multi-Petaﬂop Sky Sim-
ulation on the BG/Q,” in Proceedings of the International
Conference on High Performance Computing, Networking,
Storage and Analysis.
IEEE Computer Society Press, 2012,
p. 4.

[5] A. Canning, J. Shalf, N. Wright, S. Anderson, and M. Gajbe,
“A Hybrid MPI/OpenMP 3D FFT for Plane Wave First-
principles Materials Science Codes,” in Proceedings of
CSC12 Conference, 2012.

[6] J. M. Levesque, R. Sankaran, and R. Grout, “Hybridizing S3D
into an Exascale Application Using OpenACC: An Approach
for Moving to Multi-petaﬂops and Beyond,” in Proceedings
of the International Conference on High Performance Com-
puting, Networking, Storage and Analysis.
IEEE Computer
Society Press, 2012, pp. 15:1–15:11.

[7] A. H. Baker, R. D. Falgout, T. V. Kolev, and U. M. Yang,
“Scaling Hypre’s Multigrid Solvers to 100,000 Cores,” in
High-Performance Scientiﬁc Computing. Springer, 2012, pp.
261–279.

[8] G. Krawezik, “Performance Comparison of MPI and Three
OpenMP Programming Styles on Shared Memory Multi-
processors,” in Proceedings of the Fifteenth Annual ACM
Symposium on Parallel Algorithms and Architectures. ACM,
2003, pp. 118–127.

[9] F. Broquedis, J. Clet-Ortega, S. Moreaud, N. Furmento,
B. Goglin, G. Mercier, S. Thibault, and R. Namyst, “hwloc:
a Generic Framework for Managing Hardware Afﬁnities
in HPC Applications,” in Euromicro International Confer-
ence on Parallel, Distributed and Network-Based Processing
(PDP), 2010 18th, 2010, pp. 180–186.

[10] B. Goglin, “Managing the Topology of Heterogeneous Cluster
Nodes with Hardware Locality (hwloc),” in International
Conference on High Performance Computing & Simulation
(HPCS).

IEEE, 2014, pp. 74–81.

[11] F. Wende, T. Steinke, and A. Reinefeld, “The Impact of
Process Placement and Oversubscription on Application Per-
formance: A Case Study for Exascale Computing,” in Pro-
ceedings of the 3rd International Conference on Exascale
Applications and Software, ser. EASC ’15, 2015, pp. 13–18.

[12] S. K. Guti´errez, “The QUO Runtime Library,” Jan 2013,
Los Alamos National Laboratory LA-CC-13-076. [Online].
Available: https://github.com/lanl/libquo

[13] H. C. Edwards, C. R. Trott, and D. Sunderland, “Kokkos:
Enabling Manycore Performance Portability Through Poly-
morphic Memory Access Patterns,” Journal of Parallel and
Distributed Computing, vol. 74, no. 12, pp. 3202–3216, 2014.

[14] A. Arnold, O. Lenz, S. Kesselheim, R. Weeber, F. Fahren-
berger, D. Roehm, P. Koˇsovan, and C. Holm, “ESPResSo
3.1 — Molecular Dynamics Software for Coarse-Grained
Models,” in Meshfree Methods for Partial Differential Equa-
tions VI, ser. Lecture Notes in Computational Science and
Engineering, M. Griebel and M. A. Schweitzer, Eds., vol. 89.
Springer, 2013, pp. 1–23.

[15] R. Thakur and W. Gropp, “Test Suite for Evaluat-
ing performance of MPI
Implementations That Support
MPI THREAD MULTIPLE,” in Recent Advances in Parallel
Virtual Machine and Message Passing Interface.
Springer,
2007, pp. 46–55.

[16] R. Rabenseifner, “Hybrid Parallel Programming: Performance
Problems and Chances,” in Proceedings of the 45th Cray User
Group Conference, Ohio, 2003, pp. 12–16.

[17] R. Rabenseifner, G. Hager,
and G.  Jost,

“Hybrid
MPI/OpenMP Parallel Programming on Clusters of Multi-
Core SMP Nodes,” in Euromicro International Conference
on Parallel, Distributed and Network-based Processing,
2009 17th, 2009, pp. 427–436.


[18] E. Chow and D. Hysom, “Assessing Performance of Hybrid
MPI/OpenMP Programs on SMP Clusters,” LLNL, Tech. Rep.
UCRL-JC-143957, 2001.

[19] N. Drosinos and N. Koziris, “Performance Comparison of
Pure MPI vs Hybrid MPI-OpenMP Parallelization Models
on SMP Clusters,” in Parallel and Distributed Processing
Symposium, 2004. Proceedings. 18th International, 2004.

[20] M. P´erache, H. Jourdren, and R. Namyst, “MPC: A Uniﬁed
Parallel Runtime for Clusters of NUMA Machines,” in Eu-
ropean Conference on Parallel Processing. Springer, 2008,
pp. 78–88.

[21] C. Huang, O. Lawlor, and L. V. Kale, “Adaptive MPI,” in
International Workshop on Languages and Compilers for
Parallel Computing. Springer, 2003, pp. 306–322.

