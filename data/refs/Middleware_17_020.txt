[1] 2017. A high performance, open-source universal RPC framework. https://grpc.io.
(2017).
[2] 2017. Checkpoint/Restore In Userspace (CRIU). https://criu.org/. (2017). Accessed:
2017-09-13.
[3] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G.
Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin
Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016. TensorFlow: A System for LargeScale Machine Learning. In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16). USENIX Association, GA, 265–283. https:
//www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi
[4] Kavosh Asadi and Jason D. Williams. 2016. Sample-efficient Deep Reinforcement
Learning for Dialog Control. CoRR abs/1612.06000 (2016). http://arxiv.org/abs/
1612.06000
[5] The GPyOpt authors. 2016. GPyOpt: A Bayesian Optimization framework in
python. http://github.com/SheffieldML/GPyOpt. (2016).
[6] James Bergstra, Olivier Breuleux, Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua
Bengio. 2010. Theano: A CPU and GPU Math Compiler in Python . In Proceedings
of the 9th Python in Science Conference, Stéfan van der Walt and Jarrod Millman
(Eds.). 3 – 10.
[7] J. Bergstra, D. Yamins, and D. D. Cox. 2013. Making a Science of Model Search:
Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures. In Proceedings of the 30th International Conference on International
Conference on Machine Learning - Volume 28 (ICML’13). JMLR.org, I–115–I–123.
http://dl.acm.org/citation.cfm?id=3042817.3042832
[8] Trishul Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman.
2014. Project Adam: Building an Efficient and Scalable Deep Learning Training
System. In 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14). USENIX Association, Broomfield, CO, 571–582. https:
//www.usenix.org/conference/osdi14/technical-sessions/presentation/chilimbi
[9] François Chollet. 2015. Keras. https://github.com/fchollet/keras. (2015).
[10] Ronan Collobert, Samy Bengio, and Johnny Mariéthoz. 2002. Torch: A Modular
Machine Learning Software Library. Idiap-RR Idiap-RR-46-2002. IDIAP.
[11] Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. 2015. Speeding Up Automatic Hyperparameter Optimization of Deep Neural Networks
by Extrapolation of Learning Curves. In Proceedings of the 24th International
Conference on Artificial Intelligence (IJCAI’15). AAAI Press, 3460–3468. http:
//dl.acm.org/citation.cfm?id=2832581.2832731
[12] Eyal Even-Dar, Shie Mannor, and Yishay Mansour. 2006. Action Elimination and
Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning
Problems. Journal of machine learning research 7, Jun (2006), 1079–1105.
Deep Learning with Apache Spark
[13] Tim Hunter. 2016.
and
TensorFlow.
https://databricks.com/blog/2016/01/25/
deep-learning-with-apache-spark-and-tensorflow.html. (January 2016).
[14] F. Hutter, H. H. Hoos, and K. Leyton-Brown. 2011. Sequential Model-Based Optimization for General Algorithm Configuration. In Proc. of LION-5. 507âĂŞ523.
[15] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long,
Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Caffe: Convolutional
Architecture for Fast Feature Embedding. arXiv preprint arXiv:1408.5093 (2014).
[16] Aaron Klein, Stefan Falkner, Jost Tobias Springenberg, and Frank Hutter. 2017.
Learning curve prediction with Bayesian neural networks. Proc. of ICLR 17
(2017).
[17] Oleg Klimov. 2017.
LunarLander-v2.
https://gym.openai.com/envs/
LunarLander-v2. (2017).
[18] Brent Komer, James Bergstra, and Chris Eliasmith. 2014. Hyperopt-sklearn:
automatic hyperparameter configuration for scikit-learn. In ICML workshop on
AutoML.
[19] Alex Krizhevsky. 2017. cuda-convnet. https://code.google.com/p/cuda-convnet/.
(2017).
[20] Alex Krizhevsky and Geoffrey Hinton. 2009. Learning multiple layers of features
from tiny images. (2009). Technical report, University of Toronto.
[21] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet
Talwalkar. 2017. Hyperband: Bandit-based Configuration Evaluation for Hyperparameter Optimization. Proc. of ICLR 17 (2017).
[22] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. 2011. Hogwild:
A Lock-Free Approach to Parallelizing Stochastic Gradient Descent. In Advances
in Neural Information Processing Systems 24, J. Shawe-Taylor, R. S. Zemel, P. L.
Bartlett, F. Pereira, and K. Q. Weinberger (Eds.). Curran Associates, Inc., 693–701.
[23] Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi.
2016. Bidirectional Attention Flow for Machine Comprehension. arXiv CoRR
abs/1611.01603 (2016). http://arxiv.org/abs/1611.01603
[24] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. 2012. Practical bayesian
optimization of machine learning algorithms. In Advances in neural information
processing systems. 2951–2959.
[25] Evan R. Sparks, Ameet Talwalkar, Daniel Haas, Michael J. Franklin, Michael I.
Jordan, and Tim Kraska. Automating Model Search for Large Scale Machine
Learning. In Proceedings of the Sixth ACM Symposium on Cloud Computing (SoCC
2015). 13. DOI:https://doi.org/10.1145/2806777.2806945
[26] Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. 2014. Freeze-Thaw
Bayesian Optimization. arXiv preprint arXiv:1406.3896 (2014).
Distributed Tensor[27] Wangda Tan and Vinod Kumar Vavilapalli. 2017.
Flow Assembly on Apache Hadoop YARN. https://hortonworks.com/blog/
distributed-tensorflow-assembly-hadoop-yarn/. (March 2017).
[28] Chris Thornton, Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. 2013.
Auto-WEKA: Combined Selection and Hyperparameter Optimization of Classification Algorithms. In Proceedings of the 19th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (KDD ’13). ACM, New York,
NY, USA, 847–855. DOI:https://doi.org/10.1145/2487575.2487629
[29] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. 2016. Learning
Structured Sparsity in Deep Neural Networks. In Advances in Neural Information
Processing Systems 29, D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and
R. Garnett (Eds.). Curran Associates, Inc., 2074–2082. http://papers.nips.cc/
paper/6504-learning-structured-sparsity-in-deep-neural-networks.pdf
Open
[30] Lee Yang, Jun Shi, Bobbie Chern, and Andy Feng. 2017.
Sourcing TensorFlowOnSpark: Distributed Deep Learning on BigData Clusters.
http://yahoohadoop.tumblr.com/post/157196317141/
open-sourcing-tensorflowonspark-distributed-deep. (February 2017).
[31] Dong Yu, Adam Eversole, Mike Seltzer, Kaisheng Yao, Oleksii Kuchaiev, Yu
Zhang, Frank Seide, Zhiheng Huang, Brian Guenter, Huaming Wang, Jasha
Droppo, Geoffrey Zweig, Chris Rossbach, Jie Gao, Andreas Stolcke, Jon Currey,
Malcolm Slaney, Guoguo Chen, Amit Agarwal, Chris Basoglu, Marko Padmilac,
Alexey Kamenev, Vladimir Ivanov, Scott Cypher, Hari Parthasarathi, Bhaskar
Mitra, Baolin Peng, and Xuedong Huang. 2014. An Introduction to Computational
Networks and the Computational Network Toolkit. Technical Report.
[32] Ming Yuan and Yi Lin. 2006. Model selection and estimation in regression with
grouped variables. Journal of the Royal Statistical Society: Series B (Statistical
Methodology) 68, 1 (2006), 49–67.
[33] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent Neural
Network Regularization. arXiv CoRR abs/1409.2329 (2014). http://arxiv.org/abs/
1409.2329
