
[1] A NDOR , D., A LBERTI , C., W EISS , D., S EVERYN , A., P RESTA , A.,
G ANCHEV, K., P ETROV, S., AND C OLLINS , M. Globally normalized
transition-based neural networks. arXiv preprint arXiv:1603.06042
(2016).
[2] BASTANI , O., I OANNOU , Y., L AMPROPOULOS , L., V YTINIOTIS , D.,
N ORI , A., AND C RIMINISI , A. Measuring neural net robustness with
constraints. arXiv preprint arXiv:1605.07262 (2016).
[3] B OJARSKI , M., D EL T ESTA , D., DWORAKOWSKI , D., F IRNER , B.,
F LEPP, B., G OYAL , P., JACKEL , L. D., M ONFORT, M., M ULLER , U.,
Z HANG , J., ET AL . End to end learning for self-driving cars. arXiv
preprint arXiv:1604.07316 (2016).
[4] B OURZAC , K.
Bringing
big
neural
networks
to
self-driving
cars,
smartphones,
and
drones.
http:
//spectrum.ieee.org/computing/embedded-systems/
bringing-big-neural-networks-to-selfdriving-cars-smartphones-and-drones,
2016.
[5] C ARLINI , N., M ISHRA , P., VAIDYA , T., Z HANG , Y., S HERR , M.,
S HIELDS , C., WAGNER , D., AND Z HOU , W. Hidden voice commands.
In 25th USENIX Security Symposium (USENIX Security 16), Austin, TX
(2016).
[6] C HANDOLA , V., BANERJEE , A., AND K UMAR , V. Anomaly detection:
A survey. ACM computing surveys (CSUR) 41, 3 (2009), 15.
[7] C LEVERT, D.-A., U NTERTHINER , T., AND H OCHREITER , S. Fast and
accurate deep network learning by exponential linear units (ELUs).
arXiv preprint arXiv:1511.07289 (2015).
[8] DAHL , G. E., S TOKES , J. W., D ENG , L., AND Y U , D. Large-scale
malware classification using random projections and neural networks. In
2013 IEEE International Conference on Acoustics, Speech and Signal
Processing (2013), IEEE, pp. 3422–3426.
[9] D ENG , J., D ONG , W., S OCHER , R., L I , L.-J., L I , K., AND F EI -F EI ,
L. Imagenet: A large-scale hierarchical image database. In Computer
Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference
on (2009), IEEE, pp. 248–255.

IX. C ONCLUSION
The existence of adversarial examples limits the areas in
which deep learning can be applied. It is an open problem
to construct defenses that are robust to adversarial examples.
In an attempt to solve this problem, defensive distillation
was proposed as a general-purpose procedure to increase the
robustness of an arbitrary neural network.

15

[32] M NIH , V., K AVUKCUOGLU , K., S ILVER , D., G RAVES , A.,
A NTONOGLOU , I., W IERSTRA , D., AND R IEDMILLER , M. Playing
Atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602
(2013).
[33] M NIH , V., K AVUKCUOGLU , K., S ILVER , D., RUSU , A. A., V ENESS ,
J., B ELLEMARE , M. G., G RAVES , A., R IEDMILLER , M., F IDJELAND ,
A. K., O STROVSKI , G., ET AL . Human-level control through deep
reinforcement learning. Nature 518, 7540 (2015), 529–533.
[34] M OOSAVI -D EZFOOLI , S.-M., FAWZI , A., AND F ROSSARD , P. Deepfool: a simple and accurate method to fool deep neural networks. arXiv
preprint arXiv:1511.04599 (2015).

[10] G IUSTI , A., G UZZI , J., C IREŞAN , D. C., H E , F.-L., RODR ÍGUEZ ,
J. P., F ONTANA , F., FAESSLER , M., F ORSTER , C., S CHMIDHUBER , J.,
D I C ARO , G., ET AL . A machine learning approach to visual perception
of forest trails for mobile robots. IEEE Robotics and Automation Letters
1, 2 (2016), 661–667.
[11] G OODFELLOW, I. J., S HLENS , J., AND S ZEGEDY, C. Explaining
and harnessing adversarial examples. arXiv preprint arXiv:1412.6572
(2014).
[12] G RAHAM , B. Fractional max-pooling. arXiv preprint arXiv:1412.6071
(2014).
[13] G RAVES , A., M OHAMED , A.- R ., AND H INTON , G. Speech recognition
with deep recurrent neural networks. In 2013 IEEE international
conference on acoustics, speech and signal processing (2013), IEEE,
pp. 6645–6649.
[14] G ROSSE , K., PAPERNOT, N., M ANOHARAN , P., BACKES , M., AND
M C DANIEL , P. Adversarial perturbations against deep neural networks
for malware classification. arXiv preprint arXiv:1606.04435 (2016).
[15] G U , S., AND R IGAZIO , L. Towards deep neural network architectures
robust to adversarial examples. arXiv preprint arXiv:1412.5068 (2014).
[16] H E , K., Z HANG , X., R EN , S., AND S UN , J. Deep residual learning for
image recognition. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (2016), pp. 770–778.
[17] H INTON , G., D ENG , L., Y U , D., DAHL , G., RAHMAN M OHAMED , A.,
JAITLY, N., S ENIOR , A., VANHOUCKE , V., N GUYEN , P., S AINATH , T.,
AND K INGSBURY, B. Deep neural networks for acoustic modeling in
speech recognition. Signal Processing Magazine (2012).
[18] H INTON , G., D ENG , L., Y U , D., DAHL , G. E., M OHAMED , A.- R .,
JAITLY, N., S ENIOR , A., VANHOUCKE , V., N GUYEN , P., S AINATH ,
T. N., ET AL . Deep neural networks for acoustic modeling in speech
recognition: The shared views of four research groups. IEEE Signal
Processing Magazine 29, 6 (2012), 82–97.
[19] H INTON , G., V INYALS , O., AND D EAN , J. Distilling the knowledge in
a neural network. arXiv preprint arXiv:1503.02531 (2015).
[20] H UANG , R., X U , B., S CHUURMANS , D., AND S ZEPESV ÁRI , C. Learning with a strong adversary. CoRR, abs/1511.03034 (2015).
[21] H UANG , X., K WIATKOWSKA , M., WANG , S., AND W U , M. Safety
verification of deep neural networks. arXiv preprint arXiv:1610.06940
(2016).
[22] JANGLOV Á , D. Neural networks in mobile robot motion. Cutting Edge
Robotics 1, 1 (2005), 243.
[23] K INGMA , D., AND BA , J. Adam: A method for stochastic optimization.
arXiv preprint arXiv:1412.6980 (2014).
[24] K RIZHEVSKY, A., AND H INTON , G. Learning multiple layers of
features from tiny images.
[25] K RIZHEVSKY, A., S UTSKEVER , I., AND H INTON , G. E. ImageNet
classification with deep convolutional neural networks. In Advances
in neural information processing systems (2012), pp. 1097–1105.
[26] K URAKIN , A., G OODFELLOW, I., AND B ENGIO , S. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533 (2016).
[27] L E C UN , Y., B OTTOU , L., B ENGIO , Y., AND H AFFNER , P. Gradientbased learning applied to document recognition. Proceedings of the
IEEE 86, 11 (1998), 2278–2324.
[28] L E C UN , Y., C ORTES , C., AND B URGES , C. J. The mnist database of
handwritten digits, 1998.
[29] M AAS , A. L., H ANNUN , A. Y., AND N G , A. Y. Rectifier nonlinearities
improve neural network acoustic models. In Proc. ICML (2013), vol. 30.
[30] M ELICHER , W., U R , B., S EGRETI , S. M., KOMANDURI , S., BAUER ,
L., C HRISTIN , N., AND C RANOR , L. F. Fast, lean and accurate:
Modeling password guessability using neural networks. In Proceedings
of USENIX Security (2016).
[31] M ISHKIN , D., AND M ATAS , J. All you need is a good init. arXiv
preprint arXiv:1511.06422 (2015).

[35] PAPERNOT, N., G OODFELLOW, I., S HEATSLEY, R., F EINMAN , R., AND
M C DANIEL , P. cleverhans v1.0.0: an adversarial machine learning
library. arXiv preprint arXiv:1610.00768 (2016).
[36] PAPERNOT, N., AND M C DANIEL , P. On the effectiveness of defensive
distillation. arXiv preprint arXiv:1607.05113 (2016).
[37] PAPERNOT, N., M C DANIEL , P., AND G OODFELLOW, I. Transferability in machine learning: from phenomena to black-box attacks using
adversarial samples. arXiv preprint arXiv:1605.07277 (2016).
[38] PAPERNOT, N., M C DANIEL , P., J HA , S., F REDRIKSON , M., C ELIK ,
Z. B., AND S WAMI , A. The limitations of deep learning in adversarial
settings. In 2016 IEEE European Symposium on Security and Privacy
(EuroS&P) (2016), IEEE, pp. 372–387.
[39] PAPERNOT, N., M C DANIEL , P., W U , X., J HA , S., AND S WAMI , A.
Distillation as a defense to adversarial perturbations against deep neural
networks. IEEE Symposium on Security and Privacy (2016).
[40] PASCANU , R., S TOKES , J. W., S ANOSSIAN , H., M ARINESCU , M.,
AND T HOMAS , A. Malware classification with recurrent networks. In
2015 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP) (2015), IEEE, pp. 1916–1920.
[41] RUSSAKOVSKY, O., D ENG , J., S U , H., K RAUSE , J., S ATHEESH , S.,
M A , S., H UANG , Z., K ARPATHY, A., K HOSLA , A., B ERNSTEIN , M.,
B ERG , A. C., AND F EI -F EI , L. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV) 115,
3 (2015), 211–252.
[42] S HAHAM , U., YAMADA , Y., AND N EGAHBAN , S. Understanding
adversarial training: Increasing local stability of neural nets through
robust optimization. arXiv preprint arXiv:1511.05432 (2015).
[43] S ILVER , D., H UANG , A., M ADDISON , C. J., G UEZ , A., S IFRE , L.,
VAN D EN D RIESSCHE , G., S CHRITTWIESER , J., A NTONOGLOU , I.,
PANNEERSHELVAM , V., L ANCTOT, M., ET AL . Mastering the game
of Go with deep neural networks and tree search. Nature 529, 7587
(2016), 484–489.
[44] S PRINGENBERG , J. T., D OSOVITSKIY, A., B ROX , T., AND R IED MILLER , M. Striving for simplicity: The all convolutional net. arXiv
preprint arXiv:1412.6806 (2014).
[45] S ZEGEDY, C., VANHOUCKE , V., I OFFE , S., S HLENS , J., AND W OJNA ,
Z. Rethinking the Inception architecture for computer vision. arXiv
preprint arXiv:1512.00567 (2015).
[46] S ZEGEDY, C., Z AREMBA , W., S UTSKEVER , I., B RUNA , J., E RHAN ,
D., G OODFELLOW, I., AND F ERGUS , R. Intriguing properties of neural
networks. ICLR (2013).
[47] WARDE -FARLEY, D., AND G OODFELLOW, I. Adversarial perturbations
of deep neural networks. Advanced Structured Prediction, T. Hazan, G.
Papandreou, and D. Tarlow, Eds (2016).
[48] Y UAN , Z., L U , Y., WANG , Z., AND X UE , Y. Droid-sec: Deep learning
in android malware detection. In ACM SIGCOMM Computer Communication Review (2014), vol. 44, ACM, pp. 371–372.

