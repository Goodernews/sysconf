[1] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent Dirichlet
Allocation,” JMLR, pp. 993-1022, 2003.

[2] G. A. Miller, “WordNet: a lexical database for English,”
Communications of the ACM, pp. 39-41, 1995.

[3] J. Pennington, R. Socher, and C. Manning, “GloVe: Global
vectors for word representation,” in EMNLP, 2014, pp. 1532—
1543.

[4] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,
“Distributed representations of words and phrases and their
compositionally,” in NIPS, 2013, pp. 3111-3119.

[5] R. Das, M. Zaheer, and C. Dyer, “Gaussian LDA for topic
models with word embeddings,” in ACL, 2015, pp. 795-804.

[6] D. Q. Nguyen, R. Billingsley, L. Du, and M. Johnson,
“Improving topic models with latent feature word representations,” TACL, pp. 299-313, 2015.

[7] G. Xun, V. Gopalakrishnan, F Ma, Y. Li, J. Gao, and
A. Zhang, “Topic discovery for short texts using word embeddings,” in ICDM, 2016, pp. 1299-1304.

[8] C. Li, H. Wang, Z. Zhang, A. Sun, and Z. Ma, “Topic
modeling for short texts with auxiliary word embeddings,”
in SIGIR, 2016, pp. 165-174.

[9] D. Mimno and A. McCallum, “Topic models conditioned on
arbitrary features with Dirichlet-multinomial regression,” in
UAT, 2008, pp. 411418.

[10] D. Ramage, C. D. Manning, and S. Dumais, “Partially labeled
topic models for interpretable text mining,” in SIGKDD, 2011,
pp. 457-465.

[11] J. D. Mcauliffe and D. M. Blei, “Supervised topic models,”
in NIPS, 2008, pp. 121-128.

[12] D. Ramage, D. Hall, R. Nallapati, and C. D. Manning, “Labeled LDA: A supervised topic model for credit attribution
in multi-labeled corpora,” in EMNLP, 2009, pp. 248-256.

[13] D. Kim and A. Oh, “Hierarchical Dirichlet scaling process,”
Machine Learning, pp. 387-418, 2017.

[14] C. Hu, P. Rai, and L. Carin, “Non-negative matrix factorization for discrete data with hierarchical side-information,” in
AISTATS, 2016, pp. 1124-1132.

[15] D. Andrzejewski, X. Zhu, and M. Craven, “Incorporating
domain knowledge into topic modeling via Dirichlet forest
priors,” in ICML, 2009, pp. 25-32.

[16] P. Xie, D. Yang, and E. Xing, “Incorporating word correlation
knowledge into topic modeling,” in NAACL, 2015, pp. 725-—
734.

[17] J. Petterson, W. Buntine, S. M. Narayanamurthy, T. S. Caetano, and A. J. Smola, “Word features for Latent Dirichlet
Allocation,” in NIPS, 2010, pp. 1921-1929.

[18] L. Hong and B. D. Davison, “Empirical study of topic
modeling in twitter,” in Workshop on social media analytics,
2010, pp. 80-88.

[19] Y. Zuo, J. Wu, H. Zhang, H. Lin, F. Wang, K. Xu, and
H. Xiong, “Topic modeling of short texts: A pseudo-document
view,” in SIGKDD, 2016, pp. 2105-2114.

[20] J. Yin and J. Wang, “A Dirichlet multinomial mixture modelbased approach for short text clustering,” in SIGKDD, 2014,
pp. 233-242.

[21] R. Mehrotra, S. Sanner, W. Buntine, and L. Xie, “Improving
LDA topic models for microblogs via tweet pooling and
automatic labeling,” in SYGIR, 2013, pp. 889-892.

[22] D. Andrzejewski, X. Zhu, M. Craven, and B. Recht, “A
framework for incorporating general domain knowledge into
Latent Dirichlet Allocation using first-order logic,” in LJCAI,
2011, pp. 1171-1177.

[23] Y. Yang, D. Downey, and J. Boyd-Graber, “Efficient methods
for incorporating knowledge into topic models,” in EMNLP,
2015, pp. 308-317.

[24] H. M. Wallach, D. M. Mimno, and A. McCallum, “Rethinking
LDA: Why priors matter,’ in NJPS, 2009, pp. 1973-1981.

[25] C. Chen, L. Du, and W. Buntine, “Sampling table configurations for the hierarchical Poisson-Dirichlet process,” in
ECML, 2011, pp. 296-311.

[26] Y. Teh, M. Jordan, M. Beal, and D. Blei, “Hierarchical
Dirichlet processes,” Journal of the American Statistical Association, pp. 1566-1581, 2012.

[27] M. Zhou and L. Carin, “Negative binomial process count and
mixture modeling,” TPAMT, pp. 307-320, 2015.

[28] H. Zhao, L. Du, and W. Buntine, “Leveraging node attributes
for incomplete relational data,” in ICML, 2017, pp. 4072—
4081.

[29] W. Buntine and M. Hutter, “A Bayesian view of the
Poisson-Dirichlet process,” arXiv preprint arXiv: 1007.0296v2
[math.ST], 2012.

[30] J. Guo, W. Che, H. Wang, and T. Liu, “Revisiting embedding
features for simple semi-supervised learning,” in EMNLP,
2014, pp. 110-120.

[31] L. Yao, D. Mimno, and A. McCallum, “Efficient methods for
topic model inference on streaming document collections,” in
SIGKDD, 2009, pp. 937-946.

[32] N. Aletras and M. Stevenson, “Evaluating topic coherence
using distributional semantics,” in International Conference
on Computational Semantics, 2013, pp. 13-22.

[33] J. H. Lau, D. Newman, and T. Baldwin, “Machine reading tea
leaves: Automatically evaluating topic coherence and topic
model quality,” in EACL, 2014, pp. 530-539.

[34] D. Newman, A. Asuncion, P. Smyth, and M. Welling, “Distributed algorithms for topic models,” JMLR, pp. 1801-1828,
2009.