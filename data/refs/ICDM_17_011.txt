[1] Onur C Hamsici and Aleix M Martinez. Bayes optimality
in linear discriminant analysis. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 30(4):647-657, 2008.

[2] Amin Zollanvari and Edward R Dougherty. Random matrix
theory in pattern classification: An application to error estimation. In 20/3 Asilomar Conference on Signals, Systems and
Computers, 2013.

[3] T Tony Cai, Zhao Ren, Harrison H Zhou, et al. Estimating
structured high-dimensional covariance and precision matrices:
Optimal rates and adaptive estimation. Electronic Journal of
Statistics, 10(1):1-59, 2016.

[4] Olvi L Mangasarian. Linear and nonlinear separation of patterns
by linear programming. Operations research, 13(3):444-452,
1965.

[5] Kristin P Bennett and Olvi L Mangasarian. Robust linear
programming discrimination of two linearly inseparable sets.
Optimization methods and software, 1(1):23-34, 1992.

[6] Roger Peck and John Van Ness. The use of shrinkage estimators
in linear discriminant analysis. IEEE Transactions on Pattern
Analysis and Machine Intelligence, (5):530-537, 1982.

[7] Juwei Lu, Konstantinos N Plataniotis, and Anastasios N Venetsanopoulos. Regularization studies of linear discriminant analysis in small sample size scenarios with application to face
recognition. Pattern Recognition Letters, 26(2):181-191, 2005.

[8] Daniela M Witten and Robert Tibshirani. Covarianceregularized regression and classification for high dimensional
problems. Journal of the Royal Statistical Society: Series B
(Statistical Methodology), 71(3):615-636, 2009.

[9] Line Clemmensen, Trevor Hastie, Daniela Witten, and Bjarne

Ersbgll. Sparse discriminant analysis. Technometrics, 53(A),

2011.

[10] Jun Shao, Yazhen Wang, Xinwei Deng, Sijian Wang, et al.

Sparse linear discriminant analysis by thresholding for high

dimensional data. The Annals of statistics, 39(2):1241-1265,

2011.

[11] Peter Buhlmann and Sara Van De Geer. Statistics for high
dimensional data: methods, theory and applications. Springer,

2011.

[12] Neil D Lawrence and Bernhard Schélkopf. Estimating a kernel

fisher discriminant in the presence of label noise. In ICML,

volume 1, pages 306-313. Citeseer, 2001.

[13] Zhihua Zhang et al. Learning metrics via discriminant kernels

and multidimensional scaling: Toward expected euclidean rep
resentation. In JCML, volume 2, pages 872-879, 2003.

[14] Seung-Jean Kim, Alessandro Magnani, and Stephen Boyd.

Optimal kernel selection in kernel fisher discriminant analysis.

In ICML, pages 465-472. ACM, 2006.

[15] Zhihua Zhang, Guang Dai, Congfu Xu, and Michael I Jordan.
Regularized discriminant analysis, ridge regression and beyond.
JMLR, 11(Aug):2199-2228, 2010.

[16] Kenneth P Burnham and David R Anderson. Model selection
and multimodel inference: a practical information-theoretic
approach. Springer Science & Business Media, 2003.

[17] Jennifer A Hoeting, David Madigan, Adrian E Raftery, and
Chris T Volinsky. Bayesian model averaging: a tutorial. Statistical science, pages 382-401, 1999.

[18] Nello Cristianini and John Shawe-Taylor. Bayesian voting
schemes and large margin classifiers. Advances in Kernel
MethodsSupport Vector Learning, pages 55-68, 1999.

[19] Pascal Germain, Alexandre Lacasse, Francois Laviolette, Mario
Marchand, and Jean-Francis Roy. Risk bounds for the majority
vote: From a pac-bayesian analysis to a learning algorithm. The
Journal of Machine Learning Research, 16(1):787-860, 2015.

[20] David GT Denison. Bayesian methods for nonlinear classification and regression, volume 386. John Wiley & Sons, 2002.

[21] John Hammersley. Monte carlo methods. Springer Science &
Business Media, 2013.

[22] Philip J Davis and Philip Rabinowitz. Methods of numerical
integration. Courier Corporation, 2007.

[23] Jana Jankova, Sara van de Geer, et al. Confidence intervals
for high-dimensional inverse covariance estimation. Electronic
Journal of Statistics, 9(1):1205-1229, 2015.

[24] LR Haff. Estimation of the inverse covariance matrix: Random
mixtures of the inverse wishart matrix and the identity. The
Annals of Statistics, pages 1264-1276, 1979.

[25] S Sawyer. Wishart distributions and inverse-wishart sampling.
URL: www. math. wustl. edu/sawyer/hmhandouts/Whishart. pdf,
2007.

[26] Tom Leonard and John SJ Hsu. Bayesian inference for a
covariance matrix. The Annals of Statistics, pages 1669-1696,
1992.

[27] Santosh Srivastava, Maya R Gupta, and Béla A Frigyik.
Bayesian quadratic discriminant analysis. Journal of Machine
Learning Research, 8(Jun):1277-1305, 2007.

[28] Ignacio Alvarez, Jarad Niemi, and Matt Simpson. Bayesian

inference for a covariance matrix. arXiv preprint

arXiv: 1408.4050, 2014.

[29] John C Platt. 12 fast training of support vector machines using

sequential minimal optimization. Advances in kernel methods,

pages 185-208, 1999.

[30] Jieping Ye, Ravi Janardan, Cheong Hee Park, and Haesun Park.

An optimization criterion for generalized discriminant analysis

on undersampled problems. JEEE Transactions on Pattern

Analysis and Machine Intelligence, 26(8):982-994, 2004.

[31] Jieping Ye, Ravi Janardan, Qi Li, et al. Two-dimensional linear

discriminant analysis. In NIPS, volume 4, page 4, 2004.

[32] Jieping Ye and Qi Li. A two-stage linear discriminant analysis

via qr-decomposition. [EEE Transactions on Pattern Analysis

and Machine Intelligence, 27(6):929-941, 2005.

[33] James C. Turner and Adrienne Keller. College Health Surveil
lance Network: Epidemiology and Health Care Utilization of

College Students at U.S. 4-Year Universities. Journal of

American college health: J of ACH, page 0, June 2015.

[34] Fei Wang and Jimeng Sun. Psf: a unified patient similarity

evaluation framework through metric learning with weak su
pervision. IEEE journal of biomedical and health informatics,

19(3):1053-1060, 2015.

[35] Juwei Lu, Kostantinos N Plataniotis, and Anastasios N Venet
sanopoulos. Face recognition using Ida-based algorithms. Neu
ral Networks, IEEE Transactions on, 14(1):195—200, 2003.

[36] Hui Gao and James W Davis. Why direct Ilda is not equivalent

to Ida. Pattern Recognition, 39(5):1002—1006, 2006.

[37] Zhihua Qiao, Lan Zhou, and Jianhua Z Huang. Effective linear

discriminant analysis for high dimensional, low sample size

data. In Proceeding of the World Congress on Engineering,

volume 2, pages 2-4. Citeseer, 2008.

[38] Tony Cai and Weidong Liu. A direct estimation approach to

sparse linear discriminant analysis. Journal of the American

Statistical Association, 106(496):1566-1577, 2011.

[39] Christophe Croux, Peter Filzmoser, and Kristel Joossens. Clas
sification efficiencies for robust linear discriminant analysis.

[40] Statistica Sinica, pages 581-599, 2008.

[41] Luke Tierney and Antonietta Mira. Some adaptive monte

carlo methods for bayesian inference. Statistics in medicine,

18(1718):2507-2515, 1999.

[42] Umut A Acar, Alexander Ihler, Ramgopal Mettu, and Ozgiir

Stimer. Adaptive bayesian inference. In Neural Information

Processing Systems (NIPS), 2007.