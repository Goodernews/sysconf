[1] Matthew Otten et al, “An MPI/OpenACC implementation of
a high-order electromagnetics solver with GPUDirect communication,” The International Journal of High Performance
Computing Applications, vol. 30, no. 3, pp. 320–334, 2016.

[2] Programming Environment Research Team of RIKEN AICS
and High Performance Computing System Laboratory of
University of Tsukuba, “XcalableACC Speciﬁcation version
1.0,” http://xcalablemp.org/XACC.html.
[3] Masahiro Nakao et al, “XcalableACC: Extension of XcalableMP PGAS Language Using OpenACC for Accelerator
Clusters,” in Proceedings of the First Workshop on Accelerator Programming Using Directives, ser. WACCPD ’14, 2014,
pp. 27–36.
[4] Akihiro Tabuchi et al., “Implementation and Evaluation of
One-sided PGAS Communication in XcalableACC for Accelerated Clusters,” in International Symposium on Cluster,
Cloud and Grid Computing (CCGrid), ser. CCGrid ’17, 2017.
[5] Masahiro Nakao et al, “Productivity and Performance of the
HPC Challenge Benchmarks with the XcalableMP PGAS
Language,” in 7th International Conference on PGAS Programming Model, 2013, pp. 157–171.
[6] XcalableMP Speciﬁcation Working Group, “XcalableMP
Speciﬁcation version 1.2.1,” http://xcalablemp.org/
specification.
[7] J. A. Herdman et al, “Accelerating Hydrocodes with OpenACC, OpenCL and CUDA,” in 2012 SC Companion: High
Performance Computing, Networking Storage and Analysis,
Nov 2012, pp. 465–471.
[8] “OpenACC,” http://www.openacc-standard.org.

[9] Charles H. Koelbel et al, The High Performance Fortran
Handbook. MIT Press, 1994.

[10] Wilson, K. G., “Conﬁnement of quarks,” Phys. Rev. D,
vol. 10, pp. 2445–2459, Oct 1974. [Online]. Available:
http://link.aps.org/doi/10.1103/PhysRevD.10.2445
[11] Taisuke Boku et al, “Multi-block/multi-core SSOR preconditioner for the QCD quark solver for K computer,” in In
Proceedings of The 30th International Symposium on Lattice
Field Theory, June 2012.

[12] “HA-PACS,”
http://ccs.tsukuba.ac.jp/CCS/
eng/research-activities/projects.
[13] “Omni Compiler,” http://omni-compiler.org.
[14] Bridge++ code developing team, “Lattice QCD
code
Bridge++,”
http://bridge.kek.jp/
Lattice-code/index_e.html.
[15] Akihiro Tabuchi et al, “A Source-to-Source OpenACC Compiler for CUDA,” in Euro-Par Workhops, 2013, pp. 178–187.
[16] Andrew I. Stone et al, “Evaluating coarray fortran with
the cgpop miniapp,” in Proceedings of the Fifth Conference
on Partitioned Global Address Space Programming Models
(PGAS), October 2011.
[17] Cunningham Dave et al, “GPU Programming in a High Level
Language: Compiling X10 to CUDA,” in Proceedings of the
2011 ACM SIGPLAN X10 Workshop, ser. X10 ’11, 2011, pp.
8:1–8:10.
[18] A. Sidelnik et al, “Performance Portability with the Chapel
Language,” in 2012 IEEE 26th International Parallel and
Distributed Processing Symposium, May 2012, pp. 582–594.
[19] Koichi Shirahata and Jun Doi and Mikio Takeuchi, “Performance Analysis of Lattice QCD in X10 CUDA,” X10 Day
Tokyo 2015, 2015.
[20] S. Potluri et al, “Extending OpenSHMEM for GPU Computing,” in 2013 IEEE 27th International Symposium on Parallel
and Distributed Processing, May 2013, pp. 1001–1012.
[21] Kenta Sato et al, “GPU-ready GASNet Implementation on the
TCA Proprietary Interconnect Architecture,” in The 2016 International Conference on Computational Science and Computational Intelligence (CSCI 2016), 12 2016.
[22] Lena Oden, “GPI2 for GPUs: A PGAS framework for efﬁcient communication in hybrid clusters,” in Proceedings of the
23rd International Conference on Languages and Compilers
for Parallel Computing, 2013, pp. 461–470.
[23] H. C. Edwards and C. R. Trott, “Kokkos: Enabling Performance Portability Across Manycore Architectures,” in 2013
Extreme Scaling Workshop (xsw 2013), Aug 2013, pp. 18–24.
[24] R. D. Hornung, J. A. Keasler, “The RAJA Portability Layer:
Overview and Status,” LLNL, Tech. Rep. LLNL-TR-661403,
2014.
[25] E. Zenker and B. Worpitz and R. Widera and A. Huebl and G.
Juckeland and A. Knpfer and W. E. Nagel and M. Bussmann,
“Alpaka – An Abstraction Library for Parallel Kernel Acceleration,” in 2016 IEEE International Parallel and Distributed
Processing Symposium Workshops (IPDPSW), May 2016, pp.
631–640.
[26] Garland, Michael and Kudlur, Manjunath and Zheng,
Yili, “Designing a Uniﬁed Programming Model for
Heterogeneous Machines,” in Proceedings of the
International Conference on High Performance Computing,
Networking, Storage and Analysis, ser. SC ’12.
Los Alamitos, CA, USA: IEEE Computer Society
Press, 2012, pp. 67:1–67:11. [Online]. Available:
http://dl.acm.org/citation.cfm?id=2388996.2389087
[27] T. Hoshino et al, “A directive-based data layout abstraction
for performance portability of openacc applications,” in IEEE
2nd International Conference on Data Science and Systems,
Dec 2016, pp. 1147–1154.