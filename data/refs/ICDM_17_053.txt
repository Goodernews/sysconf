[1] R. E. Schapire, “The boosting approach to machine learning: An
overview,” in Nonlinear estimation and classiﬁcation. Springer, 2003,
pp. 149–171.
[2] R. Schapire, “The strength of weak learnability,” Machine learning,
vol. 5, no. 2, pp. 197–227, 1990.
[3] R. E. Schapire, “Explaining adaboost,” in Empirical inference. Springer,
2013, pp. 37–52.
[4] Z.-H. Zhou, Ensemble methods: foundations and algorithms. CRC
press, 2012.
[5] Y. Freund and R. E. Schapire, “A desicion-theoretic generalization of
on-line learning and an application to boosting,” in European conference
on computational learning theory. Springer, 1995, pp. 23–37.
[6] M. Mohri, A. Rostamizadeh, and A. Talwalkar, Foundations of machine
learning. MIT press, 2012.
[7] Y. Freund, “An adaptive version of the boost by majority algorithm,”
Machine learning, vol. 43, no. 3, pp. 293–318, 2001.
[8] J. Bootkrajang and A. Kabán, “Boosting in the presence of label noise,”
in Uncertainty in Artiﬁcial Intelligence. Citeseer, 2013, p. 82.
[9] P. M. Long and R. A. Servedio, “Martingale boosting,” in International
Conference on Computational Learning Theory. Springer, 2005, pp.
79–94.
[10] P. Long and R. Servedio, “Boosting the area under the roc curve,” in
Advances in neural information processing systems, 2007, pp. 945–952.
[11] P. M. Long and R. A. Servedio, “Random classiﬁcation noise defeats
all convex potential boosters,” Machine Learning, vol. 78, no. 3, pp.
287–304, 2010.
[12] R. E. Schapire, Y. Freund, P. Bartlett, and W. S. Lee, “Boosting the
margin: A new explanation for the effectiveness of voting methods,”
Annals of statistics, pp. 1651–1686, 1998.
[13] C. Rudin, I. Daubechies, and R. E. Schapire, “The dynamics of adaboost:
Cyclic behavior and convergence of margins,” Journal of Machine
Learning Research, vol. 5, no. Dec, pp. 1557–1595, 2004.
[14] L. Wang, M. Sugiyama, C. Yang, Z.-H. Zhou, and J. Feng, “On the
margin explanation of boosting algorithms.” in COLT. Citeseer, 2008,
pp. 479–490.
[15] L. Mason, P. L. Bartlett, and J. Baxter, “Improved generalization through
explicit optimization of margins,” Machine Learning, vol. 38, no. 3, pp.
243–255, 2000.
[16] T. G. Dietterich, R. H. Lathrop, and T. Lozano-Pérez, “Solving the
multiple instance problem with axis-parallel rectangles,” Artiﬁcial intelligence, vol. 89, no. 1, pp. 31–71, 1997.
[17] J. Amores, “Multiple instance classiﬁcation: Review, taxonomy and
comparative study,” Artiﬁcial Intelligence, vol. 201, pp. 81–105, 2013.
[18] T. M. Mitchell, “Machine learning,” 1997.
[19] B. Schölkopf and A. J. Smola, Learning with kernels: support vector
machines, regularization, optimization, and beyond. MIT press, 2002.
[20] A. Birnbaum and S. S. Shwartz, “Learning halfspaces with the zeroone loss: time-accuracy tradeoffs,” in Advances in Neural Information
Processing Systems, 2012, pp. 926–934.
[21] B. Van Rooyen, A. Menon, and R. C. Williamson, “Learning with
symmetric label noise: The importance of being unhinged,” in Advances
in Neural Information Processing Systems, 2015, pp. 10–18.
[22] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, “Learning with noisy labels,” in Advances in neural information processing
systems, 2013, pp. 1196–1204.
[23] A. K. Menon, B. van Rooyen, and N. Natarajan, “Learning from
binary labels with instance-dependent corruption,” arXiv preprint
arXiv:1605.00751, 2016.
[24] R. E. Schapire and Y. Freund, Boosting: Foundations and algorithms.
MIT press, 2012.
[25] O. Maron and A. L. Ratan, “Multiple-instance learning for natural scene
classiﬁcation.” in ICML, vol. 98, 1998, pp. 341–349.
[26] S. Andrews, I. Tsochantaridis, and T. Hofmann, “Support vector machines for multiple-instance learning,” in Advances in neural information
processing systems, 2002, pp. 561–568.
[27] A. Blum and A. Kalai, “A note on learning from multiple-instance
examples,” Machine Learning, vol. 30, no. 1, pp. 23–29, 1998.
[28] G. Doran and S. Ray, “Learning instance concepts from multipleinstance data with bags as distributions.” in AAAI, 2014, pp. 1802–1808.
[29] R. Rahmani, S. A. Goldman, H. Zhang, J. Krettek, and J. E. Fritts, “Localized content based image retrieval,” in Proceedings of the 7th ACM
SIGMM international workshop on Multimedia information retrieval.
ACM, 2005, pp. 227–236.
[30] B. Settles, M. Craven, and S. Ray, “Multiple-instance active learning,”
in Advances in neural information processing systems, 2008, pp. 1289–
1296.
[31] F. Briggs, X. Z. Fern, and R. Raich, “Rank-loss support instance
machines for miml instance annotation,” in Proceedings of the 18th
ACM SIGKDD international conference on Knowledge discovery and
data mining. ACM, 2012, pp. 534–542.
[32] P. Auer and R. Ortner, “A boosting approach to multiple instance
learning,” in European Conference on Machine Learning. Springer,
2004, pp. 63–74.
[33] X. Xu and E. Frank, “Logistic regression and boosting for labeled bags
of instances,” in Paciﬁc-Asia conference on knowledge discovery and
data mining. Springer, 2004, pp. 272–281.
[34] D. Ascher, P. Dubois, K. Hinsen, J. Hugunin, and T. Oliphant, “Numerical python, lawrence livermore national laboratory, livermore, california,
usa,” 2001.
[35] E. Jones, T. Oliphant, P. Peterson et al., “Open source scientiﬁc tools
for python,” 2001.
[36] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg et al.,
“Scikit-learn: Machine learning in python,” Journal of Machine Learning
Research, vol. 12, no. Oct, pp. 2825–2830, 2011.
[37] J. Demšar, “Statistical comparisons of classiﬁers over multiple data sets,”
Journal of Machine learning research, vol. 7, no. Jan, pp. 1–30, 2006.
[38] V. Tragante do O, D. Fierens, and H. Blockeel, “Instance-level accuracy
versus bag-level accuracy in multi-instance learning,” in Proceedings of
the 23rd Benelux Conference on Artiﬁcial Intelligence, 2011, p. 8.

