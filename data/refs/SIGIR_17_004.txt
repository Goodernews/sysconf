
[1] J. A. Aslam, V. Pavlu, and E. Yilmaz. 2006. A statistical method for system
evaluation using incomplete judgments. In Proc. SIGIR. 541–548.

[2] C. Buckley, D. Dimmick, I. Soboroff, and E. M. Voorhees. 2007. Bias and the
limits of pooling for large collections. Inf. Retr. 10, 6 (2007), 491–508.
[3] C. Buckley and E. M. Voorhees. 2004. Retrieval evaluation with incomplete
information. In Proc. SIGIR. 25–32.
[4] S. Büttcher, C. L. A. Clarke, P. C. K. Yeung, and I. Soboroff. 2007. Reliable
information retrieval evaluation with incomplete and biased judgements. In Proc.
SIGIR. 63–70.
[5] A. Chao and S. Lee. 1992. Estimating the number of classes via sample coverage.
J. American Statistical Association 87, 417 (1992), 210–217.
[6] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. 2009. Expected reciprocal
rank for graded relevance. In Proc. CIKM. 621–630.
[7] J. K. Jayasinghe, W. Webber, M. Sanderson, and J. S. Culpepper. 2014. Improving
test collection pools with machine learning. In Proc. Aust. Doc. Comp. Symp. 2–9.
[8] R. Kumar and S. Vassilvitskii. 2010. Generalized distances between rankings. In
Proc. WWW. 571–580.
[9] A. Lipani, M. Lupu, and A. Hanbury. 2015. Splitting water: Precision and antiprecision to reduce pool bias. In Proc. SIGIR. 103–112.
[10] A. Lipani, M. Lupu, E. Kanoulas, and A. Hanbury. 2016. The solitude of relevant
documents in the pool. In Proc. CIKM. 1989–1992.
[11] X. Lu, A. Moffat, and J. S. Culpepper. 2016. The effect of pooling and evaluation
depth on IR metrics. Inf. Retr. 19, 4 (2016), 416–445.
[12] X. Lu, A. Moffat, and J. S. Culpepper. 2016. Modeling relevance as a function of
retrieval rank. In Proc. AIRS. 3–15.
[13] A. Moffat, P. Thomas, and F. Scholer. 2013. Users versus models: What observation
tells us about effectiveness metrics. In Proc. CIKM. 659–668.
[14] A. Moffat, W. Webber, and J. Zobel. 2007. Strategic system comparisons via
targeted relevance judgments. In Proc. SIGIR. 375–382.
[15] A. Moffat and J. Zobel. 2008. Rank-biased precision for measurement of retrieval
effectiveness. ACM Trans. Information Systems 27, 1 (2008), 2:1–2:27.
[16] S. D. Ravana and A. Moffat. 2010. Score estimation, incomplete judgments, and
significance testing in IR evaluation. In Proc. AIRS. 97–109.
[17] S. E. Robertson. 2007. On document populations and measures of IR effectiveness.
In Proc. ICTIR. 9–22.
[18] T. Sakai. 2007. Alternatives to BPref. In Proc. SIGIR. 71–78.
[19] T. Sakai. 2008. Comparing metrics across TREC and NTCIR: The robustness to
pool depth bias. In Proc. SIGIR. 691–692.
[20] T. Sakai. 2008. Comparing metrics across TREC and NTCIR: The robustness to
system bias. In Proc. CIKM. 581–590.
[21] T. Sakai. 2014. Metrics, statistics, tests. In Bridging Between Information Retrieval
and Databases, N. Ferro (Ed.). Springer, 116–163.
[22] T. Schnabel, A. Swaminathan, P. I. Frazier, and T. Joachims. 2016. Unbiased
comparative evaluation of ranking functions. In Proc. ICTIR. 109–118.
[23] T. Schnabel, A. Swaminathan, and T. Joachims. 2015. Unbiased ranking evaluation
on a budget. In Proc. WWW. 935–937.
[24] E. M. Voorhees. 2014. The effect of sampling strategy on inferred measures. In
Proc. SIGIR. 1119–1122.
[25] E. M. Voorhees and D. K. Harman. 2005. TREC: Experiment and Evaluation in
Information Retrieval. The MIT Press.
[26] W. Webber and L. A. F. Park. 2009. Score adjustment for correction of pooling
bias. In Proc. SIGIR. 444–451.
[27] E. Yilmaz and J. A. Aslam. 2008. Estimating average precision when judgments
are incomplete. Knowledge and Information Systems 16, 2 (2008), 173–211.
[28] E. Yilmaz, E. Kanoulas, and J. A. Aslam. 2008. A simple and efficient sampling
method for estimating AP and NDCG. In Proc. SIGIR. 603–610.
[29] Z. Zhou. 2012. Ensemble Methods: Foundations and Algorithms. CRC press.
[30] J. Zobel. 1998. How reliable are the results of large-scale information retrieval
experiments?. In Proc. SIGIR. 307–314.

