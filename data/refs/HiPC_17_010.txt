[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet
classiﬁcation with deep convolutional neural networks,” in
Advances in neural information processing systems, 2012, pp.
1097–1105.

[2] C. Nebauer, “Evaluation of convolutional neural networks for
visual recognition,” IEEE Transactions on Neural Networks,
vol. 9, no. 4, pp. 685–696, 1998.

[3] O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, L. Deng, G. Penn,
and D. Yu, “Convolutional neural networks for speech recognition,” IEEE/ACM Transactions on audio, speech, and language processing, vol. 22, no. 10, pp. 1533–1545, 2014.

[4] P. Swietojanski, A. Ghoshal, and S. Renals, “Convolutional
neural networks for distant speech recognition,” IEEE Signal
Processing Letters, vol. 21, no. 9, pp. 1120–1124, 2014.

[5] Y. Kim, “Convolutional neural networks for sentence classiﬁcation,” arXiv preprint arXiv:1408.5882, 2014.
[6] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao,
A. Senior, P. Tucker, K. Yang, Q. V. Le et al., “Large scale
distributed deep networks,” in Advances in neural information
processing systems, 2012, pp. 1223–1231.

[7] K. Simonyan and A. Zisserman, “Very deep convolutional
networks for large-scale image recognition,” arXiv preprint
arXiv:1409.1556, 2014.

[8] Y. Qian, M. Bi, T. Tan, and K. Yu, “Very deep convolutional neural networks for noise robust speech recognition,”
IEEE/ACM Transactions on Audio, Speech, and Language
Processing, vol. 24, no. 12, pp. 2263–2276, 2016.

[9] S. Ruder, “An overview of gradient descent optimization
algorithms,” arXiv preprint arXiv:1609.04747, 2016.

[10] T. M. Chilimbi, Y. Suzue, J. Apacible, and K. Kalyanaraman,
“Project adam: Building an efﬁcient and scalable deep learning training system.” in OSDI, vol. 14, 2014, pp. 571–582.

[11] D. Das, S. Avancha, D. Mudigere, K. Vaidynathan, S. Sridharan, D. Kalamkar, B. Kaul, and P. Dubey, “Distributed
deep learning using synchronous stochastic gradient descent,”
arXiv preprint arXiv:1602.06709, 2016.
[12] F. N. Iandola, M. W. Moskewicz, K. Ashraf, and K. Keutzer,
“Firecaffe: near-linear acceleration of deep neural network
training on compute clusters,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
2016, pp. 2592–2600.
[13] H. Cui, H. Zhang, G. R. Ganger, P. B. Gibbons, and E. P.
Xing, “Geeps: Scalable deep learning on distributed gpus with
a gpu-specialized parameter server,” in Proceedings of the
Eleventh European Conference on Computer Systems. ACM,
2016, p. 4.
[14] S.-Y. Zhao and W.-J. Li, “Fast asynchronous parallel stochastic gradient descent: A lock-free approach with convergence
guarantee.” in AAAI, 2016, pp. 2379–2385.
[15] W. Zhang, S. Gupta, X. Lian, and J. Liu, “Stalenessaware async-sgd for distributed deep learning,” arXiv preprint
arXiv:1511.05950, 2015.
[16] J. Schmidhuber, “Deep learning in neural networks: An
overview,” Neural networks, vol. 61, pp. 85–117, 2015.
[17] R. Arora, A. Basu, P. Mianjy, and A. Mukherjee, “Understanding deep neural networks with rectiﬁed linear units,”
arXiv preprint arXiv:1611.01491, 2016.
[18] J. Ngiam, A. Coates, A. Lahiri, B. Prochnow, Q. V. Le, and
A. Y. Ng, “On optimization methods for deep learning,” in
Proceedings of the 28th International Conference on Machine
Learning (ICML-11), 2011, pp. 265–272.
[19] L. Bottou, “Large-scale machine learning with stochastic gradient descent,” in Proceedings of COMPSTAT’2010.
Springer, 2010, pp. 177–186.
[20] K. Chellapilla, S. Puri, and P. Simard, “High performance
convolutional neural networks for document processing,” in
Tenth International Workshop on Frontiers in Handwriting
Recognition. Suvisoft, 2006.
[21] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long,
R. Girshick, S. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for fast feature embedding,” arXiv preprint
arXiv:1408.5093, 2014.
[22] R. Collobert, S. Bengio, and J. Mariéthoz, “Torch: a modular
machine learning software library,” Idiap, Tech. Rep., 2002.
[23] E.
Shelhamer
and
.
contributors,
https://github.com/BVLC/caffe.git, 2017.

[24] N. Strom, “Scalable distributed dnn training using commodity
gpu cloud computing.” in INTERSPEECH, vol. 7, 2015, p. 10.
[25] I.-H. Chung, T. N. Sainath, B. Ramabhadran, M. Picheny,
J. Gunnels, V. Austel, U. Chauhari, and B. Kingsbury, “Parallel deep neural network training for big data on blue gene/q,”
IEEE Transactions on Parallel and Distributed Systems,
vol. 28, no. 6, pp. 1703–1714, 2017.
