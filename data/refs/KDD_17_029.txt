[1] Charu C Aggarwal, Joel L Wolf, Philip S Yu, Cecilia Procopiuc, and Jong Soo Park.
1999. Fast algorithms for projected clustering. In ACM SIGMoD Record, Vol. 28.
ACM, 61–72.

[2] Charu C. Aggarwal and Philip S. Yu. 2000. Finding Generalized Projected Clusters
in High Dimensional Spaces. In Proceedings of the 2000 ACM SIGMOD International
Conference on Management of Data (SIGMOD ’00). ACM, New York, NY, USA,
70–81. https://doi.org/10.1145/342009.335383

[3] Rakesh Agrawal, Johannes Gehrke, Dimitrios Gunopulos, and Prabhakar Ragha-
van. 1998. Automatic subspace clustering of high dimensional data for data mining

applications. Vol. 27. ACM.

[4] David Arthur and Sergei Vassilvitskii. 2007. k-means++: The advantages of
careful seeding. In Proceedings of the eighteenth annual ACM-SIAM symposium on
Discrete algorithms. Society for Industrial and Applied Mathematics, 1027–1035.
[5] Bahman Bahmani, Benjamin Moseley, Andrea Vattani, Ravi Kumar, and Sergei
Vassilvitskii. 2012. Scalable k-means++. Proceedings of the VLDB Endowment 5, 7
(2012), 622–633.

[6] Christian Böhm, Karin Kailing, Peer Kröger, and Arthur Zimek. 2004. Computing
clusters of correlation connected objects. In Proceedings of the 2004 ACM SIGMOD
international conference on Management of data. ACM, 455–466.

[7] Christian Böhm and Claudia Plant. 2015. Mining Massive Vector Data on Single
Instruction Multiple Data Microarchitectures. In Data Mining Workshop (ICDMW),
2015 IEEE International Conference on. IEEE, 597–606.

[8] Fernando De la Torre and Takeo Kanade. 2006. Discriminative cluster analysis.
In Proceedings of the 23rd international conference on Machine learning. ACM,
241–248.

[9] Inderjit S. Dhillon, Yuqiang Guan, and Brian Kulis. 2004. Kernel k-means: spectral
clustering and normalized cuts. In Proceedings of the Tenth ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and Data Mining, Seattle, Washington,
USA, August 22-25, 2004. 551–556. https://doi.org/10.1145/1014052.1014118

[10] Chris Ding and Tao Li. 2007. Adaptive dimension reduction using discriminant
analysis and k-means clustering. In Proceedings of the 24th international conference
on Machine learning. ACM, 521–528.

[11] Joseph C Dunn. 1973. A fuzzy relative of the ISODATA process and its use in

detecting compact well-separated clusters. (1973).

[12] Charles Elkan. 2003. Using the triangle inequality to accelerate k-means. In ICML,

Vol. 3. 147–153.

[13] Sebastian Goebl, Xiao He, Claudia Plant, and Christian Böhm. 2014. Finding the
optimal subspace for clustering. In Data Mining (ICDM), 2014 IEEE International
Conference on. IEEE, 130–139.

[14] Robert M. Gray and David L. Neuhoff. 1998. Quantization. IEEE transactions on

information theory 44, 6 (1998), 2325–2383.

[15] Greg Hamerly. 2010. Making k-means even faster. In Proceedings of the 2010 SIAM

international conference on data mining. SIAM, 130–140.

[16] Greg Hamerly and Charles Elkan. 2004. Learning the k in k-means. (2004).
[17] Aapo Hyvärinen and Erkki Oja. 2000. Independent component analysis: algo-

rithms and applications. Neural networks 13, 4 (2000), 411–430.

[18] Anil K Jain and Richard C Dubes. 1988. Algorithms for clustering data. Prentice-

[19] Ian Jolliffe. 2002. Principal component analysis. Wiley Online Library.
[20] Brian Kulis and Michael I Jordan. 2012. Revisiting k-means: New algorithms via
Bayesian nonparametrics. In Proceedings of the 23rd International Conference on
Ma- chine Learning (2012).

[21] Stuart Lloyd. 1982. Least squares quantization in PCM. IEEE transactions on

information theory 28, 2 (1982), 129–137.

[22] Dijun Luo, Chris HQ Ding, and Heng Huang. 2011. Linear Discriminant Analysis:

New Formulations and Overfit Analysis.. In AAAI.

[23] Andrew W Moore. 1999. Very fast EM-based mixture model clustering using
multiresolution kd-trees. Advances in Neural information processing systems
(1999), 543–549.

[24] Dan Pelleg and Andrew Moore. 1999. Accelerating exact k-means algorithms
with geometric reasoning. In Proceedings of the fifth ACM SIGKDD international
conference on Knowledge discovery and data mining. ACM, 277–281.

[25] Dan Pelleg, Andrew W Moore, and others. 2000. X-means: Extending K-means

with Efficient Estimation of the Number of Clusters.. In ICML, Vol. 1.

[26] Steven J Phillips. 2002. Acceleration of k-means and related clustering algorithms.
In Workshop on Algorithm Engineering and Experimentation. Springer, 166–177.
[27] Bernhard Schölkopf, Alexander Smola, and Klaus-Robert Müller. 1997. Kernel
principal component analysis. In International Conference on Artificial Neural
Networks. Springer, 583–588.

[28] Michael Steinbach, George Karypis, Vipin Kumar, and others. 2000. A comparison
of document clustering techniques. In KDD workshop on text mining, Vol. 400.
Boston, 525–526.

[29] Joshua B Tenenbaum, Vin De Silva, and John C Langford. 2000. A global geometric
framework for nonlinear dimensionality reduction. science 290, 5500 (2000), 2319–
2323.

[30] Max Welling and Kenichi Kurihara. 2006. Bayesian K-Means as a" Maximization-

Expectation" Algorithm.. In SDM. SIAM, 474–478.

[31] Xindong Wu, Vipin Kumar, J Ross Quinlan, Joydeep Ghosh, Qiang Yang, Hiroshi
Motoda, Geoffrey J McLachlan, Angus Ng, Bing Liu, S Yu Philip, and others. 2008.
Top 10 algorithms in data mining. Knowledge and information systems 14, 1 (2008),
1–37.

[32] Jieping Ye, Zheng Zhao, and Mingrui Wu. 2007. Discriminative K-means for
Clustering. In Advances in Neural Information Processing Systems 20, Proceedings
of the Twenty-First Annual Conference on Neural Information Processing Systems,
Vancouver, British Columbia, Canada, December 3-6, 2007. 1649–1656. http://
papers.nips.cc/paper/3176-discriminative-k-means-for-clustering

Hall, Inc.

