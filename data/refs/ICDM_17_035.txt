[1] D. Gondek and T. Hofmann, “Non-redundant data clustering,” in
IEEE ICDM 2004, pp. 75-82.

[2] E. Bae and J. Bailey, “Coala: A novel approach for the extraction
of an alternate clustering of high quality and high dissimilarity,” in
IEEE ICDM 2006, pp. 53-62.

[3] Y. Cui, X. Z. Fern, and J. G. Dy, “Non-redundant multi-view clustering via orthogonalization,” in IEEE ICDM 2007, pp. 133-142.

[4] I. Davidson and Z. Qi, “Finding alternative clusterings using constraints,” in JEEE ICDM 2008, pp. 773-778.

[5] Z. Qi and I. Davidson, “A principled and flexible framework for
finding alternative clusterings,’ in KDD. ACM, 2009, pp. 717-726.

[6] X. H. Dang and J. Bailey, “Generation of alternative clusterings using
the cami approach,” in Proceedings of the 2010 SIAM International
Conference on Data Mining. SIAM, 2010, pp. 118-129.

[7] D. Niu, J. G. Dy, and M. I. Jordan, “Multiple non-redundant spectral
clustering views,” in ICML, 2010, pp. 831-838.

[8] X. H. Dang and J. Bailey, “Generating multiple alternative clusterings via globally optimal subspaces,” Data Mining and Knowledge
Discovery, vol. 28, no. 3, pp. 569-592, 2014.

[9] D. Niu, J. G. Dy, and M. I. Jordan, “Iterative discovery of multiple
alternative clustering views,’ IEEE Transactions on Pattern Analysis
and Machine Intelligence, vol. 36, no. 7, pp. 1340-1353, 2014.

[10] S. Yang and L. Zhang, “Non-redundant multiple clustering by nonnegative matrix factorization,’ Machine Learning, pp. 1-18, 2016.

[11] A. Gretton, O. Bousquet, A. Smola, and B. Schélkopf, “Measuring statistical dependence with Hilbert-Schmidt norms,” in ALT.
Springer, 2005, pp. 63-77.

[12] K. Fukumizu, A. Gretton, X. Sun, and B. Schélkopf, “Kernel measures of conditional dependence,” in NIPS, vol. 20, 2007, pp. 489496.

[13] X. Sun, D. Janzing, B. Schélkopf, and K. Fukumizu, “A kernel-based
causal learning algorithm,” in JCML. ACM, 2007, pp. 855-862.

[14] F. R. Bach and M. I. Jordan, “Kernel independent component analysis,” Journal of Machine Learning Research, vol. 3, no. Jul, pp. 148,
2002.

[15] C. R. Baker, “Joint measures and cross-covariance operators,” Transactions of the American Mathematical Society, vol. 186, pp. 273-289,
1973.

[16] K. P. Murphy, Machine learning: A probabilistic perspective.
Press, 2012.

[17] A. Y. Ng, M. I. Jordan, Y. Weiss et al., “On spectral clustering:
Analysis and an algorithm,” in NIPS, vol. 14, no. 2, 2001, pp. 849856.

[18] R. Bhatia, Matrix Analysis. New York: Springer-Verlag, 1997.

[19] K. Fukumizu, F, R. Bach, and M. I. Jordan, “Kernel dimension
reduction in regression,” The Annals of Statistics, vol. 37, no. 4, pp.
1871-1905, 2009.

[20] M. Wang, F. Sha, and M. I. Jordan, “Unsupervised kernel dimension
reduction,” in NIPS, 2010, pp. 2379-2387.

[21] Z. Wen and W. Yin, “A feasible method for optimization with
orthogonality constraints,” Mathematical Programming, vol. 142, no.
1-2, pp. 397-434, 2013.

[22] N. X. Vinh and J. Epps, “Mincentropy: A novel information theoretic
approach for the generation of alternative clusterings,” in IEEE ICDM
2010, pp. 521-530.

[23] C. Blake and C. J. Merz, “{UCI} repository of machine leaming
databases,” 1998.

[24] Y. Hoshida, J.-P. Brunet, P. Tamayo, T. R. Golub, and J. P. Mesirov,
“Subclass mapping: Identifying common subtypes in independent
disease data sets,” PloS one, vol. 2, no. 11, p. 1195, 2007.

[25] M. Craven and S. Slattery, “Relational learning with statistical predicate invention: Better models for hypertext,’ Machine Learning,
vol. 43, no. 1, pp. 97-119, 2001.

[26] P. Jain, R. Meka, and I. S. Dhillon, “Simultaneous unsupervised learning of disparate clusterings,” Statistical Analysis and Data Mining,
vol. 1, no. 3, pp. 195-210, 2008.

[27] X.-H. Dang and J. Bailey, “A hierarchical information theoretic
technique for the discovery of non linear alternative clusterings,” in
KDD. ACM, 2010, pp. 573-582.