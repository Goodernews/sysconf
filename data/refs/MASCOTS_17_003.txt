[1] “Tesla K20X GPU accelerator,” http://www.nvidia.com/content/pdf/
kepler/tesla-k20x-bd-06397-001-v07.pdf.
[2] “XID errors,” https://docs.nvidia.com/deploy/xid-errors/.
[3] N. K. Ahmed et al., “An empirical comparison of machine learning
models for time series forecasting,” Econometric Reviews, 2010.
[4] L. N. Bairavasundaram, Characteristics, impact, and tolerance of partial
disk failures. ProQuest, 2008.
[5] G. E. Box et al., Time series analysis: forecasting and control. John
Wiley & Sons, 2015.
[6] N. V. Chawla et al., “SMOTE: synthetic minority over-sampling technique,” Journal of artiﬁcial intelligence research, 2002.
[7] P. Cortez et al., “Multi-scale internet trafﬁc forecasting using neural
networks and time series methods,” Expert Systems, 2012.
[8] C. Di Martino et al., “Lessons learned from the analysis of system
failures at petascale: The case of blue waters,” in DSN, 2014.
[9] N. El-Sayed et al., “Temperature management in data centers: why some
(might) like it hot,” SIGMETRICS, 2012.
[10] B. Fang et al., “Gpu-qin: A methodology for evaluating the error
resilience of GPGPU applications,” in ISPASS, 2014.
[11] P. Goodwin et al., “The holt-winters approach to exponential smoothing:
50 years old and going strong,” Foresight, 2010.
[12] S. Gupta et al., “Understanding and exploiting spatial properties of
system failures on extreme-scale HPC systems,” in DSN, 2015.
[13] I. Guyon, “A scaling law for the validation-set training-set size ratio,”
AT&T Bell Laboratories, 1997.
[14] S. K. S. Hari et al., “SASSIFI: Evaluating resilience of GPU applications,” in SELSE, 2015.
[15] A. A. Hwang et al., “Cosmic rays don’t strike twice: understanding the
nature of DRAM errors and the implications for system design,” in ACM
SIGPLAN Notices, 2012.
[16] R. K. Jena et al., “Soft computing methodologies in bioinformatics,”
European Journal of Scientiﬁc Research, 2009.
[17] D. Kothe et al., “Computational science requirements for leadership
computing,” ORNL Technical Report, 2007.
[18] L. M. Leemis et al., Discrete-event simulation: A ﬁrst course. Pearson
Prentice Hall, 2006.
[19] G. Li et al., “Understanding error propagation in GPGPU applications,”
in SC, 2016.
[20] J. Li et al., “Forecasting web page views: Methods and observations,”
Journal of Machine Learning Research, 2008.
[21] C. L. Mendes et al., “Deploying a large petascale system: The blue
waters experience,” Procedia Computer Science, 2014.
[22] J. Meng et al., “Best-effort parallel execution framework for recognition
and mining applications,” in IPDPS, 2009.
[23] S. Mitra et al., “Bioinformatics with soft computing,” IEEE Transactions
on Systems, Man, and Cybernetics, Part C (Applications and Reviews),
2006.
[24] S. Mitra et al., “Data mining in soft computing framework: a survey,”
IEEE Trans. Neural Networks, 2002.
[25] B. Nie et al., “A large-scale study of soft-errors on GPUs in the ﬁeld,”
in HPCA, 2016.
[26] A. Oliner et al., “What supercomputers say: A study of ﬁve system
logs,” in DSN, 2007.
[27] M. K. Patterson, “The effect of data center temperature on energy
efﬁciency,” in ITHERM, 2008.
[28] F. Provost, “Machine learning from imbalanced data sets 101,” in
AAAI’2000 workshop on imbalanced data sets, 2000.
[29] B. Schroeder et al., “Disk failures in the real world: What does an MTTF
of 1, 000, 000 hours mean to you?” in FAST, 2007.
[30] B. Schroeder et al., “A large-scale study of failures in high-performance
computing systems,” IEEE Trans. Dependable Sec. Comput., 2010.
[31] B. Schroeder et al., “Flash reliability in production: The expected and
the unexpected,” in FAST, 2016.
[32] B. Schroeder et al., “DRAM errors in the wild: A large-scale ﬁeld study,”
in ACM SIGMETRICS Performance Evaluation Review, 2009.
[33] M. Shokouhi, “Detecting seasonal queries by time-series analysis,” in
SIGIR, 2011.
[34] V. Sridharan et al., “Feng shui of supercomputer memory positional
effects in DRAM and SRAM faults,” in SC, 2013.
[35] D. Tiwari et al., “Understanding GPU errors on large-scale HPC systems
and the implications for system design and operation,” in HPCA, 2015.
[36] N. Tran et al., “Automatic ARIMA time series modeling for adaptive
I/O prefetching,” IEEE Trans. Parallel Distrib. Syst., 2004.
[37] H. L. Truong et al., “Soft computing approach to performance analysis
of parallel and distributed programs,” in Euro-Par, ser. Lecture Notes in
Computer Science. Springer, 2005.
[38] R. Venkatagiri et al., “Approxilyzer: Towards a systematic framework for
instruction-level approximate computing and its application to hardware
resiliency,” in MICRO, 2016.
[39] J. S. Vetter et al., “Keeneland: Bringing heterogeneous GPU computing
to the computational science community,” Computing in Science and
Engineering, 2011.
[40] J. Xue et al., “PRACTISE: Robust prediction of data center time series,”
in CNSM, 2015.
[41] J. Xue et al., “Proactive management of systems via hybrid analytic
techniques,” in ICCAC, 2015.
[42] K. S. Yim et al., “Hauberk: Lightweight silent data corruption error
detector for GPGPU,” in IPDPS, 2011.
[43] Z. Zhuang et al., “Capacity planning and headroom analysis for taming
database replication latency: experiences with linkedin internet trafﬁc,”
in ICPE, 2015.
