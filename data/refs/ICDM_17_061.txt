[1] Y. Kim, “Convolutional neural networks for sentence classification,”
CoRR, vol. abs/1408.5882, 2014.

[2] A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov, “Bag of tricks for
efficient text classification,’ CoRR, vol. abs/1607.01759, 2016.

[3] S. Lai, L. Xu, K. Liu, and J. Zhao, “Recurrent convolutional neural
networks for text classification,” in Proceedings of the Twenty-Ninth
AAAI Conference on Artificial Intelligence, January 25-30, 2015, Austin,
Texas, USA., 2015, pp. 2267-2273.

[4] X. Zhang, J. J. Zhao, and Y. LeCun, “Character-level convolutional
networks for text classification,’ 2015, pp. 649-657.

[5] J. Y. Lee and F. Dernoncourt, ‘Sequential short-text classification with recurrent and convolutional neural networks,’ CoRR, vol.
abs/1603.03827, 2016.

[6] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, “Distributed
representations of words and phrases and their compositionality,’ CoRR,
vol. abs/1310.4546, 2013.

[7] Z. Zhu, C. Yin, B. Qian, Y. Cheng, J. Wei, and F. Wang, “Measuring
patient similarities via a deep architecture with medical concept embedding,” in IEEE 16th International Conference on Data Mining, ICDM
2016, December 12-15, 2016, Barcelona, Spain, 2016, pp. 749-758.
[8] 'V. Mnih, N. Heess, A. Graves, and K. Kavukcuoglu, “Recurrent models
of visual attention,’ in Advances in Neural Information Processing
Systems 27: Annual Conference on Neural Information Processing
Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, 2014,
pp. 2204-2212.

[10] K. Xu, J. Ba, R. Kiros, K. Cho, A. C. Courville, R. Salakhutdinov, R. S.
Zemel, and Y. Bengio, “Show, attend and tell: Neural image caption
generation with visual attention,’ CoRR, vol. abs/1502.03044, 2015.

[11] Y. Wang, M. Huang, X. Zhu, and L. Zhao, “Attention-based LSTM
for aspect-level sentiment classification,” in Proceedings of the 2016
Conference on Empirical Methods in Natural Language Processing,
EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, 2016, pp. 606615.

[12] O. Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever, and G. E. Hinton,
“Grammar as a foreign language,” in Advances in Neural Information
Processing Systems 28: Annual Conference on Neural Information
Processing Systems 2015, December 7-12, 2015, Montreal, Quebec,
Canada, 2015, pp. 2773-2781.

[13] R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning,
“Semi-supervised recursive autoencoders for predicting sentiment distributions,” in Proceedings of the 2011 Conference on Empirical Methods
in Natural Language Processing, EMNLP 2011, 27-31 July 2011, John
McIntyre Conference Centre, Edinburgh, UK, A meeting of SIGDAT, a
Special Interest Group of the ACL, 2011, pp. 151-161.

[14] S. I. Wang and C. D. Manning, “Baselines and bigrams: Simple,
good sentiment and topic classification,” in The 50th Annual Meeting
of the Association for Computational Linguistics, Proceedings of the
Conference, July 8-14, 2012, Jeju Island, Korea - Volume 2: Short
Papers, 2012, pp. 90-94.

[15] J. P. C. G. da Silva, L. Coheur, A. C. Mendes, and A. Wichert, “From
symbolic to sub-symbolic information in question classification,” Artif:
Intell. Rev., vol. 35, no. 2, pp. 137-154, 2011.

[16] R. Socher, B. Huval, C. D. Manning, and A. Y. Ng, “Semantic compositionality through recursive matrix-vector spaces,” in Proceedings of
the 2012 Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning, EMNLPCoNLL 2012, July 12-14, 2012, Jeju Island, Korea, 2012, pp. 1201—
1211.

[17] T. Miyato, A. M. Dai, and I. Goodfellow, “Adversarial training methods
for semi-supervised text classification,’ ICLR, 2017.

[18] P. Li, L. He, X. Hu, Y. Zhang, L. Li, and X. Wu, “Concept based short
text stream classification with topic drifting detection,” in IEEE 16th
International Conference on Data Mining, ICDM 2016, December 1215, 2016, Barcelona, Spain, 2016, pp. 1009-1014.

[19] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature
hierarchies for accurate object detection and semantic segmentation,”
in Computer Vision and Pattern Recognition, 2014.

[20] Z. Huang, W. Xu, and K. Yu, “Bidirectional LSTM-CRF models for
sequence tagging,” CoRR, vol. abs/1508.01991, 2015.

[21] G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and
C. Dyer, “Neural architectures for named entity recognition,’ CoRR,
vol. abs/1603.01360, 2016.

[22] E. Strubell, P. Verga, D. Belanger, and A. McCallum, “Fast and
accurate sequence labeling with iterated dilated convolutions,”’ CoRR,
vol. abs/1702.02098, 2017.

[23] A. L. Gorin, G. Riccardi, and J. H. Wright, “How may I help you?”
Speech Communication, vol. 23, no. 1-2, pp. 113-127, 1997.

[24] F. Mairesse, M. Gasic, F. Jurcicek, S. Keizer, B. Thomson, K. Yu,
and S. Young, “Spoken language understanding from unaligned data
using discriminative classification models,” in 2009 IEEE International
Conference on Acoustics, Speech and Signal Processing, April 2009, pp.
4749-4752.

[25] P. Xu and R. Sarikaya, “Convolutional neural network based triangular
crf for joint intent detection and slot filling,’ in 20/3 IEEE Workshop
on Automatic Speech Recognition and Understanding, Dec 2013, pp.
78-83.

[26] D. Guo, G. Tiir, W. Yih, and G. Zweig, “Joint semantic utterance
classification and slot filling with recursive neural networks,” in 2014
IEEE Spoken Language Technology Workshop, SLT 2014, South Lake
Tahoe, NV, USA, December 7-10, 2014, 2014, pp. 554-559.

[27] X. Zhang and H. Wang, “A joint model of intent determination and
slot filling for spoken language understanding,” in Proceedings of the
Twenty-Fifth International Joint Conference on Artificial Intelligence,
IJCAI 2016, New York, NY, USA, 9-15 July 2016, 2016, pp. 2993-2999.
[28] B. Liu and I. Lane, “Attention-based recurrent neural network models
for joint intent detection and slot filling,” in Interspeech 2016, 2016, pp.
685-689.

[29] , “Joint online spoken language understanding and language modeling with recurrent neural networks,” in Proceedings of the SIGDIAL
2016 Conference, The 17th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, 13-15 September 2016, Los Angeles, CA,
USA, 2016, pp. 22-30.

[30] Z. C. Lipton, “A critical review of recurrent neural networks for sequence
learning,” CoRR, vol. abs/1506.00019, 2015.

[31] M. Luong, Q. V. Le, I. Sutskever, O. Vinyals, and L. Kaiser, “Multi-task
sequence to sequence learning,’ CoRR, vol. abs/1511.06114, 2015.

[32] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
Comput., vol. 9, no. 8, pp. 1735-1780, Nov. 1997.

[33] J. D. Lafferty, A. McCallum, and F. C. N. Pereira, “Conditional random
fields: Probabilistic models for segmenting and labeling sequence data,”
in Proceedings of the Eighteenth International Conference on Machine
Learning (ICML 2001), Williams College, Williamstown, MA, USA, June
28 - July 1, 2001, 2001, pp. 282-289.

[34] S. Ren, K. He, R. B. Girshick, and J. Sun, “Faster R-CNN: towards
real-time object detection with region proposal networks,’ CoRR, vol.
abs/1506.01497, 2015.

[35] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
CoRR, vol. abs/1412.6980, 2014.

[36] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, “Scikit-learn: Machine learning in Python,’ Journal of Machine
Learning Research, vol. 12, pp. 2825-2830, 2011.