[1] BOOST C++ Libraries. http://www.boost.org.
[2] Alekh Agarwal, Olivier Chapelle, Miroslav Dudík, and John Langford. 2011. A
Reliable Effective Terascale Linear Learning System. CoRR abs/1110.4198 (2011).
[3] Gene M Amdahl. 1967. Validity of the single processor approach to achieving
large scale computing capabilities. In Proceedings of the April 18-20, 1967, spring
joint computer conference. ACM, 483–485.
[4] Léon Bottou. 1991. Stochastic gradient learning in neural networks. Proceedings
of Neuro-Nımes 91, 8 (1991).
[5] Léon Bottou. 2010. Large-scale machine learning with stochastic gradient descent.
In COMPSTAT’2010. Springer, 177–186.
[6] Léon Bottou. 2012. Stochastic gradient descent tricks. In Neural Networks: Tricks
of the Trade. Springer, 421–436.
[7] Trishul Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman.
2014. Project Adam: Building an Efficient and Scalable Deep Learning Training
System. In 11th OSDI. USENIX Association, 571–582.
convnet-benchmarks.
https://github.com/soumith/
[8] Soumith Chintala.
convnet-benchmarks. (????). Accessed: 2016-11-16.
[9] Matthieu Courbariaux, Jean-Pierre David, and Yoshua Bengio. 2014. Training deep neural networks with low precision multiplications. arXiv preprint
arXiv:1412.7024 (2014).
[10] Christopher De Sa, Christopher Ré, and Kunle Olukotun. 2016. Ensuring Rapid
Mixing and Low Bias for Asynchronous Gibbs Sampling. ICML (2016).
[11] Christopher De Sa, Ce Zhang, Kunle Olukotun, and Christopher Ré. 2015. Taming
the Wild: A Unified Analysis of Hogwild!-Style Algorithms. In NIPS.
[12] Li Deng. 2012. The MNIST database of handwritten digit images for machine
learning research. IEEE Signal Processing Magazine 29, 6 (2012), 141–142.
[13] Matthew Dixon, Diego Klabjan, and Jin Hoon Bang. 2016. Classification-based
Financial Markets Prediction using Deep Neural Networks. arXiv preprint
arXiv:1603.08604 (2016).
[14] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.
2015. Deep Learning with Limited Numerical Precision. ICML (2015).
[15] Stefan Hadjis, Ce Zhang, Ioannis Mitliagkas, and Christopher Ré. 2016. Omnivore: An Optimizer for Multi-device Deep Learning on CPUs and GPUs. CoRR
abs/1606.04487 (2016).
[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep residual
learning for image recognition. arXiv preprint arXiv:1512.03385 (2015).
[17] Joseph M Hellerstein, Christoper Ré, Florian Schoppmann, Daisy Zhe Wang,
Eugene Fratkin, Aleksander Gorajek, Kee Siong Ng, Caleb Welton, Xixuan Feng,
Kun Li, and others. 2012. The MADlib analytics library: or MAD skills, the SQL.
Proceedings of the VLDB Endowment 5, 12 (2012), 1700–1711.
[18] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long,
Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Caffe: Convolutional
Architecture for Fast Feature Embedding. arXiv preprint arXiv:1408.5093 (2014).
[19] Matthew Johnson, James Saunderson, and Alan Willsky. 2013. Analyzing hogwild
parallel gaussian Gibbs sampling. In NIPS. 2715–2723.
[20] Rashid Kaleem, Sreepathi Pai, and Keshav Pingali. 2015. Stochastic gradient
descent on GPUs. In Proceedings of the 8th Workshop on General Purpose Processing
using GPUs. ACM, 81–89.
[21] David Koeplinger, Christina Delimitrou, Raghu Prabhakar, Christos Kozyrakis,
Yaqi Zhang, and Kunle Olukotun. 2016. Automatic Generation of Efficient Accelerators for Reconfigurable Hardware. In ISCA 2016: 43rd International Symposium
on Computer Architecture.
[22] Alex Krizhevsky and Geoffrey Hinton. 2009. Learning multiple layers of features
from tiny images. (2009).
[23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classification with deep convolutional neural networks. In Advances in neural information
processing systems. 1097–1105.
[24] Quoc V Le. 2013. Building high-level features using large scale unsupervised
learning. In 2013 IEEE international conference on acoustics, speech and signal
processing. IEEE, 8595–8598.
[25] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradientbased learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278–
2324.
[26] Jaekyu Lee, Hyesoon Kim, and Richard Vuduc. 2012. When prefetching works,
when it doesn’t, and why. ACM Transactions on Architecture and Code Optimization (TACO) 9, 1 (2012), 2.
[27] Ji Liu and Stephen J. Wright. 2015. Asynchronous Stochastic Coordinate Descent:
Parallelism and Convergence Properties. SIOPT 25, 1 (2015), 351–376.
[28] Ji Liu, Stephen J Wright, Christopher Ré, Victor Bittorf, and Srikrishna Sridhar.
2015. An Asynchronous Parallel Stochastic Coordinate Descent Algorithm. JMLR
16 (2015), 285–322.
[29] Shaoli Liu, Zidong Du, Jinhua Tao, Dong Han, Tao Luo, Yuan Xie, Yunji Chen,
and Tianshi Chen. 2016. Cambricon: An instruction set architecture for neural networks. In Proceedings of the 43rd International Symposium on Computer
Architecture. IEEE Press, 393–405.
[30] Yucheng Low, Joseph E Gonzalez, Aapo Kyrola, Danny Bickson, Carlos E Guestrin,
and Joseph Hellerstein. 2014. Graphlab: A new framework for parallel machine
learning. arXiv preprint arXiv:1408.2041 (2014).
[31] Horia Mania, Xinghao Pan, Dimitris Papailiopoulos, Benjamin Recht, Kannan
Ramchandran, and Michael I Jordan. 2015. Perturbed Iterate Analysis for Asynchronous Stochastic Optimization. arXiv preprint arXiv:1507.06970 (2015).
[32] George Marsaglia. 2003. Xorshift RNGs. Journal of Statistical Software 8, 1 (2003).
[33] Makoto Matsumoto and Takuji Nishimura. 1998. Mersenne twister: a 623dimensionally equidistributed uniform pseudo-random number generator. ACM
Transactions on Modeling and Computer Simulation (TOMACS) 8, 1 (1998), 3–30.
[34] Ioannis Mitliagkas, Michael Borokhovich, Alexandros G. Dimakis, and Constantine Caramanis. 2015. FrogWild!: Fast PageRank Approximations on Graph
Engines. PVLDB (2015).
[35] Andrew Y. Ng and Michael I. Jordan. 2002. On Discriminative vs. Generative
Classifiers: A comparison of logistic regression and naive Bayes. In Advances in
Neural Information Processing Systems 14.
[36] Feng Niu, Benjamin Recht, Christopher Re, and Stephen Wright. 2011. Hogwild: A
lock-free approach to parallelizing stochastic gradient descent. In NIPS. 693–701.
[37] Cyprien Noel and Simon Osindero. 2014. Dogwild!–Distributed Hogwild for
CPU & GPU. (2014).
[38] François Panneton and Pierre L’ecuyer. 2005. On the xorshift random number
generators. ACM Transactions on Modeling and Computer Simulation (TOMACS)
15, 4 (2005), 346–361.
[39] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M.
Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine
Learning in Python. Journal of Machine Learning Research 12 (2011), 2825–2830.
[40] Raghu Prabhakar, David Koeplinger, Kevin J Brown, HyoukJoong Lee, Christopher De Sa, Christos Kozyrakis, and Kunle Olukotun. 2016. Generating configurable hardware from parallel patterns. In Proceedings of 21st ASPLOS. ACM,
651–665.
[41] Ali Rahimi and Benjamin Recht. 2007. Random features for large-scale kernel
machines. In Advances in neural information processing systems. 1177–1184.
[42] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1986. Learning
representations by back-propagating errors. Nature (1986).
[43] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. 2015. ImageNet Large Scale Visual Recognition
Challenge. International Journal of Computer Vision (IJCV) 115, 3 (2015), 211–252.
https://doi.org/10.1007/s11263-015-0816-y
[44] Daniel Sanchez and Christos Kozyrakis. 2013. ZSim: fast and accurate microarchitectural simulation of thousand-core systems. ACM SIGARCH Computer
Architecture News 41, 3 (2013), 475–486.
[45] Antony W Savich and Medhat Moussa. 2011. Resource efficient arithmetic
effects on rbm neural network solution quality using mnist. In 2011 International
Conference on Reconfigurable Computing and FPGAs. IEEE, 35–40.
[46] Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 2014. 1-bit stochastic
gradient descent and its application to data-parallel distributed training of speech
DNNs.. In INTERSPEECH. 1058–1062.
[47] Evan R Sparks, Ameet Talwalkar, Virginia Smith, Jey Kottalam, Xinghao Pan,
Joseph Gonzalez, Michael J Franklin, Michael I Jordan, and Tim Kraska. 2013.
MLI: An API for distributed machine learning. In 2013 IEEE 13th International
Conference on Data Mining. IEEE, 1187–1192.
[48] Nikko Strom. 2015. Scalable Distributed DNN Training Using Commodity GPU
Cloud Computing. In Sixteenth Annual Conference of the International Speech
Communication Association.
[49] Herb Sutter. 2005. The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software. Dr. Dobb’s Journal 30, 3 (2005).
[50] Vish Viswanathan. Disclosure of H/W prefetcher control on some Intel processors. https://software.intel.com/en-us/articles/disclosure-of-hwprefetcher-control-on-some-intel-processors. (????).
[51] Keval Vora, Sai Charan Koduru, and Rajiv Gupta. 2014. ASPIRE: exploiting
asynchronous parallelism in iterative algorithms using a relaxed consistency
based DSM. ACM SIGPLAN Notices 49, 10 (2014), 861–878.
[52] Strother H Walker and David B Duncan. 1967. Estimation of the probability of
an event as a function of several independent variables. Biometrika 54, 1-2 (1967),
167–179.
[53] Samuel Williams, Andrew Waterman, and David Patterson. 2009. Roofline: an
insightful visual performance model for multicore architectures. Commun. ACM
52, 4 (2009), 65–76.
[54] Hsiang-Fu Yu, Cho-Jui Hsieh, Si Si, and Inderjit S Dhillon. 2012. Scalable Coordinate Descent Approaches to Parallel Matrix Factorization for Recommender
Systems.. In ICDM. 765–774.
[55] Chiyuan Zhang. 2016. Mocha.jl: Deep Learning for Julia. https://devblogs.nvidia.
com/parallelforall/mocha-jl-deep-learning-julia/. (2016). Accessed: 2016-11-16.
[56] Ce Zhang and Christopher Ré. 2014. DimmWitted: A study of main-memory
statistical analytics. PVLDB (2014).
[57] Shanshan Zhang, Ce Zhang, Zhao You, Rong Zheng, and Bo Xu. 2013. Asynchronous stochastic gradient descent for DNN training. In 2013 IEEE International
Conference on Acoustics, Speech and Signal Processing. IEEE, 6660–6663.
