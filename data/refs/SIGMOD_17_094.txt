
[1] I. Abraham, O. Alonso, V. Kandylas, and A. Slivkins.
Adaptive crowdsourcing algorithms for the bandit
survey problem. In COLT, pages 882–910, 2013.
[2] Y. Bachrach, T. Graepel, T. Minka, and J. Guiver.
How to grade a test without knowing the answers—a
bayesian graphical model for adaptive crowdsourcing
and aptitude testing. arXiv preprint arXiv:1206.6386,
2012.
[3] J. Besag. On the statistical analysis of dirty pictures.
Journal of the Royal Statistical Society. Series B
(Methodological), pages 259–302, 1986.
[4] R. Boim, O. Greenshpan, T. Milo, S. Novgorodov,
N. Polyzotis, and W.-C. Tan. Asking the right
questions in crowd data sourcing. In 2012 IEEE 28th
International Conference on Data Engineering, pages
1261–1264. IEEE, 2012.
[5] C. C. Cao, J. She, Y. Tong, and L. Chen. Whom to
ask?: jury selection for decision making tasks on
micro-blog services. Proceedings of the VLDB
Endowment, 5(11):1495–1506, 2012.
[6] X. Chen, Q. Lin, and D. Zhou. Optimistic knowledge
gradient policy for optimal budget allocation in
crowdsourcing.
[7] P. Dai, C. H. Lin, D. S. Weld, et al. Pomdp-based
control of workflows for crowdsourcing. Artificial
Intelligence, 202:52–85, 2013.
[8] A. P. Dawid and A. M. Skene. Maximum likelihood
estimation of observer error-rates using the em
algorithm. Applied statistics, pages 20–28, 1979.
[9] G. Demartini, D. E. Difallah, and P. Cudré-Mauroux.
Zencrowd: leveraging probabilistic reasoning and
crowdsourcing techniques for large-scale entity linking.
In Proceedings of the 21st international conference on
World Wide Web, pages 469–478. ACM, 2012.
[10] A. P. Dempster, N. M. Laird, and D. B. Rubin.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the royal statistical society. Series
B (methodological), pages 1–38, 1977.
[11] J. Fan, G. Li, B. C. Ooi, K.-l. Tan, and J. Feng. icrowd:
An adaptive crowdsourcing framework. In Proceedings
of the 2015 ACM SIGMOD International Conference
on Management of Data, pages 1015–1030. ACM, 2015.
[12] J. Feng, G. Li, H. Wang, and J. Feng. Incremental
quality inference in crowdsourcing. In International
Conference on Database Systems for Advanced
Applications, pages 453–467. Springer, 2014.
[13] P. G. Ipeirotis and E. Gabrilovich. Quizz: targeted
crowdsourcing with a billion (potential) users. In
Proceedings of the 23rd international conference on
World wide web, pages 143–154. ACM, 2014.
[14] P. G. Ipeirotis, F. Provost, V. S. Sheng, and J. Wang.
Repeated labeling using multiple noisy labelers. Data
Mining and Knowledge Discovery, 28(2):402–441, 2014.
[15] P. G. Ipeirotis, F. Provost, and J. Wang. Quality
management on amazon mechanical turk. In
Proceedings of the ACM SIGKDD workshop on human
computation, pages 64–67. ACM, 2010.
[16] M. Joglekar, H. Garcia-Molina, and A. Parameswaran.
Evaluating the crowd with confidence. In Proceedings
of the 19th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 686–694.

ACM, 2013.
[17] D. R. Karger, S. Oh, and D. Shah. Iterative learning for
reliable crowdsourcing systems. In Advances in neural
information processing systems, pages 1953–1961, 2011.
[18] D. R. Karger, S. Oh, and D. Shah. Budget-optimal task
allocation for reliable crowdsourcing systems.
Operations Research, 62(1):1–24, 2014.
[19] A. R. Khan and H. Garcia-Molina. Hybrid strategies for
finding the max with the crowd: Technical report. 2014.
[20] A. R. Khan and H. Garcia-Molina. Attribute-based
crowd entity resolution. In Proceedings of the 25th
ACM International on Conference on Information and
Knowledge Management, pages 549–558. ACM, 2016.
[21] A. Khosla, N. Jayadevaprakash, B. Yao, and F.-F. Li.
Novel dataset for fine-grained image categorization:
Stanford dogs. In Proc. CVPR Workshop on
Fine-Grained Visual Categorization (FGVC), volume 2,
2011.
[22] R. Kindermann and L. Snell. Markov random fields and
their applications. 1980.
[23] A. Kobren, C. H. Tan, P. Ipeirotis, and E. Gabrilovich.
Getting more for less: Optimized crowdsourcing with
dynamic tasks and goals. In Proceedings of the 24th
international conference on world wide web, pages
592–602. ACM, 2015.
[24] D. Koller and N. Friedman. Probabilistic graphical
models: principles and techniques. MIT press, 2009.
[25] A. Kurve, D. J. Miller, and G. Kesidis. Multicategory
crowdsourcing accounting for variable task difficulty,
worker skill, and worker intention. IEEE Transactions
on Knowledge and Data Engineering, 27(3):794–809,
2015.
[26] X. Liu, M. Lu, B. C. Ooi, Y. Shen, S. Wu, and
M. Zhang. Cdas: a crowdsourcing data analytics
system. Proceedings of the VLDB Endowment,
5(10):1040–1051, 2012.
[27] A. Marcus, A. Parameswaran, et al. Crowdsourced data
management industry and academic perspectives.
Foundations and Trends in Databases, 6(1-2):1–161,
2015.
[28] A. Marcus, E. Wu, D. Karger, S. Madden, and
R. Miller. Human-powered sorts and joins. Proceedings
of the VLDB Endowment, 5(1):13–24, 2011.
[29] J. McAuley and J. Leskovec. Hidden factors and hidden
topics: understanding rating dimensions with review
text. In Proceedings of the 7th ACM conference on
Recommender systems, pages 165–172. ACM, 2013.
[30] D. Oleson, A. Sorokin, G. P. Laughlin, V. Hester, J. Le,
and L. Biewald. Programmatic gold: Targeted and
scalable quality assurance in crowdsourcing. Human
computation, 11(11), 2011.
[31] A. G. Parameswaran, H. Garcia-Molina, H. Park,
N. Polyzotis, A. Ramesh, and J. Widom. Crowdscreen:
Algorithms for filtering data with humans. In
Proceedings of the 2012 ACM SIGMOD International
Conference on Management of Data, pages 361–372.
ACM, 2012.
[32] J. Pearl. Probabilistic reasoning in intelligent systems:
networks of plausible inference. Morgan Kaufmann,
2014.
[33] A. Ramesh, A. Parameswaran, H. Garcia-Molina, and
N. Polyzotis. Identifying reliable workers swiftly. 2012.

[34] V. C. Raykar, S. Yu, L. H. Zhao, A. Jerebko, C. Florin,
G. H. Valadez, L. Bogoni, and L. Moy. Supervised
learning from multiple experts: whom to trust when
everyone lies a bit. In Proceedings of the 26th Annual
international conference on machine learning, pages
889–896. ACM, 2009.
[35] V. S. Sheng, F. Provost, and P. G. Ipeirotis. Get
another label? improving data quality and data mining
using multiple, noisy labelers. In Proceedings of the 14th
ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 614–622. ACM, 2008.
[36] J. W. Vaughan. Adaptive task assignment for
crowdsourced classification. 2013.
[37] M. Venanzi, J. Guiver, G. Kazai, P. Kohli, and
M. Shokouhi. Community-based bayesian aggregation
models for crowdsourcing. In Proceedings of the 23rd
international conference on World wide web, pages
155–164. ACM, 2014.
[38] V. Verroios and H. Garcia-Molina. Entity resolution
with crowd errors. In 2015 IEEE 31st International
Conference on Data Engineering, pages 219–230. IEEE,
2015.
[39] V. Verroios, P. Lofgren, and H. Garcia-Molina. tdp: An
optimal-latency budget allocation strategy for
crowdsourced maximum operations. Technical report,
Stanford University.
[40] N. Vesdapunt, K. Bellare, and N. Dalvi. Crowdsourcing
algorithms for entity resolution. Proceedings of the
VLDB Endowment, 7(12):1071–1082, 2014.
[41] J. Vuurens, A. P. de Vries, and C. Eickhoff. How much
spam can you take? an analysis of crowdsourcing
results to increase accuracy. In Proc. ACM SIGIR
Workshop on Crowdsourcing for Information Retrieval
(CIR’11), pages 21–26, 2011.

[42] J. Wang, T. Kraska, M. J. Franklin, and J. Feng.
Crowder: Crowdsourcing entity resolution. Proceedings
of the VLDB Endowment, 5(11):1483–1494, 2012.
[43] P. Welinder, S. Branson, P. Perona, and S. J. Belongie.
The multidimensional wisdom of crowds. In Advances
in neural information processing systems, pages
2424–2432, 2010.
[44] P. Welinder and P. Perona. Online crowdsourcing:
rating annotators and obtaining cost-effective labels.
2010.
[45] J. Whitehill, T.-f. Wu, J. Bergsma, J. R. Movellan, and
P. L. Ruvolo. Whose vote should count more: Optimal
integration of labels from labelers of unknown expertise.
In Advances in neural information processing systems,
pages 2035–2043, 2009.
[46] Z. Zhao, F. Wei, M. Zhou, W. Chen, and W. Ng.
Crowd-selection query processing in crowdsourcing
databases: A task-driven approach. In EDBT, pages
397–408, 2015.
[47] Z. Zhao, D. Yan, W. Ng, and S. Gao. A transfer
learning based framework of crowd-selection on twitter.
In Proceedings of the 19th ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 1514–1517. ACM, 2013.
[48] Y. Zheng, R. Cheng, S. Maniu, and L. Mo. On
optimality of jury selection in crowdsourcing. In
Proceedings of the 18th International Conference on
Extending Database Technology, EDBT 2015.
OpenProceedings. org., 2015.
[49] Y. Zheng, J. Wang, G. Li, R. Cheng, and J. Feng.
Qasca: a quality-aware task assignment system for
crowdsourcing applications. In Proceedings of the 2015
ACM SIGMOD International Conference on
Management of Data, pages 1031–1046. ACM, 2015.

