[1] “Top 500 Supercomputer List,” https://www.top500.org/.
[2] D. K. Panda, “Accelerating Big Data Processing
on Modern Clusters,” in Proceedings of the
1st Workshop on Performance Analysis of Big
Data Systems, ser. PABS ’15. New York, NY,
USA: ACM, 2015, pp. 1–1. [Online]. Available:
http://doi.acm.org/10.1145/2694730.2694733
[3] “DISC,” https://www.cs.cmu.edu/ bryant/pubdir/cmu-cs07-128.pdf.
[4] “IBTA,” http://www.inﬁnibandta.org/.
[5] “Cray Aries,” http://www.cray.com/sites/default/ﬁles/resou
rces/CrayXCNetwork.pdf.
[6] M. S. Birrittella, M. Debbage, R. Huggahalli, J. Kunz,
T. Lovett, T. Rimmer, K. D. Underwood, and R. C. Zak,
“Intel Omni-Path Architecture: Enabling Scalable, High
Performance Fabrics,” in 2015 IEEE 23rd Annual Symposium on High-Performance Interconnects, Aug 2015,
pp. 1–9.
[7] “MPI,” http://mpi-forum.org/.
[8] “OpenShmem,” http://www.openshmem.org/site/.
[9] “UPC,” https://upc-lang.org/.
[10] “PSM
Programming
Guide,”
http://www.intel.com/omnipath/FabricSoftwarePublicatio
ns.
[11] S. Sur, L. Chai, H.-W. Jin, and D. K. Panda, “Shared
Receive Queue based Scalable MPI Design for Inﬁniband Clusters,” in Proceedings 20th IEEE International
Parallel Distributed Processing Symposium, April 2006,
pp. 10 pp.–.
[12] P. Boyle, A. Yamaguchi, G. Cossu, and A. Portelli, “Grid:
A Next Generation Data Parallel C++ QCD library,”
2015.
[13] V. Morozov, J. Meng, V. Vishwanath, J. R. Hammond,
K. Kumaran, and M. E. Papka, “ALCF MPI Benchmarks:
Understanding Machine-Speciﬁc Communication Behavior,” in 2012 41st International Conference on Parallel
Processing Workshops, Sept 2012, pp. 19–28.
[14] “Top Crunch,” http://www.topcrunch.org.
[15] “SPEC MPI2007,” https://www.spec.org/auto/mpi2007/Do
cs/127.wrf2.html.
[16] J. Kim, W. J. Dally, S. Scott, and D. Abts, “TechnologyDriven, Highly-Scalable Dragonﬂy Topology,” in 2008
International Symposium on Computer Architecture, June
2008, pp. 77–88.
[17] R. S. Ramanujam and B. Lin, “Destination-based
Adaptive Routing on 2D Mesh Networks,” in
Proceedings of the 6th ACM/IEEE Symposium on
Architectures for Networking and Communications
Systems, ser. ANCS ’10. New York, NY, USA:
ACM, 2010, pp. 19:1–19:12. [Online]. Available:
http://doi.acm.org/10.1145/1872007.1872030
[18] A. Singh, W. J. Dally, A. K. Gupta, and B. Towles,
“GOAL: A Load-balanced Adaptive Routing Algorithm
for Torus Networks,” in 30th Annual International Symposium on Computer Architecture, 2003. Proceedings.,
June 2003, pp. 194–205.

[19] A. Vishnu, M. Koop, A. Moody, A. R. Mamidala, S. Narravula, and D. K. Panda, “Hot-Spot Avoidance with
Multi-Pathing over Inﬁniband: An MPI Perspective,”
in Seventh IEEE International Symposium on Cluster
Computing and the Grid (CCGrid ’07), May 2007, pp.
479–486.
[20] W. D. Thakur, Rajeevand Gropp, Improving the Performance of Collective Operations in MPICH. Berlin,
Heidelberg: Springer Berlin Heidelberg, 2003, pp. 257–
267. [Online]. Available: http://dx.doi.org/10.1007/9783-540-39924-7 38
[21] J. Bruck, C.-T. Ho, S. Kipnis, and D. Weathersby,
“Efﬁcient Algorithms for All-to-All Communications in
Multi-port Message-passing Systems,” in Proceedings
of the Sixth Annual ACM Symposium on Parallel
Algorithms and Architectures, ser. SPAA ’94. New
York, NY, USA: ACM, 1994, pp. 298–309. [Online].
Available: http://doi.acm.org/10.1145/181014.181756
[22] A. Gainaru, R. L. Graham, A. Polyakov, and
G. Shainer, “Using Inﬁniband Hardware GatherScatter Capabilities to Optimize MPI All-to-All,”
in Proceedings of the 23rd European MPI Users’
Group Meeting, ser. EuroMPI 2016. New York, NY,
USA: ACM, 2016, pp. 167–179. [Online]. Available:
http://doi.acm.org/10.1145/2966884.2966918
[23] J. Worringen, “Pipelining and Overlapping for MPI Collective Operations,” in Local Computer Networks, 2003.
LCN ’03. Proceedings. 28th Annual IEEE International
Conference on, Oct 2003, pp. 548–557.
