[1] D.A. Adams. A Computation Model with Data Flow Sequencing. Stanford University, 1968.
[2] M. Aftosmis, M. Berger, and G. Adomavicius. “A par- allel multilevel method for adaptively refined Carte- sian grids with embedded boundaries”. In: AIAA’00.
[3] Scott B. Baden and Daniel Shalit. “Performance Tradeoffs in Multi-tier Formulation of a Finite Dif- ference Method”. In: ICCS 2001.
[4] B. Bao et al. “Delta Send-Recv for Dynamic Pipelin- ing in MPI Programs”. In: CCGrid 2012.
[5] D. Bonachea. GASNet Specification, v1.1. Tech. rep. UCB/CSD-02-1207. EECS Department, University of California, Berkeley, 2002.
[6] L. E. Cannon. “A Cellular Computer to Implement the Kalman Filter Algorithm”. PhD thesis. 1969.
[7] P. Colella. Defining Software Requirements for Scien- tific Computing. 2004.
[8] A. Danalis et al. “MPI-aware Compiler Optimizations for Improving Communication-computation Over- lap”. In: ICS ’09.
[9] D. Das et al. “Compiler-controlled extraction of computation-communication overlap in MPI applica- tions”. In: IPDPS 2008.
[10] Edison Design Group, Inc. - C++ Front End. U R L : https://www.edg.com/docs/edg cpp.pdf.
[11] R. D. Hornung, J. A. Keasler, and M. B. Gokhale.
Hydrodynamics Challenge Problem, Lawrence Liv- ermore National Laboratory. Tech. rep. LLNL-TR- 490254.
[12] C. Huang, O. Lawlor, and L. V. Kale ́. “Adaptive
MPI”. In: LCPC’ 04.
[13] S. Jacob and S. B. Baden. “Hiding Communication Latency with Non-SPMD, Graph-Based Execution”. In: ICCS’ 09.
[14] L. V. Kale ́. “The virtualization model of parallel programming: Runtime optimizations and the state of art.” In: LACSI’ 02.
[15] L. V. Kale ́ and S. Krishnan. “CHARM++: A Portable Concurrent Object Oriented System Based on C++”. In: OOPSLA ’93.
[16] Laxmikant V. Kale ́. “The Virtualization Model of Parallel Programming : Runtime Optimizations and the State of Art”. In: LACSI’02.
[17] H. Kamal and A. Wagner. “FG-MPI: Fine-grain MPI for multicore and clusters”. In: IPDPSW’10.
[18] I Karlin, J. Keasler, and R. Neely. LULESH 2.0 Updates and Changes. Tech. rep. LLNL-TR-641973.
[19] V. Marjanovic ́ et al. “Overlapping Communication and Computation by Using a Hybrid MPI/SMPSs
Approach”. In: ICS ’10.
[20] T. Nguyen. “Bamboo: Automatic Translation of MPI
Source into a Latency-Tolerant Form”. PhD thesis.
2014.
[21] T. Nguyen et al. “Bamboo - Preliminary scaling
results on multiple hybrid nodes of Knights Corner
and Sandy Bridge processors”. In: WOLFHPC ’13.
[22] T. Nguyen et al. “Bamboo: Translating MPI Appli- cations to a Latency-tolerant, Data-driven Form”. In:
SC’ 12.
[23] S. Pellegrini, T. Hoefler, and T. Fahringer. “Exact
Dependence Analysis for Increased Communication
Overlap”. In: EuroMPI’12.
[24] J. M. Perez, R. M. Badia, and J. Labarta. “A
dependency-aware task-based programming environ-
ment for multi-core architectures”. In: CLUSTER’08.
[25] D. Quinlan. “ROSE: Compiler Support for Object-
Oriented Frameworks”. In: CPC2000.
[26] L. I. Sedov. Similarity and Dimensional Methods in
Mechanics. 1959.
[27] G. Zheng et al. “Automatic Handling of Global
Variables for Multi-threaded MPI Programs”. In: IC-
PADS’11.
[28] Y. Zheng et al. “UPC++: a PGAS extension for C++”.
In: IPDPS’14.
