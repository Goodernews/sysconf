[1] P. Kogge, K. Bergman, S. Borkar, D. Campbell, W. Carson, W. Dally,
M. Denneau, P. Franzon, W. Harrod, K. Hill et al., “Exascale computing
study: Technology challenges in achieving exascale systems,” 2008.
[2] J. Dongarra et al., “The international exascale software project
roadmap,” International Journal of High Performance Computing Applications, p. 1094342010391989, 2011.
[3] F. Petrini, D. J. Kerbyson, and S. Pakin, “The case of the missing
supercomputer performance: Achieving optimal performance on the
8,192 processors of ASCI Q,” in Supercomputing, 2003 ACM/IEEE
Conference. IEEE, 2003, pp. 55–55.
[4] I. B. Peng, S. Markidis, and E. Laure, “The cost of synchronizing imbalanced processes in message passing systems,” in Cluster Computing
(CLUSTER), 2015 IEEE International Conference on. IEEE, 2015, pp.
408–417.
[5] I. B. Peng, S. Markidis, E. Laure, G. Kestor, and R. Gioiosa, “Idle period
propagation in message-passing applications,” in High Performance
Computing and Communications (HPCC), 2016 IEEE 18th International
Conference on. IEEE, 2016, pp. 937–944.
[6] S. Ghosh, J. R. Hammond, A. J. Pena, P. Balaji, A. H. Gebremedhin,
and B. Chapman, “One-sided interface for matrix operations using MPI3 RMA: A case study with elemental,” in Parallel Processing (ICPP),
2016 45th International Conference on. IEEE, 2016, pp. 185–194.
[7] C. E. Leiserson, “The Cilk++ concurrency platform,” The Journal of
Supercomputing, vol. 51, no. 3, pp. 244–257, 2010.
[8] L. V. Kale and S. Krishnan, CHARM++: a portable concurrent object
oriented system based on C++. ACM, 1993, vol. 28, no. 10.
[9] B. Chamberlain, D. Callahan, and H. Zima, “Parallel programmability
and the Chapel language,” Int. J. High Perform. Comput. Appl.,
vol. 21, no. 3, pp. 291–312, Aug. 2007. [Online]. Available:
http://dx.doi.org/10.1177/1094342007078442
[10] W. Gropp, E. Lusk, N. Doss, and A. Skjellum, “A high-performance,
portable implementation of the MPI message passing interface standard,”
Parallel computing, vol. 22, no. 6, pp. 789–828, 1996.
[11] L. Adhianto, S. Banerjee, M. Fagan, M. Krentel, G. Marin, J. MellorCrummey, and N. R. Tallent, “HPCTOOLKIT: Tools for performance
analysis of optimized parallel programs,” Concurr. Comput. : Pract.
Exper., vol. 22, no. 6, pp. 685–701, Apr. 2010. [Online]. Available:
http://dx.doi.org/10.1002/cpe.v22:6
[12] I. B. Peng, S. Markidis, E. Laure, D. Holmes, and M. Bull, “A data
streaming model in MPI,” in Proceedings of the 3rd Workshop on
Exascale MPI. ACM, 2015, p. 2.
[13] O. Pearce, T. Gamblin, B. R. de Supinski, M. Schulz, and N. M. Amato,
“MPMD framework for ofﬂoading load balance computation,” in Parallel and Distributed Processing Symposium, 2016 IEEE International.
IEEE, 2016, pp. 943–952.
[14] J. Dean and S. Ghemawat, “MapReduce: simpliﬁed data processing on
large clusters,” Communications of the ACM, vol. 51, no. 1, pp. 107–113,
2008.
[15] T. Hoeﬂer, A. Lumsdaine, and J. Dongarra, “Towards efﬁcient mapreduce using MPI,” in Recent Advances in Parallel Virtual Machine and
Message Passing Interface. Springer, 2009, pp. 240–249.
[16] “PUMA benchmarks and dataset downloads,” https://engineering.
purdue.edu/∼puma/datasets.htm, accessed: 2016-10-01.
[17] T. Hoeﬂer, P. Gottschling, A. Lumsdaine, and W. Rehm, “Optimizing
a conjugate gradient solver with non-blocking collective operations,”
Parallel Computing, vol. 33, no. 9, pp. 624–633, 2007.
[18] S. Markidis, G. Lapenta, and Rizwan-uddin, “Multi-scale simulations
of plasma with iPIC3D,” Mathematics and Computers in Simulation,
vol. 80, no. 7, pp. 1509–1519, 2010.
[19] J. Birn, J. Drake, M. Shay, B. Rogers, R. Denton, M. Hesse,
M. Kuznetsova, Z. Ma, A. Bhattacharjee, A. Otto et al., “Geospace environmental modeling(GEM) magnetic reconnection challenge,” Journal
of Geophysical Research, vol. 106, no. A3, pp. 3715–3719, 2001.
[20] R. Gerstenberger, M. Besta, and T. Hoeﬂer, “Enabling highly-scalable remote memory access programming with MPI-3 one sided,” in 2013 SCInternational Conference for High Performance Computing, Networking,
Storage and Analysis (SC). IEEE, 2013, pp. 1–12.
[21] P. Beckman, R. Brightwell, B. de Supinski, M. Gokhale, S. Hofmeyr,
S. Krishnamoorthy, M. Lang, B. Maccabe, J. Shalf, and M. Snir, “Exascale operating systems and runtime software report,” US Department
of Energy, Technical Report, December, 2012.
[22] T. Wang, K. Mohror, A. Moody, K. Sato, and W. Yu, “An ephemeral
burst-buffer ﬁle system for scientiﬁc applications,” in Proceedings of the
International Conference for High Performance Computing, Networking,
Storage and Analysis. IEEE Press, 2016, p. 69.
[23] K. Gilles, “The semantics of a simple language for parallel programming,” In Information Processing, vol. 74, pp. 471–475, 1974.
[24] T. Von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser, Active
messages: a mechanism for integrated communication and computation.
ACM, 1992, vol. 20, no. 2.
[25] J. J. Willcock, T. Hoeﬂer, N. G. Edmonds, and A. Lumsdaine, “Active
Pebbles: parallel programming for data-driven applications,” in Proceedings of the international conference on Supercomputing. ACM, 2011,
pp. 235–244.
