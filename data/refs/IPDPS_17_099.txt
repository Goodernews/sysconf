[1] Center for Exascale Simulation of Advanced Reactors. https://cesar.mcs.
anl.gov.
[2] Center for Exascale Simulation of Combustion in Turbulence. https:
//cesar.mcs.anl.gov.
[3] CORAL Benchmarks. https://asc.llnl.gov/CORAL-benchmarks.
[4] NAS Parallel Benchmarks. http://www.nas.nasa.gov/publications/npb.
html.
[5] Nek5000. https://nek5000.mcs.anl.gov.
[6] QBOX.
http://computation.llnl.gov/projects/
qbox-computing-structures-quantum-level.
[7] SNAP: SN Application Proxu.
http://www.nersc.gov/users/
computational-systems/cori/nersc-8-procurement/trinity-nersc-8-rfp/
nersc-8-trinity-benchmarks/snap/.
[8] Pavan Balaji, Darius Buntinas, David Goodell, William Gropp, Sameer
Kumar, Ewing Lusk, Rajeev Thakur, and Jesper Larsson Träff. MPI on
a Million Processors. Parallel Processing Letters, 21:45–60, 2011.
[9] Mohamad Chaarawi and Edgar Gabriel. Computational Science – ICCS
2008: 8th International Conference, Kraków, Poland, June 23-25, 2008,
Proceedings, Part I, chapter Evaluating Sparse Data Storage Techniques
for MPI Groups and Communicators, pages 297–306. 2008.
[10] James Dinan, Sriram Krishnamoorthy, Pavan Balaji, Jeff R. Hammond,
Manojkumar Krishnan, Vinod Tipparaju, and Abhinav Vishnu. Recent
Advances in the Message Passing Interface: 18th European MPI Users’
Group Meeting, EuroMPI 2011, Santorini, Greece, September 18-21,
2011. Proceedings, chapter Noncollective Communicator Creation in
MPI, pages 282–291. Springer Berlin Heidelberg, Berlin, Heidelberg,
2011.
[11] David Goodell, William Gropp, Xin Zhao, and Rajeev Thakur. Scalable
Memory Use in MPI: A Case Study with MPICH2. In Proceedings of
the 18th EuroMPI Conference (EuroMPI), 2011.
[12] Humaira Kamal, Seyed M. Mirtaheri, and Alan Wagner. Scalability of
Communicators and Groups in MPI. In Proceedings of the 19th ACM
International Symposium on High Performance Distributed Computing
(HPDC), 2010.
[13] Humaira Kamal and Alan Wagner. An Integrated Fine-Grain Runtime
System for MPI. Computing, 96(4):293–309, 2014.
[14] Adam Moody, Dong H. Ahn, and Bronis R. de Supinski. Exascale
Algorithms for Generalized MPI Comm Split. In Proceedings of the
18th European MPI Users’ Group Conference on Recent Advances in
the Message Passing Interface, EuroMPI’11, 2011.
[15] Paul Sack and William Gropp. A Scalable MPI Comm Split Algorithm
for Exascale Computing. In Proceedings of the 17th European MPI
Users’ Group Meeting Conference on Recent Advances in the Message
Passing Interface, 2010.
[16] M. Si, A. J. Pea, J. Hammond, P. Balaji, M. Takagi, and Y. Ishikawa.
Casper: An Asynchronous Progress Model for MPI RMA on ManyCore Architectures. In Parallel and Distributed Processing Symposium
(IPDPS), 2015 IEEE International, pages 665–676, 2015.
[17] Jesper Larsson Träff. Recent Advances in the Message Passing Interface:
17th European MPI Users’ Group Meeting, EuroMPI 2012, Stuttgart,
Germany, September 12-15, 2012. Proceedings, chapter Compact and
Efficient Implementation of the MPI Group Operations, pages 170–178.
Springer Berlin Heidelberg, Berlin, Heidelberg, 2012.
[18] M. Valiev, E.J. Bylaska, N. Govind, K. Kowalski, T.P. Straatsma,
H.J.J. Van Dam, D. Wang, J. Nieplocha, E. Apra, T.L. Windus, and
W.A. de Jong. NWChem: A comprehensive and scalable open-source
solution for large scale molecular simulations . Computer Physics
Communications, 181(9):1477–1489, 2010.
