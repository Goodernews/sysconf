[1] P. J. Braam et al., “The lustre storage architecture,” 2004.
[2] F. B. Schmuck and R. L. Haskin, “Gpfs: A shared-disk file system for
large computing clusters.” in FAST, vol. 2, 2002, pp. 231–244.
[3] M. Moore, D. Bonnie, B. Ligon, M. Marshall, W. Ligon, N. Mills,
E. Quarles, S. Sampson, S. Yang, and B. Wilson, “Orangefs: Advancing
pvfs,” FAST poster session, 2011.
[4] The HDF Group. (1997-) Hierarchical Data Format, version 5.
Http://www.hdfgroup.org/HDF5.
[5] J. Li, W. keng Liao, A. Choudhary, R. Ross, R. Thakur, W. Gropp,
R. Latham, A. Siegel, B. Gallagher, and M. Zingale, “Parallel netCDF: A
High-Performance Scientific I/O Interface,” in Proceedings of the 2003
ACM/IEEE Conference on Supercomputing, ser. SC ’03. New York,
NY, USA: ACM, 2003, pp. 39–.
[6] J. F. Lofstead, S. Klasky, K. Schwan, N. Podhorszki, and C. Jin,
“Flexible io and integration for scientific codes through the adaptable
io system (adios),” in Proceedings of the 6th international workshop
on Challenges of large applications in distributed environments, ser.
CLADE ’08. New York, NY, USA: ACM, 2008, pp. 15–24.
[7] M. Factor, K. Meth, D. Naor, O. Rodeh, and J. Satran, “Object storage:
the future building block for storage systems,” in Local to Global Data
Interoperability - Challenges and Technologies, 2005, June 2005, pp.
119–123.
[8] S. A. Weil, S. A. Brandt, E. L. Miller, D. D. E. Long, and C. Maltzahn,
“Ceph: A scalable, high-performance distributed file system,” in Proceedings of the 7th Symposium on Operating Systems Design and Implementation, ser. OSDI ’06. Berkeley, CA, USA: USENIX Association,
2006, pp. 307–320.
[9] A. Devulapalli, D. Dalessandro, P. Wyckoff, N. Ali, and P. Sadayappan,
“Integrating parallel file systems with object-based storage devices,” in
SC ’07. New York, NY, USA: ACM, 2007, pp. 27:1–27:10.
[10] J. Arnold, Openstack swift: Using, administering, and developing for
swift object storage. ” O’Reilly Media, Inc.”, 2014.
[11] J. Lofstead, I. Jimenez, C. Maltzahn, Q. Koziol, J. Bent, and E. Barton,
“Daos and friends: a proposal for an exascale storage system,” in
Proceedings of the International Conference for High Performance
Computing, Networking, Storage and Analysis. IEEE Press, 2016, p. 50.
[12] J. Arnold, Object Storage for Genomics Deploying and administering
OpenStack Swift with SwiftStack for bioinformatics. SwiftStack, 2015.
[13] M. Seltzer and N. Murphy, “Hierarchical file systems are dead,” in
Proceedings of the 12th Conference on Hot Topics in Operating Systems,
ser. HotOS’09. Berkeley, CA, USA: USENIX Association, 2009, pp.
1–1.
[14] A. Parker-Wood, D. D. E. Long, E. L. Miller, M. Seltzer, and D. Tunkelang, “Making sense of file systems through provenance and rich
metadata,” University of California, Santa Cruz, Tech. Rep. UCSCSSRC-12-01, Mar. 2012.
[15] S. Patil and G. Gibson, “Scale and concurrency of giga+: File system
directories with millions of files,” in Proceedings of the 9th USENIX
Conference on File and Stroage Technologies, ser. FAST’11. Berkeley,
CA, USA: USENIX Association, 2011, pp. 13–13.
[16] Q. Zheng, K. Ren, G. Gibson, B. W. Settlemyer, and G. Grider, “Deltafs:
Exascale file systems scale better without dedicated servers,” in 10th
Parallel Data Storage Workshop, ser. PDSW ’15, New York, NY, USA,
2015, pp. 1–6.
[17] P. G. Brown, “Overview of SciDB: Large Scale Array Storage, Processing and Analysis,” in ACM SIGMOD, 2010, pp. 963–968.
[18] MongoDB. MongoDB. Https://www.mongodb.com/.
[19] F. Bonomi, M. Mitzenmacher, R. Panigrahy, S. Singh, and G. Varghese,
“An improved construction for counting bloom filters,” in European
Symposium on Algorithms. Springer, 2006, pp. 684–695.
[20] B. Dong, X. Li, Q. Wu, L. Xiao, and L. Ruan, “A dynamic and adaptive
load balancing strategy for parallel file system with large-scale i/o
servers,” J. Parallel Distrib. Comput., vol. 72, no. 10, pp. 1254–1268,
Oct. 2012.
[21] J. Soumagne, D. Kimpe, J. A. Zounmevo, M. Chaarawi, Q. Koziol,
A. Afsahi, and R. B. Ross, “Mercury: Enabling remote procedure
call for high-performance computing,” in 2013 IEEE International
Conference on Cluster Computing, CLUSTER 2013, Indianapolis, IN,
USA, September 23-27, 2013, 2013, pp. 1–8.
[22] A. Lakshman and P. Malik, “Cassandra: structured storage system on a
p2p network,” in Proceedings of the 28th ACM symposium on Principles
of distributed computing. ACM, 2009, pp. 5–5.
[23] S. Blanas, K. Wu, S. Byna, B. Dong, and A. Shoshani, “Parallel data
analysis directly on scientific file formats,” in Proceedings of the 2014
ACM SIGMOD international conference on Management of data. ACM,
2014, pp. 385–396.
[24] O. Rodeh, “B-trees, shadowing, and clones,” Trans. Storage, vol. 3,
no. 4, pp. 2:1–2:27, Feb. 2008.
[25] C. h. Chen, W. t. Huang, C. t. Chen, and R. s. Hsiao, “Improving the
reliability of jffs2,” in 2006 International Symposium on VLSI Design,
Automation and Test, April 2006, pp. 1–4.
[26] K. Shvachko, H. Kuang, S. Radia, and R. Chansler, “The hadoop distributed file system,” in Proceedings of the 2010 IEEE 26th Symposium
on Mass Storage Systems and Technologies (MSST), ser. MSST ’10.
Washington, DC, USA: IEEE Computer Society, 2010, pp. 1–10.

[27] S. Ghemawat, H. Gobioff, and S.-T. Leung, “The google file system,” in
Proceedings of the Nineteenth ACM Symposium on Operating Systems
Principles, ser. SOSP ’03. New York, NY, USA: ACM, 2003, pp.
29–43.
[28] S. A. Brandt, E. L. Miller, D. D. Long, and L. Xue, “Efficient metadata
management in large distributed storage systems,” in Mass Storage Systems and Technologies, 2003.(MSST 2003). Proceedings. 20th IEEE/11th
NASA Goddard Conference on. IEEE, 2003, pp. 290–298.
[29] D. Zhao, Z. Zhang, X. Zhou, T. Li, K. Wang, D. Kimpe, P. Carns,
R. Ross, and I. Raicu, “Fusionfs: Toward supporting data-intensive
scientific applications on extreme-scale high-performance computing
systems,” in Big Data (Big Data), 2014 IEEE International Conference
on. IEEE, 2014, pp. 61–70.
[30] X. Li, B. Dong, L. Xiao, L. Ruan, and D. Liu, “Cefls: A cost-effective
file lookup service in a distributed metadata file system,” in CCGrid ‘12,
May 2012, pp. 25–32.
[31] F. Wang, M. Nelson, S. Oral, S. Atchley, S. Weil, B. W. Settlemyer,
B. Caldwell, and J. Hill, “Performance and scalability evaluation of
the ceph parallel file system,” in Proceedings of the 8th Parallel Data
Storage Workshop. ACM, 2013, pp. 14–19.
[32] T. Wang, A. Moody, Y. Zhu, K. Mohror, K. Sato, T. Islam, and W. Yu,
“Metakv: A key-value store for metadata management of distributed
burst buffers,” in 2017 IEEE International Parallel and Distributed
Processing Symposium (IPDPS), May 2017, pp. 1174–1183.
[33] T. Wang, S. Oral, Y. Wang, B. Settlemyer, S. Atchley, and W. Yu,
“Burstmem: A high-performance burst buffer system for scientific applications,” in Big Data (Big Data), 2014 IEEE International Conference
on. IEEE, 2014, pp. 71–79.
[34] T. Wang, S. Oral, M. Pritchard, B. Wang, and W. Yu, “Trio: burst buffer
based i/o orchestration,” in Cluster Computing (CLUSTER), 2015 IEEE
International Conference on. IEEE, 2015, pp. 194–203.
[35] D. Dai, R. B. Ross, P. Carns, D. Kimpe, and Y. Chen, “Using property
graphs for rich metadata management in hpc systems,” in Proceedings
of the 9th Parallel Data Storage Workshop, ser. PDSW ’14. Piscataway,
NJ, USA: IEEE Press, 2014, pp. 7–12.
[36] Q. Liu, J. Logan, Y. Tian et al., “Hello ADIOS: the challenges and
lessons of developing leadership class I/O frameworks,” Concurrency
and Computation: Practice and Experience, vol. 26, no. 7, pp. 1453–
1473, 2014.
[37] S. Blanas and S. Byna, “Towards exascale scientific metadata
management,” CoRR, vol. abs/1503.08482, 2015. [Online]. Available:
http://arxiv.org/abs/1503.08482
[38] S. L. Pallickara, S. Pallickara, and M. Zupanski, “Towards efficient data
search and subsetting of large-scale atmospheric datasets,” Future Gener.
Comput. Syst., vol. 28, no. 1, pp. 112–118, Jan. 2012.
[39] G. A. Gibson, D. F. Nagle, K. Amiri, J. Butler, F. W. Chang, H. Gobioff,
C. Hardin, E. Riedel, D. Rochberg, and J. Zelenka, “A cost-effective,
high-bandwidth storage architecture,” SIGPLAN Not., vol. 33, no. 11,
pp. 92–103, Oct. 1998.
[40] B. Welch, M. Unangst, Z. Abbasi, G. Gibson, B. Mueller, J. Small,
J. Zelenka, and B. Zhou, “Scalable performance of the panasas parallel
file system,” in 6th USENIX Conference on File and Storage Technologies, ser. FAST’08. Berkeley, CA, USA: USENIX Association, 2008,
pp. 2:1–2:17.