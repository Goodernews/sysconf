[1] L. B. Holder, “Graph-based data mining,” in Encyclopedia of Data
Warehousing and Mining, Second Edition. IGI Global, 2009, pp. 943–
949.
[2] J. Shi and J. Malik, “Normalized cuts and image segmentation,” IEEE
Transactions on pattern analysis and machine intelligence, vol. 22, no. 8,
pp. 888–905, 2000.
[3] U. Von Luxburg, “A tutorial on spectral clustering,” Statistics and
computing, vol. 17, no. 4, pp. 395–416, 2007.
[4] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Schölkopf, “Learning
with local and global consistency,” in Advances in neural information
processing systems, 2004, pp. 321–328.
[5] M. Belkin and P. Niyogi, “Laplacian eigenmaps for dimensionality
reduction and data representation,” Neural computation, vol. 15, no. 6,
pp. 1373–1396, 2003.
[6] J. Wright, Y. Ma, J. Mairal, G. Sapiro, T. S. Huang, and S. Yan, “Sparse
representation for computer vision and pattern recognition,” Proceedings
of the IEEE, vol. 98, no. 6, pp. 1031–1044, 2010.
[7] R. Zass and A. Shashua, “Doubly stochastic normalization for spectral
clustering,” in Advances in neural information processing systems, 2007,
pp. 1569–1576.
[8] L. Hagen and A. B. Kahng, “New spectral methods for ratio cut partitioning and clustering,” IEEE Transactions on computer-aided design
of integrated circuits and systems, vol. 11, no. 9, pp. 1074–1085, 1992.
[9] Y. Yan, C. Shen, and H. Wang, “Efﬁcient semideﬁnite spectral clustering
via lagrange duality,” IEEE Transactions on image processing, vol. 23,
no. 8, pp. 3522–3534, 2014.
[10] C. Davis and W. M. Kahan, “The rotation of eigenvectors by a
perturbation. iii,” SIAM Journal on numerical analysis, vol. 7, no. 1,
pp. 1–46, 1970.
[11] X. Wang, F. Nie, and H. Huang, “Structured doubly stochastic matrix
for graph based clustering: Structured doubly stochastic matrix,” in
Proceedings of the 22nd ACM SIGKDD International conference on
Knowledge discovery and data mining. ACM, 2016, pp. 1245–1254.
[12] B. Mohar, Y. Alavi, G. Chartrand, and O. Oellermann, “The laplacian
spectrum of graphs,” Graph theory, combinatorics, and applications,
vol. 2, no. 871-898, p. 12, 1991.
[13] I. S. Dhillon, Y. Guan, and B. Kulis, “Kernel k-means: spectral clustering
and normalized cuts,” in Proceedings of the 10th ACM SIGKDD International conference on Knowledge discovery and data mining. ACM,
2004, pp. 551–556.
[14] H. Lutkepohl, “Handbook of matrices.” Computational statistics and
Data analysis, vol. 2, no. 25, p. 243, 1997.
[15] A. Joseph, B. Yu et al., “Impact of regularization on spectral clustering,”
The Annals of Statistics, vol. 44, no. 4, pp. 1765–1791, 2016.
[16] F. R. Chung, Spectral graph theory. American Mathematical Soc.,
1997, no. 92.
[17] Y. Yu, T. Wang, and R. J. Samworth, “A useful variant of the davis–
kahan theorem for statisticians,” Biometrika, vol. 102, no. 2, pp. 315–
323, 2014.
[18] L. Zelnik-Manor and P. Perona, “Self-tuning spectral clustering,” in
Advances in neural information processing systems, 2005, pp. 1601–
1608.
[19] D. P. Bertsekas et al., “Augmented lagrangian and differentiable exact
penalty methods,” 1981.
[20] J. Von Neumann, Functional Operators (AM-22), Volume 2: The Geometry of Orthogonal Spaces.(AM-22). Princeton University Press, 2016,
vol. 2.
[21] D. Luo, H. Huang, F. Nie, and C. H. Ding, “Forging the graphs: A low
rank and positive semideﬁnite graph learning approach,” in Advances in
neural information processing systems, 2012, pp. 2960–2968.
[22] J. Khan, J. S. Wei, M. Ringner, L. H. Saal, M. Ladanyi, F. Westermann,
F. Berthold, M. Schwab, C. R. Antonescu, C. Peterson et al., “Classiﬁcation and diagnostic prediction of cancers using gene expression proﬁling
and artiﬁcial neural networks,” Nature medicine, vol. 7, no. 6, p. 673,
2001.
[23] M. Wu and B. Schölkopf, “A local learning approach for clustering,”
in Advances in neural information processing systems, 2007, pp. 1529–
1536.
[24] A. Strehl and J. Ghosh, “Cluster ensembles—a knowledge reuse framework for combining multiple partitions,” Journal of machine learning
research, vol. 3, no. Dec, pp. 583–617, 2002.
