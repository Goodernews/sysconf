[1] R. Caruana, “Multitask learning,” Machine Learning, vol. 28, no. 1,
pp. 41–75, 1997.

[2] O. Banerjee, L. E. Ghaoui, and G. Natsoulis, “Convex optimization
techniques for fitting sparse Gaussian graphical models,” in Proc. Intl.
Conf. Machine Learning. Press, 2006, pp. 89–96.

[3] J. Friedman, T. Hastie, and R. Tibshirani, “Sparse inverse covariance
estimation with the graphical lasso,” Biostatistics, vol. 9, no. 3, pp.
432–441, 2008.
[4] T. Idé, A. C. Lozano, N. Abe, and Y. Liu, “Proximity-based anomaly
detection using sparse structure learning,” in Proc. of 2009 SIAM
International Conference on Data Mining (SDM 09), 2009, pp. 97–
108.
[5] S. Hara and T. Washio, “Learning a common substructure of multiple
graphical gaussian models,” Neural Networks, vol. 38, pp. 23–38,
2013.
[6] G. Varoquaux, A. Gramfort, J.-B. Poline, and B. Thirion, “Brain
covariance selection: better individual functional connectivity models
using population prior,” in Advances in Neural Information Process-
ing Systems, 2010, pp. 2334–2342.
[7] J. Honorio and D. Samaras, “Multi-task learning of gaussian graphical
models,” in Proceedings of the 27th International Conference on
Machine Learning, ser. ICML 2010, 2010, pp. 447–454.
[8] J. Chiquet, Y. Grandvalet, and C. Ambroise, “Inferring multiple
graphical structures,” Statistics and Computing, vol. 21, no. 4, pp.
537–553, 2011.
[9] P. Danaher, P. Wang, and D. M. Witten, “The joint graphical lasso for
inverse covariance estimation across multiple classes,” Journal of the
Royal Statistical Society: Series B (Statistical Methodology), vol. 76,
no. 2, pp. 373–397, 2014.
[10] C. Gao, Y. Zhu, X. Shen, W. Pan et al., “Estimation of multiple net-
works in gaussian mixture models,” Electronic Journal of Statistics,
vol. 10, no. 1, pp. 1133–1154, 2016.
[11] C. Peterson, F. C. Stingo, and M. Vannucci, “Bayesian inference
of multiple gaussian graphical models,” Journal of the American
Statistical Association, vol. 110, no. 509, pp. 159–174, 2015.
[12] D. J. MacKay, Information theory, inference and learning algorithms.
Cambridge university press, 2003.
[13] C. M. Bishop, Pattern Recognition and Machine Learning. Springer-
Verlag, 2006.
[14] A. Corduneanu and C. M. Bishop, “Variational bayesian model selec-
tion for mixture distributions,” in Artificial intelligence and Statistics,
vol. 2001. Morgan Kaufmann Waltham, MA, 2001, pp. 27–34.
[15] K. Yamanishi, J. Takeuchi, G. Williams, and P. Milne, “On-line
unsupervised outlier detection using finite mixtures with discounting
learning algorithms,” in Proc. the Sixth ACM SIGKDD Intl. Conf. on
Knowledge Discovery and Data Mining, 2000, pp. 320–324.
[16] D. M. Tax and R. P. Duin, “Support vector data description,” Machine
Learning, vol. 54, no. 1, pp. 45–66, 2004.
[17] D. T. Phan and T. Idé, “Demystifying automatic relevance determination in probabilistic mixture models.” (submitted).
[18] M. Bahadori, Y. Liu, and D. Zhang, “Learning with minimum supervision: A general framework for transductive transfer learning,” in
Data Mining (ICDM), 2011 IEEE 11th International Conference on,
2011, pp. 61–70.
[19] X. He, G. Mourot, D. Maquin, J. Ragot, P. Beauseroy, A. Smolarz,
and E. Grall-Maës, “Multi-task learning with one-class svm,” Neurocomput., vol. 133, pp. 416–426, Jun. 2014.
[20] Y. Xiao, B. Liu, S. Y. Philip, and Z. Hao, “A robust one-class transfer
learning method with uncertain data,” Knowledge and Information
Systems, vol. 44, no. 2, pp. 407–438, 2015.
[21] W. Zhang and P. Fung, “Sparse inverse covariance matrices for low
resource speech recognition,” IEEE Transactions on Audio, Speech,
and Language Processing, vol. 21, no. 3, pp. 659–668, March 2013.
[22] H. Goldstein, “Multilevel modelling of survey data,” Journal of the
Royal Statistical Society. Series D (The Statistician), vol. 40, no. 2,
pp. 235–244, 1991.
[23] J. G. Colonna, M. Cristo, M. Salvatierra, and E. F. Nakamura, “An
incremental technique for real-time bioacoustic signal segmentation,”
Expert Systems with Applications, vol. 42, no. 21, pp. 7367–7374,
2015.