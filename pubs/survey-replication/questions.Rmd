# Research Questions {#sec:questions}

Contains a mix of both replication questions and open questions. Replication questions look at published past results on peer review effects and attempts to replicate or refute these findings on our data set. Open questions look at some controversial questions on peer-review effects for which we found opinionated answers, but little or no prior studies.

## On double-blindness

\question{Does blinding affect the quality of the reviews?}{blind-quality}

(separate confounders like conference reputation). Does-double blind really help quality in any way?
How about non-blind review where reviewers "sign" their review?
Does it lead to more balanced geography/institution/gender bias?

 - https://jamanetwork.com/journals/jama/article-abstract/380957
 
In order to look at this question, it is first necessary to look at the correlations and interactions of the other variables in our dataset that may influence the ratings of review quality by each author. From looking at correlations between conference variables, we know that there is correlation between most, therefore we must take into account there confounding variables towards our model for review quality. Also, we know further that there is correlation between the scores of the papers and the quality of the reviews as rated by authors. 

```{r blind-quality, echo=F, warning=F}
#first join the conference stats to the survey data
survey_with_blind <- survey %>%
  mutate(conference = str_sub(paper_id, end=-5), isVeryHelpful = (helpfulness == "Somewhat helpful")) %>% #turn to same conference label in order to join
  left_join(all_confs, by=c("conference" = "conference")) %>%
  dplyr::select(c("gr_overall", "submissions", "helpfulness", "understanding", "fairness", "double_blind", "age", "mean_historical_citations", "h5_index", "pc_author_ratio", "mean_authors_per_paper", "acceptance_rate", "pc_paper_ratio", "mean_review_load")) %>%
  rename(Age = age, Blind = double_blind, Citations = mean_historical_citations, h5 = h5_index, PC_Author = pc_author_ratio, Authors_Paper  = mean_authors_per_paper, Acceptance = acceptance_rate, PC_Paper = pc_paper_ratio, Review_Load = mean_review_load) %>%
  drop_na()

survey_with_blind <- na.omit(survey_with_blind)

summary(survey_with_blind) #look at data's IQR's
  # GGally::ggpairs(lower = list(continuous = wrap("points", size = 0.4, position = position_jitter(height=0.1, width=0.1), alpha = 0.2)), progress = F)

#just to confirm from earlier question, lets look at the univariate regressions between each variable and then boxplots for categorical variables

#Not Correlated - Age x PC_Author, h5 x PC_author among the conference variables
#Not Correlated - Helpfulness x All Variables
#Not Correlated - Blind with everything except possibly Acceptance and PC_Author (based on boxplots), slight association with Review Load
```


Now that we've found the correlations between variables that do exist, let us now make a model and infer from the model whether blinding affects the quality of reviews via the coefficient of the model. By making a model, in essence, we are performing an ANCOVA - because we are checking for correlations between both categorical and numeric predictors associated with a categorical effector. 

Let us make a first model depending on the overall grade for each review, the conference quality, the age of the conference, the acceptance rate, the average citations, the pc's per paper, and the review load. This model is a simple linear model, and we look at the residuals to compute the F-statistic

```{r blind-quality, echo=F, warning=F}
lrfit <- glm(helpfulness ~ 1 + Age + Citations + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, 
             data=survey_with_blind, 
             family="binomial")
coef(summary(lrfit))
```

#review-length and double-blind

Looking at this model, it can be seen that none of the variables have a significant effect, and further the model is not explanatory as it has a very high F-Stat and a very low R^2. Let's use some packages that performs exhaustive step-wise selection of our variables in order to select the most accurate model.

```{r blind-quality, echo=F, warning=F}
#using MASS
library(MASS)
fit <- glm(helpfulness ~ 1 + Age + Citations + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, data=survey_with_blind, family="binomial")
step <- stepAIC(fit, direction="both")
step$anova 
```

#add submissions in order to look at popularity of conference that PC_Author indicates
Finally, through backwards selection, it was determined that in no possible model at all - with every combination of interactions between variables - was there any reason to believe being double blind or not effected the helpfulness of reviews

We repeat the process for usefulness and understanding:
```{r blind-quality, echo=F, warning=F}
fit_fairness <- glm(fairness ~ 1 + Age + Citations + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, data=survey_with_blind, family="binomial")
step <- stepAIC(fit, direction="both")
step$anova 

fit_understanding <- glm(Blind ~ 1 + Age + Citations + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, data=survey_with_blind, family="binomial")
step <- stepAIC(fit_understanding, direction="both")
step$anova 

fit_fairness <- glm(fairness ~ Authors_Paper, data=survey_with_blind, family="binomial")
fit_understanding <- glm(understanding ~ Blind, data=survey_with_blind, family="binomial")
```

#authors/paper affects response-rate?

According to the above fitness tests, fairness is best fit by a model that consists of just its relation to the number of authors per paper, and understanding is best fit by a model that is associated only with the blinding of the conference. These tests look for pairwise correlations and penalize fits that have correlated data, using the AIC metric in order to gather information about the fit that is most strongly correlated and has the smallest amount of confounding variables. 

All in all, this data does point towards the possibility of a correlation between blinding of a conference and the understanding of a paper as rated by its authors, and further makes the argument that there is likely little such correlation between any possible combination of effector variables and blindness towards fairness and helpfulness ratings by the authors.

A more thorough test is needed for understanding, including both a logit regression and an ANCOVA. 

#caret - decisision trees and machine learning

\question{Does it lead to more citations?}{blind-citations}

Fit citations to conference reputation, then look at the residuals.

  - https://chairs-blog.acl2017.org/2017/03/02/arxiv-and-double-blind-reviewing-revisited/
  - https://2017.splashcon.org/track/splash-2017-OOPSLA#FAQ-on-Double-Blind-Reviewing
  - http://cra.org/statement-diveity-micro-50/
  - https://arxiv.org/pdf/1702.00502.pdf
  - http://www.cs.utexas.edu/users/mckinley/notes/blind-revised-2015.html
  - http://history.acm.org/pictures/tods/tods.acm.org/editorial.pdf
  - http://www.ncbi.nlm.nih.gov/pubmed/8015128
  - http://sc17.supercomputing.org/conference-overview/technical-papers/sc17-double-blind-review-policy/
  - https://publons.com/blog/who-is-using-open-peer-review/
  - http://www.cs.utexas.edu/users/mckinley/notes/blind-revised-2015.html
  - http://history.acm.org/pictures/tods/tods.acm.org/editorial.pdf
  - http://www.ncbi.nlm.nih.gov/pubmed/8015128
  - https://2017.splashcon.org/track/splash-2017-oopsla#FAQ-on-Double-Blind-Reviewing
  - https://www.sciencedirect.com/science/article/pii/S0169534707002704
  - https://scholarlykitchen.sspnet.org/2018/05/16/peer-review-autoers-reviewers-north-star/?informz=1
  - https://www.pnas.org/content/114/48/12708
  - https://arxiv.org/pdf/1802.02188.pdf
  - http://www.erinhengel.com/research/publishing_female.pdf
  - https://www.frontiersin.org/articles/10.3389/fnins.2015.00169/full
  - https://jamanetwork.com/journals/jama/article-abstract/376228

```{r blind-citations, echo=F, warning=F}
survey_with_citations <- survey %>%
  mutate(conference = str_sub(paper_id, end=-5)) %>%
  left_join(all_confs, by=c("conference" = "conference")) %>%
  left_join(papers, by=c("paper_id" = "key")) %>%
  dplyr::select(c("references", "helpfulness", "understanding", "fairness", "double_blind", "age", "mean_historical_citations", "h5_index", "pc_author_ratio", "mean_authors_per_paper", "acceptance_rate", "pc_paper_ratio", "mean_review_load")) %>%
  rename(Age = age, Blind = double_blind, Citations_Conf = mean_historical_citations, h5 = h5_index, PC_Author = pc_author_ratio, Authors_Paper  = mean_authors_per_paper, Acceptance = acceptance_rate, PC_Paper = pc_paper_ratio, Review_Load = mean_review_load)
survey_with_citations <- na.omit(survey_with_citations)

library(leaps)
leaps <- regsubsets(references ~ 1 + Age + Citations_Conf + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, data=survey_with_citations)
summary(leaps)
plot(leaps,scale="r2")

library(MASS)
fit <- lm(references ~ 1 + Age + Citations_Conf + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, data=survey_with_citations, family="binomial")
step <- stepAIC(fit, direction="both")
step$anova 
```

The best fit according to the step-reduction above appears to be towards each indicates that whether a conference is blinded or not has the most deviance on the references, making it hard to model a linear relationship between the two variables. According to both analysis' the closet relationship derived is from references associated with Age, h5, acceptance, the number of PC's as authors, the number of PC's per paper, and the number of authors per paper. 

Trying a fit anyway between hindex and citations and looking at the residuals:
```{r blind-citations, echo=F, warning=F}
fitc.lm <- lm(references ~ Citations_Conf , data = survey_with_citations)
summary(fitc.lm)
fitc.resid = resid(fitc)
plot(survey_with_citations$Citations_Conf, fitc.resid, ylab="Residuals", xlab="Citations") 
abline(0, 0)

fitc.lm <- lm(references ~ Citations_Conf * Blind , data = survey_with_citations)
summary(fitc.lm)
plot(survey_with_citations$Citations_Conf, fitc.resid, ylab="Residuals", xlab="Citations") 
abline(0, 0)
```

The fit above does point towards a high positive significant relationship between double blind conferences and references, however as before the results are inconclusive given the high amount of residual deviance and low R^2 values. More research is needed in order to account for this high deviance in the data and make a correct fit. 

\question{Does blinding reduce the h-index of authors of accepted papers?}{blind-hindex}

  - https://www.usenix.org/sites/default/files/atc19_message.pdf
  
  In order to look at this relationship, lets first consider the IQR's of those authors h-index of blind versus non-blind conferences
```{r blind-hindex, echo=F, warning=F}
survey_blind_hindex <- survey %>%
  mutate(conference = str_sub(paper_id, end=-5)) %>% 
  left_join(dplyr::select(authors, c("name", "hindex")), by = c("name" = "name")) %>%
  left_join(all_confs, by=c("conference" = "conference")) %>%
  dplyr::select(c("hindex", "double_blind", "age", "mean_historical_citations", "h5_index", "pc_author_ratio", "mean_authors_per_paper", "acceptance_rate", "pc_paper_ratio", "mean_review_load"))
survey_blind_hindex <- na.omit(survey_blind_hindex)

ggplot(survey_blind_hindex, aes(x=double_blind, y=hindex)) + geom_boxplot()
```

Looking at the boxplot,there doesn't appear to be any significant difference between the two groups, but it is likely there is one of many possible confounds messing the relationship. Let's consider how hindex correlates to all the other important numeric variables of each conference

```{r blind-hindex, echo=F, warning=F}
survey_blind_hindex <- survey_blind_hindex  %>%
  rename(Age = age, Blind = double_blind, Citations = mean_historical_citations, h5 = h5_index, PC_Author = pc_author_ratio, Authors_Paper  = mean_authors_per_paper, Acceptance = acceptance_rate, PC_Paper = pc_paper_ratio, Review_Load = mean_review_load)

GGally::ggpairs(lower = list(continuous = wrap("points", size = 0.4, position = position_jitter(height=0.1, width=0.1), alpha = 0.2)), progress = F, data=survey_blind_hindex)

```

Looking at the correlegram above, it can be seen that there appears to be no to little correlation between hindex and the rest of the variables. There is a small positive correlation between hindex and conference citations. A pairwise correlation for that relationship can be shown in a linear model: `r summary(lm(hindex ~ Citations, data=survey_blind_hindex))` and it is indeed a significant correlation. 

Now, let's do a stepwise analysis of the linear model to get the best fit that has the least amount of possible confounds.
```{r blind-hindex, echo=F, warning=F}
library(leaps)
leaps <- regsubsets(hindex ~ 1 + Age + Citations + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, data=survey_blind_hindex)
summary(leaps)
plot(leaps,scale="r2")

library(MASS)
fit <- lm(hindex ~ 1 + Age + Citations + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, data=survey_blind_hindex, family="binomial")
step <- stepAIC(fit, direction="both")
step$anova 
```

According to above, the appears to be little correlation between hindex and the blinding, in any type of possible fit

\question{Does double-blind at least bias less against junior faculty?}{blind-diversity}

fit author hindex against double-blind (use max hindex per paper, or sum, or min, or median, or mean). Correlate also to prior_subs.

  - [@tomkins17:reviewer]
  - https://www.frontiersin.org/articles/10.3389/fnins.2015.00169/full
  - https://www.researchgate.net/profile/Anthony_Tung/publication/220416127_Impact_of_double_blind_reviewing_on_SIGMOD_publication/links/55b8691d08ae9289a08d5678.pdf
  - https://arxiv.org/pdf/1802.02188.pdf
  - https://onlinelibrary.wiley.com/doi/full/10.1002/asi.22784
  - https://www.timeshighereducation.com/world-university-rankings/2019/world-ranking#!/page/0/length/25/sort_by/rank/sort_order/asc/cols/stats
  - What about country diversity? Sector diversity? country? gender? Non-english speakers Low-prestige institutions? Content based? Other types of diversity?


```{r blind-diversity, warning=F, echo=F}
survey_exp <- survey_blind_hindex %>%
  mutate(hindex = ifelse(hindex < 12, "Novice", ifelse(hindex <= 18, "Mid-career", "Experienced")))
nsimple <- glm(hindex == "Novice" ~ Blind, data=survey_exp, family = "binomial")
midsimple <- glm(hindex == "Mid-career" ~ Blind, data=survey_exp, family = "binomial")
expsimple <- glm(hindex == "Experienced" ~ Blind, data=survey_exp, family = "binomial")
summary(nsimple)
summary(midsimple)
summary(expsimple)
```
Stepwise-regression analysis
```{r blind-diversity, warning=F, echo=F}
library(MASS)

nfit <- glm(hindex == "Novice" ~ 1 + Age + Citations + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, data=survey_exp, family="binomial")
step <- stepAIC(nfit, direction="both")
step$anova 

midfit <- glm(hindex == "Mid-career" ~ 1 + Age + Citations + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, data=survey_exp, family="binomial")
step <- stepAIC(midfit, direction="both")
step$anova 

expfit <- glm(hindex == "Experienced" ~ 1 + Age + Citations + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, data=survey_exp, family="binomial")
step <- stepAIC(expfit, direction="both")
step$anova 
```

R^2, F-score and ASIc 
```{r blind-diversity, warning=F, echo=F}
library(leaps)
leaps <- regsubsets(hindex == "Novice" ~ 1 + Age + Citations + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, data=survey_exp)
summary(leaps)
plot(leaps,scale="r2")

leaps <- regsubsets(hindex == "Mid-career" ~ 1 + Age + Citations + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, data=survey_exp)
summary(leaps)
plot(leaps,scale="r2")

leaps <- regsubsets(hindex == "Experienced" ~ 1 + Age + Citations + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, data=survey_exp)
summary(leaps)
plot(leaps,scale="r2")
```


# From older paper:

Many author factors do not appear to significantly interact with double-blindness, such as: the gender, position, and research experience (based on H-index).
In terms of review scores, there do not appear to be large differences in the mean and median grades, but the distribution of grades in double-blind reviews appears wider for some categories, especially technical merit.

Double-blind reviewed conferences in our dataset do appear to accept longer papers by page count
(`r report_t(t.test(table(survey_with_confs$double_blind, survey_with_confs$mean_pages)))`)
with longer research history
(`r report_chi(chisq.test(table(survey_with_confs$double_blind, survey_with_confs$months_research)))`)
and more coauthors per paper
(`r report_t(t.test(table(survey_with_confs$double_blind, survey_with_confs$mean_authors_per_paper)))`).
These conferences are also more likely to allow rebuttals
(`r report_chi(chisq.test(table(survey_with_confs$double_blind, survey_with_confs$allow_rebuttal)))`),
and produce more reviews
(`r report_t(t.test(table(survey_with_confs$double_blind, survey_with_confs$reviews)), 2)`)
of longer length
(`r report_chi(chisq.test(table(survey_with_confs$double_blind, survey_with_confs$length)))`),
which in turn are deemed more helpful, fair, and understanding by authors (Sec. \@ref(subsec:quality)).


We doubt that double-blinding adequately explains all these phenomena [@godlee98:peer; @mcnutt90:blinding]. Instead, we suggest looking at a third factor: the reputation or prestige of a conference. Prestige may be too abstract to measure accurately, but we did collect two proxy conference metrics: the acceptance rate and H5-index (as measured by GS).
Indeed, the `r nrow(filter(all_confs, double_blind == T))` double-blind conferences in our set average a lower acceptance rate
(`r pct(mean(filter(all_confs, double_blind == T)$acceptance_rate, na.rm = T), 1, 1)`% vs.
`r pct(mean(filter(all_confs, double_blind == F)$acceptance_rate, na.rm = T), 1, 1)`%) and higher H5-index
(`r round(mean(filter(all_confs, double_blind == T)$h5_index, na.rm = T), 1)` vs.
`r round(mean(filter(all_confs, double_blind == F)$h5_index, na.rm = T), 1)`).

More competitive conferences also produce longer reviews, which are linked to better author evaluations of the reviews
(`r report_t(t.test(table(survey_with_confs$acceptance_rate, survey_with_confs$length)))`).
When correcting for conference quality, as measured by either metric, most of the previous double-blind associations fade or disappear. For example, the relationship between review length and double-blind reviews loses statistical significance when limiting ourselves to conferences with an acceptance rate of less then 0.2
(`r x <- filter(survey_with_confs, acceptance_rate >= 0.0, acceptance_rate < 0.2); report_chi(chisq.test(table(x$double_blind, x$length)))`),
or between 0.2 and 0.3
(`r x <- filter(survey_with_confs, acceptance_rate >= 0.2, acceptance_rate < 0.3); report_chi(chisq.test(table(x$double_blind, x$length)))`),
or higher than 0.3
(`r x <- filter(survey_with_confs, acceptance_rate >= 0.3, acceptance_rate < 0.9); report_chi(chisq.test(table(x$double_blind, x$length)))`).
Fig. \@ref(fig:acceptance-vs-length) depicts the relationships between these three variables.


Some studies found that a double-blind peer-review process can improve women's representation among authors [@budden08:double; @eaton19:gender; @lloyd90:gender; @wenneras01:nepotism], whereas other studies disagree [@ceci11:understanding; @lee13:bias; @tomkins17:reviewer; @ware08:peer]. Our own data does not show a significant difference in women's representation
(`r report_chi(chisq.test(table(survey_with_confs$double_blind, survey_with_confs$mapped_gender)))`).

```{r acceptance-vs-length, echo = F, message = F, warning = F, out.width = '100%', fig.cap = "Conference acceptance rate and reported review length. Observe that double-blind conferences tend to be more competitive, and more competitive conferences tend to have longer reviews."}
cbp <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#000000")
survey_with_confs %>%
  filter(!is.na(length), !is.na(acceptance_rate)) %>%
  filter(acceptance_rate > 0.14) %>% # Remove two outlier conferences
  group_by(conf, npapers, acceptance_rate, double_blind, length) %>%
  summarize(n = n()) %>%
  mutate(Review = factor(ifelse(double_blind, "Double blind", "Single blind"))) %>%
  arrange(acceptance_rate) %>%
  ggplot(aes(x = acceptance_rate, y = n, fill = forcats::fct_rev(length))) +
    geom_bar(position = "fill", stat = "identity", width = 0.005) +
    scale_x_continuous(labels = scales::percent_format()) +
    scale_y_continuous(labels = scales::percent_format()) +
    scale_fill_discrete(limits = c("1-2 Paragraphs", "Half a Page", "A Page", "Multiple Pages")) +
    facet_wrap(~ Review) +
    ylab("Reported review length (proportion)") +
    xlab("Conference acceptance rate") +
    labs(fill = "Length") +
    theme_light() +
    theme(legend.position = "top") +
    coord_flip()

#survey_with_confs %>%
#  filter(!is.na(length)) %>%
#  group_by(acceptance_rate, double_blind, length) %>%
#  summarize(n = n()) %>%
#  ggplot(aes(x = length, y = acceptance_rate, color = double_blind, size = n)) +
##         geom_boxplot(notch = T) +
#    geom_point(alpha = 0.5) +
#    geom_jitter(height = 0, width = 0.2) +
#    theme(legend.position = "top") +
#    scale_color_discrete(name = "Review policy", labels = c("Single blind", "Double blind")) +
#    xlab("Reported review length") +
#    ylab("Conference acceptance rate") +
#    coord_flip()
```


--------------------------


## On review quality

\question{What is the relationship between review length and conference reputation?}{length-reputation}

  - https://publons.com/static/Publons-Global-State-Of-Peer-Review-2018.pdf

\question{How is review length related to quality?}{review-length}

  - http://retractionwatch.com/2017/11/27/make-reviews-public-says-peer-review-expert/


\question{Do authors "like" "kind" reviews?}{kind-reviews}

  - https://nlpers.blogspot.co.uk/2015/06/some-naacl-2013-statistics-on-author.html
  - [@anderson08:towards]
  - [@papagiannaki07:author]

\question{Problems and arbitrariness in peer review}{arbitrary}

  - http://blog.mrtz.org/2014/12/15/the-nips-experiment.html
  - https://cacm.acm.org/blogs/blog-cacm/181996-the-nips-experiment/fulltext
  - [@vardi09:conferences]
  - [@wallach11:rebooting]
  - [@vines11:cointoss]
  - https://scholarlykitchen.sspnet.org/2012/07/31/the-referee-that-wasnt-there-the-ghostly-tale-of-reviewer-3-3/
  - https://dl.acm.org/citation.cfm?id=1435430https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1420798/
  - ch. 5 of "women science and technology"
  - https://www.sigarch.org/r2-the-future-of-isca-or-not/
  - https://www.sigarch.org/a-common-standard-to-fix-our-review-process-and-oh-i-was-wrong-about-one-thing/
  - https://petsymposium.org/experiment.php


\question{Effect of review days on number of submissions or author author satisfaction}{review-days}

Authors want 3m or less for reviews

  - [@editage18:perspectives]
  - [@wallach11:rebooting]

\question{Does review time materially impact quality?}{time-quality}

Does it really take about 5/6 hours per review?
Most reviewers are done in 4 weeks

  - https://senseaboutscience.org/wp-content/uploads/2016/12/Peer_Review_Survey.pdf
  - https://www.sigmetrics.org/sigmetrics2017/SIGMETRICSInfoforAuthors.html)
  - http://www.ieee-security.org/TC/Reports/2017/SP2017-PCChairReport.pdf
  - https://cacm.acm.org/magazines/2009/5/24644-program-committee-overload-in-systems/abstract
  - https://publons.com/static/Publons-Global-State-Of-Peer-Review-2018.pdf
  - https://senseaboutscience.org/wp-content/uploads/2016/12/Peer_Review_Survey.pdf
  - http://www.ieee-security.org/TC/Reports/2017/SP2017-PCChairReport.pdf
  - [@parno17:SPsurvey]



\question{Effect of reviewer load}{reviewer-load}

  - [@beverly13:findings]
  - https://cacm.acm.org/blogs/blog-cacm/165288-representative-reviewing/fulltext  --> affects name bias

\question{Should we have post-publication review/comments?}{post-pub}

  - https://scholarlykitchen.sspnet.org/2013/03/27/how-rigorous-is-the-post-publication-review-process-at-f1000-research/
  - http://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1001772
  - https://link.springer.com/article/10.1007/s11948-016-9854-2
  - https://www.sciencedirect.com/science/article/pii/S016895251500044X
  - https://search.proquest.com/docview/1697484361?pq-origsite=gscholar

\question{Is a revision cycle effective?}{revision-cycle}
How long should it be?

  - "Publish and Perish" Moshe Vardi CACM 01/2020
  - http://www.ieee-security.org/TC/Reports/2017/SP2017-PCChairReport.pdf

\question{Open peer review--no one is doing it?}{open-peer}

  - http://www.nature.com/news/watch-out-for-cheats-in-citation-game-1.20246
  - http://www.nature.com/news/let-s-make-peer-review-scientific-1.20194

\question{Can we improve reviews from one venue to the next?}{improve-reviews}

  - https://www.nature.com/articles/546352a
  - https://www.sigarch.org/a-proposal-to-coordinate-reviewing-across-computer-architecture-conferences/

\question{How does the average number of reviews compare to other fields?}{reviews-num}

  - https://scholarlykitchen.sspnet.org/2018/09/12/guest-post-what-a-new-publons-report-on-peer-review-says-about-diversity-and-more/?informz=1

\question{Are reviews unfair?}{reviews-unfair}

  - https://www.sigarch.org/r2-the-future-of-isca-or-not/

\question{Effect of over-positive PC members}{positive-pc}

  - https://www.sigarch.org/overwhelming-statistical-evidence-that-our-review-process-is-broken/
  - https://www.sigarch.org/false-accepts-and-false-rejects/

\question{Does rebuttal/author response help?}{rebuttal-help}

Probably useless

  - https://chairs-blog.acl2017.org/2017/03/27/author-response-does-it-help/
  - https://nlpers.blogspot.co.uk/2015/06/some-naacl-2013-statistics-on-author.html
  - http://www.ieee-security.org/TC/Reports/2017/SP2017-PCChairReport.pdf
  - http://sc17.supercomputing.org/program/papers-faq/
  - https://www.sigops.org/sosp/sosp17/call-for-papers.html
  - https://www.sigarch.org/how-pc-chairs-and-pc-members-can-help-improve-our-process/
  - Dave Evans, CCS: "I think the author responses are valuable, even if they don't change paper decisions since (1) they make reviewers take the early reviews more seriously and know they risk being embarrassed if they say something incorrect/lazy, (2) they give authors a sense that they had an opportunity to respond, and (3) they help PC chairs decide if a paper needs additional reviews. I think the responses did change the outcomes of some papers for CCS - one for sure, that was considered dropped after the first round, but revived by the responses and eventually accepted, and for the borderline papers, the responses were considered seriously in making decisions (but can't know for sure how things would have gone without them). In some cases, the authors responses made it easier to reject papers since they made it clear that the problems reviewers found were legitimate, and a handful of papers were withdraw by the authors after seeing the early reviews.  We had a "Discussion Committee" that assigned a PC member to oversee each paper to ensure the reviewers took the author responses into consideration, which helped a lot since for a conference of this scale it would be too much for the PC chairs to check on all papers."

\question{Do reviewers want anonymity?}{reviewer-anon}

  - https://senseaboutscience.org/wp-content/uploads/2016/12/Peer_Review_Survey.pdf

\question{Effect of reviewer award on their own submission's grade?}{reviewer-award}

  - [Search references]

\question{Effect of shepherding, where known}{shepherd}

  - [Search references]

\question{Properties of "real" papers, based on Ulfar's email from 10/4/17}{real-papers}

  1) Reviewers are less likely to agree on "real" papers.
  2) Reviewers are less likely to state high confidence on "real" papers, especially if they're saying reject/strong reject.
  - [Search references]

\question{How does the average number of reviews compare to other fields?}{num-reviews}

  - https://scholarlykitchen.sspnet.org/2018/09/12/guest-post-what-a-new-publons-report-on-peer-review-says-about-diversity-and-more/?informz=1


--------------------------

## On conference quality

\question{What matters more for a conference: scope, review length, or impact factor?}{conf-metric}

  - https://www.editage.com/files/Editage-Global-Author-Survey.pdf
  - https://peerj.com/articles/365/#p-17

\question{Correlate everything with conference metrics and policies (features)}{conf-corr}
  
First, we will look at the correlations between the key metrics of each conference. The one's chosen first are age, the mean historical citations, the h5 score (as measured by the h5 index score), the ratio between PC's and authors, the average authors per paper, the acceptance rate, the average PC's per paper, and the review load (number of reviews required per reviewer)
The correlegram below depicts these relations:
```{r conf-corr echo=F, warning=F}
#first correlations between conference metrics
all_confs %>%
  dplyr::select(c("age", "mean_historical_citations", "h5_index", "pc_author_ratio", "mean_authors_per_paper", "acceptance_rate", "pc_paper_ratio", "mean_review_load")) %>%
  rename(Age = age, Citations = mean_historical_citations, h5 = h5_index, PC_Author = pc_author_ratio, Authors_Paper  = mean_authors_per_paper, Acceptance = acceptance_rate, PC_Paper = pc_paper_ratio, Review_Load = mean_review_load) %>%
  GGally::ggpairs(lower = list(continuous = wrap("points", size = 0.4, position = position_jitter(height=0.1, width=0.1), alpha = 0.2)), progress = F)
```
Looking at the above correlegram, we can see that almost all appear to be correlated in some way or another, the question is to what degree? We will divide the correlations into those that seem obvious at first, as they are built-in to the metric in the first place and those that require further investigation.

Interesting:
-age and mean_historical_citations - clearly this will happen as conferences with more age have been cited more
-age and acceptance rate - conferences that are older will reject more papers?
-mean_historical_citations and h5_index - could it be that conferences that are cited more are rated less as more people get into them? 
-mean_historical_citations and acceptance and review-load
-h5_index and acceptance_rate - conferences that are harder to get into are rated higher?
-acceptance and pc_paper_ratio - There are more PC's so the ratings are harder for each paper?
#expander 
Built-in:
-age and h5-index - conferences that are older are more reputable?
-age and mean_historical_citations - clearly this will happen as conferences with more age have been cited more
-pc_author and pc_paper ratios  - this correlation makes sense, if there ar emore pc's there will clearly be a higher pc per paper ratio
-pc_author and review_load - that correlation is also obvious, clearly if there are more pc's there will be less review load

I haven't included the following metrics for each conference as they are measures of counts not averages: past-papers, past_citations, chairs_num, pc_size, npapers, authors_num
These scores don't take into account the time the conference has been running and the number of papers. However, I haven't included mean_historical_length or h5_median, both of these could be important as well. 

Now onto the policies of the conferences. For this, we will first look at if it is double_blind, allows rebuttals and if it is making a diversity effort. In this case, as there are only two categories - true or false, we can do a paired t-test or independent t-test depending on the context. 

\question{Diversity of reviewers}{reviewer-diversity}

(first answer why diversity is needed). Also specifically when limited to HPC (look at whpc-summit paper)
  
  - https://scholarlykitchen.sspnet.org/2018/09/07/ask-the-chefs-diversity-in-peer-review/?informz=1
  - https://scholarlykitchen.sspnet.org/2018/09/13/eight-ways-to-tackle-diversity-and-inclusion-in-peer-review/?informz=1
  - https://www.nature.com/news/journals-invite-too-few-women-to-referee-1.21337

\question{Compare to other conferences}{conf-comp}

  - ATC'19  https://www.usenix.org/sites/default/files/atc19_message.pdf
  - Compare to SIGCOMM09  https://dl.acm.org/citation.cfm?id=1568623
  - What do authors in MICRO think about the review process?  https://www.sigmicro.org/2019/01/09/micro-survey-results/


\question{Quality is multi-dimensional}{quality-multidim}

https://scholarlykitchen.sspnet.org/2019/09/16/quality-is-multi-dimensional-how-many-ways-can-you-define-quality-in-peer-review/?unapproved=84111&moderation-hash=47b56366ddaf68dffef86ee501020c54#comment-84111

\question{Textual difference between 1st-version pre-print, last version, and published version}{ver-diff}

Not just similarity, but change in meaning

  - https://scholarlykitchen.sspnet.org/2018/03/15/a-comment-on-klein-et-als-comparing-articles-to-preprints/?informz=1

\question{Do authors aim for the highest possible venue that will still accept them?}{highest-venue}

Correlate conference prestige with no. of previous attempts.

  - https://scholarlykitchen.sspnet.org/2011/12/08/is-peer-review-a-coin-toss/

\question{Do two-cycle conferences lead to better papers? Is round-based reviewing better?}{two-cycle}

  - http://from-a-to-remzi.blogspot.com/2012/07/why-you-shouldnt-feel-too-badly-about.html

\question{Effect of submission Day-of-week and months-to-publish on submissions no. and acceptance rate}{day-of-week}

Correct for confounders, which you can find by plotting.

  - [Search references]

\question{Effect of "collisions" of multiple due dates next to each other, especially with similar paper topics}{collisions}

  - https://www.sigarch.org/r2-the-future-of-isca-or-not/

\question{Do conferences with strict data policies receive fewer submissions?}{data-policy}

  - https://scholarlykitchen.sspnet.org/2018/09/25/does-adopting-a-strict-data-sharing-policy-affect-submissions/?informz=1


--------------------------

## Misc

\question{What is the relationship between prior subs and conference quality?}{prior-subs-quality}
```{r prior-subs-quality echo=F, warning=F}
library(stringr)

paper_confs <- survey %>%
  drop_na(prior_subs) %>%
  group_by(paper_id) %>%
  summarise(prior_subs = mean(prior_subs), conference = str_sub(first(paper_id), end=-5)) %>% #get average previous submissions
  left_join(select(all_confs, c("conference", "mean_historical_citations")), by=c("conference" = "conference")) %>%
  drop_na(mean_historical_citations) 
  # group_by(conference) %>% #take averages
  # summarise(prior_avg = mean(prior_subs), h5 = first(h5_median))

cor.test(paper_confs$mean_historical_citations, paper_confs$prior_subs, use="complete")

#try averages instead
paper_confs <- paper_confs %>%
  group_by(conference) %>%
  summarise(prior_avg = mean(prior_subs), h5_median = first(h5_median))

cor.test(paper_confs$h5_median, paper_confs$prior_avg, use="complete")

#produce h5_index, median and citations

```

Not correlated 

\question{Distribution of "paper readiness" and relationship to author experience}{paper-readiness}

  - http://www.ieee-security.org/TC/Reports/2017/SP2017-PCChairReport.pdf
  - https://www.nsf.gov/statistics/2018/nsb20181/assets/968/tables/tt05-18.pdf

\question{How is survey response rate affected by experience? acceptance?}{response-rate}

  - http://www.ieee-security.org/TC/Reports/2017/SP2017-PCChairReport.pdf
  - https://www.nsf.gov/statistics/2018/nsb20181/assets/968/tables/tt05-18.pdf
```{r response-rate, echo=F, warning=F}
people_with_hindex <- survey %>%
  left_join(select(authors, c("name", "hindex")), by = c("name" = "name")) %>%
  drop_na(hindex) %>%
  mutate(Experience = ifelse(hindex < 12, "Novice", ifelse(hindex <= 18, "Mid-career", "Experienced"))) %>%
  group_by(name) %>%
  filter(row_number() == 1)  
  
resp_position_exp <- people_with_hindex %>% 
  drop_na(position) %>%
  mutate(position = ifelse(position == "Professor" || position == "Assistant Professor" || position == "Associate Professor", "Professor", 
                            ifelse(position == "Student", "Student", 
                                   ifelse(position == "Goverment Researcher" || position == "Industry Researcher", "Researcher", "Other")))) %>%
  filter(!position == "Other") %>% #~ position filters ~
  group_by(position, Experience) %>%
  mutate(perc = pct(n(), nrow(authors_with_profile))) %>%
  filter(row_number() == 1)

#graph of position and percent responded divided on experience
ggplot(resp_position_exp, aes(x=position, y = perc, group=Experience, label=perc)) + 
  geom_bar(aes(fill = factor(..x..)), stat = "identity") + 
  geom_text(vjust=-0.3, size=3.5) +
  theme_minimal() +
  theme(legend.position = "none")+
  facet_grid(~Experience) +
  ylab("Response Rate %") + 
  xlab("Position") 

#do density plot, geom_density()

#graph of acceptance and percent responded with experience fill
people_with_hindex %>%  
  filter(prior_subs < 4) %>%
  ggplot(aes(x=prior_subs)) + 
  geom_bar(aes(fill = Experience, y= pct((..count..),nrow(authors_with_profile)))) + 
  theme_minimal() +
  ylab("Response Rate %") + 
  xlab("Prior Submissions") 

#flipped around
people_with_hindex %>%  
  filter(prior_subs < 4) %>%
  mutate(prior_subs=as.character(prior_subs)) %>%
  ggplot(aes(x=Experience)) + 
  geom_bar(aes(fill = prior_subs, y= pct((..count..),nrow(authors_with_profile)))) + 
  theme_minimal() +
  ylab("Response Rate %") + 
  xlab("Experience") 
```
\question{Does submission order affect paper's chances?}{sub-order}

  - http://onlinelibrary.wiley.com/doi/10.1002/asi.22747/abstract
  
```{r sub-order, echo=F, warning=F}
papers_grades_order <- survey %>% 
  group_by(paper_id) %>% 
  summarise(gr_overall = mean(gr_overall)) %>%
  left_join(select(papers, c("key", "months_to_gs", "months_to_eprint")), by=c("paper_id" = "key"))  %>%
  drop_na(gr_overall, months_to_gs, months_to_eprint)

cor.test(papers_grades_order$gr_overall,  papers_grades_order$months_to_gs)
cor.test(papers_grades_order$gr_overall, papers_grades_order$months_to_eprint)
```

\question{Relationship between english mastery and presentation scores/overall scores}{english-present}

  - [previous paper]
  
```{r english-present, echo=F, warning=F}
#take average "english-mastery" per paper  and average presentation scores for paper
eng_mastery_tbl <- survey %>%
  drop_na(gr_present) %>% #drop responses with no presentation score
  mutate(native_english = ifelse(native_english == TRUE, 1, 0)) %>%
  group_by(paper_id) %>%
  summarise(english_mastery = mean(native_english), present_avg = mean(gr_present)) 
#what do we do if two people have submitted different number of reviews, how do we determine which review is which? i'll just do weighted average for now

cor.test(eng_mastery_tbl$english_mastery, eng_mastery_tbl$present_avg) #no-correlation for any of them
```
According to the correlation measures, there does not appear to be a significant relationship, at least in our data
\question{Compare "overall" average to confidence-weighted "overall" average}{confidence}

  - [Search references]
  
```{r confidence, echo=F, warning=F}
conf_tbl <- survey %>%
  drop_na(gr_overall, gr_confidence) %>%
  group_by(paper_id) %>%
  mutate(conf_weight = gr_overall * gr_confidence) %>%
  summarise(gr_overall_avg = mean(gr_overall), gr_conf_avg = mean(conf_weight))

cor.test(conf_tbl$gr_overall_avg, conf_tbl$gr_conf_avg) #strong correlation between overall and confidence weighted overall (obviously)
#now, make nice chart for it
library(reshape)
melted <- melt(as.data.frame(conf_tbl), id=c("paper_id"))
ggplot(melted, aes(x=variable, y=value, color=variable)) +
  geom_boxplot() + 
  stat_summary(fun.y=mean, geom="point", shape=23, size=4)  +
  xlab(NULL) + 
  ylab("Grade") +
  theme_minimal()

t.test(conf_tbl$gr_conf_avg, conf_tbl$gr_overall_avg)

#blurb about more confident if positive about data
#cavat that there is no rejected paper reviews to compare to
```

\question{Produce initial stats like these}{stats}

  - https://publons.com/community/gspr

\question{Explain negative correlation between prior_subs and gr_overall}{prior-overall}

  - [Search references]


\question{How to deal with publication bias?}{pub-bias}

  - https://www.nature.com/articles/s41562-016-0034
  - http://www.nature.com/news/let-s-think-about-cognitive-bias-1.18520
  - http://www.nature.com/news/how-scientists-fool-themselves-and-how-they-can-stop-1.18517
  - http://www.nature.com/news/tool-for-detecting-publication-bias-goes-under-spotlight-1.21728
  - http://www.nature.com/news/replication-studies-bad-copy-1.10634


\question{Predicting citations from reviews}{cites-reviews}

Hypothesis: high review scores and paper citations both measure some paper quality, and should therefore be correlated
Ignoring all other factors (some of which may be very useful, such as author metrics, conf metrics, paper topics, etc.)
Outcome variable: time to first citation, 1y, 2y citations.
Predictor variables: each review grade + award


----

