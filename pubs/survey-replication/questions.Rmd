# Research Questions {#sec:questions}

Contains a mix of both replication questions and open questions. Replication questions look at published past results on peer review effects and attempts to replicate or refute these findings on our data set. Open questions look at some controversial questions on peer-review effects for which we found opinionated answers, but little or no prior studies.


## On double-blindness

__[DB.01]__ Does blinding affect the quality of the reviews? (separate confounders like conference reputation). Does-double blind really help quality in any way? How about non-blind review where reviewers "sign" their review? Does it lead to more balanced geography/institution/gender bias?
  - https://jamanetwork.com/journals/jama/article-abstract/380957

__[DB.02]__ Does it lead to more citations? Fit citations to conference reputation, then look at the residuals.
  - https://chairs-blog.acl2017.org/2017/03/02/arxiv-and-double-blind-reviewing-revisited/
  - https://2017.splashcon.org/track/splash-2017-OOPSLA#FAQ-on-Double-Blind-Reviewing
  - http://cra.org/statement-diversity-micro-50/
  - https://arxiv.org/pdf/1702.00502.pdf
  - http://www.cs.utexas.edu/users/mckinley/notes/blind-revised-2015.html
  - http://history.acm.org/pictures/tods/tods.acm.org/editorial.pdf
  - http://www.ncbi.nlm.nih.gov/pubmed/8015128
  - http://sc17.supercomputing.org/conference-overview/technical-papers/sc17-double-blind-review-policy/
  - https://publons.com/blog/who-is-using-open-peer-review/
  - http://www.cs.utexas.edu/users/mckinley/notes/blind-revised-2015.html
  - http://history.acm.org/pictures/tods/tods.acm.org/editorial.pdf
  - http://www.ncbi.nlm.nih.gov/pubmed/8015128
  - https://2017.splashcon.org/track/splash-2017-oopsla#FAQ-on-Double-Blind-Reviewing
  - https://www.sciencedirect.com/science/article/pii/S0169534707002704
  - https://scholarlykitchen.sspnet.org/2018/05/16/peer-review-autoers-reviewers-north-star/?informz=1
  - https://www.pnas.org/content/114/48/12708
  - https://arxiv.org/pdf/1802.02188.pdf
  - http://www.erinhengel.com/research/publishing_female.pdf
  - https://www.frontiersin.org/articles/10.3389/fnins.2015.00169/full
  - https://jamanetwork.com/journals/jama/article-abstract/376228


__[DB.03]__ Does blinding reduce the h-index of authors of accepted papers?
  - https://www.usenix.org/sites/default/files/atc19_message.pdf

__[DB.04]__ Does double-blind at least bias less against junior faculty? fit author hindex against double-blindWhat about country diversity? Sector diversity? Non-english speakersLow-prestige institutions?Content based?Other types of diversity?
  - https://www.frontiersin.org/articles/10.3389/fnins.2015.00169/full
  - https://www.researchgate.net/profile/Anthony_Tung/publication/220416127_Impact_of_double_blind_reviewing_on_SIGMOD_publication/links/55b8691d08ae9289a08d5678.pdf
  - https://arxiv.org/pdf/1802.02188.pdf
  - https://onlinelibrary.wiley.com/doi/full/10.1002/asi.22784
  - https://www.timeshighereducation.com/world-university-rankings/2019/world-ranking#!/page/0/length/25/sort_by/rank/sort_order/asc/cols/stats


--------------------------


## On review quality

__[Q.01]__ What is the relationship between review length and conference reputation?
  - https://publons.com/static/Publons-Global-State-Of-Peer-Review-2018.pdf

__[RQ.02]__ Do authors "like" "kind" reviews?
  - https://nlpers.blogspot.co.uk/2015/06/some-naacl-2013-statistics-on-author.html
  - [@anderson08:towards]
  - [@papagiannaki07:author]

__[RQ.03]__ Problems and arbitrariness in peer review
  - http://blog.mrtz.org/2014/12/15/the-nips-experiment.html
  - https://cacm.acm.org/blogs/blog-cacm/181996-the-nips-experiment/fulltext
  - [@vardi09:conferences]
  - [@wallach11:rebooting]
  - [@vines11:cointoss]
  - https://scholarlykitchen.sspnet.org/2012/07/31/the-referee-that-wasnt-there-the-ghostly-tale-of-reviewer-3-3/
  - https://dl.acm.org/citation.cfm?id=1435430https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1420798/
  - ch. 5 of "women science and technology"
  - https://www.sigarch.org/r2-the-future-of-isca-or-not/
  - https://www.sigarch.org/a-common-standard-to-fix-our-review-process-and-oh-i-was-wrong-about-one-thing/
  - https://petsymposium.org/experiment.php


__[RQ.04]__ Effect of review days on number of submissions or author author satisfactionAuthors want 3m or less for reviews
  - [@editage18:perspectives]
  - [@wallach11:rebooting]

__[RQ.05]__ Effect of reviewer load
  - [@beverly13:findings]

__[RQ.06]__ Does review time materially impact quality?
  - https://www.sigmetrics.org/sigmetrics2017/SIGMETRICSInfoforAuthors.html)
  - http://www.ieee-security.org/TC/Reports/2017/SP2017-PCChairReport.pdf
  - https://cacm.acm.org/magazines/2009/5/24644-program-committee-overload-in-systems/abstract

__[RQ.07]__ Should we have post-publication review/comments?
  - https://scholarlykitchen.sspnet.org/2013/03/27/how-rigorous-is-the-post-publication-review-process-at-f1000-research/
  - http://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1001772
  - https://link.springer.com/article/10.1007/s11948-016-9854-2
  - https://www.sciencedirect.com/science/article/pii/S016895251500044X
  - https://search.proquest.com/docview/1697484361?pq-origsite=gscholar

__[RQ.08]__ Is a revision cycle effective? How long?
  - http://www.ieee-security.org/TC/Reports/2017/SP2017-PCChairReport.pdf

__[RQ.08]__ Open peer review--no one is doing it?
  - http://www.nature.com/news/watch-out-for-cheats-in-citation-game-1.20246
  - http://www.nature.com/news/let-s-make-peer-review-scientific-1.20194

__[RQ.09]__ Can we improve reviews from one venue to the next?
  - https://www.nature.com/articles/546352a
  - https://www.sigarch.org/a-proposal-to-coordinate-reviewing-across-computer-architecture-conferences/

__[RQ.10]__ How does the average number of reviews compare to other fields?
  - https://scholarlykitchen.sspnet.org/2018/09/12/guest-post-what-a-new-publons-report-on-peer-review-says-about-diversity-and-more/?informz=1

__[RQ.11]__ How is review length related to quality?
  - http://retractionwatch.com/2017/11/27/make-reviews-public-says-peer-review-expert/

__[RQ.12]__ Are reviews unfair?
  - https://www.sigarch.org/r2-the-future-of-isca-or-not/

__[RQ.13]__ Effect of over-positive PC members
  - https://www.sigarch.org/overwhelming-statistical-evidence-that-our-review-process-is-broken/
  - https://www.sigarch.org/false-accepts-and-false-rejects/

__[RQ.14]__ Does rebuttal/author response help? probably useless
  - https://chairs-blog.acl2017.org/2017/03/27/author-response-does-it-help/
  - https://nlpers.blogspot.co.uk/2015/06/some-naacl-2013-statistics-on-author.html
  - http://www.ieee-security.org/TC/Reports/2017/SP2017-PCChairReport.pdf
  - http://sc17.supercomputing.org/program/papers-faq/
  - https://www.sigops.org/sosp/sosp17/call-for-papers.html
  - https://www.sigarch.org/how-pc-chairs-and-pc-members-can-help-improve-our-process/
  - Dave Evans, CCS: "I think the author responses are valuable, even if they don't change paper decisions since (1) they make reviewers take the early reviews more seriously and know they risk being embarrassed if they say something incorrect/lazy, (2) they give authors a sense that they had an opportunity to respond, and (3) they help PC chairs decide if a paper needs additional reviews. I think the responses did change the outcomes of some papers for CCS - one for sure, that was considered dropped after the first round, but revived by the responses and eventually accepted, and for the borderline papers, the responses were considered seriously in making decisions (but can't know for sure how things would have gone without them). In some cases, the authors responses made it easier to reject papers since they made it clear that the problems reviewers found were legitimate, and a handful of papers were withdraw by the authors after seeing the early reviews.  We had a "Discussion Committee" that assigned a PC member to oversee each paper to ensure the reviewers took the author responses into consideration, which helped a lot since for a conference of this scale it would be too much for the PC chairs to check on all papers."

__[RQ.15]__ Do reviewers want anonymity?
  - https://senseaboutscience.org/wp-content/uploads/2016/12/Peer_Review_Survey.pdf

__[RQ.16]__ Most reviewers are done in 4 weeks
  - https://senseaboutscience.org/wp-content/uploads/2016/12/Peer_Review_Survey.pdf

__[RQ.17]__ Does it really take about 5/6 hours per review?
  - https://publons.com/static/Publons-Global-State-Of-Peer-Review-2018.pdf
  - https://senseaboutscience.org/wp-content/uploads/2016/12/Peer_Review_Survey.pdf
  - http://www.ieee-security.org/TC/Reports/2017/SP2017-PCChairReport.pdf

__[RQ.18]__ Effect of reviewer award on their own submission's grade?
  - [Search references]

__[RQ.19]__ Effect of shepherding, where known.
  - [Search references]

__[RQ.20]__ Properties of "real" papers, based on Ulfar's email from 10/4/17
  1) Reviewers are less likely to agree on "real" papers.
  2) Reviewers are less likely to state high confidence on "real" papers, especially if they're saying reject/strong reject.
  - [Search references]

__[RQ.21]__ How does the average number of reviews compare to other fields?
  - https://scholarlykitchen.sspnet.org/2018/09/12/guest-post-what-a-new-publons-report-on-peer-review-says-about-diversity-and-more/?informz=1


--------------------------

## On conference quality

__[CQ.01]__ What matters more for a conference: scope, review length, or impact factor?
  - https://www.editage.com/files/Editage-Global-Author-Survey.pdf
  - https://peerj.com/articles/365/#p-17

__[CQ.02]__ Correlate everything with conference metrics and policies (features)
  - [Search references]

__[CQ.03]__ Diversity of reviewers (first answer why diversity is needed). Also specifically when limited to HPC (look at whpc-summit paper)
  - https://scholarlykitchen.sspnet.org/2018/09/07/ask-the-chefs-diversity-in-peer-review/?informz=1
  - https://scholarlykitchen.sspnet.org/2018/09/13/eight-ways-to-tackle-diversity-and-inclusion-in-peer-review/?informz=1
  - https://www.nature.com/news/journals-invite-too-few-women-to-referee-1.21337

__[CQ.04]__ Compare to other conferences:
* ATC'19  https://www.usenix.org/sites/default/files/atc19_message.pdf
* Compare to SIGCOMM09  https://dl.acm.org/citation.cfm?id=1568623
* What do authors in MICRO think about the review process?  https://www.sigmicro.org/2019/01/09/micro-survey-results/


__[CQ.05]__ Quality is multi-dimensional:
https://scholarlykitchen.sspnet.org/2019/09/16/quality-is-multi-dimensional-how-many-ways-can-you-define-quality-in-peer-review/?unapproved=84111&moderation-hash=47b56366ddaf68dffef86ee501020c54#comment-84111

__[CQ.06]__ Textual difference between 1st-version pre-print, last version, and published version. Not just similarity, but change in meaning
  - https://scholarlykitchen.sspnet.org/2018/03/15/a-comment-on-klein-et-als-comparing-articles-to-preprints/?informz=1

__[CQ.07]__ Do authors aim for the highest possible venue that will still accept them? Correlate conference prestige with no. of previous attempts.
  - https://scholarlykitchen.sspnet.org/2011/12/08/is-peer-review-a-coin-toss/

__[CQ.08]__ Do two-cycle conferences lead to better papers? Is round-based reviewing better?
  - http://from-a-to-remzi.blogspot.com/2012/07/why-you-shouldnt-feel-too-badly-about.html

__[CQ.09]__ Effect of submission Day-of-week and months-to-publish on submissions no. and acceptance rate. Correct for confounders, which you can find by plotting.
  - [Search references]

__[CQ.10]__ Effect of "collisions" of multiple due dates next to each other, especially with similar paper topics
  - https://www.sigarch.org/r2-the-future-of-isca-or-not/

__[CQ.11]__ Do conferences with strict data policies receive fewer submissions?
  - https://scholarlykitchen.sspnet.org/2018/09/25/does-adopting-a-strict-data-sharing-policy-affect-submissions/?informz=1


--------------------------

## Misc

__[MI.01]__ Are students and professors responding to the same paper differently?

```{r student-prof-setup, echo=F, warning=F}
survey_with_profs <- survey %>% 
  filter(position %in% c("Student", "Professor", "Associate Professor", "Assistant Professor")) %>%
  mutate(position = case_when(
    position == "Student" ~ "Student", 
    position == "Professor" ~ "Professor", 
    position == "Assistant Professor" ~ "Professor", 
    position == "Associate Professor" ~ "Professor"), understanding = as.numeric(understanding), fairness = as.numeric(fairness), helpfulness = as.numeric(helpfulness) )%>%
  group_by(paper_id, position) %>%  #all reviews are averaged by professor or student
  summarise_at(c("understanding", "helpfulness", "fairness"), funs(mean), na.rm = TRUE) %>%
  drop_na(understanding) %>%
  group_by(paper_id) %>%
  filter(position %in% c("Student", "Professor"),  n() > 1) %>%
  transmute(Understanding = understanding - lag(understanding), Helpfulness = helpfulness - lag(helpfulness), Fairness = fairness - lag(fairness)) %>%
  drop_na(Understanding) 
```
Students often have a different perspective and experience in the research process, and it could be that students therefore have different ideas about the rebuttal process, including helpfulness, the grades of the reviewers and quite a few subjective measures we've asked of our authors. 

Of the subjective measures we've asked of are helpfulness, fairness and understanding. Could it be that professors rate higher or at least respond differently in some respect then students in these measures for papers worked on with students? For our survey results, there was around 20 papers that met the criteria of having had review scores provided by both professor and student. We looked at these results and took the difference in review scores between students and professors, then graphed these differences on a boxplot. The graph is below:

```{r student-prof-setup, echo=F, warning=F} 
melted <- melt(as.data.frame(survey_with_profs))
melted %>% ggplot(aes(x = variable, y = value, color = variable)) +
    geom_boxplot(notch = F) +
    stat_summary(fun.y=mean, geom="point", shape=23, size=4) +
    theme(legend.position = "none") +
    ylab("Deviation in Grades") +
    xlab("Category")
```

Looking at these boxplots, we can see that in understanding and helpfulness, the results where fairly symetric, indicating that there was perhaps not much difference between the professors and students in these ratings. However, looking at the helpfulness boxplot, we can see that there is a skew towards the professors having less positive results then students in terms of the helpfulness of the reviews. However, the conversion between these factors and numeric variables is not exactly linear, so these results only point towards a trend. In order to verify these results it is necessary to run a chi-square test or another significance test. This skew also brings about another question, does the experience of an author change the ratings of helpfulness of paper reviews? In order to answer this question, it would be necessary to look at the hindex of the authors between each paper and do a chi-square test for those differences.

__[MI.02]__ Do more senior authors have fewer rejections (look for either first author, survey respondend rank, or max GS metrics in authors)
```{r student-senior-setup, echo=F, warning=F}
#join survey data with h-index of author
survey_with_hindex <- survey %>%
  left_join(select(authors, c("name", "hindex")), by = c("name" = "name"))

hindex_factr_levels <- c("novice", "mid-career", "veteran")
#turn hindex into factor 
survey_with_factor <- survey_with_hindex %>%
  mutate(hindex_discr = case_when(
    hindex > 18 ~ "veteran", 
    hindex > 13 ~ "mid-career", 
    hindex <= 12 ~ "novice", 
    TRUE ~ "less then or equal to ten")) %>%
  mutate(hindex_discr = factor(hindex_discr, levels = hindex_factr_levels))

pct_ovr_18 <- pct( nrow(filter(survey_with_factor, hindex_discr =="veteran")),nrow(survey_with_factor))
pct_under_12 <- pct( nrow(filter(survey_with_factor, hindex_discr == "novice")),nrow(survey_with_factor))
#figure out correlation between these two and come up with small narrative, write small narratives and clean this data up and push it
```

Senior authors have more experience in the review process and consequently may have an easier time in getting there papers accepted, without rejections. In order to measure seniority, we use the hindex scores of the authors and join this dataset with our survey responses. Among those that responded, we can test for correlation between hindex and there prior submissions of the paper. 

First to note, is that among our respondents, there was `r pct_ovr_18` percent of respondents with an h-index over 18, while there was `r pct_under_12` percent under 12 Therefore there is a much larger proption of authors with less experience in our survey responses. In order to measure the difference here we can do a correlation between hindex and the rejections for the authors. 

```{r student-senior-setup, echo=F, warning=F}
hindex_corr <- survey_with_hindex %>% #remove repeats
  group_by(paper_id, name) %>%
  mutate(hindex = hindex, prior_subs = first(prior_subs))
rcorr(hindex_corr$hindex, hindex_corr$prior_subs)
```
Running this correlation, we can see that there is some correlation between the hindex and the rejection count, however it does not appear to be significant. However, if we do a chi-square test on experience against prior submissions, we get the following:
```{r student-senior-setup, echo=F, warning=F}
chisq.test(table(survey_with_factor$hindex_discr,survey_with_factor$prior_subs))
```
This indicates that may be some significant relation between these two variables, however further statistical tests are needed.

__[MI.03]__ Distribution of "paper readiness" and relationship to author experience
  - http://www.ieee-security.org/TC/Reports/2017/SP2017-PCChairReport.pdf
  - https://www.nsf.gov/statistics/2018/nsb20181/assets/968/tables/tt05-18.pdf

__[MI.04]__ How is survey response rate affected by experience? acceptance?
  - http://www.ieee-security.org/TC/Reports/2017/SP2017-PCChairReport.pdf
  - https://www.nsf.gov/statistics/2018/nsb20181/assets/968/tables/tt05-18.pdf

__[MI.05]__ Does submission order affect paper's chances?
  - http://onlinelibrary.wiley.com/doi/10.1002/asi.22747/abstract

__[MI.06]__ Do award-winning papers have an easier path through peer review?


Many conferences confer an award to one or more papers, such as "Best Paper", "Best Student Paper", and "Best Artifact". Presumably, these papers received the award because their peer reviews agreed on its high quality, and ranked it accordingly. This presumption leads to the testable predictions that award-winning papers have shorter submission histories, higher review scores, fewer dissenting reviews, and higher reviewer confidence.

```{r award-setup, echo=F, warning=F}
survey_with_award <- survey %>%
  left_join(select(papers, c("key", "award")), by = c("paper_id" = "key"))

award_means <- survey_with_award %>%
  group_by(paper_id) %>%
  summarise_all(funs(mean(., na.rm = T))) %>%
  ungroup()
```

To test these hypotheses, we marked every award winning paper in our dataset, and interesected these with the papers in our survey responses.
We have responses from `r nrow(filter(award_means, award == 1))` papers that won an award. Of these,
`r pct(nrow(filter(award_means, award == 1, prior_subs == 0)), nrow(filter(award_means, award == 1, !is.nan(prior_subs))))`%
Were accepted on the first attempt, compared to
`r pct(nrow(filter(award_means, award == 0, prior_subs == 0)), nrow(filter(award_means, award == 0, !is.nan(prior_subs))))`%
of the papers who did not win an award.
(`r report_chi(chisq.test(table(award_means$award, award_means$prior_subs == 0)))`),
which is not as significant as we might expect. Looking at paper history from a different perspective, award-winning papers had only been rejected
`r mean(filter(award_means, award == 1)$prior_subs, na.rm = T) %>% round(2)`
times on average prior to acceptance, compared to 
`r mean(filter(award_means, award == 0)$prior_subs, na.rm = T) %>% round(2)`
with other papers, and all but one have been accepted by the third attempt. This is still not a very significant difference
(`r report_t(t.test(table(award_means$award, award_means$prior_subs)))`).

One possible explanation to this metric's similarity is that awards are somewhat arbitrary or subjective, and the winning papers are not substantially different than the rest.^[See for example this lively debate: https://academia.stackexchange.com/questions/114072]
Another explanation may be that these papers are ineed of higher perceived quality than the rest, but to reach this point they too had to go through numerous iterations of improvement.^[See for example Eric Price's explanation on his award-winning paper in the NIPS experiment http://blog.mrtz.org/2014/12/15/the-nips-experiment.html].

The second explanation seems more plausible when we consider review scores. To see this, we aggregated all the review scores we received for each paper across all papers and all review categories (not all reviews included all categories). The review grade distributions is shown in Fig. \@ref(fig:review-grades-award) for both award-winning and regular papers.

```{r review-grades-award, echo=F, message=F, warning=F, out.width = '100%', fig.cap = "Review grades distributions by category and award. Grades are normalized to the range [0:1]. Mean and median grades are represented by diamonds and bars, respectively. N represents the number of scores received in each category." }
library(reshape2)
survey_with_award %>%
  select(c("award", starts_with("gr_"))) %>%
  rename(Award = award, Overall = gr_overall, Technical = gr_technical, Presentation = gr_present, Impact = gr_impact, Originality = gr_originality, Relevance = gr_relevance, Confidence = gr_confidence) %>%
  melt() %>%
  rename(Category = variable, Grade = value) %>%
  filter(!is.na(Grade)) %>%
  group_by(Category, Award) %>%
  add_count() %>%
  ungroup() %>%
  ggplot(aes(x = Category, y = Grade, color = Category)) +
         geom_text(aes(y = 1.0, label = paste0("N=",n)), vjust = -0.5, size = 3) +
         geom_boxplot(notch = F) +
         stat_summary(fun.y=mean, geom="point", shape=23, size=4) +
         theme(legend.position = "none", axis.text.x = element_text(angle = 45)) +
         ylab("Normalized grade") +
         facet_wrap(. ~ Award)
```


As expected, award-winning papers score higher across all categories, and only the sparse "Relevance" category showed a weak statistical significance of $p>0.014$ using T-test (even though most reviews of award winning papers had a perfect relevance score). This is not a coincidence, as many conferences select, or at least screen for award papers based on review scores.

On the other hand, the same figure fails to affirm the two remaining predictions, as it shows wider range bands for most categories in the reviews of award-winning papers, and nearly identical mean and median reviewer confidence grades regardless of award. That said, the lowest grades received by award winning papers are higher than those of regular papers for most categories. So perhaps the distinctive trait of award-winning papers, other than higher scores, is that they elicit few negative opinions, if any.


__[MI.07]__ Relationship between english mastery and presentation scores/overall scores
  - [previous paper]

__[MI.08]__ Compare "overall" average to confidence-weighted "overall" average
  - [Search references]

__[MI.09]__ Correlation among different review scores. How correlated are all the grades? conversely, if one reviewer loves a paper and another hates it, can they disagree even on "presentation"?

In order to answer this question, we derive a correlation matrix between the review grades of various reviews, dropping na's in a pairwise Pearson correlation matrix calculation. The correlations are shown below: 
```{r award-setup, echo=F, warning=F}
survey_reviews <- survey %>%
  select(gr_technical, gr_present, gr_impact, gr_originality, gr_relevance, gr_confidence)
rev_corr <- cor(as.data.frame(survey_reviews), use="complete.obs")
rev_corr
```
Looking at these correlations, we can see that they are all highly correlated to each other, the most significant correlation between technical grades and the impact grades. To visualize these correlations, we plot them in a correlogram, show below:
`r corrplot(rev_corr, type="upper")`
(Further research needed)

__[MI.10]__ Produce initial stats like these:
  - https://publons.com/community/gspr

__[MI.11]__ Explain negative correlation between prior_subs and gr_overall
  - [Search references]
  
__[MI.12]__ How to deal with publication bias?
  - https://www.nature.com/articles/s41562-016-0034
  - http://www.nature.com/news/let-s-think-about-cognitive-bias-1.18520
  - http://www.nature.com/news/how-scientists-fool-themselves-and-how-they-can-stop-1.18517
  - http://www.nature.com/news/tool-for-detecting-publication-bias-goes-under-spotlight-1.21728
  - http://www.nature.com/news/replication-studies-bad-copy-1.10634
