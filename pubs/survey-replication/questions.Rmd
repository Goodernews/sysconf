# Research Questions {#sec:questions}

Contains a mix of both replication questions and open questions. Replication questions look at published past results on peer review effects and attempts to replicate or refute these findings on our data set. Open questions look at some controversial questions on peer-review effects for which we found opinionated answers, but little or no prior studies.

## On double-blindness

\question{Does blinding affect the quality of the reviews?}{blind-quality}

(separate confounders like conference reputation). Does-double blind really help quality in any way?
How about non-blind review where reviewers "sign" their review?
Does it lead to more balanced geography/institution/gender bias?

 - https://jamanetwork.com/journals/jama/article-abstract/380957

Higher quality conference leads to longer reviews? p. 39 of [@publons18:peer] Is review length correlated to english-abilities of reviewers?

 - https://www.sciping.com/wp-content/uploads/2018/09/Publons-Global-State-Of-Peer-Review-2018.pdf

 
In order to look at this question, it is first necessary to look at the correlations and interactions of the other variables in our dataset that may influence the ratings of review quality by each author. From looking at correlations between conference variables, we know that there is correlation between most, therefore we must take into account there confounding variables towards our model for review quality. Also, we know further that there is correlation between the scores of the papers and the quality of the reviews as rated by authors. 

```{r blind-quality, echo=F, warning=F}
#first join the conference stats to the survey data
survey_with_blind <- survey %>%
  mutate(conference = str_sub(paper_id, end=-5), isVeryHelpful = (helpfulness == "Somewhat helpful")) %>% #turn to same conference label in order to join
  left_join(all_confs, by=c("conference" = "conference")) %>%
  dplyr::select(c("gr_overall", "submissions", "helpfulness", "understanding", "fairness", "double_blind", "age", "mean_historical_citations", "h5_index", "pc_author_ratio", "mean_authors_per_paper", "acceptance_rate", "pc_paper_ratio", "mean_review_load")) %>%
  rename(Age = age, Blind = double_blind, Citations = mean_historical_citations, h5 = h5_index, PC_Author = pc_author_ratio, Authors_Paper  = mean_authors_per_paper, Acceptance = acceptance_rate, PC_Paper = pc_paper_ratio, Review_Load = mean_review_load) %>%
  drop_na()

survey_with_blind <- na.omit(survey_with_blind)

summary(survey_with_blind) #look at data's IQR's
  # GGally::ggpairs(lower = list(continuous = wrap("points", size = 0.4, position = position_jitter(height=0.1, width=0.1), alpha = 0.2)), progress = F)

#just to confirm from earlier question, lets look at the univariate regressions between each variable and then boxplots for categorical variables

#Not Correlated - Age x PC_Author, h5 x PC_author among the conference variables
#Not Correlated - Helpfulness x All Variables
#Not Correlated - Blind with everything except possibly Acceptance and PC_Author (based on boxplots), slight association with Review Load
```


Now that we've found the correlations between variables that do exist, let us now make a model and infer from the model whether blinding affects the quality of reviews via the coefficient of the model. By making a model, in essence, we are performing an ANCOVA - because we are checking for correlations between both categorical and numeric predictors associated with a categorical effector. 

Let us make a first model depending on the overall grade for each review, the conference quality, the age of the conference, the acceptance rate, the average citations, the pc's per paper, and the review load. This model is a simple linear model, and we look at the residuals to compute the F-statistic

```{r blind-quality-glm, echo=F, warning=F}
lrfit <- glm(helpfulness ~ 1 + Age + Citations + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, 
             data=survey_with_blind, 
             family="binomial")
coef(summary(lrfit))
```

#review-length and double-blind

Looking at this model, it can be seen that none of the variables have a significant effect, and further the model is not explanatory as it has a very high F-Stat and a very low R^2. Let's use some packages that performs exhaustive step-wise selection of our variables in order to select the most accurate model.

```{r blind-quality-stepwise, echo=F, warning=F}
#using MASS
library(MASS)
fit <- glm(helpfulness ~ 1 + Age + Citations + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, data=survey_with_blind, family="binomial")
step <- stepAIC(fit, direction="both")
step$anova 
```

#add submissions in order to look at popularity of conference that PC_Author indicates
Finally, through backwards selection, it was determined that in no possible model at all - with every combination of interactions between variables - was there any reason to believe being double blind or not effected the helpfulness of reviews

We repeat the process for usefulness and understanding:
```{r blind-quality-submissions, echo=F, warning=F}
fit_fairness <- glm(fairness ~ 1 + Age + Citations + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, data=survey_with_blind, family="binomial")
step <- stepAIC(fit, direction="both")
step$anova 

fit_understanding <- glm(Blind ~ 1 + Age + Citations + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, data=survey_with_blind, family="binomial")
step <- stepAIC(fit_understanding, direction="both")
step$anova 

fit_fairness <- glm(fairness ~ Authors_Paper, data=survey_with_blind, family="binomial")
fit_understanding <- glm(understanding ~ Blind, data=survey_with_blind, family="binomial")
```

###authors/paper affects response-rate?

According to the above fitness tests, fairness is best fit by a model that consists of just its relation to the number of authors per paper, and understanding is best fit by a model that is associated only with the blinding of the conference. These tests look for pairwise correlations and penalize fits that have correlated data, using the AIC metric in order to gather information about the fit that is most strongly correlated and has the smallest amount of confounding variables. 

All in all, this data does point towards the possibility of a correlation between blinding of a conference and the understanding of a paper as rated by its authors, and further makes the argument that there is likely little such correlation between any possible combination of effector variables and blindness towards fairness and helpfulness ratings by the authors.

A more thorough test is needed for understanding, including both a logit regression and an ANCOVA. 

###caret - decisision trees and machine learning

\question{Does it lead to more citations?}{blind-citations}

Fit citations to conference reputation, then look at the residuals.

  - https://chairs-blog.acl2017.org/2017/03/02/arxiv-and-double-blind-reviewing-revisited/
  - https://2017.splashcon.org/track/splash-2017-OOPSLA#FAQ-on-Double-Blind-Reviewing
  - http://cra.org/statement-diveity-micro-50/
  - https://arxiv.org/pdf/1702.00502.pdf
  - http://www.cs.utexas.edu/users/mckinley/notes/blind-revised-2015.html
  - http://history.acm.org/pictures/tods/tods.acm.org/editorial.pdf
  - http://www.ncbi.nlm.nih.gov/pubmed/8015128
  - http://sc17.supercomputing.org/conference-overview/technical-papers/sc17-double-blind-review-policy/
  - https://publons.com/blog/who-is-using-open-peer-review/
  - http://www.cs.utexas.edu/users/mckinley/notes/blind-revised-2015.html
  - http://history.acm.org/pictures/tods/tods.acm.org/editorial.pdf
  - http://www.ncbi.nlm.nih.gov/pubmed/8015128
  - https://2017.splashcon.org/track/splash-2017-oopsla#FAQ-on-Double-Blind-Reviewing
  - https://www.sciencedirect.com/science/article/pii/S0169534707002704
  - https://scholarlykitchen.sspnet.org/2018/05/16/peer-review-autoers-reviewers-north-star/?informz=1
  - https://www.pnas.org/content/114/48/12708
  - https://arxiv.org/pdf/1802.02188.pdf
  - http://www.erinhengel.com/research/publishing_female.pdf
  - https://www.frontiersin.org/articles/10.3389/fnins.2015.00169/full
  - https://jamanetwork.com/journals/jama/article-abstract/376228

```{r blind-citations, echo=F, warning=F}
survey_with_citations <- survey %>%
  mutate(conference = str_sub(paper_id, end=-5)) %>%
  left_join(all_confs, by=c("conference" = "conference")) %>%
  left_join(papers, by=c("paper_id" = "key")) %>%
  dplyr::select(c("references", "helpfulness", "understanding", "fairness", "double_blind", "age", "mean_historical_citations", "h5_index", "pc_author_ratio", "mean_authors_per_paper", "acceptance_rate", "pc_paper_ratio", "mean_review_load")) %>%
  rename(Age = age, Blind = double_blind, Citations_Conf = mean_historical_citations, h5 = h5_index, PC_Author = pc_author_ratio, Authors_Paper  = mean_authors_per_paper, Acceptance = acceptance_rate, PC_Paper = pc_paper_ratio, Review_Load = mean_review_load)
survey_with_citations <- na.omit(survey_with_citations)

#library(leaps)
#leaps <- regsubsets(references ~ 1 + Age + Citations_Conf + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, data=survey_with_citations)
#summary(leaps)
#plot(leaps,scale="r2")

library(MASS)
fit <- lm(references ~ 1 + Age + Citations_Conf + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, data=survey_with_citations, family="binomial")
step <- stepAIC(fit, direction="both")
step$anova 
```

The best fit according to the step-reduction above appears to be towards each indicates that whether a conference is blinded or not has the most deviance on the references, making it hard to model a linear relationship between the two variables. According to both analysis' the closet relationship derived is from references associated with Age, h5, acceptance, the number of PC's as authors, the number of PC's per paper, and the number of authors per paper. 

Trying a fit anyway between hindex and citations and looking at the residuals:
```{r blind-citations-hindex, echo=F, warning=F}
fitc.lm <- lm(references ~ Citations_Conf , data = survey_with_citations)
summary(fitc.lm)
fitc.resid = resid(fitc.lm)
plot(survey_with_citations$Citations_Conf, fitc.resid, ylab="Residuals", xlab="Citations") 
abline(0, 0)

fitc.lm <- lm(references ~ Citations_Conf * Blind , data = survey_with_citations)
summary(fitc.lm)
plot(survey_with_citations$Citations_Conf, fitc.resid, ylab="Residuals", xlab="Citations") 
abline(0, 0)
```

The fit above does point towards a high positive significant relationship between double blind conferences and references, however as before the results are inconclusive given the high amount of residual deviance and low R^2 values. More research is needed in order to account for this high deviance in the data and make a correct fit. 

\question{Does blinding reduce the h-index of authors of accepted papers?}{blind-hindex}

  - https://www.usenix.org/sites/default/files/atc19_message.pdf
  
  In order to look at this relationship, lets first consider the IQR's of those authors h-index of blind versus non-blind conferences
```{r blind-hindex, echo=F, warning=F}
survey_blind_hindex <- survey %>%
  mutate(conference = str_sub(paper_id, end=-5)) %>% 
  left_join(dplyr::select(authors, c("name", "hindex")), by = c("name" = "name")) %>%
  left_join(all_confs, by=c("conference" = "conference")) %>%
  dplyr::select(c("hindex", "double_blind", "age", "mean_historical_citations", "h5_index", "pc_author_ratio", "mean_authors_per_paper", "acceptance_rate", "pc_paper_ratio", "mean_review_load"))
survey_blind_hindex <- na.omit(survey_blind_hindex)

ggplot(survey_blind_hindex, aes(x=double_blind, y=hindex)) + geom_boxplot()
```

Looking at the boxplot,there doesn't appear to be any significant difference between the two groups, but it is likely there is one of many possible confounds messing the relationship. Let's consider how hindex correlates to all the other important numeric variables of each conference

```{r blind-hindex-corr, echo=F, warning=F}
survey_blind_hindex <- survey_blind_hindex  %>%
  rename(Age = age, Blind = double_blind, Citations = mean_historical_citations, h5 = h5_index, PC_Author = pc_author_ratio, Authors_Paper  = mean_authors_per_paper, Acceptance = acceptance_rate, PC_Paper = pc_paper_ratio, Review_Load = mean_review_load)

GGally::ggpairs(lower = list(continuous = wrap("points", size = 0.4, position = position_jitter(height=0.1, width=0.1), alpha = 0.2)), progress = F, data=survey_blind_hindex)

```

Looking at the correlegram above, it can be seen that there appears to be no to little correlation between hindex and the rest of the variables. There is a small positive correlation between hindex and conference citations. A pairwise correlation for that relationship can be shown in a linear model: `r summary(lm(hindex ~ Citations, data=survey_blind_hindex))` and it is indeed a significant correlation. 

Now, let's do a stepwise analysis of the linear model to get the best fit that has the least amount of possible confounds.
```{r blind-hindex-model, echo=F, warning=F}
#library(leaps)
#leaps <- regsubsets(hindex ~ 1 + Age + Citations + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, data=survey_blind_hindex)
#summary(leaps)
#plot(leaps,scale="r2")

library(MASS)
fit <- lm(hindex ~ 1 + Age + Citations + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, data=survey_blind_hindex, family="binomial")
step <- stepAIC(fit, direction="both")
step$anova 
```

According to above, the appears to be little correlation between hindex and the blinding, in any type of possible fit

\question{Does double-blind bias less against junior faculty?}{blind-diversity}

fit author hindex against double-blind (use max hindex per paper, or sum, or min, or median, or mean). Correlate also to prior_subs.

  - [@tomkins17:reviewer]
  - https://www.frontiersin.org/articles/10.3389/fnins.2015.00169/full
  - https://www.researchgate.net/profile/Anthony_Tung/publication/220416127_Impact_of_double_blind_reviewing_on_SIGMOD_publication/links/55b8691d08ae9289a08d5678.pdf
  - https://arxiv.org/pdf/1802.02188.pdf
  - https://onlinelibrary.wiley.com/doi/full/10.1002/asi.22784
  - https://www.timeshighereducation.com/world-university-rankings/2019/world-ranking#!/page/0/length/25/sort_by/rank/sort_order/asc/cols/stats
  - What about country diversity? Sector diversity? country? gender? Non-english speakers Low-prestige institutions? Content based? Other types of diversity?


```{r blind-diversity, warning=F, echo=F}
survey_exp <- survey_blind_hindex %>%
  mutate(hindex = ifelse(hindex < 12, "Novice", ifelse(hindex <= 18, "Mid-career", "Experienced")))
nsimple <- glm(hindex == "Novice" ~ Blind, data=survey_exp, family = "binomial")
midsimple <- glm(hindex == "Mid-career" ~ Blind, data=survey_exp, family = "binomial")
expsimple <- glm(hindex == "Experienced" ~ Blind, data=survey_exp, family = "binomial")
summary(nsimple)
summary(midsimple)
summary(expsimple)
```
Stepwise-regression analysis
```{r blind-diversity-stepwise, warning=F, echo=F}
library(MASS)

nfit <- glm(hindex == "Novice" ~ 1 + Age + Citations + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, data=survey_exp, family="binomial")
step <- stepAIC(nfit, direction="both")
step$anova 

midfit <- glm(hindex == "Mid-career" ~ 1 + Age + Citations + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, data=survey_exp, family="binomial")
step <- stepAIC(midfit, direction="both")
step$anova 

expfit <- glm(hindex == "Experienced" ~ 1 + Age + Citations + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, data=survey_exp, family="binomial")
step <- stepAIC(expfit, direction="both")
step$anova 
```

R^2, F-score and ASIc 
```{r blind-diversity-AIC, warning=F, echo=F}
#library(leaps)
#leaps <- regsubsets(hindex == "Novice" ~ 1 + Age + Citations + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, data=survey_exp)
#summary(leaps)
#plot(leaps,scale="r2")

#leaps <- regsubsets(hindex == "Mid-career" ~ 1 + Age + Citations + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, data=survey_exp)
#summary(leaps)
#plot(leaps,scale="r2")

#leaps <- regsubsets(hindex == "Experienced" ~ 1 + Age + Citations + h5 + PC_Author + Authors_Paper + Acceptance + PC_Paper + Review_Load + Blind, data=survey_exp)
#summary(leaps)
#plot(leaps,scale="r2")
```


# From older paper:

Many author factors do not appear to significantly interact with double-blindness, such as: the gender, position, and research experience (based on H-index).
In terms of review scores, there do not appear to be large differences in the mean and median grades, but the distribution of grades in double-blind reviews appears wider for some categories, especially technical merit.

```{r echo=F}
survey_with_confs <- survey %>%
  mutate(conference = gsub("_\\d\\d\\d", "", paper_id)) %>%
  left_join(all_confs)
```

Double-blind reviewed conferences in our dataset do appear to accept longer papers by page count
(`r report_t(t.test(table(survey_with_confs$double_blind, survey_with_confs$mean_pages)))`)
with longer research history
(`r report_chi(chisq.test(table(survey_with_confs$double_blind, survey_with_confs$months_research)))`)
and more coauthors per paper
(`r report_t(t.test(table(survey_with_confs$double_blind, survey_with_confs$mean_authors_per_paper)))`).
These conferences are also more likely to allow rebuttals
(`r report_chi(chisq.test(table(survey_with_confs$double_blind, survey_with_confs$allow_rebuttal)))`),
and produce more reviews
(`r report_t(t.test(table(survey_with_confs$double_blind, survey_with_confs$reviews)), 2)`)
of longer length
(`r report_chi(chisq.test(table(survey_with_confs$double_blind, survey_with_confs$length)))`),
which in turn are deemed more helpful, fair, and understanding by authors (Sec. \@ref(subsec:quality)).


We doubt that double-blinding adequately explains all these phenomena [@godlee98:peer; @mcnutt90:blinding]. Instead, we suggest looking at a third factor: the reputation or prestige of a conference. Prestige may be too abstract to measure accurately, but we did collect two proxy conference metrics: the acceptance rate and H5-index (as measured by GS).
Indeed, the `r nrow(filter(all_confs, double_blind == T))` double-blind conferences in our set average a lower acceptance rate
(`r pct(mean(filter(all_confs, double_blind == T)$acceptance_rate, na.rm = T), 1, 1)`% vs.
`r pct(mean(filter(all_confs, double_blind == F)$acceptance_rate, na.rm = T), 1, 1)`%) and higher H5-index
(`r round(mean(filter(all_confs, double_blind == T)$h5_index, na.rm = T), 1)` vs.
`r round(mean(filter(all_confs, double_blind == F)$h5_index, na.rm = T), 1)`).

More competitive conferences also produce longer reviews, which are linked to better author evaluations of the reviews
(`r report_t(t.test(table(survey_with_confs$acceptance_rate, survey_with_confs$length)))`).
When correcting for conference quality, as measured by either metric, most of the previous double-blind associations fade or disappear. For example, the relationship between review length and double-blind reviews loses statistical significance when limiting ourselves to conferences with an acceptance rate of less then 0.2
(`r x <- filter(survey_with_confs, acceptance_rate >= 0.0, acceptance_rate < 0.2); report_chi(chisq.test(table(x$double_blind, x$length)))`),
or between 0.2 and 0.3
(`r x <- filter(survey_with_confs, acceptance_rate >= 0.2, acceptance_rate < 0.3); report_chi(chisq.test(table(x$double_blind, x$length)))`),
or higher than 0.3
(`r x <- filter(survey_with_confs, acceptance_rate >= 0.3, acceptance_rate < 0.9); report_chi(chisq.test(table(x$double_blind, x$length)))`).
Fig. \@ref(fig:acceptance-vs-length) depicts the relationships between these three variables.


Some studies found that a double-blind peer-review process can improve women's representation among authors [@budden08:double; @eaton19:gender; @lloyd90:gender; @wenneras01:nepotism], whereas other studies disagree [@ceci11:understanding; @lee13:bias; @tomkins17:reviewer; @ware08:peer]. Our own data does not show a significant difference in women's representation
(`r report_chi(chisq.test(table(survey_with_confs$double_blind, survey_with_confs$mapped_gender)))`).

```{r acceptance-vs-length, echo = F, message = F, warning = F, out.width = '100%', fig.cap = "Conference acceptance rate and reported review length. Observe that double-blind conferences tend to be more competitive, and more competitive conferences tend to have longer reviews."}
cbp <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#000000")
survey_with_confs %>%
  filter(!is.na(length), !is.na(acceptance_rate)) %>%
  filter(acceptance_rate > 0.14) %>% # Remove two outlier conferences
  group_by(conf, npapers, acceptance_rate, double_blind, length) %>%
  summarize(n = n()) %>%
  mutate(Review = factor(ifelse(double_blind, "Double blind", "Single blind"))) %>%
  arrange(acceptance_rate) %>%
  ggplot(aes(x = acceptance_rate, y = n, fill = forcats::fct_rev(length))) +
    geom_bar(position = "fill", stat = "identity", width = 0.005) +
    scale_x_continuous(labels = scales::percent_format()) +
    scale_y_continuous(labels = scales::percent_format()) +
    scale_fill_discrete(limits = c("1-2 Paragraphs", "Half a Page", "A Page", "Multiple Pages")) +
    facet_wrap(~ Review) +
    ylab("Reported review length (proportion)") +
    xlab("Conference acceptance rate") +
    labs(fill = "Length") +
    theme_light() +
    theme(legend.position = "top") +
    coord_flip()

#survey_with_confs %>%
#  filter(!is.na(length)) %>%
#  group_by(acceptance_rate, double_blind, length) %>%
#  summarize(n = n()) %>%
#  ggplot(aes(x = length, y = acceptance_rate, color = double_blind, size = n)) +
##         geom_boxplot(notch = T) +
#    geom_point(alpha = 0.5) +
#    geom_jitter(height = 0, width = 0.2) +
#    theme(legend.position = "top") +
#    scale_color_discrete(name = "Review policy", labels = c("Single blind", "Double blind")) +
#    xlab("Reported review length") +
#    ylab("Conference acceptance rate") +
#    coord_flip()
```


