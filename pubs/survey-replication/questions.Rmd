# Research Questions {#sec:questions}

Contains a mix of both replication questions and open questions. Replication questions look at published past results on peer review effects and attempts to replicate or refute these findings on our data set. Open questions look at some controversial questions on peer-review effects for which we found opinionated answers, but little or no prior studies.

## On double-blindness

\question{Does blinding affect the quality of the reviews?}{blind-quality}

(separate confounders like conference reputation). Does-double blind really help quality in any way?
How about non-blind review where reviewers "sign" their review?
Does it lead to more balanced geography/institution/gender bias?

 - https://jamanetwork.com/journals/jama/article-abstract/380957
 
In order to look at this question, it is first necessary to look at the correlations and interactions of the other variables in our dataset that may influence the ratings of review quality by each author. From looking at correlations between conference variables, we know that there is correlation between most, therefore we must take into account there confounding variables towards our model for review quality. Also, we know further that there is correlation between the scores of the papers and the quality of the reviews as rated by authors. 

```{r blind-quality, echo=F, warning=F}
#first join the conference stats to the survey data
survey_with_blind <- survey %>%
  mutate(conference = str_sub(paper_id, end=-5), isVeryHelpful = (helpfulness == "Somewhat helpful")) %>% #turn to same conference label in order to join
  left_join(all_confs, by=c("conference" = "conference")) %>%
  select(-c(conference))

survey_with_blind %>%
  select(c("helpfulness", "age", "mean_historical_citations", "h5_index", "pc_author_ratio", "mean_authors_per_paper", "acceptance_rate", "pc_paper_ratio", "mean_review_load")) %>%
  rename(Age = age, Citations = mean_historical_citations, h5 = h5_index, PC_Author = pc_author_ratio, Authors_Paper  = mean_authors_per_paper, Acceptance = acceptance_rate, PC_Paper = pc_paper_ratio, Review_Load = mean_review_load) %>%
  GGally::ggpairs(lower = list(continuous = wrap("points", size = 0.4, position = position_jitter(height=0.1, width=0.1), alpha = 0.2)), progress = F)
```

Let us make a first model depending on the overall grade for each review, the conference quality, the age of the conference, the acceptance rate, the average citations, the pc's per paper, and the review load. 
```{r blind-quality, echo=F, warning=F}
lrfit <- glm(helpfulness ~ 1 + double_blind + age + h5_index + mean_historical_citations + pc_author_ratio  , 
             data=survey_with_blind, 
             family="binomial")
coef(summary(lrfit))

#in order to make our models the best fit, we can look at the deviance measure of GOF 
```


\question{Does it lead to more citations?}{blind-citations}

Fit citations to conference reputation, then look at the residuals.

  - https://chairs-blog.acl2017.org/2017/03/02/arxiv-and-double-blind-reviewing-revisited/
  - https://2017.splashcon.org/track/splash-2017-OOPSLA#FAQ-on-Double-Blind-Reviewing
  - http://cra.org/statement-diversity-micro-50/
  - https://arxiv.org/pdf/1702.00502.pdf
  - http://www.cs.utexas.edu/users/mckinley/notes/blind-revised-2015.html
  - http://history.acm.org/pictures/tods/tods.acm.org/editorial.pdf
  - http://www.ncbi.nlm.nih.gov/pubmed/8015128
  - http://sc17.supercomputing.org/conference-overview/technical-papers/sc17-double-blind-review-policy/
  - https://publons.com/blog/who-is-using-open-peer-review/
  - http://www.cs.utexas.edu/users/mckinley/notes/blind-revised-2015.html
  - http://history.acm.org/pictures/tods/tods.acm.org/editorial.pdf
  - http://www.ncbi.nlm.nih.gov/pubmed/8015128
  - https://2017.splashcon.org/track/splash-2017-oopsla#FAQ-on-Double-Blind-Reviewing
  - https://www.sciencedirect.com/science/article/pii/S0169534707002704
  - https://scholarlykitchen.sspnet.org/2018/05/16/peer-review-autoers-reviewers-north-star/?informz=1
  - https://www.pnas.org/content/114/48/12708
  - https://arxiv.org/pdf/1802.02188.pdf
  - http://www.erinhengel.com/research/publishing_female.pdf
  - https://www.frontiersin.org/articles/10.3389/fnins.2015.00169/full
  - https://jamanetwork.com/journals/jama/article-abstract/376228


\question{Does blinding reduce the h-index of authors of accepted papers?}{blind-hindex}

  - https://www.usenix.org/sites/default/files/atc19_message.pdf

\question{Does double-blind at least bias less against junior faculty?}{blind-diversity}

fit author hindex against double-blind
What about country diversity? Sector diversity? Non-english speakers Low-prestige institutions? Content based? Other types of diversity?

  - https://www.frontiersin.org/articles/10.3389/fnins.2015.00169/full
  - https://www.researchgate.net/profile/Anthony_Tung/publication/220416127_Impact_of_double_blind_reviewing_on_SIGMOD_publication/links/55b8691d08ae9289a08d5678.pdf
  - https://arxiv.org/pdf/1802.02188.pdf
  - https://onlinelibrary.wiley.com/doi/full/10.1002/asi.22784
  - https://www.timeshighereducation.com/world-university-rankings/2019/world-ranking#!/page/0/length/25/sort_by/rank/sort_order/asc/cols/stats


--------------------------


## On review quality

\question{What is the relationship between review length and conference reputation?}{length-reputation}

  - https://publons.com/static/Publons-Global-State-Of-Peer-Review-2018.pdf

\question{How is review length related to quality?}{review-length}

  - http://retractionwatch.com/2017/11/27/make-reviews-public-says-peer-review-expert/


\question{Do authors "like" "kind" reviews?}{kind-reviews}

  - https://nlpers.blogspot.co.uk/2015/06/some-naacl-2013-statistics-on-author.html
  - [@anderson08:towards]
  - [@papagiannaki07:author]

\question{Problems and arbitrariness in peer review}{arbitrary}

  - http://blog.mrtz.org/2014/12/15/the-nips-experiment.html
  - https://cacm.acm.org/blogs/blog-cacm/181996-the-nips-experiment/fulltext
  - [@vardi09:conferences]
  - [@wallach11:rebooting]
  - [@vines11:cointoss]
  - https://scholarlykitchen.sspnet.org/2012/07/31/the-referee-that-wasnt-there-the-ghostly-tale-of-reviewer-3-3/
  - https://dl.acm.org/citation.cfm?id=1435430https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1420798/
  - ch. 5 of "women science and technology"
  - https://www.sigarch.org/r2-the-future-of-isca-or-not/
  - https://www.sigarch.org/a-common-standard-to-fix-our-review-process-and-oh-i-was-wrong-about-one-thing/
  - https://petsymposium.org/experiment.php


\question{Effect of review days on number of submissions or author author satisfaction}{review-days}

Authors want 3m or less for reviews

  - [@editage18:perspectives]
  - [@wallach11:rebooting]

\question{Does review time materially impact quality?}{time-quality}

Does it really take about 5/6 hours per review?
Most reviewers are done in 4 weeks

  - https://senseaboutscience.org/wp-content/uploads/2016/12/Peer_Review_Survey.pdf
  - https://www.sigmetrics.org/sigmetrics2017/SIGMETRICSInfoforAuthors.html)
  - http://www.ieee-security.org/TC/Reports/2017/SP2017-PCChairReport.pdf
  - https://cacm.acm.org/magazines/2009/5/24644-program-committee-overload-in-systems/abstract
  - https://publons.com/static/Publons-Global-State-Of-Peer-Review-2018.pdf
  - https://senseaboutscience.org/wp-content/uploads/2016/12/Peer_Review_Survey.pdf
  - http://www.ieee-security.org/TC/Reports/2017/SP2017-PCChairReport.pdf



\question{Effect of reviewer load}{reviewer-load}

  - [@beverly13:findings]

\question{Should we have post-publication review/comments?}{post-pub}

  - https://scholarlykitchen.sspnet.org/2013/03/27/how-rigorous-is-the-post-publication-review-process-at-f1000-research/
  - http://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1001772
  - https://link.springer.com/article/10.1007/s11948-016-9854-2
  - https://www.sciencedirect.com/science/article/pii/S016895251500044X
  - https://search.proquest.com/docview/1697484361?pq-origsite=gscholar

\question{Is a revision cycle effective?}{revision-cycle}

How long should it be?

  - http://www.ieee-security.org/TC/Reports/2017/SP2017-PCChairReport.pdf

\question{Open peer review--no one is doing it?}{open-peer}

  - http://www.nature.com/news/watch-out-for-cheats-in-citation-game-1.20246
  - http://www.nature.com/news/let-s-make-peer-review-scientific-1.20194

\question{Can we improve reviews from one venue to the next?}{improve-reviews}

  - https://www.nature.com/articles/546352a
  - https://www.sigarch.org/a-proposal-to-coordinate-reviewing-across-computer-architecture-conferences/

\question{How does the average number of reviews compare to other fields?}{reviews-num}

  - https://scholarlykitchen.sspnet.org/2018/09/12/guest-post-what-a-new-publons-report-on-peer-review-says-about-diversity-and-more/?informz=1

\question{Are reviews unfair?}{reviews-unfair}

  - https://www.sigarch.org/r2-the-future-of-isca-or-not/

\question{Effect of over-positive PC members}{positive-pc}

  - https://www.sigarch.org/overwhelming-statistical-evidence-that-our-review-process-is-broken/
  - https://www.sigarch.org/false-accepts-and-false-rejects/

\question{Does rebuttal/author response help?}{rebuttal-help}

Probably useless

  - https://chairs-blog.acl2017.org/2017/03/27/author-response-does-it-help/
  - https://nlpers.blogspot.co.uk/2015/06/some-naacl-2013-statistics-on-author.html
  - http://www.ieee-security.org/TC/Reports/2017/SP2017-PCChairReport.pdf
  - http://sc17.supercomputing.org/program/papers-faq/
  - https://www.sigops.org/sosp/sosp17/call-for-papers.html
  - https://www.sigarch.org/how-pc-chairs-and-pc-members-can-help-improve-our-process/
  - Dave Evans, CCS: "I think the author responses are valuable, even if they don't change paper decisions since (1) they make reviewers take the early reviews more seriously and know they risk being embarrassed if they say something incorrect/lazy, (2) they give authors a sense that they had an opportunity to respond, and (3) they help PC chairs decide if a paper needs additional reviews. I think the responses did change the outcomes of some papers for CCS - one for sure, that was considered dropped after the first round, but revived by the responses and eventually accepted, and for the borderline papers, the responses were considered seriously in making decisions (but can't know for sure how things would have gone without them). In some cases, the authors responses made it easier to reject papers since they made it clear that the problems reviewers found were legitimate, and a handful of papers were withdraw by the authors after seeing the early reviews.  We had a "Discussion Committee" that assigned a PC member to oversee each paper to ensure the reviewers took the author responses into consideration, which helped a lot since for a conference of this scale it would be too much for the PC chairs to check on all papers."

\question{Do reviewers want anonymity?}{reviewer-anon}

  - https://senseaboutscience.org/wp-content/uploads/2016/12/Peer_Review_Survey.pdf

\question{Effect of reviewer award on their own submission's grade?}{reviewer-award}

  - [Search references]

\question{Effect of shepherding, where known}{shepherd}

  - [Search references]

\question{Properties of "real" papers, based on Ulfar's email from 10/4/17}{real-papers}

  1) Reviewers are less likely to agree on "real" papers.
  2) Reviewers are less likely to state high confidence on "real" papers, especially if they're saying reject/strong reject.
  - [Search references]

\question{How does the average number of reviews compare to other fields?}{num-reviews}

  - https://scholarlykitchen.sspnet.org/2018/09/12/guest-post-what-a-new-publons-report-on-peer-review-says-about-diversity-and-more/?informz=1


--------------------------

## On conference quality

\question{What matters more for a conference: scope, review length, or impact factor?}{conf-metric}

  - https://www.editage.com/files/Editage-Global-Author-Survey.pdf
  - https://peerj.com/articles/365/#p-17

\question{Correlate everything with conference metrics and policies (features)}{conf-corr}
  
First, we will look at the correlations between the key metrics of each conference. The one's chosen first are age, the mean historical citations, the h5 score (as measured by the h5 index score), the ratio between PC's and authors, the average authors per paper, the acceptance rate, the average PC's per paper, and the review load (number of reviews required per reviewer)
The correlegram below depicts these relations:
```{r conf-corr echo=F, warning=F}
#first correlations between conference metrics
all_confs %>%
  select(c("age", "mean_historical_citations", "h5_index", "pc_author_ratio", "mean_authors_per_paper", "acceptance_rate", "pc_paper_ratio", "mean_review_load")) %>%
  rename(Age = age, Citations = mean_historical_citations, h5 = h5_index, PC_Author = pc_author_ratio, Authors_Paper  = mean_authors_per_paper, Acceptance = acceptance_rate, PC_Paper = pc_paper_ratio, Review_Load = mean_review_load) %>%
  GGally::ggpairs(lower = list(continuous = wrap("points", size = 0.4, position = position_jitter(height=0.1, width=0.1), alpha = 0.2)), progress = F)
```
Looking at the above correlegram, we can see that almost all appear to be correlated in some way or another, the question is to what degree? We will divide the correlations into those that seem obvious at first, as they are built-in to the metric in the first place and those that require further investigation.

Interesting:
-age and h5-index - conferences that are older are more reputable?
-age and acceptance rate - conferences that are older will reject more papers?
-mean_historical_citations and h5_index - could it be that conferences that are cited more are rated less as more people get into them? 
-mean_historical_citations and acceptance and review-load
-h5_index and acceptance_rate - conferences that are harder to get into are rated higher?
-acceptance and pc_paper_ratio - There are more PC's so the ratings are harder for each paper?

Built-in:
-age and mean_historical_citations - clearly this will happen as conferences with more age have been cited more
-pc_author and pc_paper ratios  - this correlation makes sense, if there ar emore pc's there will clearly be a higher pc per paper ratio
-pc_author and review_load - that correlation is also obvious, clearly if there are more pc's there will be less review load

I haven't included the following metrics for each conference as they are measures of counts not averages: past-papers, past_citations, chairs_num, pc_size, npapers, authors_num
These scores don't take into account the time the conference has been running and the number of papers. However, I haven't included mean_historical_length or h5_median, both of these could be important as well. 

Now onto the policies of the conferences. For this, we will first look at if it is double_blind, allows rebuttals and if it is making a diversity effort. In this case, as there are only two categories - true or false, we can do a paired t-test or independent t-test depending on the context. 

\question{Diversity of reviewers}{reviewer-diversity}

(first answer why diversity is needed). Also specifically when limited to HPC (look at whpc-summit paper)
  
  - https://scholarlykitchen.sspnet.org/2018/09/07/ask-the-chefs-diversity-in-peer-review/?informz=1
  - https://scholarlykitchen.sspnet.org/2018/09/13/eight-ways-to-tackle-diversity-and-inclusion-in-peer-review/?informz=1
  - https://www.nature.com/news/journals-invite-too-few-women-to-referee-1.21337

\question{Compare to other conferences}{conf-comp}

  - ATC'19  https://www.usenix.org/sites/default/files/atc19_message.pdf
  - Compare to SIGCOMM09  https://dl.acm.org/citation.cfm?id=1568623
  - What do authors in MICRO think about the review process?  https://www.sigmicro.org/2019/01/09/micro-survey-results/


\question{Quality is multi-dimensional}{quality-multidim}

https://scholarlykitchen.sspnet.org/2019/09/16/quality-is-multi-dimensional-how-many-ways-can-you-define-quality-in-peer-review/?unapproved=84111&moderation-hash=47b56366ddaf68dffef86ee501020c54#comment-84111

\question{Textual difference between 1st-version pre-print, last version, and published version}{ver-diff}

Not just similarity, but change in meaning

  - https://scholarlykitchen.sspnet.org/2018/03/15/a-comment-on-klein-et-als-comparing-articles-to-preprints/?informz=1

\question{Do authors aim for the highest possible venue that will still accept them?}{highest-venue}

Correlate conference prestige with no. of previous attempts.

  - https://scholarlykitchen.sspnet.org/2011/12/08/is-peer-review-a-coin-toss/

\question{Do two-cycle conferences lead to better papers? Is round-based reviewing better?}{two-cycle}

  - http://from-a-to-remzi.blogspot.com/2012/07/why-you-shouldnt-feel-too-badly-about.html

\question{Effect of submission Day-of-week and months-to-publish on submissions no. and acceptance rate}{day-of-week}

Correct for confounders, which you can find by plotting.

  - [Search references]

\question{Effect of "collisions" of multiple due dates next to each other, especially with similar paper topics}{collisions}

  - https://www.sigarch.org/r2-the-future-of-isca-or-not/

\question{Do conferences with strict data policies receive fewer submissions?}{data-policy}

  - https://scholarlykitchen.sspnet.org/2018/09/25/does-adopting-a-strict-data-sharing-policy-affect-submissions/?informz=1


--------------------------

## Misc

\question{What is the relationship between prior subs and conference quality?}{prior-subs-quality}
```{r prior-subs-quality echo=F, warning=F}
library(stringr)

paper_confs <- survey %>%
  drop_na(prior_subs) %>%
  group_by(paper_id) %>%
  summarise(prior_subs = mean(prior_subs), conference = str_sub(first(paper_id), end=-5)) %>% #get average previous submissions
  left_join(select(all_confs, c("conference", "mean_historical_citations")), by=c("conference" = "conference")) %>%
  drop_na(mean_historical_citations) 
  # group_by(conference) %>% #take averages
  # summarise(prior_avg = mean(prior_subs), h5 = first(h5_median))

cor.test(paper_confs$mean_historical_citations, paper_confs$prior_subs, use="complete")

#try averages instead
paper_confs <- paper_confs %>%
  group_by(conference) %>%
  summarise(prior_avg = mean(prior_subs), h5_median = first(h5_median))

cor.test(paper_confs$h5_median, paper_confs$prior_avg, use="complete")

#produce h5_index, median and citations

```

Not correlated 

\question{Distribution of "paper readiness" and relationship to author experience}{paper-readiness}

  - http://www.ieee-security.org/TC/Reports/2017/SP2017-PCChairReport.pdf
  - https://www.nsf.gov/statistics/2018/nsb20181/assets/968/tables/tt05-18.pdf

\question{How is survey response rate affected by experience? acceptance?}{response-rate}

  - http://www.ieee-security.org/TC/Reports/2017/SP2017-PCChairReport.pdf
  - https://www.nsf.gov/statistics/2018/nsb20181/assets/968/tables/tt05-18.pdf
```{r response-rate, echo=F, warning=F}
people_with_hindex <- survey %>%
  left_join(select(authors, c("name", "hindex")), by = c("name" = "name")) %>%
  drop_na(hindex) %>%
  mutate(Experience = ifelse(hindex < 12, "Novice", ifelse(hindex <= 18, "Mid-career", "Experienced"))) %>%
  group_by(name) %>%
  filter(row_number() == 1)  
  
resp_position_exp <- people_with_hindex %>% 
  drop_na(position) %>%
  mutate(position = ifelse(position == "Professor" || position == "Assistant Professor" || position == "Associate Professor", "Professor", 
                            ifelse(position == "Student", "Student", 
                                   ifelse(position == "Goverment Researcher" || position == "Industry Researcher", "Researcher", "Other")))) %>%
  filter(!position == "Other") %>% #~ position filters ~
  group_by(position, Experience) %>%
  mutate(perc = pct(n(), nrow(authors_with_profile))) %>%
  filter(row_number() == 1)

#graph of position and percent responded divided on experience
ggplot(resp_position_exp, aes(x=position, y = perc, group=Experience, label=perc)) + 
  geom_bar(aes(fill = factor(..x..)), stat = "identity") + 
  geom_text(vjust=-0.3, size=3.5) +
  theme_minimal() +
  theme(legend.position = "none")+
  facet_grid(~Experience) +
  ylab("Response Rate %") + 
  xlab("Position") 

#do density plot, geom_density()

#graph of acceptance and percent responded with experience fill
people_with_hindex %>%  
  filter(prior_subs < 4) %>%
  ggplot(aes(x=prior_subs)) + 
  geom_bar(aes(fill = Experience, y= pct((..count..),nrow(authors_with_profile)))) + 
  theme_minimal() +
  ylab("Response Rate %") + 
  xlab("Prior Submissions") 

#flipped around
people_with_hindex %>%  
  filter(prior_subs < 4) %>%
  mutate(prior_subs=as.character(prior_subs)) %>%
  ggplot(aes(x=Experience)) + 
  geom_bar(aes(fill = prior_subs, y= pct((..count..),nrow(authors_with_profile)))) + 
  theme_minimal() +
  ylab("Response Rate %") + 
  xlab("Experience") 
```
\question{Does submission order affect paper's chances?}{sub-order}

  - http://onlinelibrary.wiley.com/doi/10.1002/asi.22747/abstract
  
```{r sub-order, echo=F, warning=F}
papers_grades_order <- survey %>% 
  group_by(paper_id) %>% 
  summarise(gr_overall = mean(gr_overall)) %>%
  left_join(select(papers, c("key", "months_to_gs", "months_to_eprint")), by=c("paper_id" = "key"))  %>%
  drop_na(gr_overall, months_to_gs, months_to_eprint)

cor.test(papers_grades_order$gr_overall,  papers_grades_order$months_to_gs)
cor.test(papers_grades_order$gr_overall, papers_grades_order$months_to_eprint)
```

\question{Relationship between english mastery and presentation scores/overall scores}{english-present}

  - [previous paper]
  
```{r english-present, echo=F, warning=F}
#take average "english-mastery" per paper  and average presentation scores for paper
eng_mastery_tbl <- survey %>%
  drop_na(gr_present) %>% #drop responses with no presentation score
  mutate(native_english = ifelse(native_english == TRUE, 1, 0)) %>%
  group_by(paper_id) %>%
  summarise(english_mastery = mean(native_english), present_avg = mean(gr_present)) 
#what do we do if two people have submitted different number of reviews, how do we determine which review is which? i'll just do weighted average for now

cor.test(eng_mastery_tbl$english_mastery, eng_mastery_tbl$present_avg) #no-correlation for any of them
```
According to the correlation measures, there does not appear to be a significant relationship, at least in our data
\question{Compare "overall" average to confidence-weighted "overall" average}{confidence}

  - [Search references]
  
```{r confidence, echo=F, warning=F}
conf_tbl <- survey %>%
  drop_na(gr_overall, gr_confidence) %>%
  group_by(paper_id) %>%
  mutate(conf_weight = gr_overall * gr_confidence) %>%
  summarise(gr_overall_avg = mean(gr_overall), gr_conf_avg = mean(conf_weight))

cor.test(conf_tbl$gr_overall_avg, conf_tbl$gr_conf_avg) #strong correlation between overall and confidence weighted overall (obviously)
#now, make nice chart for it
library(reshape)
melted <- melt(as.data.frame(conf_tbl), id=c("paper_id"))
ggplot(melted, aes(x=variable, y=value, color=variable)) +
  geom_boxplot() + 
  stat_summary(fun.y=mean, geom="point", shape=23, size=4)  +
  xlab(NULL) + 
  ylab("Grade") +
  theme_minimal()

t.test(conf_tbl$gr_conf_avg, conf_tbl$gr_overall_avg)

#blurb about more confident if positive about data
#cavat that there is no rejected paper reviews to compare to
```

\question{Produce initial stats like these}{stats}

  - https://publons.com/community/gspr

\question{Explain negative correlation between prior_subs and gr_overall}{prior-overall}

  - [Search references]


\question{How to deal with publication bias?}{pub-bias}

  - https://www.nature.com/articles/s41562-016-0034
  - http://www.nature.com/news/let-s-think-about-cognitive-bias-1.18520
  - http://www.nature.com/news/how-scientists-fool-themselves-and-how-they-can-stop-1.18517
  - http://www.nature.com/news/tool-for-detecting-publication-bias-goes-under-spotlight-1.21728
  - http://www.nature.com/news/replication-studies-bad-copy-1.10634


\question{Predicting citations from reviews}{cites-reviews}

Hypothesis: high review scores and paper citations both measure some paper quality, and should therefore be correlated
Ignoring all other factors (some of which may be very useful, such as author metrics, conf metrics, paper topics, etc.)
Outcome variable: time to first citation, 1y, 2y citations.
Predictor variables: each review grade + award


----

