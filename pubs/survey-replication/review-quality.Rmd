# Quality Metrics in Peer Review

# Review Quality {#sec:review-quality}

\question{What is the correlation in review scores across categories?}{score-corr}


Ideally, peer review represents an expert unbiased judgement of a paper along multiple indepdent critiera [@hames08:peer Ch. 9]. The reality is a bit murkier. To assess the independence of reviews, we look next at the correlation of grades among review categories.

All conferences ask reviewers to rank papers on an overall acceptance decision, usually on a discrete scale with 4--6 values. Most conferences also request grading papers along several other dimensions, such as clarity of presentation, technical merit, potential impact, etc. Each conference uses its own categorical scale for these, so to compare between them, we have simplified the grades and normalized them to a range of [0:1], where the lowest choice in a category is assigned a grade of zero. This normalization introduces some grade distortion, because the original scale may not have been evenly spaced, it does allow us to approximate the numeric correlation between thousands of pairwise grades.

When each dimension of a paper is judged completely independetly and objectively from all others, we would expect zero corelation between categories. This is not a realistic expectation, since all categories inform the overall acceptance decision, and to some extent each other. But as the numbers in Fig. \@ref(fig:grade-correlations) show, all categories are actually positively correlated with each other, including the reviewer's confidence! (All p-values are near zero.)

Take for example the presentation and technical categories. A paper may be poorly written and describe a brilliant technical idea, and vice-versa. We have no a-priori reason to expect that an author's writing skills and technical skills are strongly correlated. Certainly, particularly bad writing can detract the reviewer to the point of completely missing the technical merits, but because we are only looking at accepted papers, the likelihood of having many such papers in our dataset is low. We would therefore expect weak correlation between the two categories, and the scatter plots certainly show examples of uncorrelated pairs. But the overall Pearson corerlation of 
`r round(cor(survey$gr_technical, survey$gr_present, use = "pair"), 2)`
is relatively high (second only to the correlation between technical merit and impact).

We believe these positive correlations are the result of reviewers forming an opinion of a paper that colors their judgement along all categories [@smith06:peer]. The different categories are not graded independently. The implication here is that bias across categories may be high enough to bring into question the usefulness of having multiple categories in the first place. A well-argued overall grade may provide less noisy signal to final acceptance decision.

```{r grade-correlations, echo=F, warning=F, cache=T, fig.cap = "Pearson correlations between grades in different review categories."}
survey %>%
  select(starts_with("gr_")) %>%
  rename(Overall = gr_overall, Technical = gr_technical, Presentation = gr_present, Impact = gr_impact, Originality = gr_originality, Relevance = gr_relevance, Confidence = gr_confidence) %>%
  GGally::ggpairs(lower = list(continuous = wrap("points", size = 0.4, position = position_jitter(height=0.1, width=0.1), alpha = 0.2)), progress = F)
```



\question{Do award-winning papers have an easier path through peer review?}{award-papers}

Many conferences confer an award to one or more papers, such as "Best Paper", "Best Student Paper", and "Best Artifact". Presumably, these papers received the award because their peer reviews agreed on its high quality, and ranked it accordingly. This presumption leads to the testable predictions that award-winning papers have shorter submission histories, higher review scores, fewer dissenting reviews, and higher reviewer confidence.

```{r award-setup, echo=F, warning=F}
#survey_with_award <- survey %>%
#  left_join(select(papers, c("key", "award")), by = c("paper_id" = "key"))

#award_means <- survey_with_award %>%
#  group_by(paper_id) %>%
#  summarise_all(funs(mean(., na.rm = T))) %>%
#  ungroup()
```

To test these hypotheses, we marked every award winning paper in our dataset, and interesected these with the papers in our survey responses.
We have responses from `#r nrow(filter(award_means, award == 1))` papers that won an award. Of these,
`#r pct(nrow(filter(award_means, award == 1, prior_subs == 0)), nrow(filter(award_means, award == 1, !is.nan(prior_subs))))`%
Were accepted on the first attempt, compared to
`#r pct(nrow(filter(award_means, award == 0, prior_subs == 0)), nrow(filter(award_means, award == 0, !is.nan(prior_subs))))`%
of the papers who did not win an award.
(`#r report_chi(chisq.test(table(award_means$award, award_means$prior_subs == 0)))`),
which is not as significant as we might expect. Looking at paper history from a different perspective, award-winning papers had only been rejected
`#r mean(filter(award_means, award == 1)$prior_subs, na.rm = T) %>% round(2)`
times on average prior to acceptance, compared to 
`#r mean(filter(award_means, award == 0)$prior_subs, na.rm = T) %>% round(2)`
with other papers, and all but one have been accepted by the third attempt. This is still not a very significant difference
(`#r report_t(t.test(table(award_means$award, award_means$prior_subs)))`).

One possible explanation to this metric's similarity is that awards are somewhat arbitrary or subjective, and the winning papers are not substantially different than the rest.^[See for example this lively debate: https://academia.stackexchange.com/questions/114072]
Another explanation may be that these papers are ineed of higher perceived quality than the rest, but to reach this point they too had to go through numerous iterations of improvement.^[See for example Eric Price's explanation on his award-winning paper in the NIPS experiment http://blog.mrtz.org/2014/12/15/the-nips-experiment.html].

The second explanation seems more plausible when we consider review scores. To see this, we aggregated all the review scores we received for each paper across all papers and all review categories (not all reviews included all categories). The review grade distributions is shown in Fig. \@ref(fig:review-grades-award) for both award-winning and regular papers.

```{r review-grades-award, echo=F, message=F, warning=F, out.width = '100%', fig.cap = "Review grades distributions by category and award. Grades are normalized to the range [0:1]. Mean and median grades are represented by diamonds and bars, respectively. N represents the number of scores received in each category."}
#cats = c("Overall", "Technical", "Presentation", "Impact", "Originality", "Relevance", "Confidence")
#survey_with_award %>%
#  select(c("award", starts_with("gr_"))) %>%
#  rename(Award = award, Overall = gr_overall, Technical = gr_technical, Presentation = gr_present, Impact = gr_impact, Originality = gr_originality, Relevance = gr_relevance, Confidence = gr_confidence) %>%
#  pivot_longer(cols = cats, names_to = "Category", values_to = "Grade") %>%
#  filter(!is.na(Grade)) %>%
#  group_by(Category, Award) %>%
#  add_count() %>%
#  ungroup() %>%
#  mutate(Category = factor(Category, levels = cats),
#         Award = recode(factor(Award), "FALSE" = "Regular", "TRUE" = "Award-winning")) %>%
#  ggplot(aes(x = Category, y = Grade, color = Category)) +
#         geom_text(aes(y = 1.0, label = paste0("N=",n)), vjust = -0.5, size = 3) +
#         geom_boxplot(notch = F) +
#         stat_summary(fun.y = mean, geom = "point", shape = 23, size = 4) +
#         theme(legend.position = "none", axis.text.x = element_text(angle = 45)) +
#         ylab("Normalized grade") +
#         facet_wrap(. ~ Award)
```


As expected, award-winning papers score higher across all categories, and only the sparse "Relevance" category showed a weak statistical significance of $p>0.014$ using T-test (even though most reviews of award winning papers had a perfect relevance score). This is not a coincidence, as many conferences select, or at least screen for award papers based on review scores.

On the other hand, the same figure fails to affirm the two remaining predictions. It shows wider range bands for most categories in the reviews of award-winning papers, partially explained by the smaller samples. It also shows nearly identical mean and median reviewer confidence grades, regardless of award. That said, the lowest grades received by award-winning papers are higher than those of regular papers for most categories. So perhaps the distinctive trait of award-winning papers, other than higher scores, is that they elicit few negative opinions, if any.


--------------------------


## On review quality

\question{What is the relationship between review length and conference reputation?}{length-reputation}

  - https://publons.com/static/Publons-Global-State-Of-Peer-Review-2018.pdf

\question{How is review length related to quality?}{review-length}

  - http://retractionwatch.com/2017/11/27/make-reviews-public-says-peer-review-expert/


\question{Do authors "like" "kind" reviews?}{kind-reviews}

  - https://nlpers.blogspot.co.uk/2015/06/some-naacl-2013-statistics-on-author.html
  - [@anderson08:towards]
  - [@papagiannaki07:author]

\question{Problems and arbitrariness in peer review}{arbitrary}

  - http://blog.mrtz.org/2014/12/15/the-nips-experiment.html
  - https://cacm.acm.org/blogs/blog-cacm/181996-the-nips-experiment/fulltext
  - [@vardi09:conferences]
  - [@wallach11:rebooting]
  - [@vines11:cointoss]
  - https://scholarlykitchen.sspnet.org/2012/07/31/the-referee-that-wasnt-there-the-ghostly-tale-of-reviewer-3-3/
  - https://dl.acm.org/citation.cfm?id=1435430https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1420798/
  - ch. 5 of "women science and technology"
  - https://www.sigarch.org/r2-the-future-of-isca-or-not/
  - https://www.sigarch.org/a-common-standard-to-fix-our-review-process-and-oh-i-was-wrong-about-one-thing/
  - https://petsymposium.org/experiment.php


\question{Effect of review days on number of submissions or author author satisfaction}{review-days}

Authors want 3m or less for reviews

  - [@editage18:perspectives]
  - [@wallach11:rebooting]

\question{Does review time materially impact quality?}{time-quality}

Does it really take about 5/6 hours per review?
Most reviewers are done in 4 weeks

  - https://senseaboutscience.org/wp-content/uploads/2016/12/Peer_Review_Survey.pdf
  - https://www.sigmetrics.org/sigmetrics2017/SIGMETRICSInfoforAuthors.html)
  - http://www.ieee-security.org/TC/Reports/2017/SP2017-PCChairReport.pdf
  - https://cacm.acm.org/magazines/2009/5/24644-program-committee-overload-in-systems/abstract
  - https://publons.com/static/Publons-Global-State-Of-Peer-Review-2018.pdf
  - https://senseaboutscience.org/wp-content/uploads/2016/12/Peer_Review_Survey.pdf
  - http://www.ieee-security.org/TC/Reports/2017/SP2017-PCChairReport.pdf
  - [@parno17:SPsurvey]



\question{Effect of reviewer load}{reviewer-load}

  - [@beverly13:findings]
  - https://cacm.acm.org/blogs/blog-cacm/165288-representative-reviewing/fulltext  --> affects name bias

\question{Should we have post-publication review/comments?}{post-pub}

  - https://scholarlykitchen.sspnet.org/2013/03/27/how-rigorous-is-the-post-publication-review-process-at-f1000-research/
  - http://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1001772
  - https://link.springer.com/article/10.1007/s11948-016-9854-2
  - https://www.sciencedirect.com/science/article/pii/S016895251500044X
  - https://search.proquest.com/docview/1697484361?pq-origsite=gscholar

\question{Is a revision cycle effective?}{revision-cycle}
How long should it be?

  - "Publish and Perish" Moshe Vardi CACM 01/2020
  - http://www.ieee-security.org/TC/Reports/2017/SP2017-PCChairReport.pdf

\question{Open peer review--no one is doing it?}{open-peer}

  - http://www.nature.com/news/watch-out-for-cheats-in-citation-game-1.20246
  - http://www.nature.com/news/let-s-make-peer-review-scientific-1.20194

\question{Can we improve reviews from one venue to the next?}{improve-reviews}

  - https://www.nature.com/articles/546352a
  - https://www.sigarch.org/a-proposal-to-coordinate-reviewing-across-computer-architecture-conferences/

\question{How does the average number of reviews compare to other fields?}{reviews-num}

  - https://scholarlykitchen.sspnet.org/2018/09/12/guest-post-what-a-new-publons-report-on-peer-review-says-about-diversity-and-more/?informz=1

\question{Are reviews unfair?}{reviews-unfair}

  - https://www.sigarch.org/r2-the-future-of-isca-or-not/

\question{Effect of over-positive PC members}{positive-pc}

  - https://www.sigarch.org/overwhelming-statistical-evidence-that-our-review-process-is-broken/
  - https://www.sigarch.org/false-accepts-and-false-rejects/

\question{Does rebuttal/author response help?}{rebuttal-help}

Probably useless

  - https://chairs-blog.acl2017.org/2017/03/27/author-response-does-it-help/
  - https://nlpers.blogspot.co.uk/2015/06/some-naacl-2013-statistics-on-author.html
  - http://www.ieee-security.org/TC/Reports/2017/SP2017-PCChairReport.pdf
  - http://sc17.supercomputing.org/program/papers-faq/
  - https://www.sigops.org/sosp/sosp17/call-for-papers.html
  - https://www.sigarch.org/how-pc-chairs-and-pc-members-can-help-improve-our-process/
  - Dave Evans, CCS: "I think the author responses are valuable, even if they don't change paper decisions since (1) they make reviewers take the early reviews more seriously and know they risk being embarrassed if they say something incorrect/lazy, (2) they give authors a sense that they had an opportunity to respond, and (3) they help PC chairs decide if a paper needs additional reviews. I think the responses did change the outcomes of some papers for CCS - one for sure, that was considered dropped after the first round, but revived by the responses and eventually accepted, and for the borderline papers, the responses were considered seriously in making decisions (but can't know for sure how things would have gone without them). In some cases, the authors responses made it easier to reject papers since they made it clear that the problems reviewers found were legitimate, and a handful of papers were withdraw by the authors after seeing the early reviews.  We had a "Discussion Committee" that assigned a PC member to oversee each paper to ensure the reviewers took the author responses into consideration, which helped a lot since for a conference of this scale it would be too much for the PC chairs to check on all papers."

\question{Do reviewers want anonymity?}{reviewer-anon}

  - https://senseaboutscience.org/wp-content/uploads/2016/12/Peer_Review_Survey.pdf

\question{Effect of reviewer award on their own submission's grade?}{reviewer-award}

  - [Search references]

\question{Effect of shepherding, where known}{shepherd}

  - [Search references]

\question{Properties of "real" papers, based on Ulfar's email from 10/4/17}{real-papers}

  1) Reviewers are less likely to agree on "real" papers.
  2) Reviewers are less likely to state high confidence on "real" papers, especially if they're saying reject/strong reject.
  - [Search references]

\question{How does the average number of reviews compare to other fields?}{num-reviews}

  - https://scholarlykitchen.sspnet.org/2018/09/12/guest-post-what-a-new-publons-report-on-peer-review-says-about-diversity-and-more/?informz=1


--------------------------

## On conference quality

\question{What matters more for a conference: scope, review length, or impact factor?}{conf-metric}

  - https://www.editage.com/files/Editage-Global-Author-Survey.pdf
  - https://peerj.com/articles/365/#p-17

\question{Correlate everything with conference metrics and policies (features)}{conf-corr}
  
First, we will look at the correlations between the key metrics of each conference. The one's chosen first are age, the mean historical citations, the h5 score (as measured by the h5 index score), the ratio between PC's and authors, the average authors per paper, the acceptance rate, the average PC's per paper, and the review load (number of reviews required per reviewer)
The correlegram below depicts these relations:
```{r conf-corr, echo=F, warning=F}
#first correlations between conference metrics
#all_confs %>%
#  dplyr::select(c("age", "mean_historical_citations", "h5_index", "pc_author_ratio", "mean_authors_per_paper", "acceptance_rate", "pc_paper_ratio", "mean_review_load")) %>%
#  rename(Age = age, Citations = mean_historical_citations, h5 = h5_index, PC_Author = pc_author_ratio, Authors_Paper  = mean_authors_per_paper, Acceptance = acceptance_rate, PC_Paper = pc_paper_ratio, Review_Load = mean_review_load) %>%
#  GGally::ggpairs(lower = list(continuous = wrap("points", size = 0.4, position = position_jitter(height=0.1, width=0.1), alpha = 0.2)), progress = F)
```
Looking at the above correlegram, we can see that almost all appear to be correlated in some way or another, the question is to what degree? We will divide the correlations into those that seem obvious at first, as they are built-in to the metric in the first place and those that require further investigation.

Interesting:
-age and mean_historical_citations - clearly this will happen as conferences with more age have been cited more
-age and acceptance rate - conferences that are older will reject more papers?
-mean_historical_citations and h5_index - could it be that conferences that are cited more are rated less as more people get into them? 
-mean_historical_citations and acceptance and review-load
-h5_index and acceptance_rate - conferences that are harder to get into are rated higher?
-acceptance and pc_paper_ratio - There are more PC's so the ratings are harder for each paper?
#expander 
Built-in:
-age and h5-index - conferences that are older are more reputable?
-age and mean_historical_citations - clearly this will happen as conferences with more age have been cited more
-pc_author and pc_paper ratios  - this correlation makes sense, if there ar emore pc's there will clearly be a higher pc per paper ratio
-pc_author and review_load - that correlation is also obvious, clearly if there are more pc's there will be less review load

I haven't included the following metrics for each conference as they are measures of counts not averages: past-papers, past_citations, chairs_num, pc_size, npapers, authors_num
These scores don't take into account the time the conference has been running and the number of papers. However, I haven't included mean_historical_length or h5_median, both of these could be important as well. 

Now onto the policies of the conferences. For this, we will first look at if it is double_blind, allows rebuttals and if it is making a diversity effort. In this case, as there are only two categories - true or false, we can do a paired t-test or independent t-test depending on the context. 

\question{Diversity of reviewers}{reviewer-diversity}

(first answer why diversity is needed). Also specifically when limited to HPC (look at whpc-summit paper)
  
  - https://scholarlykitchen.sspnet.org/2018/09/07/ask-the-chefs-diversity-in-peer-review/?informz=1
  - https://scholarlykitchen.sspnet.org/2018/09/13/eight-ways-to-tackle-diversity-and-inclusion-in-peer-review/?informz=1
  - https://www.nature.com/news/journals-invite-too-few-women-to-referee-1.21337

\question{Compare to other conferences}{conf-comp}

  - ATC'19  https://www.usenix.org/sites/default/files/atc19_message.pdf
  - Compare to SIGCOMM09  https://dl.acm.org/citation.cfm?id=1568623
  - What do authors in MICRO think about the review process?  https://www.sigmicro.org/2019/01/09/micro-survey-results/


\question{Quality is multi-dimensional}{quality-multidim}

https://scholarlykitchen.sspnet.org/2019/09/16/quality-is-multi-dimensional-how-many-ways-can-you-define-quality-in-peer-review/?unapproved=84111&moderation-hash=47b56366ddaf68dffef86ee501020c54#comment-84111

\question{Textual difference between 1st-version pre-print, last version, and published version}{ver-diff}

Not just similarity, but change in meaning

  - https://scholarlykitchen.sspnet.org/2018/03/15/a-comment-on-klein-et-als-comparing-articles-to-preprints/?informz=1

\question{Do authors aim for the highest possible venue that will still accept them?}{highest-venue}

Correlate conference prestige with no. of previous attempts.

  - https://scholarlykitchen.sspnet.org/2011/12/08/is-peer-review-a-coin-toss/

\question{Do two-cycle conferences lead to better papers? Is round-based reviewing better?}{two-cycle}

  - http://from-a-to-remzi.blogspot.com/2012/07/why-you-shouldnt-feel-too-badly-about.html

\question{Effect of submission Day-of-week and months-to-publish on submissions no. and acceptance rate}{day-of-week}

Correct for confounders, which you can find by plotting.

  - [Search references]

\question{Effect of "collisions" of multiple due dates next to each other, especially with similar paper topics}{collisions}

  - https://www.sigarch.org/r2-the-future-of-isca-or-not/

\question{Do conferences with strict data policies receive fewer submissions?}{data-policy}

  - https://scholarlykitchen.sspnet.org/2018/09/25/does-adopting-a-strict-data-sharing-policy-affect-submissions/?informz=1



\question{What is the relationship between prior subs and conference quality?}{prior-subs-quality}
```{r prior-subs-quality, echo=F, warning=F}
library(stringr)

#paper_confs <- survey %>%
#  drop_na(prior_subs) %>%
#  group_by(paper_id) %>%
#  summarise(prior_subs = mean(prior_subs), conference = str_sub(first(paper_id), end=-5)) %>% #get average previous submissions
#  left_join(select(all_confs, c("conference", "mean_historical_citations")), by=c("conference" = "conference")) %>%
#  drop_na(mean_historical_citations) 
#  # group_by(conference) %>% #take averages
#  # summarise(prior_avg = mean(prior_subs), h5 = first(h5_median))

#cor.test(paper_confs$mean_historical_citations, paper_confs$prior_subs, use="complete")

#try averages instead
#paper_confs <- paper_confs %>%
#  group_by(conference) %>%
#  summarise(prior_avg = mean(prior_subs), h5_median = first(h5_median))

#cor.test(paper_confs$h5_median, paper_confs$prior_avg, use="complete")

##produce h5_index, median and citations

```

Not correlated 

\question{Predicting citations from reviews}{cites-reviews}

Hypothesis: high review scores and paper citations both measure some paper quality, and should therefore be correlated
Ignoring all other factors (some of which may be very useful, such as author metrics, conf metrics, paper topics, etc.)
Outcome variable: time to first citation, 1y, 2y citations.
Predictor variables: each review grade + award

\question{Do more senior authors have fewer rejections?}{senior-reject}

This addresses some of the survivorship bias

(look for either first author, survey respondend rank, or max GS metrics in authors)

```{r student-senior-rej, echo=F, warning=F}
##join survey data with h-index of author
#hindex_factr_levels <- c("novice", "mid-career", "veteran")
##turn hindex into factor 
#survey_with_factor <- survey_with_hindex %>%
#  mutate(hindex_discr = case_when(
#    hindex > 18 ~ "veteran", 
#    hindex > 13 ~ "mid-career", 
#    hindex <= 12 ~ "novice", 
#    TRUE ~ "less then or equal to ten")) %>%
#  mutate(hindex_discr = factor(hindex_discr, levels = hindex_factr_levels))

#pct_ovr_18 <- pct( nrow(filter(survey_with_factor, hindex_discr =="veteran")),nrow(survey_with_factor))
#pct_under_12 <- pct( nrow(filter(survey_with_factor, hindex_discr == "novice")),nrow(survey_with_factor))
##figure out correlation between these two and come up with small narrative, write small narratives and clean this data up and push it
```

Senior authors have more experience in the review process and consequently may have an easier time in getting there papers accepted, without rejections. In order to measure seniority, we use the hindex scores of the authors and join this dataset with our survey responses. Among those that responded, we can test for correlation between hindex and there prior submissions of the paper. 

First to note, is that among our respondents, there was `r #pct_ovr_18` percent of respondents with an h-index over 18, while there was `r #pct_under_12` percent under 12 Therefore there is a much larger proption of authors with less experience in our survey responses. In order to measure the difference here we can do a correlation between hindex and the rejections for the authors. 

```{r student-senior-1, echo=F, warning=F}
#hindex_corr <- survey_with_hindex %>% #remove repeats
#  group_by(paper_id, name) %>%
#  mutate(hindex = hindex, prior_subs = first(prior_subs))
#cor(hindex_corr$hindex, hindex_corr$prior_subs, use = "pair")
```
Running this correlation, we can see that there is some correlation between the hindex and the rejection count, however it does not appear to be significant. However, if we do a chi-square test on experience against prior submissions, we get the following:
`r #report_chi(chisq.test(table(survey_with_factor$hindex_discr,survey_with_factor$prior_subs)))`
This indicates that may be some significant relation between these two variables, however further statistical tests are needed.

