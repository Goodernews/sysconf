document,SMOG,Flesch.Kincaid,confrence,num_words,abstract,abstract.SMOG,abstract.Flesch.Kincaid,abstract.num_words
ASPLOS_17_001.txt,16.863396642778785,15.458962435350696,ASPLOS,7702,"

Many emerging applications such as IoT, wearables, im-
plantables, and sensor networks are power- and energy-
constrained. These applications rely on ultra-low-power pro-
cessors that have rapidly become the most abundant type of
processor manufactured today. In the ultra-low-power em-
bedded systems used by these applications, peak power and
energy requirements are the primary factors that determine
critical system characteristics, such as size, weight, cost, and
lifetime. While the power and energy requirements of these
systems tend to be application-specific, conventional techniques for rating peak power and energy cannot accurately
bound the power and energy requirements of an application
running on a processor, leading to over-provisioning that in-
creases system size and weight. In this paper, we present an
automated technique that performs hardware-software co-
analysis of the application and ultra-low-power processor in
an embedded system to determine application-specific peak
power and energy requirements. Our technique provides
more accurate, tighter bounds than conventional techniques
for determining peak power and energy requirements, re-
porting 15% lower peak power and 17% lower peak energy,
on average, than a conventional approach based on profiling
and guardbanding. Compared to an aggressive stressmark-
based approach, our technique reports power and energy
bounds that are 26% and 26% lower, respectively, on av-
erage. Also, unlike conventional approaches, our technique
reports guaranteed bounds on peak power and energy in-
dependent of an application’s input set. Tighter bounds on
peak power and energy can be exploited to reduce system
size, weight, and cost.
",18.001758247042904,17.209019607843135,257
ASPLOS_17_002.txt,16.67345388539084,15.18088484138644,ASPLOS,8553,"

Guaranteeing Quality-of-Service (QoS) of latency-sensitive
applications while improving server utilization through application co-location is important yet challenging in modern datacenters. The key challenge is that when applications
are co-located on a server, performance interference due to
resource contention can be detrimental to the application
QoS. Although prior work has proposed techniques to identify “safe” co-locations where application QoS is satisfied
by predicting the performance interference on multicores, no
such prediction technique on accelerators such as GPUs.

In this work, we present Prophet, an approach to precisely
predict the performance degradation of latency-sensitive applications on accelerators due to application co-location.
We analyzed the performance interference on accelerators
through a real system investigation and found that unlike on
multicores where the key contentious resources are shared
caches and main memory bandwidth, the key contentious resources on accelerators are instead processing elements, accelerator memory bandwidth and PCIe bandwidth. Based on
this observation, we designed interference models that enable the precise prediction for processing element, accelerator memory bandwidth and PCIe bandwidth contention on
real hardware. By using a novel technique to forecast solo-run execution traces of the co-located applications using interference models, Prophet can accurately predict the performance degradation of latency-sensitive applications on



non-preemptive accelerators. Using Prophet, we can identify “safe” co-locations on accelerators to improve utilization without violating the QoS target. Our evaluation shows
that Prophet can predict the performance degradation with
an average prediction error 5.47% on real systems. Meanwhile, based on the prediction, Prophet achieves accelerator
utilization improvements of 49.9% on average while maintaining the QoS target of latency-sensitive applications.
",20.64404841556777,19.69436263736264,276
ASPLOS_17_003.txt,15.046147545017252,13.527279016360911,ASPLOS,8070,"

Recent work shows that dynamic memory allocation consumes nearly 7% of all cycles in Google datacenters. With
the trend towards increased specialization of hardware, we
propose Mallacc, an in-core hardware accelerator designed
for broad use across a number of high-performance, modern memory allocators. The design of Mallacc is quite different from traditional throughput-oriented hardware accelerators. Because memory allocation requests tend to be
very frequent, fast, and interspersed inside other application
code, accelerators must be optimized for latency rather than
throughput and area overheads must be kept to a bare minimum. Mallacc accelerates the three primary operations of a
typical memory allocation request: size class computation,
retrieval of a free memory block, and sampling of memory
usage. Our results show that malloc latency can be reduced
by up to 50% with a hardware cost of less than 1500 µm2 of
silicon area, less than 0.006% of a typical high-performance
processor core.
",17.693802365651003,16.48753246753247,156
ASPLOS_17_004.txt,15.764962155845346,14.211218745227619,ASPLOS,8705,"

Complex code bases with several layers of abstractions have
abundant inefficiencies that affect the execution time. Value
redundancy is a kind of inefficiency where the same values
are repeatedly computed, stored, or retrieved over the course
of execution. Not all redundancies can be easily detected
or eliminated with compiler optimization passes due to the
inherent limitations of the static analysis.

Microscopic observation of whole executions at instruction-

and operand-level granularity breaks down abstractions and
helps recognize redundancies that masquerade in complex
programs. We have developed REDSPY—a fine-grained
profiler to pinpoint and quantify redundant operations in
program executions. Value redundancy may happen over
time at same locations or in adjacent locations, and thus it
has temporal and spatial locality. REDSPY identifies both
temporal and spatial value locality. Furthermore, REDSPY
is capable of identifying values that are approximately the
same, enabling optimization opportunities in HPC codes that
often use floating point computations. REDSPY provides
intuitive optimization guidance by apportioning redundancies to their provenance—source lines and execution calling contexts. REDSPY pinpointed dramatically high volume
of redundancies in programs that were optimization targets
for decades, such as SPEC CPU2006 suite, Rodinia benchmark, and NWChem—a production computational chemistry code. Guided by REDSPY, we were able to eliminate
redundancies that resulted in significant speedups.
",17.54021597470382,16.725198463508324,214
ASPLOS_17_005.txt,14.614779932904572,12.428875002851296,ASPLOS,8031,"

We propose translation-enabled memory prefetching
optimizations or TEMPO, a low-overhead hardware mechanism to boost memory performance by exploiting the operating system’s (OS’) virtual memory subsystem. We are
the first to make the following observations: (1) a substantial fraction (20-40%) of DRAM references in modern big-data workloads are devoted to accessing page tables; and
(2) when memory references require page table lookups
in DRAM, the vast majority of them (98%+) also look up
DRAM for the subsequent data access. TEMPO exploits
these observations to enable DRAM row-buffer and on-
chip cache prefetching of the data that page tables point to.
TEMPO requires trivial changes to the memory controller
(under 3% additional area), no OS or application changes,
and improves performance by 10-30% and energy by 1-14%.
",20.503739492662863,18.848176691729325,134
ASPLOS_17_006.txt,14.901951003080807,12.484796939633526,ASPLOS,6560,"

Dynamic scripting languages are becoming more and more
widely adopted not only for fast prototyping but also for developing production-grade applications. They provide high-
productivity programming environments featuring high levels of abstraction with powerful built-in functions, automatic memory management, object-oriented programming
paradigm and dynamic typing. However, their flexible, dynamic type systems easily become the source of inefficiency
in terms of instruction count, memory footprint, and energy consumption. This overhead makes it challenging to deploy these high-productivity programming technologies on
emerging single-board computers for IoT applications.
Addressing this challenge, this paper introduces Typed
Architectures, a high-efficiency, low-cost execution substrate for dynamic scripting languages, where each data
variable retains high-level type information at an ISA level.
Typed Architectures calculate and check the dynamic type
of each variable implicitly in hardware, rather than explicitly
in software, hence significantly reducing instruction count
for dynamic type checking. Besides, Typed Architectures
introduce polymorphic instructions (e.g., xadd), which are
bound to the correct native instruction at runtime within
the pipeline (e.g., add or fadd) to efficiently implement
polymorphic operators. Finally, Typed Architectures provide
hardware support for flexible yet efficient type tag extraction
and insertion, capturing common data layout patterns of tag-
value pairs. Our evaluation using a fully synthesizable RISC-
V RTL design on FPGA shows that Typed Architectures
achieve geomean speedups of 11.2% and 9.9% with maximum speedups of 32.6% and 43.5% for two production-
grade scripting engines for JavaScript and Lua, respectively.
Moreover, Typed Architectures improve the energy-delay
product (EDP) by 19.3% for JavaScript and 16.5% for Lua
with an area overhead of 1.6% at a 40nm technology node.
",20.075844112070616,18.570553113553114,283
ASPLOS_17_007.txt,14.857430866204353,13.19391585760518,ASPLOS,8947,"
The slotted-page structure is a database page format
commonly used for managing variable-length records. In
this work, we develop a novel “failure-atomic slottedPage structure” for persistent memory that leverages byteaddressability and durability of persistent memory to minimize redundant write operations used to maintain consistency in traditional database systems. Failure-atomic slottedpaging consists of two key elements: (i) in-place commit
per page using hardware transactional memory and (ii) slotheader logging that logs the commit mark of each page. The
proposed scheme is implemented in SQLite and compared
against NVWAL, the current state-of-the-art scheme. Our
performance study shows that our failure-atomic slottedpaging shows optimal performance for database transactions
that insert a single record. For transactions that touch more
than one database page, our proposed slot-header logging
scheme minimizes the logging overhead by avoiding duplicating pages and logging only the metadata of the dirty
pages. Overall, we find that our failure-atomic slotted-page
management scheme reduces database logging overhead to
1/6 and improves query response time by up to 33% compared to NVWAL.

Categories and Subject Descriptors H.2.7 [Database
Management):Logging and recovery

Keywords Non-volatile memory; Database buffer caching;
Slotted page structure; Database recovery

",18.377959752453624,16.971666666666668,206
ASPLOS_17_008.txt,15.702998854712352,13.784602976700697,ASPLOS,7454,"
Transactional memory (TM) has been the focus of numerous
studies, and it is supported in processors such as the IBM
Blue Gene/Q and Intel Haswell. Many studies have used the
STAMP benchmark suite to evaluate their designs. However,
the speedups obtained for the STAMP benchmarks on all
TM systems we know of are quite limited; for example, with
64 threads on the IBM Blue Gene/Q, we observe a median
speedup of 1.4X using the Blue Gene/Q hardware transactional memory (HTM), and a median speedup of 4.1X using
a software transactional memory (STM).

What limits the performance of these benchmarks on
TMs? In this paper, we argue that the problem lies with the
programming model and data structures used to write them.
To make this point, we articulate two principles that we
believe must be embodied in any scalable program and argue
that STAMP programs violate both of them. By modifying
the STAMP programs to satisfy both principles, we produce
a new set of programs that we call the Stampede suite. Its
median speedup on the Blue Gene/Q is 8.0X when using an
STM. The two principles also permit us to simplify the TM
design. Using this new STM with the Stampede benchmarks,
we obtain a median speedup of 17.7X with 64 threads on
the Blue Gene/Q and 13.2X with 32 threads on an Intel
Westmere system.

These results suggest that HTM and STM designs will
benefit if more attention is paid to the division of labor
between application programs, systems software, and hardware.

Keywords transactions; transactional memory; programming models; scalability; Stampede benchmarks



",13.6886701853816,11.649297752808987,273
ASPLOS_17_009.txt,16.123142277362437,14.990963051887427,ASPLOS,8789,"
Memory consistency models (MCMs) which govern intermodule interactions in a shared memory system, are a significant, yet often under-appreciated, aspect of system design.
MCMs are defined at the various layers of the hardware-
software stack, requiring thoroughly verified specifications,
compilers, and implementations at the interfaces between
layers. Current verification techniques evaluate segments of
the system stack in isolation, such as proving compiler mappings from a high-level language (HLL) to an ISA or proving
validity of a microarchitectural implementation of an ISA.

This paper makes a case for full-stack MCM verification
and provides a toolflow, TriCheck, capable of verifying that
the HLL, compiler, ISA, and implementation collectively uphold MCM requirements. The work showcases TriCheck’s
ability to evaluate a proposed ISA MCM in order to ensure
that each layer and each mapping is correct and complete.
Specifically, we apply TriCheck to the open source RISCV ISA [55], seeking to verify accurate, efficient, and legal
compilations from C11. We uncover under-specifications and
potential inefficiencies in the current RISC-V ISA documentation and identify possible solutions for each. As an example,
we find that a RISC-V-compliant microarchitecture allows
144 outcomes forbidden by C11 to be observed out of 1,701
litmus tests examined. Overall, this paper demonstrates the
necessity of full-stack verification for detecting MCM-related
bugs in the hardware-software stack.

Keywords
computer architecture, heterogeneous paral-
lelism, memory consistency, shared memory, verification,
compilation, C11, RISC-V
",17.58133193835471,17.380372384937242,242
ASPLOS_17_010.txt,14.79667108994365,13.021741421964077,ASPLOS,8268,"
Emerging non-volatile memory (NVM) technologies promise
durability with read and write latencies comparable to
volatile memory (DRAM). We define Persistent Memory
(PM) as NVM accessed with byte addressability at low latency via normal memory instructions. Persistent-memory
applications ensure the consistency of persistent data by
inserting ordering points between writes to PM allowing
the construction of higher-level transaction mechanisms. An
epoch is a set of writes to PM between ordering points.

To put systems research in PM on a firmer footing, we developed and analyzed a PM benchmark suite called WHISPER that comprises ten PM applications we gathered to
cover all current interfaces to PM. A quantitative analysis
reveals several insights: (a) only 4% of writes in PM-aware
applications are to PM and the rest are to volatile memory,
(b) software transactions are often implemented with 5 to 50
ordering points (c) 75% of epochs update exactly one 64B
cache line, (d) 80% of epochs from the same thread depend
on previous epochs from the same thread, while few epochs
depend on epochs from other threads.

Based on our analysis, we propose the Hands-off Persistence System (HOPS) to track updates to PM in hardware.
Current hardware design requires applications to force data
to PM as each epoch ends. HOPS provides high-level ISA
primitives for applications to express durability and ordering constraints separately and enforces them automatically,
while achieving 24.3% better performance over current approaches to persistence.

CCS Concepts


Information systems Storage class memory

Keywords Persistent memory (PM); Non-volatile memory
(NVM); Storage-class memory; Caches; Benchmark



",17.80541091248751,16.78746743295019,263
ASPLOS_17_011.txt,16.500499454590873,14.571731596179017,ASPLOS,7040,"
This paper presents PRORACE, a dynamic data race detector
practical for production runs. It is lightweight, but still offers
high race detection capability. To track memory accesses,
PRORACE leverages instruction sampling using the performance monitoring unit (PMU) in commodity processors.
Our PMU driver enables PRORACE to sample more memory accesses at a lower cost compared to the state-of-the-art
Linux driver. Moreover, PRORACE uses PMU-provided execution contexts including register states and program path,
and reconstructs unsampled memory accesses offline. This
technique allows PRORACE to overcome inherent limitations of sampling and improve the detection coverage by performing data race detection on the trace with not only sampled but also reconstructed memory accesses. Experiments
using racy production software including apache and mysql
shows that, with a reasonable offline cost, PRORACE incurs
only 2.6% overhead at runtime with 27.5% detection probability with a sampling period of 10,000.

",18.699421769314853,16.440816326530612,151
ASPLOS_17_012.txt,15.931614510713498,14.301498746627626,ASPLOS,9114,"
Specialized hardware accelerators have performance and
energy-efficiency advantages over general-purpose processors. To fully realize these benefits and aid programmability, accelerators may share a physical and virtual address
space and full cache coherence with the host system. However, allowing accelerators — particularly those designed by
third parties — to directly communicate with host coherence
protocols poses several problems. Host coherence protocols
are complex, vary between companies, and may be proprietary, increasing burden on accelerator designers. Bugs in
the accelerator implementation may cause crashes and other
serious consequences to the host system.

We propose Crossing Guard, a coherence interface between the host coherence system and accelerators. The
Crossing Guard interface provides the accelerator designer
with a standardized set of coherence messages that are simple enough to aid in design of bug-free coherent caches. At
the same time, they are sufficiently complex to allow customized and optimized accelerator caches with performance
comparable to using the host protocol. The Crossing Guard
hardware is implemented as part of the trusted host, and provides complete safety to the host coherence system, even in
the presence of a pathologically buggy accelerator cache.

CCS Concepts ¢eHardware — Hardware accelerators;
Robustness; eComputer systems organization — Heterogeneous (hybrid) systems

Keywords accelerators, cache coherence, coherence interfaces



",18.026119701940384,17.4440243902439,205
ASPLOS_17_013.txt,17.42787496031928,16.411927346954105,ASPLOS,7791,"
Building a trustworthy life-critical embedded system requires deep reasoning about the potential effects that sequences of machine instructions can have on full system operation. Rather than trying to analyze complete binaries and
the countless ways their instructions can interact with one
another — memory, side effects, control registers, implicit
state, etc. — we explore a new approach. We propose an architecture controlled by a thin computational layer designed
to tightly correspond with the lambda calculus, drawing on
principles of functional programming to bring the assembly much closer to myriad reasoning frameworks, such as
the Coq proof assistant. This approach allows assemblylevel verified versions of critical code to operate safely in
tandem with arbitrary code, including imperative and unverified system components, without the need for large supporting trusted computing bases. We demonstrate that this
computational layer can be built in such a way as to simultaneously provide full programmability and compact,
precise, and complete semantics, while still using hardware resources comparable to normal embedded systems.
To demonstrate the practicality of this approach, our FPGAimplemented prototype runs an embedded medical application which monitors and treats life-threatening arrhythmias.
Though the system integrates untrusted and imperative components, our architecture allows for the formal verification
of multiple properties of the end-to-end system, including a
proof of correctness of the assembly-level implementation



of the core algorithm, the integrity of trusted data via a noninterference proof, and a guarantee that our prototype meets
critical timing requirements.

",22.076135915942103,21.444096812278634,243
ASPLOS_17_014.txt,15.12602000501552,13.958231403202323,ASPLOS,7918,"
Asynchronous programming model is commonly used in
mobile systems and Web 2.0 environments. Asynchronous
race detectors use algorithms that are an order of magnitude
performance and space inefficient compared to conventional
data race detectors. We solve this problem by identifying
and addressing two important problems in reasoning about
causality between asynchronous events.

Unlike conventional signal-wait operations, establishing
causal order between two asynchronous events is fundamentally more challenging as there is no common handle they
operate on. We propose a new primitive named ASYNCCLOCK that addresses this problem by explicitly tracking
causally preceding events, and show that ASYNCCLOCK can
handle a wide variety of asynchronous causality models. We
also address the important scalability problem of efficiently
identifying heirless events whose metadata can be reclaimed.

We built the first single-pass, non-graph-based Android
race detector using our algorithm and applied it to find errors
in 20 popular applications. Our tool incurs about 6x performance overhead, which is several times more efficient than
the state-of-the-art solution. It also scales well with the execution length. We used our tool to find 147 previously unknown harmful races.

CCS Concepts «Software and its engineering — Software testing and debugging; e Theory of computation
— Program analysis

Keywords Data Races; Causality; Happens-before; EventDriven; Asynchronous; Android
",16.35954948731386,15.451020059752455,215
ASPLOS_17_015.txt,13.651687916286654,11.643221342378606,ASPLOS,8472,"
High-performance servers are non-uniform memory access
(NUMA) machines. To fully leverage these machines, programmers need efficient concurrent data structures that are
aware of the NUMA performance artifacts. We propose Node
Replication (NR), a black-box approach to obtaining such
data structures. NR takes an arbitrary sequential data structure and automatically transforms it into a NUMA-aware concurrent data structure satisfying linearizability. Using NR requires no expertise in concurrent data structure design, and
the result is free of concurrency bugs. NR draws ideas from
two disciplines: shared-memory algorithms and distributed
systems. Briefly, NR implements a NUMA-aware shared log,
and then uses the log to replicate data structures consistently
across NUMA nodes. NR is best suited for contended data
structures, where it can outperform lock-free algorithms by
3.1x, and lock-based solutions by 30x. To show the benefits
of NR to a real application, we apply NR to the data structures of Redis, an in-memory storage system. The result outperforms other methods by up to 14x. The cost of NR is additional memory for its log and replicas.

",14.02287970970757,11.998991008991009,184
ASPLOS_17_016.txt,15.503387374645175,13.697594847212208,ASPLOS,8491,"
Existing distributed asynchronous graph processing systems
employ checkpointing to capture globally consistent snapshots and rollback all machines to most recent checkpoint to
recover from machine failures. In this paper we argue that
recovery in distributed asynchronous graph processing does
not require the entire execution state to be rolled back to
a globally consistent state due to the relaxed asynchronous
execution semantics. We define the properties required in the
recovered state for it to be usable for correct asynchronous
processing and develop CoRAL, a lightweight checkpointing and recovery algorithm. First, this algorithm carries out
confined recovery that only rolls back graph execution states
of the failed machines to affect recovery. Second, it relies
upon lightweight checkpoints that capture locally consistent
snapshots with a reduced peak network bandwidth requirement. Our experiments using real-world graphs show that
our technique recovers from failures and finishes processing
1.5x to 3.2x faster compared to the traditional asynchronous
checkpointing and recovery mechanism when failures impact
1 to 6 machines of a 16 machine cluster. Moreover, capturing
locally consistent snapshots significantly reduces intermittent
high peak bandwidth usage required to save the snapshots
— the average reduction in 99th percentile bandwidth ranges
from 22% to 51% while 1 to 6 snapshot replicas are being
maintained.

",19.5731925562951,18.398793342579754,209
ASPLOS_17_017.txt,15.438029552798298,13.897357143911552,ASPLOS,8727,"
Continuous processing of a streaming graph maintains an
approximate result of the iterative computation on a recent
version of the graph. Upon a user query, the accurate result
on the current graph can be quickly computed by feeding the
approximate results to the iterative computation — a form of
incremental computation that corrects the (small amount of)
error in the approximate result. Despite the effectiveness of
this approach in processing growing graphs, it is generally
not applicable when edge deletions are present — existing approximations can lead to either incorrect results (e.g., monotonic computations terminate at an incorrect minima/maxima)
or poor performance (e.g., with approximations, convergence
takes longer than performing the computation from scratch).
This paper presents KickStarter, a runtime technique that
can trim the approximate values for a subset of vertices
impacted by the deleted edges. The trimmed approximation is
both safe and profitable, enabling the computation to produce
correct results and converge quickly. KickStarter works for
a class of monotonic graph algorithms and can be readily
incorporated in any existing streaming graph system. Our
experiments with four streaming algorithms on five large
graphs demonstrate that trimming not only produces correct
results but also accelerates these algorithms by 8.5-23.7x.

Categories and Subject Descriptors H.2.4 [Database Management Systems]: Parallel databases; H.3.4 [Information
Storage and Retrieval]: Systems and Software

General Terms Language, Measurements, Performance

Keywords graph processing, value dependence, streaming
graphs



",19.661554785965695,18.96396551724138,239
ASPLOS_17_018.txt,15.925321434696272,15.103582877511894,ASPLOS,8578,"
Applications written to run on conventional operating systems
typically depend on OS abstractions like processes, pipes,
signals, sockets, and a shared file system. Porting these
applications to the web currently requires extensive rewriting
or hosting significant portions of code server-side because
browsers present a nontraditional runtime environment that
lacks OS functionality.

This paper presents BROWSIX, a framework that bridges
the considerable gap between conventional operating systems and the browser, enabling unmodified programs expecting a Unix-like environment to run directly in the browser.
BROWSIX comprises two core parts: (1) a JavaScript-only
system that makes core Unix features (including pipes, concurrent processes, signals, sockets, and a shared file system)
available to web applications; and (2) extended JavaScript runtimes for C, C++, Go, and Node.js that support running programs written in these languages as processes in the browser.
BROWSIX supports running a POSIX shell, making it straightforward to connect applications together via pipes.

We illustrate BROWSIX’s capabilities via case studies that
demonstrate how it eases porting legacy applications to the
browser and enables new functionality. We demonstrate a
Brows Ix-enabled I4TpX editor that operates by executing
unmodified versions of pdfLaTeX and BibTeX. This browseronly MTpX editor can render documents in seconds, making
it fast enough to be practical. We further demonstrate how
BROWSIX lets us port a client-server application to run
entirely in the browser for disconnected operation. Creating
these applications required less than 50 lines of glue code and
no code modifications, demonstrating how easily BROWSIX
can be used to build sophisticated web applications from
existing parts without modification.



CCS Concepts «Information systems — Browsers;
¢ Software and its engineering — Operating systems

Keywords Browsix, browser, operating system, JavaScript
",18.9813275721742,18.63140425531915,282
ASPLOS_17_019.txt,15.421292144905347,13.945549647823967,ASPLOS,6945,"
Convolutional Neural Networks (CNN) are a class of Artificial Neural Networks (ANN) that are highly efficient at
the pattern recognition tasks that underlie difficult AI problems in a variety of domains, such as speech recognition,
object recognition, and natural language processing. CNNs
are, however, computationally intensive to train.

This paper presents the first characterization of the performance optimization opportunities for training CNNs on
CPUs. Our characterization includes insights based on the
structure of the network itself (i.e., intrinsic arithmetic intensity of the convolution and its scalability under parallelism)
as well as dynamic properties of its execution (i.e., sparsity
of the computation).

Given this characterization, we present an automatic
framework called spg-CNN for optimizing CNN training on
CPUs. It comprises of a computation scheduler for efficient
parallel execution, and two code generators: one that optimizes for sparsity, and the other that optimizes for spatial
reuse in convolutions.

We evaluate spg-CNN using convolutions from a variety
of real world benchmarks, and show that spg-CNN can train
CNNs faster than state-of-the-art approaches by an order of
magnitude.

",18.99602597827317,18.300793650793654,183
ASPLOS_17_020.txt,16.202141388371004,14.667892033079081,ASPLOS,8971,"
There has been a significant amount of effort invested in
designing scheduling transformations such as loop tiling
and loop fusion that rearrange the execution of dynamic
instances of loop nests to place operations that access the
same data close together temporally. In recent years, there
has been interest in designing similar transformations that
operate on recursive programs, but until now these transformations have only considered simple scenarios: multiple
recursions to be fused, or a recursion nested inside a simple
loop. This paper develops the first set of scheduling transformations for nested recursions: recursive methods that call
other recursive methods. These are the recursive analog to
nested loops. We present a transformation called recursion
twisting that automatically improves locality at all levels of
the memory hierarchy, and show that this transformation can
yield substantial performance improvements across several
benchmarks that exhibit nested recursion.

Categories and Subject Descriptors D.3.4 [Processors]:
Code generation, Compilers, Optimization

Keywords Nested Recursion; Locality Optimization
",20.267338824336647,18.429367088607595,160
ASPLOS_17_021.txt,16.083856869542608,14.451186355441973,ASPLOS,7997,"
Cache is designed to exploit locality; however, the role of onchip L1 data caches on modern GPUs is often awkward. The
locality among global memory requests from different SMs
(Streaming Multiprocessors) is predominantly harvested by
the commonly-shared L2 with long access latency; while the
in-core locality, which is crucial for performance delivery, is
handled explicitly by user-controlled scratchpad memory. In
this work, we disclose another type of data locality that has
been long ignored but with performance boosting potential
— the inter-CTA locality. Exploiting such locality is rather
challenging due to unclear hardware feasibility, unknown
and inaccessible underlying CTA scheduler, and small incore cache capacity. To address these issues, we first conduct
a thorough empirical exploration on various modern GPUs
and demonstrate that inter-CTA locality can be harvested,
both spatially and temporally, on L1 or L1/Tex unified cache.
Through further quantification process, we prove the significance and commonality of such locality among GPU applications, and discuss whether such reuse is exploitable. By
leveraging these insights, we propose the concept of CTAClustering and its associated software-based techniques to
reshape the default CTA scheduling in order to group the
CTAs with potential reuse together on the same SM. Our
techniques require no hardware modification and can be directly deployed on existing GPUs. In addition, we incorporate these techniques into an integrated framework for automatic inter-CTA locality optimization. We evaluate our techniques using a wide range of popular GPU applications on
all modern generations of NVIDIA GPU architectures. The

results show that our proposed techniques significantly improve cache performance through reducing L2 cache transactions by 55%, 65%, 29%, 28% on average for Fermi, Kepler, Maxwell and Pascal, respectively, leading to an average of 1.46x, 1.48x, 1.45x, 1.41x (up to 3.8x, 3.6x, 3.1x,
3.3x) performance speedups for applications with algorithmrelated inter-CTA reuse.

CCS Concepts «Computer systems organization —
Single instruction, multiple data; ¢ Software and its engineering — Runtime environments

Keywords GPU, CTA, cache locality, performance optimization, runtime tool
",18.599290044081553,18.197107250755284,339
ASPLOS_17_022.txt,14.344582303996482,11.97618093203598,ASPLOS,8650,"
Software fault isolation (SFI) is an important technique for
the construction of secure operating systems, web browsers,
and other extensible software. We demonstrate that superoptimization can dramatically improve the performance of
Google Native Client, a SFI system that ships inside the
Google Chrome Browser. Key to our results are new techniques for superoptimization of loops: we propose a new architecture for superoptimization tools that incorporates both
a fully sound verification technique to ensure correctness and
a bounded verification technique to guide the search to optimized code. In our evaluation we optimize 13 libe string
functions, formally verify the correctness of the optimizations and report a median and average speedup of 25% over
the libraries shipped by Google.

",17.693802365651003,18.7115170940171,118
ASPLOS_17_023.txt,14.951256379313953,12.635222801996573,ASPLOS,8749,"
Emerging non-volatile memory (NVM) offers non-volatility,
byte-addressability and fast access at the same time. To make
the best use of these properties, it has been shown by empirical evidence that programs should access NVM directly
through CPU load and store instructions, so that the overhead of a traditional file system or database can be avoided.
Thus, durable transactions become a common choice of applications for accessing persistent memory data in a crash
consistent manner. However, existing durable transaction
systems employ either undo logging, which requires a fence
for every memory write, or redo logging, which requires
intercepting all memory reads within transactions.

This paper presents DUDETM, a crash-consistent durable
transaction system that avoids the drawbacks of both undo
logging and redo logging. DUDETM uses shadow DRAM
to decouple the execution of a durable transaction into three
fully asynchronous steps. The advantage is that only minimal fences and no memory read instrumentation are required. This design also enables an out-of-the-box transactional memory (TM) to be used as an independent component in our system. The evaluation results show that
DUDETM adds durability to aTM system with only 7.4% ~
24.6% throughput degradation. Compared to the existing
durable transaction systems, DUDETM provides 1.7x to
4.4x higher throughput. Moreover, DUDETM can be implemented with existing hardware TMs with minor hardware
modifications, leading to a further 1.7x speedup.



",16.691746362846608,14.779328063241106,235
ASPLOS_17_024.txt,14.746455172624128,13.18917382832046,ASPLOS,8772,"
Remote access to NVMe Flash enables flexible scaling and
high utilization of Flash capacity and IOPS within a datacenter. However, existing systems for remote Flash access either
introduce significant performance overheads or fail to isolate
the multiple remote clients sharing each Flash device. We
present ReFlex, a software-based system for remote Flash
access, that provides nearly identical performance to accessing local Flash. ReFlex uses a dataplane kernel to closely
integrate networking and storage processing to achieve low
latency and high throughput at low resource requirements.
Specifically, ReFlex can serve up to 850K IOPS per core
over TCP/IP networking, while adding 21 us over direct access to local Flash. ReFlex uses a QoS scheduler that can
enforce tail latency and throughput service-level objectives
(SLOs) for thousands of remote clients. We show that ReFlex allows applications to use remote Flash while maintaining their original performance with local Flash.

Categories and Subject Descriptors D.4.2 [Operating Systems]: Storage Management

Keywords Flash; I/O scheduling; network storage; QoS

",15.247664890283005,14.243465568862277,169
ASPLOS_17_025.txt,17.012228118437186,15.555024271844662,ASPLOS,8740,"
The popularization of video capture devices has created
strong storage demand for encoded videos. Approximate
storage can ease this demand by enabling denser storage at
the expense of occasional errors. Unfortunately, even minor storage errors, such as bit flips, can result in major visual damage in encoded videos. Similarly, video encryption,
widely employed for privacy and digital rights management,
may create long dependencies between bits that show little
or no tolerance to storage errors.

In this paper we propose VideoApp, a novel and efficient methodology to compute bit-level reliability requirements for encoded videos by tracking visual and metadata
dependencies within encoded bitstreams. We further show
how VideoApp can be used to trade video quality for storage
density in an optimal way. We integrate our methodology
into a popular H.264 encoder to partition an encoded video
stream into multiple streams that can receive different levels of error correction according to their reliability needs.
When applied to a dense and highly error-prone multi-level
cell storage substrate, our variable error correction mechanism reduces the error correction overhead by half under
the most error-intolerant encoder settings, achieving quality/density points that neither compression nor approximation can achieve alone. Finally, we define the basic invariants needed to support encrypted approximate video storage.
We present an analysis of block cipher modes of operation,
showing that some are fully compatible with approximation,
enabling approximate and secure video storage systems.

CCS Concepts «Computer systems organization — Reliability; eHardware — Memory and dense storage;
eSecurity and privacy — Security requirements; Digital
rights management; e Computing methodologies > Jmage compression



",19.1951169018618,18.69527411519778,262
ASPLOS_17_026.txt,14.332403373139023,12.894280673641848,ASPLOS,8549,"
With Solid State Disks (SSDs) offering high degrees of parallelism, SSD controllers place data and direct requests to
exploit the maximum offered hardware parallelism. In the
quest to maximize parallelism and utilization, sub-requests
of a request that are directed to different flash chips by the
scheduler can experience differential wait times since their
individual queues are not coordinated and load balanced at
all times. Since the macro request is considered complete
only when its last sub-request completes, some of its subrequests that complete earlier have to necessarily wait for
this last sub-request. This paper opens the door to anew class
of schedulers to leverage such slack between sub-requests
in order to improve response times. Specifically, the paper
presents the design and implementation of a slack-enabled
re-ordering scheduler, called Slacker, for sub-requests issued to each flash chip. Layered under a modern SSD request scheduler, Slacker estimates the slack of each incoming sub-request to a flash chip and allows them to jump
ahead of existing sub-requests with sufficient slack so as
to not detrimentally impact their response times. Slacker is
simple to implement and imposes only marginal additions
to the hardware. Using a spectrum of 21 workloads with diverse read-write characteristics, we show that Slacker provides as much as 19.5%, 13% and 14.5% improvement in
response times, with average improvements of 12%, 6.5%
and 8.5%, for write-intensive, read-intensive and read-write
balanced workloads, respectively.

CCS Concepts eHardware — External storage; Nonvolatile memory

Keywords SSD, Scheduling, Intra-Request Slack



",16.45884130781739,15.886744487678342,262
ASPLOS_17_027.txt,14.813077771397033,13.26136947400737,ASPLOS,9729,"
There is more than a decade-long history of using static analysis to find bugs in systems such as Linux. Most of the existing
static analyses developed for these systems are simple checkers that find bugs based on pattern matching. Despite the
presence of many sophisticated interprocedural analyses, few
of them have been employed to improve checkers for systems code due to their complex implementations and poor
scalability.

In this paper, we revisit the scalability problem of interprocedural static analysis from a “Big Data” perspective. That
is, we turn sophisticated code analysis into Big Data analytics and leverage novel data processing techniques to solve
this traditional programming language problem. We develop
Graspan, a disk-based parallel graph system that uses an
edge-pair centric computation model to compute dynamic
transitive closures on very large program graphs.

We implement context-sensitive pointer/alias and dataflow
analyses on Graspan. An evaluation of these analyses on
large codebases such as Linux shows that their Graspan
implementations scale to millions of lines of code and are
much simpler than their original implementations. Moreover,
we show that these analyses can be used to augment the
existing checkers; these augmented checkers uncovered 132
new NULL pointer bugs and 1308 unnecessary NULL tests in
Linux 4.4.0-re5, PostgreSQL 8.3.9, and Apache httpd 2.2.18.

Categories and Subject Descriptors ""3.2 [Logics and
Meaning of Programs]: Semantics of Programming
Languages—program analysis; H.3.4 [information Storage
and Retrieval]: Systems and Software

General Terms Language, Measurements, Performance



Keywords Static analysis, graph processing, disk-based
systems
",17.00531248756302,16.47584251968504,263
ASPLOS_17_028.txt,15.300240255967587,13.70797640352312,ASPLOS,7412,"
With the recent advance of wearable devices and Internet of
Things (IoTs), it becomes attractive to implement the Deep
Convolutional Neural Networks (DCNNs) in embedded and
portable systems. Currently, executing the software-based
DCNNs requires high-performance servers, restricting the
widespread deployment on embedded and mobile IoT devices. To overcome this obstacle, considerable research efforts have been made to develop highly-parallel and specialized DCNN accelerators using GPGPUs, FPGAs or ASICs.
Stochastic Computing (SC), which uses a bit-stream to
represent a number within [-1, 1] by counting the number
of ones in the bit-stream, has high potential for implementing DCNNs with high scalability and ultra-low hardware
footprint. Since multiplications and additions can be calculated using AND gates and multiplexers in SC, significant
reductions in power (energy) and hardware footprint can be
achieved compared to the conventional binary arithmetic implementations. The tremendous savings in power (energy)
and hardware resources allow immense design space for enhancing scalability and robustness for hardware DCNNs.
This paper presents SC-DCNN, the first comprehensive
design and optimization framework of SC-based DCNNs,
using a bottom-up approach. We first present the designs of
function blocks that perform the basic operations in DCNN,
including inner product, pooling, and activation function.
Then we propose four designs of feature extraction blocks,
which are in charge of extracting features from input feature maps, by connecting different basic function blocks
with joint optimization. Moreover, the efficient weight storage methods are proposed to reduce the area and power
(energy) consumption. Putting all together, with feature extraction blocks carefully selected, SC-DCNN is holistically
optimized to minimize area and power (energy) consumption while maintaining high network accuracy. Experimental results demonstrate that the LeNet5 implemented in SCDCNN consumes only 17 mm? area and 1.53 W power,
achieves throughput of 781250 images/s, area efficiency of
45946 images/s/mm?, and energy efficiency of 510734 images/J.

",17.73682964510965,16.261269841269847,317
ASPLOS_17_029.txt,15.459667526655526,13.75313480771683,ASPLOS,6448,"
As the next-generation manufacturing driven force, 3D
printing technology is having a transformative effect on various industrial domains and has been widely applied in a
broad spectrum of applications. It also progresses towards
other versatile fields with portable battery-powered 3D printers working on a limited energy budget. While reducing
manufacturing energy is an essential challenge in industrial
sustainability and national economics, this growing trend
motivates us to explore the energy consumption of the 3D
printer for the purpose of energy efficiency. To this end, we
perform an in-depth analysis of energy consumption in commercial, off-the-shelf 3D printers from an instruction-level
perspective. We build an instruction-level energy model and
an energy profiler to analyze the energy cost during the
fabrication process. From the insights obtained by the energy profiler, we propose and implement a cross-layer energy optimization solution, called 3DGates, which spans the
instruction-set, the compiler and the firmware. We evaluate
3DGates over 338 benchmarks on a 3D printer and achieve
an overall energy reduction of 25%.

CCS Concepts eComputer systems organization —
Embedded and cyber-physical systems; Special purpose
systems; Sensors and actuators; Firmware; e General and
reference — Cross-computing tools and techniques

Keywords 3D Printers; Energy Characterization and Optimization; G-code Instruction rofiling

",19.661554785965695,18.623301886792458,213
ASPLOS_17_030.txt,14.303976858825663,12.058865996943418,ASPLOS,8482,"
Processors and operating systems (OSes) support multiple memory page sizes. Superpages increase Translation
Lookaside Buffer (TLB) hits, while small pages provide
fine-grained memory protection. Ideally, TLBs should perform well for any distribution of page sizes. In reality,
set-associative TLBs — used frequently for their energyefficiency compared to fully-associative TLBs — cannot (easily) support multiple page sizes concurrently. Instead, commercial systems typically implement separate set-associative
TLBs for different page sizes. This means that when superpages are allocated aggressively, TLB misses may, counterintuitively, increase even if entries for small pages remain
unused (and vice-versa).

We invent MIX TLBs, energy-frugal set-associative
structures that concurrently support all page sizes by exploiting superpage allocation patterns. MIX TLBs boost the
performance (often by 10-30%) of big-memory applications
on native CPUs, virtualized CPUs, and GPUs. MIX TLBs
are simple and require no OS or program changes.

CCS Concepts eComputer systems organization —>
Pipeline computing; Multicore architectures

Keywords Virtual memory; TLB; superpages; coalescing
",15.903189008614273,15.202074074074073,162
ASPLOS_17_031.txt,14.443759383750667,12.440831223689788,ASPLOS,10284,"
Direct network I/O allows network controllers (NICs) to expose multiple instances of themselves, to be used by untrusted
software without a trusted intermediary. Direct I/O thus frees
researchers from legacy software, fueling studies that innovate in multitenant setups. Such studies, however, overwhelmingly ignore one serious problem: direct memory accesses
(DMAs) of NICs disallow page faults, forcing systems to either pin entire address spaces to physical memory and thereby
hinder memory utilization, or resort to APIs that pin/unpin
memory buffers before/after they are DMAed, which complicates the programming model and hampers performance.

We solve this problem by designing and implementing
page fault support for InfiniBand and Ethernet NICs. A main
challenge we tackle—unique to NICs—is handling receive
DMaAs that trigger page faults, leaving the NIC without
memory to store the incoming data. We demonstrate that
our solution provides all the benefits associated with “regular”
virtual memory, notably (1) a simpler programming model
that rids users from the need to pin, and (2) the ability
to employ all the canonical memory optimizations, such
as Memory overcommitment and demand-paging based on
actual use. We show that, as a result, benchmark performance
improves by up to 1.9x.

",18.08858127442927,17.264857142857142,202
ASPLOS_17_032.txt,14.412955874186938,12.731448490014614,ASPLOS,8187,"
With exploding traffic is stuffing existing network infrastructure, today’s telecommunication and cloud service
providers resort to Network Function Virtualization (NFV)
for greater agility and economics. Pioneer service provider
such as AT&T proposes to adopt container in NFV to
achieve shorter Virtualized Network Function (VNF) provisioning time and better runtime performance. However,
we characterize typical NFV workloads on the containers
and find that the performance is unsatisfactory. We observe
that the shared host OS network stack is the main bottleneck, where the traffic flow processing involves a large
amount of intermediate memory buffers and results in significant last level cache pollution. Existing OS memory
allocation policies fail to exploit the locality and data sharing information among buffers.

In this paper, we propose NetContainer, a software
framework that achieves fine-grained hardware resource
management for containerized NF'V platform. NetContainer
employs a cache access overheads guided page coloring
scheme to coordinately address the inter-flow cache access
overheads and intra-flow cache access overheads. It maps
the memory buffer pages that manifest low cache access
overheads (across a flow or among the flows) to the same
last level cache partition. NetContainer exploits a footprint
theory based method to estimate the cache access overheads and a Min-Cost Max-Flow model to guide the
memory buffer mappings. We implement the NetContainer
in Linux kernel and extensively evaluate it with real NFV
workloads. Experimental results show that NetContainer
outperforms conventional page coloring-based memory
allocator by 48% in terms of successful call rate.

Keywords NFV; Container; Networking; Page Coloring;
Session Initiation Protocol;
",16.728156217252725,15.526007751937986,261
ASPLOS_17_033.txt,14.796558624071348,12.486394651858387,ASPLOS,8117,"
GPUs are widely adopted in HPC and cloud computing platforms to accelerate general-purpose workloads. However,
modern GPUs do not support flexible preemption, leading
to performance and priority inversion problems in multitasking environments.

In this paper, we propose and develop FLEP, the first software system that enables flexible kernel preemption and kernel scheduling on commodity GPUs. The FLEP compilation
engine transforms the GPU program into preemptable forms,
which can be interrupted during execution and yield all or
part of the streaming multi-processors (SMs) in the GPU.
The FLEP runtime engine intercepts all kernel invocations
and determines which kernels and how those kernels should
be preempted and scheduled. Experimental results on twokernel co-runs demonstrate up to 24.2X speedup for highpriority kernels and up to 27X improvement on normalized
average turnaround time for kernels with the same priority.
FLEP reduces the preemption latency by up to 41% compared to yielding the whole GPU when the waiting kernels
only need several SMs. With all the benefits, FLEP only
introduces 2.5% runtime overhead, which is substantially
lower than the kernel slicing approach.

CCS Concepts «Software and its engineering — Multiprocessing / multiprogramming / multitasking

Keywords
scheduling

Preemption; Multi-tasking; GPGPU; Kernel
",18.10804710084791,16.42112244897959,199
ASPLOS_17_034.txt,15.142600324211951,13.756006129328838,ASPLOS,6163,"
Latent Dirichlet Allocation (LDA) is a popular tool for analyzing discrete count data such as text and images. Applications require LDA to handle both large datasets and a large
number of topics. Though distributed CPU systems have
been used, GPU-based systems have emerged as a promising alternative because of the high computational power and
memory bandwidth of GPUs. However, existing GPU-based
LDA systems cannot support a large number of topics because they use algorithms on dense data structures whose
time and space complexity is linear to the number of topics.

In this paper, we propose SaberLDA, a GPU-based
LDA system that implements a sparsity-aware algorithm to
achieve sublinear time complexity and scales well to learn
a large number of topics. To address the challenges introduced by sparsity, we propose a novel data layout, a new
warp-based sampling kernel, and an efficient sparse count
matrix updating algorithm that improves locality, makes efficient utilization of GPU warps, and reduces memory consumption. Experiments show that SaberLDA can learn from
billions-token-scale data with up to 10,000 topics, which is
almost two orders of magnitude larger than that of the previous GPU-based systems. With a single GPU card, SaberLDA
is able to learn 10,000 topics from a dataset of billions of tokens in a few hours, which is only achievable with clusters
with tens of machines before.
",16.52667757954773,15.682500000000001,233
ASPLOS_17_035.txt,15.163798381676667,13.213939693438,ASPLOS,6251,"
Cloud services are becoming increasingly globalized and
data-center workloads are expanding exponentially. GPU and
FPGA-based clouds have illustrated improvements in power
and performance by accelerating compute-intensive workloads. ASIC-based clouds are a promising way to optimize
the Total Cost of Ownership (TCO) of a given datacenter
computation (e.g. YouTube transcoding) by reducing both
energy consumption and marginal computation cost.

The feasibility of an ASIC Cloud for a particular application is directly gated by the ability to manage the NonRecurring Engineering (NRE) costs of designing and fabricating the ASIC, so that it is significantly lower (e.g. 2x)
than the TCO of the best available alternative.

In this paper, we show that technology node selection is a
major tool for managing ASIC Cloud NRE, and allows the
designer to trade off an accelerator’s excess energy efficiency
and cost performance for lower total cost. We explore NRE
and cross-technology optimization of ASIC Clouds for four
different applications: Bitcoin mining, YouTube-style video
transcoding, Litecoin, and Deep Learning. We address these
challenges and show large reductions in the NRE, potentially
enabling ASIC Clouds to address a wider variety of datacenter
workloads. Our results suggest that advanced nodes like 16nm
will lead to sub-optimal TCO for many workloads, and that
use of older nodes like 65nm can enable a greater diversity
of ASIC Clouds.

Keywords NRE, ASIC Cloud, TCO, datacenter, accelerator

",17.122413403193683,15.48653246753247,235
ASPLOS_17_036.txt,15.491431649005374,13.576600547695119,ASPLOS,7512,"
As graphics processing units (GPUs) are broadly adopted,
running multiple applications on a GPU at the same time is
beginning to attract wide attention. Recent proposals on multitasking GPUs have focused on either spatial multitasking,
which partitions GPU resource at a streaming multiprocessor (SM) granularity, or simultaneous multikernel (SMK),
which runs multiple kernels on the same SM. However, multitasking performance varies heavily depending on the resource partitions within each scheme, and the application
mixes. In this paper, we propose GPU Maestro that performs
dynamic resource management for efficient utilization of
multitasking GPUs. GPU Maestro can discover the best performing GPU resource partition exploiting both spatial multitasking and SMK. Furthermore, dynamism within a kernel
and interference between the kernels are automatically considered because GPU Maestro finds the best performing partition through direct measurements. Evaluations show that
GPU Maestro can improve average system throughput by
20.2% and 13.9% over the baseline spatial multitasking and
SMK, respectively.

CCS Concepts eComputer systems organization —>
Single instruction, multiple data; eHardware — Onchip resource management; e Software and its engineering — Multiprocessing / multiprogramming / multitasking

Keywords Graphics Processing Unit; Multitasking; Resource Management
",18.511140095513987,18.02742486338798,185
ASPLOS_17_037.txt,16.32524537670112,14.006023270812552,ASPLOS,8247,"
We present a methodology for identifying security critical
properties for use in the dynamic verification of a processor. Such verification has been shown to be an effective way
to prevent exploits of vulnerabilities in the processor, given
a meaningful set of security properties. We use known processor errata to establish an initial set of security-critical invariants of the processor. We then use machine learning to
infer an additional set of invariants that are not tied to any
particular, known vulnerability, yet are critical to security.

We build a tool chain implementing the approach and
evaluate it for the open-source OR1200 RISC processor.
We find that our tool can identify 19 (86.4%) of the 22
manually crafted security-critical properties from prior work
and generates 3 new security properties not covered in prior
work.

",17.879347455551382,15.561296296296295,137
ASPLOS_17_038.txt,14.771371093290924,12.74150537634409,ASPLOS,9124,"
Hardware-based mechanisms for software isolation are becoming increasingly popular, but implementing these mechanisms correctly has proved difficult, undermining the root
of security. This work introduces an effective way to formally verify important properties of such hardware security
mechanisms. In our approach, hardware is developed using
a lightweight security-typed hardware description language
(HDL) that performs static information flow analysis. We
show the practicality of our approach by implementing and
verifying a simplified but realistic multi-core prototype of
the ARM TrustZone architecture. To make the security-typed
HDL expressive enough to verify a realistic processor, we develop new type system features. Our experiments suggest that
information flow analysis is efficient, and programmer effort
is modest. We also show that information flow constraints are
an effective way to detect hardware vulnerabilities, including
several found in commercial processors.

",18.3970566412798,17.107090719499478,138
ASPLOS_17_039.txt,14.649819433416425,13.420500729750035,ASPLOS,10384,"
Java provides security and robustness by building a highlevel security model atop the foundation of memory protection. Unfortunately, any native code linked into a Java program — including the million lines used to implement the
standard library — is able to bypass both the memory protection and the higher-level policies. We present a hardwareassisted implementation of the Java native code interface,
which extends the guarantees required for Java’s security
model to native code.

Our design supports safe direct access to buffers owned
by the JVM, including hardware-enforced read-only access
where appropriate. We also present Java language syntax
to declaratively describe isolated compartments for native
code.

We show that it is possible to preserve the memory safety
and isolation requirements of the Java security model in C
code, allowing native code to run in the same process as Java
code with the same impact on security as running equivalent
Java code. Our approach has a negligible impact on performance, compared with the existing unsafe native code interface. We demonstrate a prototype implementation running
on the CHERI microprocessor synthesized in FPGA.

",16.218646115125612,15.801666666666673,182
ASPLOS_17_040.txt,14.501689262937365,12.523633506748137,ASPLOS,8954,"
Researchers are actively exploring techniques to enforce
control-flow integrity (CFI), which restricts program execution to a predefined set of targets for each indirect control transfer to prevent code-reuse attacks. While hardwareassisted CFI enforcement may have the potential for advantages in performance and flexibility over software instrumentation, current hardware-assisted defenses are either
incomplete (i.e., do not enforce all control transfers) or less
efficient in comparison. We find that the recent introduction
of hardware features to log complete control-flow traces,
such as Intel Processor Trace (PT), provides an opportunity
to explore how efficient and flexible a hardware-assisted CFI
enforcement system may become. While Intel PT was designed to aid in offline debugging and failure diagnosis, we
explore its effectiveness for online CFI enforcement over unmodified binaries by designing a parallelized method for enforcing various types of CFI policies. We have implemented
a prototype called GRIFFIN in the Linux 4.2 kernel that enables complete CFI enforcement over a variety of software,
including the Firefox browser and its jitted code. Our experiments show that GRIFFIN can enforce fine-grained CFI
policies with shadow stack as recommended by researchers
at a performance that is comparable to software-only instrumentation techniques. In addition, we find that alternative
logging approaches yield significant performance improvements for trace processing, identifying opportunities for further hardware assistance.

CCS Concepts eSecurity and privacy — Operating systems security
Keywords Intel Processor Trace, Control-Flow Integrity

",19.78447435784618,19.465,239
ASPLOS_17_041.txt,15.93059798066328,14.358496411654926,ASPLOS,8523,"
Cloud providers routinely schedule multiple applications per
physical host to increase efficiency. The resulting interference on shared resources often leads to performance degradation and, more importantly, security vulnerabilities. Interference can leak important information ranging from a service’s placement to confidential data, like private keys.

We present Bolt, a practical system that accurately detects the type and characteristics of applications sharing a
cloud platform based on the interference an adversary sees
on shared resources. Bolt leverages online data mining techniques that only require 2-5 seconds for detection. In a multiuser study on EC2, Bolt correctly identifies the characteristics of 385 out of 436 diverse workloads. Extracting this information enables a wide spectrum of previously-impractical
cloud attacks, including denial of service attacks (DoS) that
increase tail latency by 140x, as well as resource freeing
(RFA) and co-residency attacks. Finally, we show that while
advanced isolation mechanisms, such as cache partitioning
lower detection accuracy, they are insufficient to eliminate
these vulnerabilities altogether. To do so, one must either
disallow core sharing, or only allow it between threads of
the same application, leading to significant inefficiencies and
performance penalties.

*CCS Concepts: «Security and privacy — Systems security; * Computer systems organization — Cloud computing

*Keywords: cloud computing; security; interference; isolation; datacenter; latency; denial of service attack; data mining

",18.458006810337128,17.679364485981313,216
ASPLOS_17_042.txt,16.643613559503354,15.807745737460689,ASPLOS,6911,"
The computation for today’s intelligent personal assistants
such as Apple Siri, Google Now, and Microsoft Cortana, is
performed in the cloud. This cloud-only approach requires
significant amounts of data to be sent to the cloud over the
wireless network and puts significant computational pressure on the datacenter. However, as the computational resources in mobile devices become more powerful and energy efficient, questions arise as to whether this cloud-only
processing is desirable moving forward, and what are the
implications of pushing some or all of this compute to the
mobile devices on the edge.

In this paper, we examine the status quo approach of
cloud-only processing and investigate computation partitioning strategies that effectively leverage both the cycles in the
cloud and on the mobile device to achieve low latency, low
energy consumption, and high datacenter throughput for this
class of intelligent applications. Our study uses 8 intelligent
applications spanning computer vision, speech, and natural
language domains, all employing state-of-the-art Deep Neural Networks (DNNs) as the core machine learning technique. We find that given the characteristics of DNN algorithms, a fine-grained, layer-level computation partitioning
strategy based on the data and computation variations of
each layer within a DNN has significant latency and energy
advantages over the status quo approach.

Using this insight, we design Neurosurgeon, a lightweight scheduler to automatically partition DNN computation between mobile devices and datacenters at the granularity of neural network layers. Neurosurgeon does not
require per-application profiling. It adapts to various DNN
architectures, hardware platforms, wireless networks, and
server load levels, intelligently partitioning computation for

best latency or best mobile energy. We evaluate Neurosur-—
geon on a state-of-the-art mobile development platform and
show that it improves end-to-end latency by 3.1 on average and up to 40.7x, reduces mobile energy consumption
by 59.5% on average and up to 94.7%, and improves datacenter throughput by 1.5 on average and up to 6.7x.

Keywords mobile computing; cloud computing; deep neural networks; intelligent applications

",19.10251660953655,18.728679653679652,344
ASPLOS_17_043.txt,15.89244996593532,14.059003841476606,ASPLOS,7990,"
The advent of new memory technologies that are denser
and cheaper than commodity DRAM has renewed interest
in two-tiered main memory schemes. Infrequently accessed
application data can be stored in such memories to achieve
significant memory cost savings. Past research on two-tiered
main memory has assumed a 4KB page size. However,
2MB huge pages are performance critical in cloud applications with large memory footprints, especially in virtualized cloud environments, where nested paging drastically increases the cost of 4KB page management. We present Thermostat, an application-transparent huge-page-aware mechanism to place pages in a dual-technology hybrid memory
system while achieving both the cost advantages of two-tiered memory and performance advantages of transparent
huge pages. We present an online page classification mechanism that accurately classifies both 4KB and 2MB pages
as hot or cold while incurring no observable performance
overhead across several representative cloud applications.
We implement Thermostat in Linux kernel version 4.5 and
evaluate its effectiveness on representative cloud computing
workloads running under KVM virtualization. We emulate
slow memory with performance characteristics approximating near-future high-density memory technology and show
that Thermostat migrates up to 50% of application footprint
to slow memory while limiting performance degradation to
3%, thereby reducing memory cost up to 30%.

CCS Concepts

• Software and its engineering → Memory mangement

Keywords Cloud Computing, Operating systems
",19.399008522586517,17.602666666666668,226
ASPLOS_17_044.txt,15.850880102106235,14.34604363346708,ASPLOS,8457,"
Energy efficiency is one of the most important design considerations in running modern datacenters. Datacenter operating systems rely on software techniques such as execution
migration to achieve energy efficiency across pools of machines. Execution migration is possible in datacenters today
because they consist mainly of homogeneous-ISA machines.
However, recent market trends indicate that alternate ISAs
such as ARM and PowerPC are pushing into the datacenter,
meaning current execution migration techniques are no longer
applicable. How can execution migration be applied in future
heterogeneous-ISA datacenters?

In this work we present a compiler, runtime, and an operating system extension for enabling execution migration between
heterogeneous-ISA servers. We present a new multi-ISA binary
architecture and heterogeneous-OS containers for facilitating
efficient migration of natively-compiled applications. We build
and evaluate a prototype of our design and demonstrate energy savings of up to 66% for a workload running on an ARM
and an x86 server interconnected by a high-speed network.

",17.410965686947208,17.178004658385095,162
ASPLOS_17_045.txt,16.045693215258485,14.355407323055363,ASPLOS,7108,"
The memory consistency model is a fundamental part of any
shared memory architecture or programming model. Modern
weak memory models are notoriously difficult to define and
to implement correctly. Most real-world programming languages, compilers, and (micro)architectures therefore rely
heavily on black-box testing methodologies. The success of
such techniques requires that the suite of litmus tests used
to perform the testing be comprehensive—it should ideally
stress all obscure comer cases of the model and of its implementation. Most litmus test suites today are generated from
some combination of manual effort and randomization; however, the complex and subtle nature of contemporary memory models means that manual effort is both error-prone and
subject to incomplete coverage.

This paper presents a methodology for synthesizing comprehensive litmus test suites directly from a memory model
specification. By construction, these suites contain all tests
satisfying a minimality criterion: that no synchronization
mechanism in the test can be weakened without causing
new behaviors to become observable. We formalize this notion using the Alloy modeling language, and we apply it to
a number of existing and newly-proposed memory models.
Our results show not only that this synthesis technique can
automatically reproduce all manually-generated tests from
existing suites, but also that it discovers new tests that are
not as well studied.

CCS Concepts eComputer systems organization —>
Multicore architectures; e Hardware — Coverage metrics; e Software and its engineering — Synchronization

Keywords memory consistency models, litmus tests, synchronization, synthesis
",18.026119701940384,17.477639344262297,244
ASPLOS_17_046.txt,14.76955116536869,13.17805178615949,ASPLOS,9144,"
In big data and cloud computing era, reliability of distributed
systems is extremely important. Unfortunately, distributed
concurrency bugs, referred to as DCbugs, widely exist. They
hide in the large state space of distributed cloud systems
and manifest non-deterministically depending on the timing
of distributed computation and communication. Effective
techniques to detect DCbugs are desired.

This paper presents a pilot solution, DCatch, in the world
of DCbug detection. DCatch predicts DCbugs by analyzing
correct execution of distributed systems. To build DCatch,
we design a set of happens-before rules that model a wide
variety of communication and concurrency mechanisms in
real-world distributed cloud systems. We then build runtime tracing and trace analysis tools to effectively identify
concurrent conflicting memory accesses in these systems.
Finally, we design tools to help prune false positives and
trigger DCbugs.

We have evaluated DCatch on four representative opensource distributed cloud systems, Cassandra, Hadoop MapReduce, HBase, and ZooKeeper. By monitoring correct execution of seven workloads on these systems, DCatch reports
32 DCbugs, with 20 of them being truly harmful.

CCS Concepts Software and its engineering — Cloud
computing; Software reliability; Software testing and debugging

Keywords Concurrency Bugs, Distributed Systems, Bug
Detection, Cloud Computing

",15.359359093739592,14.412500000000005,198
ASPLOS_17_047.txt,14.372944249313704,12.630439173004955,ASPLOS,10134,"
We present Castor, a record/replay system for multi-core
applications that provides consistently low and predictable
overheads. With Castor, developers can leave record and
replay on by default, making it practical to record and
reproduce production bugs, or employ fault tolerance to
recover from hardware failures.
Castor is inspired by several observations: First, an efficient mechanism for logging non-deterministic events is critical for recording demanding workloads with low overhead.
Through careful use of hardware we were able to increase log
throughput by 10× or more, e.g., we could record a server
handling 10× more requests per second for the same record
overhead. Second, most applications can be recorded without
modifying source code by using the compiler to instrument
language level sources of non-determinism, in conjunction
with more familiar techniques like shared library interposition. Third, while Castor cannot deterministically replay all
data races, this limitation is generally unimportant in practice,
contrary to what prior work has assumed.
Castor currently supports applications written in C, C++,
and Go on FreeBSD. We have evaluated Castor on parallel
and server workloads, including a commercial implementation of memcached in Go, which runs Castor in production.



CCS Concepts

• Software and its engineering → Operating systems

Keywords Multi-Core Replay, Replay Debugging, FaultTolerance
",16.594172100314452,15.965821596244133,210
ASPLOS_17_048.txt,14.866192472650049,13.276958800332089,ASPLOS,7221,"
Software optimization is constantly a serious concern for developing high-performance systems. To accelerate the workflow execution of a specific functionality, software developers usually define and implement a fast path to speed up the
critical and commonly executed functions in the workflow.
However, producing a bug-free fast path is nontrivial. Our
study on the Linux kernel discloses that a committed fast
path can have up to 19 follow-up patches for bug fixing, and
most of them are deep semantic bugs, which are difficult to
be pinpointed by existing bug-finding tools.

In this paper, we present such a new category of software
bugs based on our fast-path bug study across various system
software including virtual memory manager, file systems,
network, and device drivers. We investigate their root causes
and identify five error-prone aspects in a fast path: path state,
trigger condition, path output, fault handling, and assistant
data structure. We find that many of the deep bugs can be
prevented by applying static analysis incorporating simple
semantic information. We extract a set of rules based on our
findings and build a toolkit PALLAS to check fast-path bugs.
The evaluation results show that PALLAS can effectively reveal fast-path bugs in a variety of systems including Linux
kernel, mobile operating system, software-defined networking system, and web browser.

Categories and Subject Descriptors D.2.5 [Software Engineering]: Testing and Debugging; D.4.5 [Operating Systems]: Reliability

Keywords Software Optimization; Fast Path; Semantic
Bugs; Static Analysis

",16.52667757954773,15.588748987854252,250
ASPLOS_17_049.txt,15.924969227343503,14.77968395886582,ASPLOS,7642,"
DRAM cells need periodic refresh to maintain data integrity.
With high capacity DRAMs, DRAM refresh poses a significant performance bottleneck as the number of rows to be
refreshed (and hence the refresh cycle time, tRFC) for each
refresh command increases. Modern day DRAMs perform
refresh at a rank-level, while LPDDRs used in mobile environments support refresh at a per-bank level. Rank-level
refresh degrades the performance significantly since none of
the banks in a rank can serve the on-demand requests. Perbank refresh alleviates some of the performance bottlenecks
as the other banks in a rank are available for on-demand requests. Typical DRAM retention time is in the order of several milliseconds, viz, 64msec for environments operating in
temperatures below 85 deg C and 32msec for environments
operating above 85 deg C.

With systems moving towards increased consolidation
(e.g., virtualized environments), DRAM refresh becomes
a significant bottleneck as it reduces the available overall DRAM bandwidth per task. In this work, we propose
a hardware-software co-design to mitigate DRAM refresh
overheads by exposing the hardware address-mapping and
DRAM refresh schedule to the operating system (OS). In
our co-design, we propose a novel per-bank refresh schedule in the hardware which augments memory partitioning
in the OS. Supported by the novel per-bank refresh schedule and memory-partitioning, we propose a refresh-aware
process scheduling algorithm in the OS which schedules applications on cores such that none of the on-demand requests
from the applications are stalled by refreshes. The evaluation
of our proposed co-design using multi-programmed workloads from the SPEC CPU2006, STREAM and NAS suites

show significant performance improvements compared to
the previously proposed hardware-only approaches.

CCS Concepts eComputer systems organization —>
General-Hardware/software interfaces; e Hardware >
DRAM Memory; ¢Software — Operating Systems

Keywords DRAM tefresh, Operating Systems, Task Scheduling, Hardware-software co-design.

",15.903189008614273,15.737658730158731,314
ASPLOS_17_050.txt,15.61687899452966,13.183823450315987,ASPLOS,8023,"
Data prefetching and cache replacement algorithms have
been intensively studied in the design of high performance
microprocessors. Typically, the data prefetcher operates in
the private caches and does not interact with the replacement policy in the shared Last-Level Cache (LLC). Similarly, most replacement policies do not consider demand and
prefetch requests as different types of requests. In particular, program counter (PC)-based replacement policies cannot learn from prefetch requests since the data prefetcher
does not generate a PC value. PC-based policies can also
be negatively affected by compiler optimizations. In this
paper, we propose a holistic cache management technique
called Kill-the-PC (KPC) that overcomes the weaknesses of
traditional prefetching and replacement policy algorithms.
KPC cache management has three novel contributions. First,
a prefetcher which approximates the future use distance of
prefetch requests based on its prediction confidence. Second,
a simple replacement policy provides similar or better performance than current state-of-the-art PC-based prediction
using global hysteresis. Third, KPC integrates prefetching
and replacement policy into a whole system which is greater
than the sum of its parts. Information from the prefetcher
is used to improve the performance of the replacement policy and vice-versa. Finally, KPC removes the need to propagate the PC through entire on-chip cache hierarchy while
providing a holistic cache management approach with better performance than state-of-the-art PC-, and non-PC-based
schemes. Our evaluation shows that KPC provides 8% better
performance than the best combination of existing prefetcher
and replacement policy for multi-core workloads.

Categories and Subject Descriptors B.3.2 [Memory Structures]: Cache memories

Keywords Memory Hierarchy, Data Prefetching, Cache
Replacement Policy
",16.954822765917157,14.320652398143373,279
ASPLOS_17_051.txt,15.169353473064934,13.394707537639196,ASPLOS,5755,"
The high accuracy of deep neural networks (NNs) has led
to the development of NN accelerators that improve performance by two orders of magnitude. However, scaling these
accelerators for higher performance with increasingly larger
NNs exacerbates the cost and energy overheads of their
memory systems, including the on-chip SRAM buffers and
the off-chip DRAM channels.

This paper presents the hardware architecture and software scheduling and partitioning techniques for TETRIS, a
scalable NN accelerator using 3D memory. First, we show
that the high throughput and low energy characteristics of
3D memory allow us to rebalance the NN accelerator design,
using more area for processing elements and less area for
SRAM buffers. Second, we move portions of the NN computations close to the DRAM banks to decrease bandwidth
pressure and increase performance and energy efficiency.
Third, we show that despite the use of small SRAM buffers,
the presence of 3D memory simplifies dataflow scheduling for NN computations. We present an analytical scheduling scheme that matches the efficiency of schedules derived
through exhaustive search. Finally, we develop a hybrid partitioning scheme that parallelizes the NN computations over
multiple accelerators. Overall, we show that TETRIS improves the performance by 4.1x and reduces the energy by
1.5x over NN accelerators with conventional, low-power
DRAM memory systems.

CCS Concepts eComputer systems organization —>
Neural networks

Keywords 3D memory, neural networks, acceleration, dataflow

scheduling, partitioning

",17.58133193835471,16.40601298701299,233
ASPLOS_17_052.txt,17.02255974131777,15.805796779469237,ASPLOS,7739,"
NUMA (non-uniform memory access) servers are commonly used in high-performance computing and datacenters. Within each server, a processor-interconnect (e.g.,
Intel QPI, AMD HyperTransport) is used to communicate between the different sockets or nodes. In this work,
we explore the impact of the processor-interconnect on
overall performance —- in particular, the performance unfairness caused by processor-interconnect arbitration. It is
well known that locally-fair arbitration does not guarantee globally-fair bandwidth sharing as closer nodes receive
more bandwidth in a multi-hop network. However, this work
demonstrates that the opposite can occur in a commodity
NUMA server where remote nodes receive higher bandwidth
(and perform better). We analyze this problem and identify that this occurs because of external concentration used
in router micro-architectures for processor-interconnects
without globally-aware arbitration. While accessing remote
memory can occur in any NUMA system, performance unfairness (or performance variation) is more critical in cloud
computing and virtual machines with shared resources. We
demonstrate how this unfairness creates significant performance variation when a workload is executed on the Xen
virtualization platform. We then provide analysis using synthetic workloads to better understand the source of unfairness and eliminate the impact of other shared resources,
including the shared last-level cache and main memory. To
provide fairness, we propose a novel, history-based arbitration that tracks the history of arbitration grants made in
the previous history window. A weighted arbitration is done
based on the history to provide global fairness. Through

simulations, we show our proposed history-based arbitration can provide global fairness and minimize the processorinterconnect performance unfairness at low cost.

CCS Concepts «Computer systems organization > Jnterconnection architectures

Keywords Processor-interconnect,; NUMA servers; arbitration; router concentration
",17.99237782463571,16.51859649122807,286
ASPLOS_17_053.txt,15.511044010423916,13.85241480929135,ASPLOS,9430,"
Distributed transactional storage is an important service in
today’s data centers. Achieving high performance without
high complexity is often a challenge for these systems due
to sophisticated consistency protocols and multiple layers
of abstraction. In this paper we show how to combine two
emerging technologies—Software-Defined Flash (SDF) and
precise synchronized clocks—to improve performance and
reduce complexity for transactional storage within the data
center.

We present a distributed transactional system (called MILANA) as a layer above a durable multi-version key-value
store (called SEMEL) for read-heavy workloads within a data
center. SEMEL exploits write behavior of SSDs to maintain
a time-ordered sequence of versions for each key efficiently
and durably. MILANA adds a variant of optimistic concurrency control above SEMEL’s API to service read requests
from a consistent snapshot and to enable clients to make fast
local commit or abort decisions for read-only transactions.

Experiments with the prototype reveal up to 43% lower
transaction abort rates using [EEE Precision Time Protocol
(PTP) vs. the standard Network Time Protocol (NTP). Under
the Retwis benchmark, client-local validation of read-only
transactions yields a 35% reduction in latency and 55%
increase in transaction throughput.

CCS Concepts eInformation systems — Distributed
storage

Keywords clock-synchronization; non-volatile memory;
distributed storage systems; strong consistency; transactions

",16.99224021665606,16.495925925925928,219
ASPLOS_17_054.txt,15.021129683784007,13.90683902784647,ASPLOS,7564,"
The emergence of programmable network devices and the
increasing data traffic of datacenters motivate the idea of
in-network computation. By offloading compute operations
onto intermediate networking devices (e.g., switches, network accelerators, middleboxes), one can (1) serve network
requests on the fly with low latency; (2) reduce datacenter
traffic and mitigate network congestion; and (3) save energy
by running servers in a low-power mode. However, since
(1) existing switch technology doesn’t provide general computing capabilities, and (2) commodity datacenter networks
are complex (e.g., hierarchical fat-tree topologies, multipath
communication), enabling in-network computation inside a
datacenter is challenging.

In this paper, as a step towards in-network computing, we
present IncBricks, an in-network caching fabric with basic
computing primitives. IncBricks is a hardware-software codesigned system that supports caching in the network using a
programmable network middlebox. As a key-value store accelerator, our prototype lowers request latency by over 30%
and doubles throughput for 1024 byte values in a common
cluster configuration. Our results demonstrate the effectiveness of in-network computing and that efficient datacenter
network request processing is possible if we carefully split
the computation across the different programmable computing elements in a datacenter, including programmable
switches, network accelerators, and end hosts.

",20.267338824336647,20.346366158113735,210
ASPLOS_17_055.txt,16.946048551805386,15.194914692892862,ASPLOS,7454,"
Due to imbalances in technology scaling, the energy consumption of data storage and communication by far exceeds
the energy consumption of actual data production, i.e., computation. As a consequence, recomputing data can become
more energy-efficient than storing and retrieving precomputed data. At the same time, recomputation can relax the
pressure on the memory hierarchy and the communication
bandwidth. This study hence assesses the energy efficiency
prospects of trading computation for communication. We
introduce an illustrative proof-of-concept design, identify
practical limitations, and provide design guidelines.
",17.122413403193683,16.5592183908046,89
ASPLOS_17_056.txt,15.766247114938995,14.195115951384675,ASPLOS,7522,"
Conventional off-chip voltage regulators are typically bulky
and slow, and are inefficient at exploiting system and workload variability using Dynamic Voltage and Frequency Scaling (DVFS). On-die integration of voltage regulators has the
potential to increase the energy efficiency of computer systems by enabling power control at a fine granularity in both
space and time. The energy conversion efficiency of on-chip
regulators, however, is typically much lower than off-chip
regulators, which results in significant energy losses. Finegrained power control and high voltage regulator efficiency
are difficult to achieve simultaneously, with either emerging
on-chip or conventional off-chip regulators.

A voltage conversion framework that relies on a hierarchy
of off-chip switching regulators and on-chip linear regulators is proposed to enable fine-grained power control with
a regulator efficiency greater than 90%. A DVFS control
policy that is based on a reinforcement learning (RL) approach is developed to exploit the proposed framework. Percore RL agents learn and improve their control policies independently, while retaining the ability to coordinate their
actions to accomplish system level power management objectives. When evaluated on a mix of 14 parallel and 13 multiprogrammed workloads, the proposed voltage conversion
framework achieves 18% greater energy efficiency than a
conventional framework that uses on-chip switching regulators. Moreover, when the RL based DVFS control policy is
used to control the proposed voltage conversion framework,
the system achieves a 21% higher energy efficiency over a
baseline oracle policy with coarse-grained power control capability.

CCS Concepts «Computer systems organization — Architectures; e Hardware — Power and energy

Keywords Computer architecture; Power, energy, and thermal management.

",19.083932058031824,18.682149253731342,269
ATC_17_001.txt,13.96064878648128,12.185931900586255,ATC,7148,"

Virtual machines (VMs) that try to isolate untrusted
code are widely used in practice. However, it is often
possible to trigger zero-day flaws in the host Operating
System (OS) from inside of such virtualized systems. In
this paper, we propose a new security metric showing
strong correlation between “popular paths” and kernel
vulnerabilities. We verify that the OS kernel paths accessed by popular applications in everyday use contain
significantly fewer security bugs than less-used paths.
We then demonstrate that this observation is useful in
practice by building a prototype system which locks an
application into using only popular OS kernel paths. By
doing so, we demonstrate that we can prevent the triggering of zero-day kernel bugs significantly better than
three other competing approaches, and argue that this is
a practical approach to secure system design.

",15.470042427545799,14.50028985507247,139
ATC_17_002.txt,14.229593243241187,13.38026269166102,ATC,8509,"

Porting Linux device drivers to target more recent and
older Linux kernel versions to compensate for the everchanging kernel interface is a continual problem for
Linux device driver developers. Acquiring information
about interface changes is a necessary, but tedious and
error prone, part of this task. In this paper, we propose
two tools, Prequel and gcc-reduce, to help the developer
collect the needed information. Prequel provides language support for querying git commit histories, while
gcc-reduce translates error messages produced by compiling a driver with a target kernel into appropriate Prequel queries. We have used our approach in porting 33
device driver files over up to 3 years of Linux kernel history, amounting to hundreds of thousands of commits.
In these experiments, for 3/4 of the porting issues, our
approach highlighted commits that enabled solving the
porting task. For many porting issues, our approach retrieves relevant commits in 30 seconds or less.

",13.25671669890799,13.90818181818182,155
ATC_17_003.txt,13.50910700611447,11.41000804870103,ATC,8487,"

The operating system is tasked with maintaining the
coherency of per-core TLBs, necessitating costly synchronization operations, notably to invalidate stale
mappings. As core-counts increase, the overhead of
TLB synchronization likewise increases and hinders
scalability, whereas existing software optimizations
that attempt to alleviate the problem (like batching)
are lacking.

We address this problem by revising the TLB
synchronization subsystem. We introduce several
techniques that detect cases whereby soon-to-be
invalidated mappings are cached by only one TLB
or not cached at all, allowing us to entirely avoid
the cost of synchronization. In contrast to existing
optimizations, our approach leverages hardware
page access tracking. We implement our techniques
in Linux and find that they reduce the number of
TLB invalidations by up to 98% on average and thus
improve performance by up to 78%. Evaluations
show that while our techniques may introduce
overheads of up to 9% when memory mappings
are never removed, these overheads can be avoided
by simple hardware enhancements.

",16.439396014739867,15.788361086765995,164
ATC_17_004.txt,13.00547502291781,11.678236972349136,ATC,7507,"
With the high throughput offered by solid-state drives
(SSDs), multi-SSD volumes have become an attractive
storage solution for big data applications. Unfortunately, the IO stack in current operating systems imposes a number of volume-level limitations, such as pervolume based IO processing in the block layer, single
flush thread per volume for buffer cache management,
locks for parallel IOs on a file, all of which lower the
performance that could otherwise be achieved on multiSSD volumes. To address this problem, we propose a
new design of per-drive IO processing that separates two
key functionalities of IO batching and IO serving in the
IO stack. Specifically, we design and develop Falcon1
that consists of two major components: Falcon IO Management Layer that batches the incoming IOs at the volume level, and Falcon Block Layer that parallelizes IO
serving on the SSD level in a new block layer. Compared to the current practice, Falcon significantly speeds
up direct random file read and write on an 8-SSD volume by 1.77× and 1.59× respectively, and also shows
strong scalability across different numbers of drives and
various storage controllers. In addition, Falcon improves
the performance of a variety of applications by 1.69×.
",16.32212239822248,17.91509803921569,205
ATC_17_005.txt,14.697785290350886,13.404851117806817,ATC,7787,"
Troubleshooting network performance issues is a challenging task especially in large-scale data center networks. This paper presents deTector, a network monitoring system that is able to detect and localize network failures (manifested mainly by packet losses) accurately in near real time while minimizing the monitoring overhead. deTector achieves this goal by tightly
coupling detection and localization and carefully selecting probe paths so that packet losses can be localized
only according to end-to-end observations without the
help of additional tools (e.g., tracert). In particular, we
quantify the desirable properties of the matrix of probe
paths, i.e., coverage and identifiability, and leverage an
efﬁcient greedy algorithm with a good approximation ratio and fast speed to select probe paths. We also propose
a loss localization method according to loss patterns in
a data center network. Our algorithm analysis, experimental evaluation on a Fattree testbed and supplementary
large-scale simulation validate the scalability, feasibility
and effectiveness of deTector.
",20.075844112070616,20.63179617834395,161
ATC_17_006.txt,15.203237407356955,13.019926946107784,ATC,7310,"
Current IaaS clouds provide performance guarantee on
CPU and memory but no quantitative network performance for VM instances. Our measurements from three
production IaaS clouds show that for the VMs with same
CPU and memory, or similar pricing, the difference in
bandwidth performance can be as much as 16x, which
reveals a severe price-performance anomaly due to a lack
of pricing for bandwidth guarantee. Considering the low
network utilization in cloud-scale datacenters, we address this by presenting SoftBW, a system that enables
pricing bandwidth with over commitment on bandwidth
guarantee. SoftBW leverages usage-based charging to
guarantee price-performance consistency among tenants,
and implements a fulfillment based scheduling to provide bandwidth/fairness guarantee under bandwidth over
commitment. Both testbed experiments and large-scale
simulation results validate SoftBW’s ability of providing
efficient bandwidth guarantee, and show that by using
bandwidth over commitment, SoftBW increases 3.9x
network utilization while incurring less than 5% guarantee failure.
",20.267338824336647,18.8058064516129,158
ATC_17_007.txt,16.23332324115118,14.60264865250154,ATC,7762,"
In this paper, we propose a novel approach to manage
the throughput vs visibility latency tradeoff that emerges
when enforcing causal consistency in geo-replicated systems. Our approach consists in allowing full concurrency
when processing local updates and using a deferred local
serialisation procedure before shipping updates to remote
datacenters. This strategy allows to implement inexpensive mechanisms to ensure system consistency requirements while avoiding intrusive effects on update operations, a major performance limitation of previous systems. We have implemented our approach as a variant
of Riak KV. Our evaluation shows that we outperform
sequencer-based approaches by almost an order of magnitude in the maximum achievable throughput. Furthermore, unlike previous sequencer-free solutions, our approach reaches nearly optimal remote update visibility
latencies without limiting throughput.
",18.062587368997235,18.1174,126
ATC_17_008.txt,14.654440547930143,12.353929181257545,ATC,7400,"
Understanding the performance of data-parallel workloads when resource-constrained has significant practical
importance but unfortunately has received only limited
attention. This paper identifies, quantifies and demonstrates memory elasticity, an intrinsic property of dataparallel tasks. Memory elasticity allows tasks to run with
significantly less memory than they would ideally need
while only paying a moderate performance penalty. For
example, we find that given as little as 10% of ideal memory, PageRank and NutchIndexing Hadoop reducers become only 1.2x/1.75x and 1.08x slower. We show that
memory elasticity is prevalent in the Hadoop, Spark, Tez
and Flink frameworks. We also show that memory elasticity is predictable in nature by building simple models
for Hadoop and extending them to Tez and Spark.

To demonstrate the potential benefits of leveraging
memory elasticity, this paper further explores its application to cluster scheduling. In this setting, we observe
that the resource vs. time trade-off enabled by memory
elasticity becomes a task queuing time vs. task runtime
trade-off. Tasks may complete faster when scheduled
with less memory because their waiting time is reduced.
We show that a scheduler can turn this task-level tradeoff into improved job completion time and cluster-wide
memory utilization. We have integrated memory elasticity into Apache YARN. We show gains of up to 60% in
average job completion time on a 50-node Hadoop cluster. Extensive simulations show similar improvements
over a large number of scenarios.
",15.903189008614273,13.925063291139239,241
ATC_17_009.txt,16.335523135448913,13.956252989000483,ATC,8115,"
Streaming video algorithms dynamically select between
different versions of a video to deliver the highest quality
version that can be viewed without buffering over the
client’s connection. To improve the quality for viewers,
the backing video service can generate more and/or better
versions, but at a significant computational overhead.
Processing all videos uploaded to Facebook in the most
intensive way would require a prohibitively large cluster.
Facebook’s video popularity distribution is highly skewed,
however, with analysis on sampled videos showing 1% of
them accounting for 83% of the total watch time by users.
Thus, if we can predict the future popularity of videos, we
can focus the intensive processing on those videos that
improve the quality of the most watch time.
To address this challenge, we designed Chess, the first
popularity prediction algorithm that is both scalable and
accurate. Chess is scalable because, unlike the state-ofthe-art approaches, it requires only constant space per
video, enabling it to handle Facebook’s video workload.
Chess is accurate because it delivers superior predictions
using a combination of historical access patterns with
social signals in a unified online learning framework. We
have built a video prediction service, ChessVPS, using
our new algorithm that can handle Facebook’s workload
with only four machines. We find that re-encoding popular
videos predicted by ChessVPS enables a higher percentage
of total user watch time to benefit from intensive encoding,
with less overhead than a recent production heuristic, e.g.,
80% of watch time with one-third as much overhead.
",18.35118385865746,16.292761904761907,258
ATC_17_010.txt,15.351857567395324,13.764830594370213,ATC,7788,"
The current primary concern of out-of-core graph processing systems is improving disk I/O locality, which
leads to certain restrictions on their programming and execution models. Although improving the locality, these
constraints also restrict the expressiveness. As a result, only sub-optimal algorithms are supported for many
kinds of applications. When compared with the optimal algorithms, these supported algorithms typically incur sequential, but much larger, amount of disk I/O.

In this paper, we explore a fundamentally different
tradeoff: less total amount of I/O rather than better locality. We show that out-of-core graph processing systems uniquely provide the opportunities to lift the restrictions of the programming and execution model (e.g., process each loaded block at most once, neighborhood constraint) in a feasible manner, which enable efficient algorithms that require drastically less number of iterations.
To demonstrate the ideas, we build CLIP, a novel out-ofcore graph processing system designed with the principle
of “squeezing out all the value of loaded data”. With the
more expressive programming model and more flexible
execution, CLIP enables more efficient algorithms that
require much less amount of total disk I/O. Our experiments show that the algorithms that can be only implemented in CLIP are much faster than the original disklocality-optimized algorithms in many real-world cases
(up to tens or even thousands of times speedup).
",18.377959752453624,17.152901982378854,229
ATC_17_011.txt,16.48448037999441,15.798426694075328,ATC,4629,"
With more devices connected, delays and jitter at the WiFi
hop become more prevalent, and correct functioning during network congestion becomes more important. However, two important performance issues prevent modern
WiFi from reaching its potential: increased latency under
load caused by excessive queueing (i.e. bufferbloat) and
the 802.11 performance anomaly.

To remedy these issues, we present a novel two-part
solution. We design a new queueing scheme that eliminates bufferbloat in the wireless setting. Leveraging
this queueing scheme, we then design an airtime fairness
scheduler that operates at the access point and doesn’t require any changes to clients.

We evaluate our solution using both a theoretical model
and experiments in a testbed environment, formulating
a suitable analytical model in the process. We show that
our solution achieves an order of magnitude reduction in
latency under load, large improvements in multi-station
throughput, and nearly perfect airtime fairness for both
TCP and downstream UDP traffic. Further experiments
with application traffic confirm that the solution provides
significant performance gains for real-world traffic.We
develop a production quality implementation of our solution in the Linux kernel, the platform powering most access points outside of the managed enterprise setting. The
implementation has been accepted into the mainline kernel distribution, making it available for deployment on
billions of devices running Linux today.
",17.251386760058843,16.38015360983103,222
ATC_17_012.txt,15.525326909830266,13.614821165316396,ATC,7321,"
Next-generation genome sequencing technology has
reached a point at which it is becoming cost-effective to
sequence all patients. Biobanks and researchers are faced
with an oncoming deluge of genomic data, whose processing requires new and scalable bioinformatics architectures and systems. Processing raw genetic sequence
data is computationally expensive and datasets are large.
Current software systems can require many hours to process a single genome and generally run only on a single
computer. Common file formats are monolithic and roworiented, a barrier to distributed computation.

To address these challenges, we built Persona, a
cluster-scale, high-throughput bioinformatics framework. Persona currently supports paired-read alignment,
sorting, and duplicate marking using well-known algorithms and techniques. Persona can significantly reduce
end-to-end processing times for bioinformatics computations. A new Aggregate Genomic Data (AGD) format
unifies sample data and analysis results, while enabling
efficient distributed computation and I/O.

In a case study on sequence alignment, Persona sustains 1.353 gigabases aligned per second with 101 base
pair reads on a 32-node cluster and can align a full
genome in ~16.7 seconds using the SNAP algorithm.
Our results demonstrate that: (1) alignment computation with Persona scales linearly across servers with no
measurable completion-time imbalance and negligible
framework overheads; (2) on a single server, sorting with
Persona and AGD is up to 2.3x faster than commonly
used tools, while duplicate marking is 3x faster; (3) with
AGD, a 7 node COTS network storage system can service up to 60 alignment compute nodes; (4) server cost
dominates for a balanced system running Persona, while
long-term data storage dwarfs the cost of computation.
",17.332850977054683,15.960153180153181,276
ATC_17_013.txt,13.760222810521444,11.594030896721062,ATC,6197,"
Recent GPUs enable Peer-to-Peer Direct Memory Access (P2P) from fast peripheral devices like NVMe SSDs
to exclude the CPU from the data path between them
for efficiency. Unfortunately, using P2P to access files
is challenging because of the subtleties of low-level nonstandard interfaces, which bypass the OS file I/O layers
and may hurt system performance.

SPIN integrates P2P into the standard OS file I/O stack,
dynamically activating P2P where appropriate, transparently to the user. It combines P2P with page cache
accesses, re-enables read-ahead for sequential reads,
all while maintaining standard POSIX FS consistency,
portability across GPUs and SSDs, and compatibility
with virtual block devices such as software RAID.

We evaluate SPIN on NVIDIA and AMD GPUs using standard file I/O benchmarks, application traces and
end-to-end experiments. SPIN achieves significant performance speedups across a wide range of workloads, exceeding P2P throughput by up to an order of magnitude.
It also boosts the performance of an aerial imagery rendering application by 2.6x by dynamically adapting to
its input-dependent file access pattern, and enables 3.3 x
higher throughput for a GPU-accelerated log server.
",17.451712890111917,16.18735119047619,195
ATC_17_014.txt,17.094715597493337,16.300094404759722,ATC,7338,"
Deep learning models can take weeks to train on a single
GPU-equipped machine, necessitating scaling out DL
training to a GPU-cluster. However, current distributed
DL implementations can scale poorly due to substantial
parameter synchronization over the network, because the
high throughput of GPUs allows more data batches to be
processed per unit time than CPUs, leading to more frequent network synchronization. We present Poseidon, an
efficient communication architecture for distributed DL
on GPUs. Poseidon exploits the layered model structures
in DL programs to overlap communication and computation, reducing bursty network communication. Moreover, Poseidon uses a hybrid communication scheme that
optimizes the number of bytes required to synchronize
each layer, according to layer properties and the number of machines. We show that Poseidon is applicable
to different DL frameworks by plugging Poseidon into
Caffe and TensorFlow. We show that Poseidon enables
Caffe and TensorFlow to achieve 15.5x speed-up on
16 single-GPU machines, even with limited bandwidth
(10GbE) and the challenging VGG19-22K network for
image classification. Moreover, Poseidon-enabled TensorFlow achieves 31.5x speed-up with 32 single-GPU
machines on Inception-V3, a 50% improvement over the
open-source TensorFlow (20x speed-up).
",16.678067442207542,15.939489795918366,199
ATC_17_015.txt,15.498279670224122,13.461863494880358,ATC,7381,"
Recent advances in storage (e.g., DDR4, SSD, NVM)
and accelerators (e.g., GPU, Xeon-Phi, FPGA) provide
the opportunity to efficiently process large-scale graphs
on a single machine. In this paper, we present Garaph,
a GPU-accelerated graph processing system on a single
machine with secondary storage as memory extension.
Garaph is novel in three ways. First, Garaph proposes a vertex replication degree customization scheme that
maximizes the GPU utilization given vertices’ degrees
and space constraints. Second, Garaph adopts a balanced
edge-based partition ensuring work balance over CPU
threads, and also a hybrid of notify-pull and pull computation models optimized for fast graph processing on
the CPU. Third, Garaph uses a dynamic workload assignment scheme which takes into account both characteristics of processing elements and graph algorithms. Our
evaluation with six widely used graph applications on
seven real-world graphs shows that Garaph significantly
outperforms existing state-of-art CPU-based and GPUbased graph processing systems, getting up to 5.36x
speedup over the fastest among them.
",16.26309291913925,15.614047619047621,172
ATC_17_016.txt,15.842052363570328,13.64593166290885,ATC,8109,"
Dynamic tainting tracks the influence of certain inputs
(taint sources) through execution and it is a powerful
tool for information flow analysis and security. Taint
tracking has primarily targeted CPU program executions.
Motivated by recent recognition of information leaking
in GPU memory and GPU-resident malware, this paper
presents the first design and prototype implementation of
a taint tracking system on GPUs. Our design combines
a static binary instrumentation with dynamic tainting at
runtime. We present new performance optimizations by
exploiting unique GPU characteristics—a large portion
of instructions on GPU runtime parameters and constant
memory can be safely eliminated from taint tracking;
large GPU register file allows fast maintenance of a hot
portion of the taint map. Experiments show that these
techniques improved the GPU taint tracking performance
by 5 to 20 times for a range of image processing, data
encryption, and deep learning applications. We further
demonstrate that GPU taint tracking can enable zeroing
sensitive data to minimize information leaking as well as
identifying and countering GPU-resident malware.
",17.93193317476759,16.571060985797832,172
ATC_17_017.txt,16.85318252631207,16.273649695153164,ATC,9769,"
Modern hypervisor designs for both ARM and x86
virtualization rely on running an operating system kernel,
the hypervisor OS kernel, to support hypervisor functionality.
While x86 hypervisors effectively leverage architectural
support to run the kernel, existing ARM hypervisors map
poorly to the virtualization features of the ARM architecture,
resulting in worse performance. We identify the key reason
for this problem is the need to multiplex kernel mode state
between the hypervisor and virtual machines, which each
run their own kernel. To address this problem, we take a
fundamentally different approach to hypervisor design that
runs the hypervisor together with its OS kernel in a separate
CPU mode from kernel mode. Using this approach, we
redesign KVM/ARM to leverage a separate ARM CPU
mode for running both the hypervisor and its OS kernel.
We show what changes are required in Linux to implement
this on current ARM hardware as well as how newer ARM
architectural support can be used to support this approach
without any changes to Linux other than to KVM/ARM
itself. We show that our redesign and optimizations can
result in an order of magnitude performance improvement for
KVM/ARM, and can provide faster performance than x86 on
key hypervisor operations. As a result, many aspects of our
design have been successfully merged into mainline Linux.
",16.52667757954773,16.207711187214613,220
ATC_17_018.txt,14.800144850241193,12.974950078757956,ATC,6904,"
Public cloud software marketplaces already offer users a
wealth of choice in operating systems, database management systems, financial software, and virtual networking, all deployable and configurable at the click of a button. Unfortunately, this level of customization has not
extended to emerging hypervisor-level services, partly
because traditional virtual machines (VMs) are fully controlled by only one hypervisor at a time. Currently, a VM
in a cloud platform cannot concurrently use hypervisorlevel services from multiple third-parties in a compartmentalized manner. We propose the notion of a multihypervisor VM, which is an unmodified guest that can simultaneously use services from multiple coresident, but
isolated, hypervisors. We present a new virtualization architecture, called Span virtualization, that leverages nesting to allow multiple hypervisors to concurrently control
a guest’s memory, virtual CPU, and I/O resources. Our
prototype of Span virtualization on the KVM/QEMU
platform enables a guest to use services such as introspection, network monitoring, guest mirroring, and hypervisor refresh, with performance comparable to traditional nested VMs.
",20.425298281703412,20.079518072289158,168
ATC_17_019.txt,15.096187278703951,12.566159022311322,ATC,7437,"
Data centers are evolving to host heterogeneous workloads on shared clusters to reduce the operational cost
and achieve higher resource utilization. However, it is
challenging to schedule heterogeneous workloads with
diverse resource requirements and QoS constraints. On
the one hand, latency-critical jobs need to be scheduled
as soon as they are submitted to avoid any queuing delays. On the other hand, best-effort long jobs should
be allowed to occupy the cluster when there are idle
resources to improve cluster utilization. The challenge
lies in how to minimize the queuing delays of short jobs
while maximizing cluster utilization. Existing solutions
either forcibly kill long jobs to guarantee low latency for
short jobs or disable preemption to optimize utilization.
Hybrid approaches with resource reservations have been
proposed but need to be tuned for specific workloads.

In this paper, we propose and develop Bic-C, a
container-based resource management framework for
Big Data cluster computing. The key design is to leverage lightweight virtualization, a.k.a, containers to make
tasks preemptable in cluster scheduling. We devise two
types of preemption strategies: immediate and graceful preemptions and show their effectiveness and tradeoffs with loosely-coupled MapReduce workloads as well
as iterative, in-memory Spark workloads. Based on the
mechanisms for task preemption, we further develop a
preemptive fair share cluster scheduler. We have implemented BIG-C in YARN. Our evaluation with synthetic
and production workloads shows that low-latency and
high utilization can be both attained when scheduling
heterogeneous workloads on a contended cluster.
",15.299343439825492,13.47147410358566,254
ATC_17_020.txt,16.780388724211193,15.424456352762594,ATC,2553,"
When synchronization primitives such as locking and
read-copy update (RCU) execute within virtual machines (VMs), preemption can cause multi-second latency spikes, increasing peak memory footprint and fragmentation inside VMs, which in turn may trigger swapping or VM ballooning. The resulting CPU utilization
and memory footprint increases can negate the serverconsolidation benefits of virtualization. Although preemption of lock holders in VMs has been well-studied,
the corresponding solutions do not apply to RCU due to
its exceedingly lightweight read-side primitives.

This paper presents the first evaluation of RCU-reader
preemption in a virtualized environment. Our evaluation
shows 50% increase in the peak memory footprint and
155% increase in fragmentation for a microbenchmark,
23.71% increase in average kernel CPU utilization, 2.9 x
increase in the CPU time to compute a grace period and
2.18x increase in the average grace period duration for
the Postmark benchmark.
",19.083932058031824,18.508958904109594,150
ATC_17_021.txt,16.53530059490887,15.24691294508158,ATC,8220,"
A number of security mechanisms have been proposed
to harden programs written in unsafe languages, each
of which mitigates a specific type of memory error. Intuitively, enforcing multiple security mechanisms on a
target program will improve its overall security. However,
this is not yet a viable approach in practice because the execution slowdown caused by various security mechanisms
is often non-linearly accumulated, making the combined
protection prohibitively expensive; further, most security
mechanisms are designed for independent or isolated uses
and thus are often in conflict with each other, making it
impossible to fuse them in a straightforward way.

In this paper, we present BUNSHIN, an N-versionbased system that enables different and even conflicting
security mechanisms to be combined to secure a program
while at the same time reducing the execution slowdown.
In particular, we propose an automated mechanism to
distribute runtime security checks in multiple program
variants in such a way that conflicts between security
checks are inherently eliminated and execution slowdown
is minimized with parallel execution. We also present
an N-version execution engine to seamlessly synchronize
these variants so that all distributed security checks work
together to guarantee the security of a target program.
",21.493009986710348,21.356767676767678,199
ATC_17_022.txt,15.491945009452667,14.291596197577878,ATC,7919,"
Trusted execution support in modern CPUs, as offered by
Intel SGX enclaves, can protect applications in untrusted
environments. While prior work has shown that legacy
applications can run in their entirety inside enclaves, this
results in a large trusted computing base (TCB). Instead,
we explore an approach in which we partition an application and use an enclave to protect only security-sensitive
data and functions, thus obtaining a smaller TCB.
We describe Glamdring, the first source-level partitioning framework that secures applications written in
C using Intel SGX. A developer first annotates securitysensitive application data. Glamdring then automatically
partitions the application into untrusted and enclave
parts: (i) to preserve data confidentiality, Glamdring uses
dataflow analysis to identify functions that may be exposed to sensitive data; (ii) for data integrity, it uses backward slicing to identify functions that may affect sensitive
data. Glamdring then places security-sensitive functions
inside the enclave, and adds runtime checks and cryptographic operations at the enclave boundary to protect it
from attack. Our evaluation of Glamdring with the Memcached store, the LibreSSL library, and the Digital Bitbox
bitcoin wallet shows that it achieves small TCB sizes and
has acceptable performance overheads.
",17.451712890111917,18.208644688644686,196
ATC_17_023.txt,14.103384218730401,12.141580495682451,ATC,8283,"
Feature-rich mass-market operating systems have large
trusted computing bases (TCBs) and a long history of
vulnerabilities. Systems like Overshadow, InkTag or
Haven attempt to remove the operating system (OS) from
the TCB of applications while retaining its functionality.
However, the untrusted OS’s control of most physical
resources puts it in a much better position to launch sidechannel attacks than traditional unprivileged side-channel
attackers. Initial attacks focused on the page-fault channel, demonstrating significant information leakage for
three legacy applications.

We present two new side channels for an untrusted OS
which use timer interrupts and cache misses to achieve
higher temporal and spatial resolution than the page-fault
channel. We leverage the untrusted OS’s control over
hardware to reduce noise in the side channels to enable
successful attacks in just a single run of the target. We
demonstrate that our side channels enable attacks against
new SGX applications such as VC3 that were designed
not to trust the OS. We also show a new attack against
libjpeg that extracts images with two orders of magnitude more information than the page-fault channel attack.
",15.579741850924794,14.413614864864869,188
ATC_17_024.txt,14.700341548467172,12.649773919905307,ATC,3442,"
Container technology is being adopted as a mainstream
platform for IT solutions because of high degree of
agility, reusability and portability it offers. However,
there are challenges to be addressed for successful adoption. First, it is difﬁcult to establish the full pedigree
of images downloaded from public registries. Some
might have vulnerabilities introduced unintentionally
through rounds of updates by different users. Second,
non-conformance to the immutable software deployment
policies, such as those promoted by the DevOps principles, introduces vulnerabilities and the loss of control
over deployed software. In this study, we investigate
containers deployed in a production cloud to derive a
set of recommended approaches to address these challenges. Our analysis reveals evidences that (i), images
of unresolved pedigree have introduced vulnerabilities
to containers belonging to third parties; (ii), updates to
live public containers are common, defying the tenet that
deployed software is immutable; and (iii), scanning containers or images alone is insufﬁcient to eradicate vulnerabilities from public containers. We advocate for better systems support for tracking image provenance and
resolving disruptive changes to containers, and propose
practices that container users should adopt to limit the
vulnerability of their containers.
",18.903936251131103,17.7537107329843,194
ATC_17_025.txt,15.254787174112831,12.855675778718211,ATC,7496,"
Web application performance heavily relies on the hit rate
of DRAM key-value caches. Current DRAM caches statically partition memory across applications that share the
cache. This results in under utilization and limits cache hit
rates. We present Memshare, a DRAM key-value cache
that dynamically manages memory across applications.
Memshare provides a resource sharing model that guarantees reserved memory to different applications while
dynamically pooling and sharing the remaining memory
to optimize overall hit rate.

Key-value caches are typically memory capacity bound,
which leaves cache server CPU and memory bandwidth
idle. Memshare leverages these resources with a logstructured design that allows it to provide better hit rates
than conventional caches by dynamically re-partitioning
memory among applications. We implemented Memshare
and ran it on a week-long trace from a commercial memcached provider. Memshare increases the combined hit
rate of the applications in the trace from 84.7% to 90.8%,
and it reduces the total number of misses by 39.7% without significantly affecting cache throughput or latency.
Even for single-tenant applications, Memshare increases
the average hit rate of the state-of-the-art key-value cache
by an additional 2.7%.
",16.404322709996244,14.145916666666668,197
ATC_17_026.txt,14.58107990017238,12.71711509668085,ATC,7456,"
Recent in-memory database systems leverage advanced
hardware features like RDMA to provide transactional
processing at millions of transactions per second. Distributed transaction processing systems can scale to even
higher rates, especially for partitionable workloads. Unfortunately, these high rates are challenging to sustain
during partition reconfiguration events. In this paper, we
first show that state-of-the-art approaches would cause
notable performance disruption under fast transaction
processing. To this end, this paper presents DrTM+B,
a live reconfiguration approach that seamlessly repartitions data while causing little performance disruption to
running transactions. DrTM+B uses a pre-copy based
mechanism, where excessive data transfer is avoided by
leveraging properties commonly found in recent transactional systems. DrTM+B’s reconfiguration plans reduce data movement by preferring existing data replicas, while data is asynchronously copied from multiple
replicas in parallel. It further reuses the log forwarding
mechanism in primary-backup replication to seamlessly
track and forward dirty database tuples, avoiding iterative copying costs. To commit a reconfiguration plan in a
transactionally safe way, DrTM+B designs a cooperative
commit protocol to perform data and state synchronizations among replicas. Evaluation on a working system
based on DrTM+R with 3-way replication using typical
OLTP workloads like TPC-C and SmallBank shows that
DrTM+B incurs only very small performance degradation during live reconfiguration. Both the reconfiguration
time and the downtime are also minimal.
",17.015998829145012,16.12727272727273,232
ATC_17_027.txt,12.505229122004451,9.887879536092587,ATC,5068,"
Hybrid memory systems consisting of DRAM and
Non-Volatile Memory are promising to persist data fast.
The index design of existing key-value stores for hybrid
memory fails to utilize its specific performance characteristics: fast writes in DRAM, slow writes in NVM, and
similar reads in DRAM and NVM. This paper presents
HiKYV, a persistent key-value store with the central idea
of constructing a hybrid index in hybrid memory. To
support rich key-value operations efficiently, HiKV exploits the distinct merits of hash index and B+-Tree index. HiKV builds and persists the hash index in NVM to
retain its inherent ability of fast index searching. HiKV
builds the B+-Tree index in DRAM to support range
scan and avoids long NVM writes for maintaining consistency of the two indexes. Furthermore, HiKV applies
differential concurrency schemes to hybrid index and
adopts ordered-write consistency to ensure crash consistency. For single-threaded performance, HiKV outperforms the state-of-the-art NVM-based key-value stores
by reducing latency up to 86.6%, and for multi-threaded
performance, HiKV increases the throughput by up to
6.4x under YCSB workloads.
",14.90622815163357,13.103115079365082,190
ATC_17_028.txt,13.196295847233852,11.068082198319754,ATC,4902,"
We present TRIAD, a new persistent key-value (KV)
store based on Log-Structured Merge (LSM) trees.
TRIAD improves LSM KV throughput by reducing the
write amplification arising in the maintenance of the
LSM tree structure. Although occurring in the background, write amplification consumes significant CPU
and I/O resources. By reducing write amplification,
TRIAD allows these resources to be used instead to improve user-facing throughput.

TRIAD uses a holistic combination of three techniques. At the LSM memory component level, TRIAD
leverages skew in data popularity to avoid frequent I/O
operations on the most popular keys. At the storage
level, TRIAD amortizes management costs by deferring
and batching multiple I/O operations. At the commit log
level, TRIAD avoids duplicate writes to storage.

We implement TRIAD as an extension of Facebook’s
RocksDB and evaluate it with production and synthetic
workloads. With these workloads, TRIAD yields up to
193% improvement in throughput. It reduces write amplification by a factor of up to 4x, and decreases the
amount of I/O by an order of magnitude.
",13.747043046817236,11.568181818181824,178
ATC_17_029.txt,15.686049864025119,14.545056179775283,ATC,6259,"
The ability to record and replay program executions
with low overhead enables many applications, such
as reverse-execution debugging, debugging of hard-toreproduce test failures, and “black box” forensic analysis of failures in deployed systems. Existing record-andreplay approaches limit deployability by recording an entire virtual machine (heavyweight), modifying the OS
kernel (adding deployment and maintenance costs), requiring pervasive code instrumentation (imposing significant performance and complexity overhead), or modifying compilers and runtime systems (limiting generality).
We investigated whether it is possible to build a practical record-and-replay system avoiding all these issues.
The answer turns out to be yes — if the CPU and operating system meet certain non-obvious constraints. Fortunately modern Intel CPUs, Linux kernels and user-space
frameworks do meet these constraints, although this has
only become true recently. With some novel optimizations, our system RR records and replays real-world lowparallelism workloads with low overhead, with an entirely user-space implementation, using stock hardware,
compilers, runtimes and operating systems. RR forms the
basis of an open-source reverse-execution debugger seeing significant use in practice. We present the design and
implementation of RR, describe its performance on a variety of workloads, and identify constraints on hardware
and operating system design required to support our approach.
",18.903936251131103,18.76334330143541,210
ATC_17_030.txt,14.471496955478685,13.613811780007431,ATC,6907,"
This paper proposes the use of machine learning techniques to make storage systems more reliable in the face
of sector errors. Sector errors are partial drive failures,
where individual sectors on a drive become unavailable,
and occur at a high rate in both hard disk drives and solid
state drives. The data in the affected sectors can only
be recovered through redundancy in the system (e.g. another drive in the same RAID) and is lost if the error
is encountered while the system operates in degraded
mode, e.g. during RAID reconstruction.

In this paper, we explore a range of different machine
learning techniques and show that sector errors can be
predicted ahead of time with high accuracy. Prediction is
robust, even when only little training data or only training data for a different drive model is available. We also
discuss a number of possible use cases for improving
storage system reliability through the use of sector error
predictors. We evaluate one such use case in detail: We
show that the mean time to detecting errors (and hence
the window of vulnerability to data loss) can be greatly
reduced by adapting the speed of a scrubber based on
error predictions.
",14.348710955821954,14.827796123474517,202
ATC_17_031.txt,14.94437977177897,12.89125854594975,ATC,6129,"
We present a new technique, H3, for reproducing
Heisenbugs in production runs on commercial hardware.
H3 integrates the hardware control flow tracing capability provided in recent Intel processors with symbolic
constraint analysis. Compared to a state-of-the-art solution, CLAP, this integration allows H3 to reproduce
failures with much lower runtime overhead and much
more compact trace. Moreover, it allows us to develop
a highly effective core-based constraint reduction technique that significantly reduces the complexity of the
generated symbolic constraints. H3 has been implemented for C/C++ and evaluated on both popular benchmarks and real-world applications. It reproduces realworld Heisenbugs with overhead ranging between 1.4%23.4%, up to 8X more efficient than CLAP, and incurs
only 4.9% runtime overhead on PARSEC benchmarks.
",16.728156217252725,14.592559055118112,129
ATC_17_032.txt,14.642724376502208,12.989243530152926,ATC,7414,"
Multi-Version Execution (MVE) deploys multiple versions of the same program, typically synchronizing their
execution at the level of system calls. By default, MVE
requires all deployed versions to issue the same sequence
of system calls, which limits the types of versions which
can be deployed.

In this paper, we propose a Domain-Specific Language (DSL) to reconcile expected divergences between
different program versions deployed through MVE. We
evaluate the DSL by adding it to an existing MVE system (Varan) and testing it via three scenarios: (1) deploying the same program under different configurations,
(2) deploying different releases of the same program, and
(3) deploying dynamic analyses in parallel with the native execution. We also present an algorithm to automatically extract DSL rules from pairs of system call traces.
Our results show that each scenario requires a small number of simple rules (at most 14 rules in each case) and that
writing DSL rules can be partially automated.
",16.11434528070225,15.442025316455702,159
ATC_17_033.txt,14.30093382909072,12.524031437072427,ATC,7560,"
The performance of an OS’s networking stack can be
measured by its achieved throughput, CPU utilization,
latency, and per-flow fairness. To be able to drive increasing line-rates at 10Gbps and beyond, modern OS
networking stacks rely on a number of important hardware and software optimizations, including but not limited to using multiple transmit and receive queues and
segmentation offloading. Unfortunately, we have observed that these optimizations lead to substantial flowlevel unfairness.

We describe Titan, an extension to the Linux networking stack that systematically addresses unfairness arising
in different operating conditions. Across both fine and
coarse timescales and when NIC queues are undersubscribed and oversubscribed, we find that the Titan can
reduce unfairness by 58% or more when compared with
the best performing Linux configuration. We also find
that improving fairness can lead to a reduction in tail
flow completion times for flows in an all-to-all shuffle
in a cluster of servers.
",17.315433740611066,15.34435483870968,157
ATC_17_034.txt,14.361759988170036,12.546572260163895,ATC,7452,"
Crowdsourcing mobile user’s network performance
has become an effective way of understanding and improving mobile network performance and user qualityof-experience. However, the current measurement
method is still based on the landline measurement
paradigm in which a measurement app measures the path
to fixed (measurement or web) servers. In this work, we
introduce a new paradigm of measuring per-app mobile
network performance. We design and implement MopEye, an Android app to measure network round-trip delay for each app whenever there is app traffic. This opportunistic measurement can be conducted automatically
without user intervention. Therefore, it can facilitate a
large-scale and long-term crowdsourcing of mobile network performance. In the course of implementing MopEye, we have overcome a suite of challenges to make
the continuous latency monitoring lightweight and accurate. We have deployed MopEye to Google Play for
an IRB-approved crowdsourcing study in a period of ten
months, which obtains over five million measurements
from 6,266 Android apps on 2,351 smartphones. The
analysis reveals a number of new findings on the per-app
network performance and mobile DNS performance.
",15.760457277294734,13.23842490842491,186
ATC_17_035.txt,15.732027447692403,14.240159309748243,ATC,5959,"
Due to their performance and flexibility, FPGAs are an
attractive platform for the execution of network functions. It has been a challenge for a long time though
to make FPGA programming accessible to a large audience of developers. An appealing solution is to compile
code from a general-purpose language to hardware using
high-level synthesis. Unfortunately, current approaches
to implement rich network functionality are insufficient
because they lack: (i) libraries with abstractions for common network operations and data structures, (ii) bindings
to the underlying “substrate” on the FPGA, and (iii) debugging and profiling support.
This paper describes Emu, a new standard library for
an FPGA hardware compiler that enables developers to
rapidly create and deploy network functionality. Emu allows for high-performance designs without being bound
to particular packet processing paradigms. Furthermore,
it supports running the same programs on CPUs, in
Mininet, and on FPGAs, providing a better development environment that includes advanced debugging capabilities. We demonstrate that network functions implemented using Emu have only negligible resource and
performance overheads compared with natively-written
hardware versions.
",16.827784334635936,15.838750000000001,178
ATC_17_036.txt,13.86626420597387,11.957903408077087,ATC,7798,"
Virtual cloud network services let users have their own
private networks in the public cloud. IPsec gateways are
growing in importance accordingly as they provide VPN
connections for customers to remotely access these private networks. Major cloud providers offer IPsec gateway functions to tenants using virtual machines (VMs)
running a software IPsec gateway inside. However, dedicating individual IPsec gateway VMs to each tenant results in significant resource waste due to the strong isolation mechanism of VMs.

In this paper, we design Protego, a distributed IPsec
gateway service designed for multitenancy. By separating the control plane and the data plane of an IPsec
gateway, Protego achieves high availability with active
redundancy. Furthermore, Protego elastically scales in
and out by seamlessly migrating IPsec tunnels between
the data nodes without compromising their throughput.
Our evaluation and simulation based on production data
show that Protego together with a simple resource provisioning algorithm saves more than 80% of the resources
compared with allocating independent VMs.
",15.579741850924794,15.367500000000003,161
ATC_17_037.txt,15.325249324791049,13.265135262666231,ATC,7213,"
Recent approximation algorithms (e.g., CounterStacks,
SHARDS and AET) make lightweight, continuouslyupdated miss ratio curves (MRCs) practical for online
modeling and control of LRU caches. For more complex
cache-replacement policies, scaled-down simulation, introduced with SHARDS, offers a general method for emulating a given cache size by using a miniature cache processing a small spatially-hashed sample of requests.

We present the first detailed study evaluating the effectiveness of this approach for modeling non-LRU algorithms, including ARC, LIRS and OPT. Experiments
with over a hundred real-world traces demonstrate that
scaled-down MRCs are extremely accurate while requiring dramatically less space and time than full simulation.

We propose an efficient, generic framework for dynamic optimization using multiple scaled-down simulations to explore candidate cache configurations simultaneously. Experiments demonstrate significant improvements from automatic adaptation of parameters including the stack size limit in LIRS, and queue sizes in 2Q.

Finally, we introduce SLIDE, a new approach inspired
by Talus that uses scaled-down MRCs to remove performance cliffs automatically. SLIDE performs shadow
partitioning transparently within a single unified cache,
avoiding the problem of migrating state between distinct
caches when partition boundaries change. Experiments
demonstrate that SLIDE improves miss ratios for many
cache policies, with large gains in the presence of cliffs.
",18.903936251131103,17.51702380952381,212
ATC_17_038.txt,14.30394559478821,12.799355901038002,ATC,7742,"
Today’s web applications rely heavily on caching to reduce latency and backend load, using services like Redis
or Memcached that employ inflexible caching algorithms.
But the needs of each application vary, and significant performance gains can be achieved with a tailored strategy,
e.g., incorporating cost of fetching, expiration time, and
so forth. Existing strategies are fundamentally limited,
however, because they rely on data structures to maintain
a total ordering of the cached items.

Inspired by Redis’s use of random sampling for eviction (in lieu of a data structure) and recent theoretical justification for this approach, we design a new caching algorithm for web applications called hyperbolic caching. Unlike prior schemes, hyperbolic caching decays item priorities at variable rates and continuously reorders many items
at once. By combining random sampling with lazy evaluation of the hyperbolic priority function, we gain complete
flexibility in customizing the function. For example, we
describe extensions that incorporate item cost, expiration
time, and windowing. We also introduce the notion of a
cost class in order to measure the costs and manipulate the
priorities of all items belonging to a related group.

We design a hyperbolic caching variant for several production systems from leading cloud providers. We implement our scheme in Redis and the Django web framework.
Using real and simulated traces, we show that hyperbolic
caching reduces miss rates by ~10-20% over competitive
baselines tailored to the application, and improves end-toend throughput by ~5-10%.
",16.133371291317395,15.460022446689116,245
ATC_17_039.txt,13.969126861187881,12.436941743716382,ATC,6382,"
Control planes of cloud frameworks trade off between
scheduling granularity and performance. Centralized
systems schedule at task granularity, but only schedule
a few thousand tasks per second. Distributed systems
schedule hundreds of thousands of tasks per second but
changing the schedule is costly.
We present execution templates, a control plane abstraction that can schedule hundreds of thousands of
tasks per second while supporting fine-grained, per-task
scheduling decisions. Execution templates leverage a
program’s repetitive control flow to cache blocks of
frequently-executed tasks. Executing a task in a template
requires sending a single message. Large-scale scheduling changes install new templates, while small changes
apply edits to existing templates.
Evaluations of execution templates in Nimbus, a
data analytics framework, find that they provide the
fine-grained scheduling flexibility of centralized control
planes while matching the strong scaling of distributed
ones. Execution templates support complex, real-world
applications, such as a fluid simulation with a triply
nested loop and data dependent branches.
",13.731508374201272,13.136172839506173,164
ATC_17_040.txt,15.426865362138287,13.710927840403347,ATC,7038,"
Software projects that use a compiled language are built
hundreds of thousands of times during their lifespan.
Hence, the compiler is invoked over and over again on an
incrementally changing source base. As previous work
has shown, up to 97 percent of these invocations are redundant and do not lead to an altered compilation result.
In order to avoid such redundant builds, many developers
use caching tools that are based on textual hashing of the
source files. However, these tools fail in the presence of
modifications that leave the compilation result unchanged.
Especially for C projects, where module-interface definitions are imported textually with the C preprocessor,
modifications to header files lead to many redundant compilations.

In this paper, we present the cHash approach and compiler extension to quickly detect modifications on the
language level that will not lead to a changed compilation
result. By calculating a hash over the abstract syntax tree,
we achieve a high precision at comparatively low costs.
While cHash is light-weight and build system agnostic,
it can cancel 80 percent of all compiler invocations early
and reduce the build-time of incremental builds by up to
51 percent. In comparison to the state-of-the-art CCache
tool, cHash is at least 30 percent more precise in detecting
redundant compilations.
",13.968273953766033,12.500666666666671,217
ATC_17_041.txt,13.644598463511748,12.367878853398853,ATC,7490,"
Microsoft Azure Storage is a global cloud storage system
with a footprint in 38 geographic regions. To protect
customer data against catastrophic data center failures, it
optionally replicates data to secondary DCs hundreds of
miles away. Using Microsoft OneDrive as an example,
this paper illustrates the characteristics of typical cloud
storage workloads and the opportunity to lower storage
cost for geo-redundancy with erasure coding.

The paper presents the design, implementation and
evaluation of Giza — a strongly consistent, versioned object store that applies erasure coding across global data
centers. The key technical challenge Giza addresses is
to achieve single cross-DC round trip latency for the
common contention-free workload, while also maintaining strong consistency when there are conflicting access.
Giza addresses the challenge with a novel implementation of well-known distributed consensus algorithms
tailored for restricted cloud storage APIs. Giza is deployed to 11 DCs across 3 continents and experimental
results demonstrate that it achieves our design goals.
",17.315433740611066,18.048144654088052,160
ATC_17_042.txt,14.479396534829888,12.38619179152795,ATC,6813,"
Fast query services are important to improve overall performance of large-scale storage systems when handling
a large number of files. Open-addressing cuckoo hash
schemes have been widely used to support query services
due to the salient features of simplicity and ease of
use. Conventional schemes are unfortunately inadequate
to address the potential problem of having endless
loops during item insertion, which degrades the query
performance. To address the problem, we propose a costefficient cuckoo hashing scheme, named SmartCuckoo.
The idea behind SmartCuckoo is to represent the hashing
relationship as a directed pseudoforest and use it to
track item placements for accurately predetermining
the occurrence of endless loop. SmartCuckoo can
efficiently predetermine insertion failures without paying
a high cost of carrying out step-by-step probing. We
have implemented SmartCuckoo in a large-scale cloud
storage system. Extensive evaluations using three realworld traces and the YCSB benchmark demonstrate
the efficiency and efficacy of SmartCuckoo. We have
released the source code of SmartCuckoo for public use.
",15.021129683784007,13.212971887550204,167
ATC_17_043.txt,13.930668464705018,12.253201011521153,ATC,8044,"
We propose repair pipelining, a technique that speeds up
the repair performance in general erasure-coded storage.
By pipelining the repair of failed data in small-size units
across storage nodes, repair pipelining reduces the repair
time to approximately the same as the normal read time
to the same amount of data in homogeneous environments. We further extend repair pipelining for heterogeneous environments. We implement a repair pipelining prototype called ECPipe and integrate it as a middleware system into two open-source distributed storage
systems HDFS and QES. Experiments on a local testbed
and Amazon EC2 show that repair pipelining significantly improves the performance of both degraded reads
and full-node recovery over existing repair techniques.
",16.647925096878797,16.244206896551727,117
ATC_17_044.txt,14.148317110815729,12.705682953124128,ATC,3140,"
Erasure coding (EC) has been widely used in cloud
storage systems because it effectively reduces storage
redundancy while providing the same level of durability.
However, EC introduces significant overhead to small
write operations which perform partial write to an entire
EC group. This has been a major barrier for EC to
be widely adopted in small-write-intensive systems such
as virtual disk service. Parity logging (PL) appends
parity changes to a journal to accelerate partial writes.
However, since previous PL schemes have to perform a
time-consuming write-after-read for each partial write,
i.e., read the current value of the data and then compute
and write the parity delta, their write performance is still
much lower than that of replication-based storage.
This paper presents PARI X, a speculative partial write
scheme for fast parity logging. We transform the original
formula of parity calculation, so as to use the data deltas
(between the current/original data values), instead of
the parity deltas, to calculate the parities during journal
replay. For each partial write, this allows PARI X to
speculatively log only the current value of the data. The
original value is needed only once in a journal when
performing the first write to the data. For a series of
n partial writes to the same data, PARI X performs pure
write (instead of write-after-read) for the last n − 1 ones
while only introducing a small penalty of an extra network RTT (round-trip time) to the first one. Evaluation
results show that PARI X remarkably outperforms stateof-the-art PL schemes in partial write performance.
",14.554592549557764,13.736801346801347,271
ATC_17_045.txt,14.799372854733818,12.454664046034402,ATC,7143,"
Energy-based billing as well as energy-efficient software require accurate knowledge of energy consumption.
Model-based energy accounting and external measurement hardware are the main methods to obtain energy
data, but cost and the need for frequent recalibration have
impeded their large-scale adoption. Running Average
Power Limit (RAPL) by Intel® enables non-intrusive,
off-the-shelf energy monitoring, but only on a per-socket
level. To enable apportioning of energy to individual applications we present E-Team, a non-intrusive,
scheduler-based, easy-to-use energy-accounting mechanism. By leveraging RAPL, our method can be used on
any Intel system built after 2011 without the need for external infrastructure, application modification, or model
calibration. E-Team allows starting and stopping measurements at arbitrary points in time while maintaining a
low performance overhead. E-Team provides high accuracy, compared to external instrumentation, with an error
of less than 3.5 %.
",17.451712890111917,15.328817407757807,152
ATC_17_046.txt,14.296508354672088,13.002615376826604,ATC,8691,"
Application scalability is a critical aspect to efficiently
use NUMA machines with many cores. To achieve that,
various techniques ranging from task placement to data
sharding are used in practice. However, from the perspective of an operating system, these techniques often do not
work as expected because various subsystems in the OS
interact and share data structures among themselves, resulting in scalability bottlenecks. Although current OSes
attempt to tackle this problem by introducing a wide range
of synchronization primitives such as spinlock and mutex, the widely used synchronization mechanisms are not
designed to handle both under- and over-subscribed scenarios in a scalable fashion. In particular, the current
blocking synchronization primitives that are designed to
address both scenarios are NUMA oblivious, meaning
that they suffer from cache-line contention in an undersubscribed situation, and even worse, inherently spur long
scheduler intervention, which leads to sub-optimal performance in an over-subscribed situation.

In this work, we present several design choices to implement scalable blocking synchronization primitives that
can address both under- and over-subscribed scenarios.
Such design decisions include memory-efficient NUMAaware locks (favorable for deployment) and schedulingaware, scalable parking and wake-up strategies. To validate our design choices, we implement two new blocking
synchronization primitives, which are variants of mutex
and read-write semaphore in the Linux kernel. Our evaluation shows that these locks can scale real-world applications by 1.2-1.6x and some of the file system operations
up to 4.7x in both under- and over-subscribed scenarios.
Moreover, they use 1.5—-10x less memory than the stateof-the-art NUMA-aware locks on a 120-core machine.
",17.467979349516824,17.27259259259259,275
ATC_17_047.txt,14.210583711579421,12.665152976570806,ATC,6900,"
Stream analytics on real-time events has an insatiable demand for throughput and latency. Its performance on a
single machine is central to meeting this demand, even in
a distributed system. This paper presents a novel stream
processing engine called StreamBox that exploits the
parallelism and memory hierarchy of modern multicore
hardware. StreamBox executes a pipeline of transforms
over records that may arrive out-of-order. As records arrive, it groups the records into ordered epochs delineated
by watermarks. A watermark guarantees no subsequent
record’s event timestamp will precede it.

Our contribution is to produce and manage abundant parallelism by generalizing out-of-order record processing within each epoch to out-of-order epoch processing and by dynamically prioritizing epochs to optimize latency. We introduce a data structure called cascading containers, which dynamically manages concurrency and dependences among epochs in the transform
pipeline. StreamBox creates sequential memory layout
of records in epochs and steers them to optimize NUMA
locality. On a 56-core machine, StreamBox processes
records up to 38 GB/sec (38M Records/sec) with 50 ms
latency.
",14.554592549557764,13.804407821229052,181
ATC_17_048.txt,14.635584396666587,12.498376027339923,ATC,4948,"
Graph processing systems are used in a wide variety of
fields, ranging from biology to social networks, and a
large number of such systems have been described in the
recent literature. We perform a systematic comparison
of various techniques proposed to speed up in-memory
multicore graph processing. In addition, we take an endto-end view of execution time, including not only algorithm execution time, but also pre-processing time and
the time to load the graph input data from storage.

More specifically, we study various data structures to
represent the graph in memory, various approaches to
pre-processing and various ways to structure the graph
computation. We also investigate approaches to improve
cache locality, synchronization, and NUMA-awareness.
In doing so, we take our inspiration from a number of
graph processing systems, and implement the techniques
they propose in a single system. We then selectively enable different techniques, allowing us to assess their benefits in isolation and independent of unrelated implementation considerations.

Our main observation is that the cost of pre-processing
in many circumstances dominates the cost of algorithm
execution, calling into question the benefits of proposed
algorithmic optimizations that rely on extensive preprocessing. Equally surprising, using radix sort turns
out to be the most efficient way of pre-processing the
graph input data into adjacency lists, when the graph input data is already in memory or is loaded from fast storage. Furthermore, we adapt a technique developed for
out-of-core graph processing, and show that it significantly improves cache locality. Finally, we demonstrate
that NUMA-awareness and its attendant pre-processing
costs are beneficial only on large machines and for certain algorithms.
",17.845785984406827,16.34183135704875,277
ATC_17_049.txt,15.531306060372259,13.71722186264298,ATC,8107,"Intel SGX hardware enables applications to protect themselves from potentially-malicious OSes or hyper- visors. In cloud computing and other systems, many users and applications could benefit from SGX. Unfortu- nately, current applications will not work out-of-the-box on SGX. Although previous work has shown that a li- brary OS can execute unmodified applications on SGX, a belief has developed that a library OS will be ruinous for performance and TCB size, making application code modification an implicit prerequisite to adopting SGX.
This paper demonstrates that these concerns are exag- gerated, and that a fully-featured library OS can rapidly deploy unmodified applications on SGX with overheads comparable to applications modified to use “shim” lay- ers. We present a port of Graphene to SGX, as well as a number of improvements to make the security bene- fits of SGX more usable, such as integrity support for dynamically-loaded libraries, and secure multi-process support. Graphene-SGX supports a wide range of un- modified applications, including Apache, GCC, and the R interpreter. The performance overheads of Graphene- SGX range from matching a Linux process to less than 2× in most single-process cases; these overheads are largely attributable to current SGX hardware or missed opportunities to optimize Graphene internals, and are not necessarily fundamental to leaving the application un- modified. Graphene-SGX is open-source and has been used concurrently by other groups for SGX research.
",21.344311912025617,21.298487394957984,237
ATC_17_050.txt,14.875922410659523,13.554998065533674,ATC,7827,"
How to preserve users’ privacy while supporting high-utility
analytics for low-latency stream processing?

To answer this question: we describe the design,
implementation and evaluation of PRIVAPPROX, a data
analytics system for privacy-preserving stream processing.
PRIVAPPROX provides three important properties: (i)
Privacy: zero-knowledge privacy guarantee for users, a
privacy bound tighter than the state-of-the-art differential
privacy; (ii) Utility: an interface for data analysts to
systematically explore the trade-offs between the output
accuracy (with error estimation) and the query execution
budget; (iii) Latency: near real-time stream processing based
on a scalable “synchronization-free” distributed architecture.

The key idea behind our approach is to marry two techniques together, namely, sampling (used for approximate
computation) and randomized response (used for privacypreserving analytics). The resulting marriage is complementary — it achieves stronger privacy guarantees, and also
improves the performance for stream analytics.
",20.829396946661657,20.13379310344828,146
ATC_17_051.txt,14.946909199031175,13.884730035927006,ATC,7431,"
A popular community repository such as Docker Hub,
PyPI, or RubyGems distributes tens of thousands of software projects to millions of users. The large number of
projects and users make these repositories attractive targets for exploitation. After a repository compromise, a
malicious party can launch a number of attacks on unsuspecting users, including rollback attacks that revert
projects to obsolete and vulnerable versions. Unfortunately, due to the rapid rate at which packages are updated, existing techniques that protect against rollback
attacks would cause each user to download 2-3 times the
size of an average package in metadata each month, making them impractical to deploy.

In this work, we develop a system called Mercury that
uses a novel technique to compactly disseminate version information while still protecting against rollback
attacks. Due to a different technique for dealing with
key revocation, users are protected from rollback attacks,
even if the software repository is compromised. This
technique is bandwidth-efficient, especially when delta
compression is used to transmit only the differences between previous and current lists of version information.
An analysis we performed for the Python community
shows that once Mercury is deployed on PyPI, each user
will only download metadata each month that is about
3.5% the size of an average package. Our work has been
incorporated into the latest versions of TUF, which is being integrated by Haskell, OCaml, RubyGems, Python,
and CoreOS, and is being used in production by LEAP,
Flynn, and Docker.
",16.32212239822248,16.35874316939891,246
ATC_17_052.txt,15.613653632860277,14.436674614031663,ATC,7524,"
Discovering the security vulnerabilities of commercial
off-the-shelf (COTS) operating systems (OSes) is challenging because they not only are huge and complex, but
also lack detailed debug information. Concolic testing,
which generates all feasible inputs of a program by using
symbolic execution and tests the program with the generated inputs, is one of the most promising approaches
to solve this problem. Unfortunately, the state-of-the-art
concolic testing tools do not scale well for testing COTS
OSes because of state explosion. Indeed, they often fail
to find a single bug (or crash) in COTS OSes despite their
long execution time.

In this paper, we propose CAB-FUZZ (Context-Aware
and Boundary-focused), a practical concolic testing tool
to quickly explore interesting paths that are highly likely
triggering real bugs without debug information. First,
CAB-FUuUZz prioritizes the boundary states of arrays
and loops, inspired by the fact that many vulnerabilities
originate from a lack of proper boundary checks. Second, CAB-Fuzz exploits real programs interacting with
COTS OSes to construct proper contexts to explore deep
and complex kernel states without debug information. We
applied CAB-FUZZ to Windows 7 and Windows Server
2008 and found 2/ undisclosed unique crashes, including two local privilege escalation vulnerabilities (CVE2015-6098 and CVE-2016-0040) and one information
disclosure vulnerability in a cryptography driver (CVE2016-7219). CAB-Fuzz found vulnerabilities that are
non-trivial to discover; five vulnerabilities have existed
for 14 years, and we could trigger them even in the initial
version of Windows XP (August 2001).
",16.32212239822248,16.383802083333336,257
ATC_17_053.txt,13.20190855032514,10.81797773283851,ATC,7937,"
Emerging non-volatile main memory (NVMM) unlocks
the performance potential of applications by storing persistent data in the main memory. Such applications
require a lightweight persistent transactional memory
(PTM) system, instead of a heavyweight filesystem or
database, to have fast access to data. In a PTM system,
the memory usage, both capacity and bandwidth, plays a
key role in dictating performance and efficiency. Existing memory management mechanisms for PTMs generate high memory fragmentation, high write traffic and a
large number of persist barriers, since data is first written
to a log and then to the main data store.

In this paper, we present a log-structured NVMM system that not only maintains NVMM in a compact manner
but also reduces the write traffic and the number of persist barriers needed for executing transactions. All data
allocations and modifications are appended to the log
which becomes the location of the data. Further, we address a unique challenge of log-structured memory management by designing a tree-based address translation
mechanism where access granularities are flexible and
different from allocation granularities. Our results show
that the new system enjoys up to 89.9% higher transaction throughput and up to 82.8% lower write traffic than
a traditional PTM system.
",17.267425705330172,15.758871951219515,208
ATC_17_054.txt,15.31624406262705,13.454046060414864,ATC,8181,"
Fast, byte-addressable NVM promises near cache latency and near memory bus throughput for file system
operations. However, unanticipated cache line eviction
may lead to disordered metadata update and thus existing NVM file systems (NVMEFS) use synchronous cache
flushes to ensure consistency, which extends critical path
latency.

In this paper, we revisit soft updates, an intriguing
idea that eliminates most synchronous metadata updates
through delayed writes and dependency tracking, in the
context of NVMFS. We show that on one hand byteaddressability of NVM significantly simplifies dependency tracking and enforcement by allowing better directory organization and closely matching the per-pointer
dependency tracking of soft updates. On the other hand,
per-cache-line failure atomicity of NVM cannot ensure
the correctness of soft updates, which relies on block
write atomicity; page cache, which is necessary for dual
views in soft updates, becomes inefficient due to double
writes and duplicated metadata. To guarantee the correctness and consistency without synchronous cache flushes
and page cache, we propose pointer-based dual views,
which shares most data structures but uses different pointers in different views, to allow delayed persistency and
eliminate file system checking after a crash. In this
way, our system, namely SoupFS1 , significantly shortens the critical path latency by delaying almost all synchronous cache flushes. We have implemented SoupFS as
a POSIX-compliant file system for Linux and evaluated
it against state-of-the-art NVMFS like PMFS and NOVA.
Performance results show that SoupFS can have notably
lower latency and modestly higher throughput compared
to existing NVMFS.
",17.755912252390015,17.209019607843135,256
ATC_17_055.txt,14.864739340645041,12.619243516999365,ATC,7158,"
In hypervisor-based virtualization environments, translation lookaside buffers (TLBs) misses may induce twodimensional page table walks, which may incur a long
access latency, and this issue becomes worse with ever
increasing memory capacity. To reduce the overhead
of TLB misses, large pages (e.g., 2M-pages) are widely
supported in modern hardware platforms to reduce the
number of page table entries. However, memory management with large pages can be inefficient in deduplication, leading to low utilization of memory, which is a
precious resource for a variety of applications.

To simultaneously enjoy benefits of high performance
by accessing memory with large pages (e.g., 2M-pages)
and high deduplication rate by managing memory with
base pages (e.g., 4K-pages), we propose Smart Memory
Deduplciation, or SmartMD in short, which is an adaptive and efficient management scheme for mixed-page
memory. Specifically, we propose two lightweight
schemes to accurately monitor pages’ access frequency
and repetition rate, and present a dynamic and adaptive conversion scheme to selectively split or reconstruct
large pages. We implement a prototype system and conduct extensive experiments with various workloads. Experiment results show that SmartMD can simultaneously
achieve high access performance similar to systems using large pages, and achieves a deduplication rate similar
to that applying aggressive deduplication scheme (i.e.,
KSM) at the same time on base pages.
",19.7143461543385,19.229953917050697,222
ATC_17_056.txt,14.968284150121264,12.876059098305962,ATC,7960,"
We develop an approach for the automatic and elastic
management of memory in shared clusters executing data
analytics applications. Our approach, called ElasticMem,
comprises a technique for dynamically changing memory
limits in Java virtual machines, models to predict memory usage and garbage collection cost, and a scheduling
algorithm that dynamically reallocates memory between
applications. Experiments with our prototype implementation show that our approach outperforms static memory
allocation leading to fewer query failures when memory is
scarce, up to 80% lower garbage collection overheads, and
up to 30% lower query times when memory is abundant.
",21.786851204258245,21.610851063829788,95
ATC_17_057.txt,15.268469308843638,13.854391512922053,ATC,8796,"
In this paper, we comprehensively investigate the file
fragmentation problem on mobile flash storage. From
our evaluation study with real Android smartphones, we
observed two interesting points on file fragmentation on
flash storage. First, defragmentation on mobile flash
storage is essential for high I/O performance on Android
smartphones because file fragmentation, which is a recurring problem (even after defragmentation), can significantly degrade I/O performance. Second, file fragmentation affects flash storage quite differently than HDDs.
When files are fragmented on flash storage, the logical fragmentation and the physical fragmentation are decoupled and a performance degradation mostly comes
from logical fragmentation. Motivated by our observations, we propose a novel defragger, janus defragger
(janusd), which supports two defraggers, janusdL for a
logical defragger and janusdP for a physical defragger.
JanusdL, which takes advantage of flash storage’s internal logical to physical mapping table, supports logical
defragmentation without data copies. JanusdL is very
effective for most fragmented files while not sacrificing
the flash lifetime. JanusdP, which is useful for physically fragmented files but requires data copies, is invoked
only when absolutely necessary. By adaptively selecting
janusdL and janusdP, janusd achieves the effect of full
file defragmentation without reducing the flash lifetime.
Our experimental results show that janusd can achieve at
least the same level of I/O performance improvement as
e4defrag without affecting the flash lifetime, thus making janusd an attractive defragmentation solution for mobile flash storage.
",16.908762484321528,16.003799533799537,236
ATC_17_058.txt,15.083918704305347,13.316651225080555,ATC,7278,"
Non-volatile memory (NVM) and remote direct memory
access (RDMA) provide extremely high performance
in storage and network hardware. However, existing
distributed file systems strictly isolate file system and
network layers, and the heavy layered software designs leave high-speed hardware under-exploited. In
this paper, we propose an RDMA-enabled distributed
persistent memory file system, Octopus, to redesign file
system internal mechanisms by closely coupling NVM
and RDMA features. For data operations, Octopus
directly accesses a shared persistent memory pool to
reduce memory copying overhead, and actively fetches
and pushes data all in clients to re-balance the load between the server and network. For metadata operations,
Octopus introduces self-identified RPC for immediate
notification between file systems and networking, and
an efficient distributed transaction mechanism for consistency. Evaluations show that Octopus achieves nearly
the raw bandwidth for large I/Os and orders of magnitude
better performance than existing distributed file systems.
",19.117987234576393,18.032124183006534,154
ATC_17_059.txt,15.071051467149328,12.8778003532104,ATC,7530,"
For data durability, many applications rely on synchronous operations such as an fsync() system call.
However, latency-sensitive synchronous operations can
be delayed under the compound transaction scheme of
the current journaling technique. Because a compound
transaction includes irrelevant data and metadata, as well
as the data and metadata of fsynced file, the latency of
an fsync call can be unexpectedly long. In this paper,
we first analyze various factors that may delay an fsync
operation, and propose a novel hybrid journaling technique, called ijournaling, which journals only the corresponding file-level transaction for an fsync call, while
recording a normal journal transaction during periodic
journaling. The file-level transaction journal has only
the related metadata updates of the fsynced file. By removing several factors detrimental to fsync latency, the
proposed technique can reduce the fsync latency, mitigate the interference between fsync-intensive threads,
and provide high manycore scalability. Experiments using a smartphone and a desktop computer showed significant improvements in fsync latency through the use of
ijournaling.
",18.08858127442927,16.808095238095238,169
ATC_17_060.txt,14.910137052850317,13.268157075009913,ATC,8356,"
Datacenters can use distributed file systems to store data
for batch processing on the same servers that run latencycritical services. Taking advantage of this storage capacity involves minimizing interference with the co-located
services, while implementing user-friendly, efficient, and
scalable file system access. Unfortunately, current systems fail one or more of these requirements, and must
be manually partitioned across independent subclusters.
Thus, in this paper, we introduce techniques for automatically and transparently scaling such file systems to entire resource-harvesting datacenters. We create a layer of
software in front of the existing metadata managers, assign servers to subclusters to minimize interference and
data movement, and smartly migrate data across subclusters in the background. We implement our techniques in
HDFS, and evaluate them using simulation of 10 production datacenters and a real 4k-server deployment. Our
results show that our techniques produce high file access
performance, and high data durability and availability,
while migrating a limited amount of data. We recently
deployed our system onto 30k servers in Bing’s datacenters, and discuss lessons from this deployment.
",16.975882523387877,16.321818181818184,178
CCGrid_17_001.txt,15.412030082604126,14.370249007309585,CCGrid,7008,"
A goal of cloud service management is to design
self-adaptable auto-scaler to react to workload fluctuations and
changing the resources assigned. The key problem is how and
when to add/remove resources in order to meet agreed servicelevel agreements. Reducing application cost and guaranteeing
service-level agreements (SLAs) are two critical factors of dynamic controller design. In this paper, we compare two dynamic
learning strategies based on a fuzzy logic system, which learns
and modifies fuzzy scaling rules at runtime. A self-adaptive fuzzy
logic controller is combined with two reinforcement learning
(RL) approaches: (i) Fuzzy SARSA learning (FSL) and (ii) Fuzzy
Q-learning (FQL). As an off-policy approach, Q-learning learns
independent of the policy currently followed, whereas SARSA
as an on-policy always incorporates the actual agent’s behavior
and leads to faster learning. Both approaches are implemented
and compared in their advantages and disadvantages, here in
the OpenStack cloud platform. We demonstrate that both autoscaling approaches can handle various load traffic situations,
sudden and periodic, and delivering resources on demand while
reducing operating costs and preventing SLA violations. The
experimental results demonstrate that FSL and FOL have acceptable performance in terms of adjusted number of virtual machine
targeted to optimize SLA compliance and response time.

Keywords-Cloud Computing; Orchestration; Controller; Fuzzy
Logic;Q-Learning; SARSA; OpenStack

",17.122413403193683,15.50773303167421,223
CCGrid_17_002.txt,16.919566844278062,16.113675085545484,CCGrid,6496,"
In this paper, we propose a data acquisition and
analysis framework for materials-to-devices processes, named
4CeeD, that focuses on the immense potential of capturing,
accurately curating, correlating, and coordinating materialsto-devices digital data in a real-time and trusted manner
before fully archiving and publishing them for wide access and
sharing. In particular, 4CeeD consists of: (i) a curation service
for collecting data from experimental instruments, curating,
and wrapping of data with extensive metadata in real-time
and in a trusted manner, (ii) a cloudlet for caching collected
data from curation service and coordinating data transfer
with the back-end, and (iii) a cloud-based coordination service
for storing data, extracting meta-data, analyzing and finding
correlations among the data. Our evaluation results show that
our proposed approach is able to help researchers significantly
save time and cost spent on experiments, and is efficient
in dealing with high-volume and fast-changing workload of
heterogeneous types of experimental data.

",23.72669961743115,28.06049689440994,162
CCGrid_17_003.txt,16.52667757954773,15.122347780126848,CCGrid,3793,"
Hybrid cloud bursting (i.e., leasing temporary offpremise cloud resources to boost the overall capacity during
peak utilization) is a popular and cost-effective way to deal
with the increasing complexity of big data analytics. It is
particularly promising for iterative MapReduce applications that
reuse massive amounts of input data at each iteration, which
compensates for the high overhead and cost of concurrent data
transfers from the on-premise to the off-premise VMs over a
weak inter-site link that is of limited capacity. In this paper
we study how to combine various MapReduce data locality
techniques designed for hybrid cloud bursting in order to achieve
scalability for iterative MapReduce applications in a cost-effective
fashion. This is a non-trivial problem due to the complex
interaction between the data movements over the weak link
and the scheduling of computational tasks that have to adapt
to the shifting data distribution. We show that using the right
combination of techniques, iterative MapReduce applications can
scale well in a hybrid cloud bursting scenario and come even close
to the scalability observed in single sites.

Keywords-Hybrid Cloud; Big Data Analytics; Data locality; /O
and Data Management; Scheduling

",18.7741,19.353717948717954,197
CCGrid_17_004.txt,15.34768225266718,13.105141713746253,CCGrid,7382,"
The emergence of Internet of Things (IoT) is
participating to the increase of data- and energy-hungry
applications. As connected devices do not yet offer enough
capabilities for sustaining these applications, users perform
computation offloading to the cloud. To avoid network bottlenecks and reduce the costs associated to data movement, edge
cloud solutions have started being deployed, thus improving the
Quality of Service. In this paper, we advocate for leveraging
on-site renewable energy production in the different edge cloud
nodes to green IoT systems while offering improved QoS
compared to core cloud solutions. We propose an analytic
model to decide whether to offload computation from the
objects to the edge or to the core Cloud, depending on the
renewable energy availability and the desired application QoS.
This model is validated on our application use-case that deals
with video stream analysis from vehicle cameras.

",17.693802365651003,15.888793103448279,146
CCGrid_17_005.txt,14.730988012329881,14.5079533777492,CCGrid,6233,"
In the large-scale Wireless Metropolitan Area Network (WMAN) consisting of many wireless Access Points (APs),
choosing the appropriate position to place cloudlet is very important for reducing the user’s access delay. For service provider, it is
always very costly to deployment cloudlets. How many cloudlets
should be placed in a WMAN and how much resource each
cloudlet should have is very important for the service provider.
In this paper, we study the cloudlet placement and resource
allocation problem in a large-scale Wireless WMAN, we formulate
the problem as an novel cloudlet placement problem that given
an average access delay between mobile users and the cloudlets,
place K cloudlets to some strategic locations in the WMAN with
the objective to minimize the number of use cloudlet K. We
then propose an exact solution to the problem by formulating
it as an Integer Linear Programming (ILP). Due to the poor
scalability of the ILP, we devise a clustering algorithm K-Medoids
(KM) for the problem. For a special case of the problem where
all cloudlets computing capabilities have been given, we propose
an efficient heuristic for it. We finally evaluate the performance
of the proposed algorithms through experimental simulations.
Simulation result demonstrates that the proposed algorithms are
effective.

Keywords— Cloudlet placement, cloudlet number minimization, resource allocation, heuristic algorithms.

",15.112257680678326,14.076220183486239,220
CCGrid_17_006.txt,14.813165584870244,12.486864614511958,CCGrid,2769,"
We analyze YARN container overhead and present
early results of reducing its overhead by dynamically adjusting
the input split size. YARN is designed as a generic resource
manager that decouples programming models from resource
management infrastructures. We demonstrate that YARN’s
generic design incurs significant overhead because each container must perform various initialization steps, including
authentication. To reduce container overhead without changing
the existing YARN framework significantly, we propose leveraging the input split, which is the logical representation of physical
HDFS blocks. With input splits, we can combine multiple
HDFS blocks and increase the input size of each container,
thereby enabling a single map wave and reducing the number
of containers and their initialization overhead. Experimental
results shows that we can avoid recurring container overhead
by selecting the right size for input splits and reducing the
number of containers.

",18.599290044081553,16.723478260869566,140
CCGrid_17_007.txt,16.951756855520017,15.234613572915084,CCGrid,7129,"
Large data analysis problems often involve a large
number of variables, and the corresponding analysis algorithms
may examine all variable combinations to find the optimal
solution. For example, to model the time required to complete a
scientific workflow, we need to consider the impact of dozens of
parameters. To reduce the model building time and reduce the
likelihood of overfitting, we look to variable selection methods
to identify the critical variables for the performance model.
In this work, we create a combination of variable selection
and performance prediction methods that is as effective as the
exhaustive search approach when the exhaustive search could be
completed in a reasonable amount of time. To handle the cases
where the exhaustive search is too time consuming, we develop the
parallelized variable selection algorithm. Additionally, we develop
a parallel grouping mechanism that further reduces the variable
selection time by 70%.

As a case study, we exercise the variable selection technique
with the performance measurement data from the Palomar
Transient Factory (PTF) workflow. The application scientists have
determined that about 50 variables and parameters are important
to the performance of the workflows. Our tests show that the
Sequential Backward Selection algorithm is able to approximate
the optimal subset relatively quickly. By reducing the number of
variables used to build the model from 50 to 4, we are able to
maintain the prediction quality while reducing the model building
time by a factor of 6. Using the parallelization and grouping
techniques we developed in this work, the variable selection
process was reduced from over 18 hours to 15 minutes while
ending up with the same variable subset.

",18.243605946275583,16.44253606172425,272
CCGrid_17_008.txt,16.218646115125612,15.755988061815515,CCGrid,6850,"
Big data are increasingly collected and stored in
a highly distributed infrastructures due to the development
of sensor network, cloud computing, IoT and mobile computing among many other emerging technologies. In practice,
the majority of existing big-data-processing frameworks (e.g.,
Hadoop and Spark) are designed based on the single-cluster
setup with the assumptions of centralized management and
homogeneous connectivity which makes them sub-optimal and
sometimes infeasible to apply for scenarios that require implementing data analytics jobs on highly distributed data sets
(across racks, data centers or multi-organizations). In order
to tackle this challenge, we present HDM-MC, a multi-cluster
big data processing framework which is designed to enable
the capability of performing large scale data analytics across
multi-clusters with minimum extra overhead due to additional
scheduling requirements. In this paper, we present the architecture and realization of the system. In addition, we evaluate
the performance of our framework in comparison to other
state-of-art single cluster big data processing frameworks.

",19.686211704642208,20.40007185628743,169
CCGrid_17_009.txt,16.331033593023204,14.695107199842894,CCGrid,4381,"
Troubleshooting in an infrastructure-as-a-Service
daaS) cloud platform is an inherently difficult task because it is
a multi-player as well as multi-layer environment where tenant
and provider effectively share administrative duties. To address
these concerns, we present our work on CloudSight in which
cloud providers allow tenants greater system-wide visibility
through a transparency-as-a-service abstraction. We present
the design, implementation, and evaluation of CloudSight in
the OpenStack cloud platform. We also develop two example
applications that make use of the CloudSight abstraction and
use the applications to explore real cloud problems.

",15.903189008614273,15.758877551020412,99
CCGrid_17_010.txt,13.77620161395005,11.881230075885611,CCGrid,6649,"
Live Virtual Machine (VM) migration offers a couple of benefits to cloud providers and users, but it is limited within
a data center. With the development of cloud computing and
the cooperation between data centers, live VM migration is also
desired across data centers. Based on a detailed analysis of VM
deployment models and the nature of VM image data, we design
and implement a new migration framework called CBase. The
key concept of CBase is a newly introduced central base image
repository for reliable and efficient data sharing between VMs
and data centers. With this central base image repository, live
VM migration and further performance optimizations are made
possible. The results from an extensive experiment show that
CBase is able to support VM migration efficiently, outperforming
existing solutions in terms of total migration time and network
traffic.

Index Terms—Cloud computing, Data center, Live VM migration, VM storage migration, Three-layer image structure, Data
deduplication

",15.903189008614273,15.188608058608065,157
CCGrid_17_011.txt,14.808363749432733,13.160029467989371,CCGrid,6363,"
Workflows are a widely used abstraction for
describing large scientific applications and running them on
distributed systems. However, most workflow systems have been
silent on the question of what execution environment each task
in the workflow is expected to run in. Consequently, a workflow
may run successfully in the environment it was created, but
fail on other platforms due to the differences in execution
environment. Container-based schedulers have recently arisen
as a potential solution to this problem, adopting containers
to distribute computing resources and deliver well-defined
execution environments to applications. In this paper, we consider how to connect workflow system to container schedulers
with minimal performance loss and higher system efficiency.
As an example of current technology, we use Makeflow and
Mesos. We present five design challenges, and address them by
using four configurations that connecting workflow system to
container scheduler from different level of the infrastructure.
In order to take full advantage of the resource sharing schema
of Mesos, we enable the resource monitor of Makeflow to
dynamically update the task resource requirement. We explore
the performance of a large bioinformatics workflow, and
observe that using Makeflow, Work Queue and the Resource
monitor together not only increase the transfer throughput but
also achieves highest resource usage rate.

Keywords-Workflow; Container; Scheduling

",17.122413403193683,15.253196261682245,215
CCGrid_17_012.txt,14.024525171496233,12.538250386383858,CCGrid,6185,"
The power and procurement cost of bandwidth in
system-wide networks has forced a steady drop in the byte/flop
ratio. This trend of computation becoming faster relative to
the network is expected to hold. In this paper, we explore
how cost-oriented task placement enables reducing the cost
of system-wide networks by enabling high performance even
on tapered topologies where more bandwidth is provisioned
at lower levels. We describe APHiD, an efficient hierarchical
placement algorithm that uses new techniques to improve
the quality of heuristic solutions and reduces the demand on
high-level, expensive bandwidth in hierarchical topologies. We
apply APHID to a tapered fat-tree, demonstrating that APHiD
maintains application scalability even for severely tapered
network configurations. Using simulation, we show that for
tapered networks APHiD improves performance by more than
50% over random placement and even 15% in some cases over
costlier, state-of-the-art placement algorithms.

",16.32212239822248,15.949210526315792,153
CCGrid_17_013.txt,14.142726877243955,12.798458265334666,CCGrid,6483,"
Running Big Data applications in the cloud has
become extremely popular in recent times. To enable the storage
of data for these applications, cloud-based distributed storage
solutions are a must. OpenStack Swift is an object storage service
which is widely used for such purposes. Swift is one of the
main components of the OpenStack software package. Although
Swift has become extremely popular in recent times, its proxy
server based design limits the overall throughput and scalability
of the cluster. Swift is based on the traditional TCP/IP sockets
based communication which has known performance issues such
as context-switch and buffer copies for each message transfer.
Modern high-performance interconnects such as InfiniBand and
RoCE offer advanced features such as RDMA and provide high
bandwidth and low latency communication. In this paper, we
propose two new designs to improve the performance and scalability of Swift. We propose changes to the Swift architecture and
operation design. We propose high-performance implementations
of network communication and I/O modules based on RDMA
to provide the fastest possible object transfer. In addition, we
use efficient hashing algorithms to accelerate object verification
in Swift. Experimental evaluations with microbenchmarks, Swift
stack benchmark (ssbench), and synthetic application workloads
reveal up to 2x and 7.3x performance improvement with our two
proposed designs for put and get operations. To the best of our
knowledge, this is the first work towards accelerating OpenStack
Swift with RDMA over high-performance interconnects in the
literature.

Keywords-OpenStack, Swift, RDMA, High-performance interconnects

",14.138980108339055,12.782380952380954,254
CCGrid_17_014.txt,16.851985718397255,16.438772545218644,CCGrid,5537,"
NVIDIA GPUDirect is a family of technologies
aimed at optimizing data movement among GPUs (P2P) or
between GPUs and third-party devices (RDMA). GPUDirect
Asyne, introduced in CUDA 8.0, is a new addition which
allows direct synchronization between GPU and third party
devices. For example, Async allows an NVIDIA GPU to directly
trigger and poll for completion of communication operations
queued to an InfiniBand Connect-IB network adapter, removing
CPU involvement from the critical path in GPU accelerated
applications. In this paper, we present the building blocks of
GPUDirect Async and explain the supported usage models of
this new technology. We also present a performance evaluation
using a micro-benchmark and a synthetic stencil benchmark.
Finally, we demonstrate the use of Async in a few multi-GPU
MPI applications: HPGMG-FV (geometric multi-grid), achieving
up to 25% improvement in total execution time; CoMD-CUDA
(classical molecular dynamics), reducing communications times
up to 30%; LULESH2-CUDA, achieving an average performance
improvement of 13% in the total execution time.

Index Terms—GPUDirect Async; CUDA 8.0; InfiniBand; asynchronous communications.

",18.548980349730343,17.33809523809524,180
CCGrid_17_015.txt,14.735707215402421,13.410976384623535,CCGrid,2689,"
Among the low-diameter, high-radix networks being
deployed in next-generation HPC systems, dual-rail fat-tree
networks are a promising approach. Adding additional injection
connections (rails) to one or more network planes allows multirail fat-tree networks to alleviate communication bottlenecks.
These multi-rail networks necessitate new design considerations,
such as routing choices, job placements, and scalability of rails.
We extend our fat-tree network model in the CODES parallel
simulation framework to support multi-rail and multi-plane
configurations in addition to different types of static routing,
resulting in a powerful research vehicle for fat-tree network analysis. Our detailed packet-level simulations use communication
traces from real applications to make performance predictions
and to evaluate the impact of single- and multi-rail networks in
conjunction with schemes for injection rail selection and intraplane routing.

",17.58133193835471,17.747913043478267,139
CCGrid_17_016.txt,16.68892709610593,14.942108653602194,CCGrid,4430,"
Cloud virtualization technology is shifting towards
light-weight containers, which provide isolated environments
for running cloud-based services. The emerging trends such as
container-based micro-service architectures and hybrid cloud
deployments result in increased traffic volumes between the
micro-services, mobility of the communication endpoints, and
some of the communication taking place over untrusted networks. Yet, the services are typically designed with the assumption of scalable, persistent and secure connectivity. In this
paper, we present the SynAPTIC architecture, which enables
secure and persistent connectivity between mobile containers,
especially in the hybrid cloud and in multi-tenant cloud networks. The solution is based on the standardized Host Identity
Protocol (HIP) that tenants can deploy on top of existing cloud
infrastructure independently of their cloud provider. Optional
cloud-provider extensions based on Software-Defined Networking (SDN) further optimize the networking architecture. Our
qualitative and quantitative evaluation shows that SynAPTIC
performs better than some of the existing solutions.

Index Terms—containers, docker, security, HIP, SDN, mobility

",17.971250198000288,16.652560975609756,165
CCGrid_17_017.txt,15.536931494314562,13.697268513136482,CCGrid,8298,"
Virtualization helps to deploy the functionality of
expensive and rigid hardware appliances on scalable virtual
resources running on commodity servers. However, optimal
resource provisioning for non-trivial services is still an open
problem. While there have been efforts to answer the questions of
when to provision additional resources in a running service, and
how many resources are needed, the question of what should be
provisioned has not been investigated, in particular, for complex
applications or services, which consist of a set of connected
components, where each component in turn potentially consists of
multiple component instances (e.g., VMs or containers). Each instance of a component can be run in different flavors (i.e., number
of cores or amount of memory), while the service constructed by
the combination of these component configurations must satisfy
the customer Service Level Objective (SLO). In this work, we
offer to service providers an answer to the what to deploy
question by introducing RConf, a system that automatically
chooses the optimal combination of component instances for nontrivial network services. In particular, we propose an analytical
model based on robust queuing theory that is able to accurately
model arbitrary components, and develop an algorithm that
finds the combination of their instances, such that the overall
utilization of the running instances is maximized while meeting
SLO requirements.

Index Terms—Robust queue; Resource configuration; Performance of complex services; Optimization

",20.803037591832787,20.091804511278202,231
CCGrid_17_018.txt,15.154443776002186,13.793529885937247,CCGrid,7800,"
Simulation is a fast, controlled, and reproducible
way to evaluate new algorithms for distributed computing
platforms in a variety of conditions. However, the realism of
simulations is rarely assessed, which critically questions the
applicability of a whole range of findings.

In this paper, we present our efforts to build platform models
from application traces, to allow for the accurate simulation of
file transfers across a distributed infrastructure. File transfers
are key to performance, as the variability of file transfer times
has important consequences on the dataflow of the application.
We present a methodology to build realistic platform models
from application traces and provide a quantitative evaluation of
the accuracy of the derived simulations. Results show that the
proposed models are able to correctly capture real-life variability
and significantly outperform the state-of-the-art model.

",16.92669308720184,16.763235294117653,137
CCGrid_17_019.txt,16.479419663050535,14.875546023458991,CCGrid,5808,"
Graph Processing has been widely used to capture
complex data dependency and uncover relationship insights.
Due to the ever-growing graph scale and algorithm complexity,
distributed graph processing has become more and more popular.
In this paper, we investigate how to balance performance and cost
for large scale graph processing on configurable virtual machines
(VMs). We analyze the system architecture and implementation
details of a Pregel-like distributed graph processing framework
and develop a system-aware model to predict the execution
time. Consequently, cost effective execution scenarios are recommended by selecting a certain number of VMs with specified
capability subject to the predefined resource price and user
preference. Experiments using synthetic and real world graphs
have verified that system-aware model can achieve much higher
prediction accuracy than popular machine-learning models which
treat graph processing framework as a black box. As a result,
the recommended execution scenarios have comparable cost
efficiency to the optimal scenarios.

Index terms— Distributed Graph Processing, Performance
Modelling and Prediction, Cost Effective Execution, Resource
Virtualization and Provisioning

",17.971250198000288,16.738023255813953,173
CCGrid_17_020.txt,14.76877833306607,13.068093981809742,CCGrid,5051,"
Nowadays, nested VMs are often being used to
address compatibility issues, security concerns, software scaling and continuous integration scenarios. With the increased
adoption of nested VMs, there is a need for newer techniques
to troubleshoot any unexpected behavior. Because of privacy
and security issues, ease of deployment and execution overhead,
these investigation techniques should preferably limit their data
collection in most cases to the physical host level, without internal
access to the VMs. This paper introduces the Nested Virtual
Machine Detection Algorithm (NDA) - a host hypervisor based
analysis method which can investigate the performance of nested
VMs. NDA can uncover the CPU overhead entailed by the host
hypervisor and guest hypervisors, and compare it to the CPU
usage of Nested VMs. We further developed several graphical
views, for the TraceCompass trace visualization tool, to display
the virtual CPUs of VMs and their corresponding nested VMs,
along with their states. These approaches are based on host
hypervisor tracing, which brings a lower overhead (around 1%)
as compared to other approaches. Based on our analysis and the
implemented graphical views, our techniques can quickly detect
different problems and their root causes, such as unexpected
delays inside nested VMs.

Keywords—Nested Virtualization; KVM; Performance Analysis;
LTTng; TraceCompass; Process State

",20.267338824336647,20.655339805825246,207
CCGrid_17_021.txt,14.633937201125022,12.095043969870286,CCGrid,6135,"
We present a new method for scheduling independent tasks on a parallel machine composed of identical
processors. This problem has been studied extensively for a long
time with many variants. We are interested here in designing a
generic algorithm in the on-line non-preemptive setting whose
performance is good for various objectives. The basic idea of
this algorithm is to detect some problematic tasks that are
responsible for the delay of other shorter tasks. Then the
former tasks are redirected to be executed in a dedicated part
of the machine. We show through an extensive experimental
campaign that this method is effective and in most cases
is closer to some standard lower bounds than the base-line
method for the problem.

Keywords-on-line scheduling, non-preemptive, independent
tasks, stretch, redirections

",14.756829357015494,12.235497835497839,133
CCGrid_17_022.txt,17.966341267752572,16.90977441715437,CCGrid,5884,"
In the recent years, overlay networks have emerged
as a crucial platform for deployment of various distributed applications. Many of these applications rely on data redundancy techniques, such as erasure coding, to achieve higher fault tolerance.
However, erasure coding applied in large scale overlay networks
entails various overheads in terms of storage, latency and data
rebuilding costs. These overheads are largely attributed to the
selected erasure coding scheme and the encoded chunk placement
in the overlay network. This paper explores a multi-objective
optimization approach for identifying appropriate erasure coding
schemes and encoded chunk placement in overlay networks. The
uniqueness of our approach lies in the consideration of multiple
erasure coding objectives such as encoding rate and redundancy
factor, with overlay network performance characteristics like
storage consumption, latency and system reliability. Our approach enables a variety of tradeoff solutions with respect to
these objectives to be identified in the form of a Pareto front.
To solve this problem, we propose a novel two stage multiobjective evolutionary algorithm, where the first stage determines
the optimal set of encoding schemes, while the second stage
optimizes placement of the corresponding encoded data chunks
in overlay networks of varying sizes. We study the performance
of our method by generating and analyzing the Pareto optimal
sets of tradeoff solutions. Experimental results demonstrate that
the Pareto optimal set produced by our multi-objective approach
includes and even dominates the chunk placements delivered by
a related state-of-the-art weighted sum method.

Keywords—Erasure coding, peer-to-peer, overlay network,
multi-objective optimization, Pareto optimal set.

",18.43863989570496,17.40972027972028,261
CCGrid_17_023.txt,17.413020483833883,16.190194817213364,CCGrid,7318,"
Infrastructure as a Service (IaaS) providers typically offer multiple service classes to deal with the wide variety
of users adopting this cloud computing model. In this scenario,
IaaS providers need to perform efficient admission control and
capacity planning in order to minimize infrastructure costs,
while fulfilling the different Service Level Objectives (SLOs)
defined for all service classes offered. However, most of the
previous work on this field consider a single resource dimension
— typically CPU — when making such management decisions.
We show that this approach will either increase infrastructure
costs due to over-provisioning, or violate SLOs due to lack
of capacity for the resource dimensions being ignored. To fill
this gap, we propose admission control and capacity planning
methods that consider multiple service classes and multiple
resource dimensions. Our results show that our admission
control method can guarantee a high availability SLO fulfillment in scenarios where both CPU and memory can become
the bottleneck resource. Moreover, we show that our capacity
planning method can find the minimum capacity required for
both CPU and memory to meet SLOs with good accuracy. We
also analyze how the load variation on one resource dimension
can affect another, highlighting the need to manage resources
for multiple dimensions simultaneously.

Keywords-admission control; capacity planning; cloud computing; performance models; simulation.

",18.599290044081553,16.787071651090347,215
CCGrid_17_024.txt,17.50936553159778,16.17587455009869,CCGrid,8463,"
In virtualized datacenters (vDCs), dynamic consolidation of virtual machines (VMs) is used as one of the most
common techniques to achieve both energy- and resource- utilization efficiency. Live migrations of VMs are used for dynamic
consolidation but due to dynamic resource demand variation
of VMs may lead to frequent and non-optimal migrations.
Assuming deterministic workload of the VMs may ensure the
most energy/resource-efficient VM allocations but eventually may
lead to significant resource contention or under-utilization if
the workload varies significantly over time. On the other hand,
adopting a conservative approach by allocating VMs depending
on their peak demand may lead to low utilization, if the peak
occurs infrequently or for a short period of time. Therefore,
in this work we design a robust VM migration scheme that
strikes a balance between protection for resource contention and
additional energy costs due to powering on more servers while
considering uncertainties on VMs resource demands. We use the
theory of I robustness and derive a robust Mixed Integer Linear
programming (MILP) formulation. Due to the complexity, the
problem is hard to solve for online optimization and we propose
a novel heuristic based on Tabu search. Using several scenarios,
we show that that the proposed heuristic can achieve near optimal
solution qualities in a short time and scales well with the instance
sizes. Moreover, we quantitatively analyze the trade-off between
energy cost versus protection level and robustness.

Keywords—Virtualized datacenters; VM Consolidation; VM Migration; Energy Efficiency; Tabu Search; Robust Optimization;
Resource Allocation.

",18.026119701940384,16.710992094861663,254
CCGrid_17_025.txt,14.71426603190282,12.854165725056625,CCGrid,6406,"
Event correlation is a cornerstone for process discovery over event logs crossing multiple data sources. The
computed correlation rules and process instances will greatly
help us to unleash the power of process mining. However,
exploring all possible event correlations over a log could be
time consuming, especially when the log is large. State-of-theart methods based on MapReduce designed to handle this challenge have offered significant performance improvements over
standalone implementations. However, all existing techniques are
still based on a conventional generating-and-pruning scheme.
Therefore, event partitioning across multiple machines is often
inefficient. In this paper, following the principle of filtering-andverification, we propose a new algorithm, called RF-GraP, which
provides a more efficient correlation over distributed systems.
We present the detailed implementation of our approach and
conduct a quantitative evaluation using the Spark platform.
Experimental results demonstrate that the proposed method is
indeed efficient. Compared to the state-of-the-art, we are able
to achieve significant performance speedups with obviously less
network communication.

Keywords-event correlation; process mining; service computing; data partitioning; big data; data-intensive computing

",15.186304673781336,14.786903096903096,183
CCGrid_17_026.txt,13.8440620124452,11.442335940565364,CCGrid,6035,"
This paper proposes a scalable and efficient cache
update technique to improve the performance of in-memory
cluster computing in Spark, a popular open-source system for
big data computing. Although the memory cache speeds up data
processing in Spark, its data immutability constraint requires
reloading the whole RDD when part of its data is updated. Such
constraint makes the RDD update inefficient. To address this
problem, we divide an RDD into partitions, and propose the
partial-update RDD (PRDD) method to enable users to replace
individual partition(s) of an RDD. We devise two solutions to the
RDD partition problem — a dynamic programming algorithm and
a nonlinear programming method. Experiment results suggest
that, PRDD achieves 4.32x speedup when compared with the
original RDD in Spark. We apply PRDD to a billing system for
Chunghwa Telecomm, the largest telecommunication company in
Taiwan. Our result shows that the PRDD based billing system
outperforms the original billing system in CHT by a factor of
24x in throughput. We also evaluate PRDD using the TPC-H
benchmark, which also yields promising result.

Keywords—big data computing, cache update, Spark, resilient
distributed dataset, telecom billing system

",14.554592549557764,13.162583333333334,194
CCGrid_17_027.txt,15.881881088844896,14.551574981506139,CCGrid,6744,"
Due to its simplicity and scalability, MapReduce
has become a de facto standard computing model for big data
processing. Since the original MapReduce model was only appropriate for embarrassingly parallel batch processing, many
follow-up studies have focused on improving the efficiency
and performance of the model. Spark follows one of these
recent trends by providing in-memory processing capability to
reduce slow disk I/O for iterative computing tasks. However, the
acceleration of Spark’s in-memory processing using graphics
processing units (GPUs) is challenging due to its deep memory
hierarchy and host-to-GPU communication overhead. In this
paper, we introduce a novel GPU-accelerated MapReduce
framework that extends Spark’s in-memory processing so that
iterative computing is performed only in the GPU memory.
Having discovered that the main bottleneck in the current
Spark system for GPU computing is data communication on a
Java virtual machine, we propose a modification of the current
Spark implementation to bypass expensive data management
for iterative task offloading to GPUs. We also propose a
novel GPU in-memory processing and caching framework that
minimizes host-to-GPU communication via lazy evaluation and
reuses GPU memory over multiple mapper executions. The proposed system employs message-passing interface (MPI)-based
data synchronization for inter-worker communication so that
more complicated iterative computing tasks, such as iterative
numerical solvers, can be efficiently handled. We demonstrate
the performance of our system in terms of several iterative
computing tasks in big data processing applications, including
machine learning and scientific computing. We achieved up to
50 times speed up over conventional Spark and about 10 times
speed up over GPU-accelerated Spark.

Keywords-Spark, MapReduce, GPU, In-memory Computing

",18.63122029486172,17.810844155844155,283
CCGrid_17_028.txt,17.849301816266916,17.08138649616185,CCGrid,7743,"
In state-of-the-art Software Transactional Memory (STM) systems, threads carry out the execution of transactions as non-interruptible tasks. Hence, a thread can react
to the injection of a higher priority transactional task and
take care of its processing only at the end of the currently
executed transaction. In this article we pursue a paradigm
shift where the execution of an in-memory transaction is
carried out as a preemptable task, so that a thread can start
processing a higher priority transactional task before finalizing
its current transaction. We achieve this goal in an applicationtransparent manner, by only relying on Operating System
facilities we include in our preemptive STM architecture. With
our approach we are able to re-evaluate CPU assignment
across transactions along a same thread every few tens of
microseconds. This is mandatory for an effective priority-aware
architecture given the typically finer-grain nature of in-memory
transactions compared to their counterpart in database systems. We integrated our preemptive STM architecture with
the TinySTM package, and released it as open source. We
also provide the results of an experimental assessment of our
proposal based on running a port of the TPC-C benchmark to
the STM environment.

",17.553077303434723,15.929777227722774,203
CCGrid_17_029.txt,15.617909005465652,13.857414790996788,CCGrid,4951,"
In-situ analytics have recently emerged as a promising approach to reduce I/O, network, and storage congestion
for scientific data analysis. At the same time, Mean Time To
Failure (MTTF) has also continuously decreased for HPC clusters,
making failures during the execution of a simulation more likely.
It is difficult to apply Checkpointing and Restart (C/R) to a
simulation executed with in-situ analytics, because we not only
need to store analytic and simulation states, but also need to
ensure consistency between them.

To solve the above problem, we present a novel solution to
apply C/R approach. The solution exploits a common property
of analytics applications, which is that they fit the reductionstyle processing pattern. Based on the observation that the
global reduction result is correct if all local reduction objects
have been processed, only global reduction objects and progress
of the reduction procedure is checkpointed, together with an
application-level checkpoint of the simulation. We also discuss
the specific details for handling time-sharing and space sharing
modes, which are suitable for multi-core and many-core environments, respectively.

We have evaluated our approach in the context of Smart
middleware for developing in-situ analytics, which provides a
Map-Reduce like API. We demonstrate the low overheads of our
approach with different scientific simulations and analytics tasks.

",16.45884130781739,15.824675716440428,222
CCGrid_17_030.txt,14.932604731484979,13.70856805222974,CCGrid,4693,"
Concurrent multithreaded access to the Message
Passing Interface (MPI) is gaining importance to support
emerging hybrid MPI applications. The interoperability between
threads and MPI, however, is complex and renders efficient
implementations nontrivial. Prior studies showed that threads
waiting for communication progress (waiting threads) often
interfere with others (active threads) and degrade their progress.
This situation occurs when both classes of threads compete for
the same MPI resource and ownership passing to waiting threads
does not guarantee communication to advance. The best-known
practical solution prioritizes active threads and adapts first-infirst-out arbitration within each class. This approach, however,
suffers from residual wasted resource acquisitions (waste) and
ignores data locality, thus resulting in poor scalability.

In this work, we propose thread synchronization improvements
to eliminate waste while preserving data locality in a production
MPI implementation. First, we leverage MPI knowledge and a
fast synchronization method to eliminate waste and accelerate
progress. Second, we rely on a cooperative progress model that
dynamically elects and restricts a single waiting thread to drive
a communication context for improved data locality. Third, we
prioritize active threads and synchronize them with a localitypreserving lock that is hierarchical and exploits unbounded bias
for high throughput. Results show significant improvement in
synthetic microbenchmarks and two MPI+OpenMP applications.

Index Terms—MPI; threads; OpenMP; thread safety; lock;
mutex; synchronization

",15.134931603849854,14.959234234234238,222
CCGrid_17_031.txt,14.847065617897258,13.809319194061505,CCGrid,2833,"
Data replication is a key technique to achieve
data availability, reliability, and optimized performance in
distributed storage systems and data centers. In recent years,
with the emergence of new storage devices, heterogeneous
object-based storage system, such as a storage system with
the co-existence of hard disk drives and solid state drives,
have become increasingly attractive as they combine merits of
different storage devices to deliver better promise. However,
existing data replication schemes do not place data based
on heterogeneous device characteristics as well as considering
distinct data access patterns. In this paper, we introduce a
novel data replication scheme PRS to achieve efficient data
replication for heterogeneous storage systems. Different from
traditional schemes, the PRS groups objects according to
data access patterns and distributes replicas to heterogeneous
devices with their features. It uses a pseudo random algorithm
to optimize replica layout by considering storage device performance and capacity. The experimental results confirm that
PRS is a highly efficient replication scheme for heterogeneous
storage systems.

Keywords-data replication; heterogeneous storage; objectbased storage; access pattern; data distribution

",16.975882523387877,17.572083333333335,178
CCGrid_17_032.txt,13.95137733561527,13.309942511332512,CCGrid,8060,"
A large number of cloud datastores have been
developed to handle the cloud OLTP workload. Double caching
problem where the same data resides both at the user buffer
and the kernel buffer has been identified as one of the problems
and has been largely solved by using direct I/O mode to bypass
the kernel buffer. However, maintaining the caching layer only
in user-level has the disadvantage that the user process may
monopolize memory resources and that it is difficult to fully
utilize the system memory due to the risks of the forced
termination of the process or the unpredictable performance
degradation in case of memory pressure.

In this paper, we propose a new I/O mode, DBIO, to
efficiently exploit OS kernel buffer as a victim cache for userlevel file content cache, enjoying the strengths of kernel-level
cache rather than just skipping it. DBIO provides the new
file read/write function calls, which enable user programs to
dynamically choose the right I/O behavior based on their
context when issuing I/Os instead of when opening the file. On
the cloud key-value store workloads and the traditional OLTP
workloads with the modified version of MySQL/InnoDB, DBIO
improves the in-memory cache hit ratio and the transaction
performance compared to both buffered and direct I/O mode,
fully utilizing the user buffer and the kernel buffer without
double caching.

Keywords-double caching; victim cache; buffer pool; page
cache;

",15.343465313023842,16.811865407319953,243
CCGrid_17_033.txt,13.871548416118785,11.480244304882262,CCGrid,4200,"
Solid State Drives (SSD) are integrated together
with Hard Disk Drives (HDD) in Hybrid Storage Systems (HSS)
for Cloud environment. When it comes to storing data, some
placement strategies are used to find the best location (SSD
or HDD). These strategies should minimize the cost of data
placement while satisfying Service Level Objectives (SLO). This
paper presents two Cost based Object Placement Strategies
(COPS) for DBaaS objects in HSS: a Genetic based approach
(G-COPS) and an ad-hoc Heuristic approach (H-COPS) based
on incremental optimization. While G-COPS proved to be
closer to the optimal solution in case of small instances, HCOPS showed a better scalability as it approached the exact
solution even for large instances (by 10% in average). In
addition, H-COPS showed small execution times (few seconds)
even for large instances which makes it a good candidate to
be used in runtime. Both H-COPS and G-COPS performed
better than state-of-the-art solutions as they satisfied SLOs
while reducing the overall cost by more than 40% for problems
of small and large instances.

Keywords-DBaaS, Hybrid storage, SSD, Optimization, Cloud

",14.554592549557764,12.028191489361706,189
CCGrid_17_034.txt,14.610829932237657,12.781410068963336,CCGrid,7380,"
 Security and privacy in cloud computing are
critical components for various organizations that depend on
the cloud in their daily operations. Customers’ data and the
organizations’ proprietary information have been subject to
various attacks in the past. In this paper, we develop a set
of Moving Target Defense (MTD) strategies that randomize
the location of the Virtual Machines (VMs) to harden the
cloud against a class of Multi-Armed Bandit (MAB) policybased attacks. These attack policies capture the behavior of
adversaries that seek to explore the allocation of VMs in the
cloud and exploit the ones that provide the highest rewards
(e.g., access to critical datasets, ability to observe credit card
transactions, etc). We assess through simulation experiments
the performance of our MTD strategies, showing that they
can make MAB policy-based attacks no more effective than
random attack policies. Additionally, we show the effects of
critical parameters — such as discount factors, the time between
randomizing the locations of the VMs and variance in the
rewards obtained - on the performance of our defenses. We
validate our results through simulations and a real OpenStack
system implementation in our lab to assess migration times and
down times under different system loads.

",18.243605946275583,17.19965541995693,201
CCGrid_17_035.txt,13.445238502366085,11.13082784093832,CCGrid,8420,"
MapReduce is a framework for processing large
data sets much used in the context of cloud computing. MapReduce implementations like Hadoop can tolerate crashes and
file corruptions, but not arbitrary faults. Unfortunately, there
is evidence that arbitrary faults do occur and can affect the
correctness of MapReduce job executions. Furthermore, many
outages of major cloud offerings have been reported, raising
concerns about the dependence on a single cloud.

In this paper we propose a novel execution system that allows
to scale out MapReduce computations to a cloud-of-clouds and
tolerate arbitrary faults, malicious faults, and cloud outages. Our
system, Chrysaor, is based on a fine-grained replication scheme
that tolerates faults at the task level. Our solution has three
important properties: it tolerates the above-mentioned classes of
faults at reasonable cost; it requires minimal modifications to
the users’ applications; and it does not involve changes to the
Hadoop source code. We performed an extensive evaluation of our
system in Amazon EC2, showing that our fine-grained solution is
efficient in terms of computation by recovering only faulty tasks.
This is achieved without incurring a significant penalty for the
baseline case (i.e., without faults) in most workloads.

",16.18397175987059,14.384949494949492,200
CCGrid_17_036.txt,15.136825477255204,13.482250169809607,CCGrid,6171,"
Network messaging delay historically constitutes a
large portion of the wall-clock time for High Performance
Computing (HPC) applications, as these applications run on
many nodes and involve intensive communication among their
tasks. Dragonfly network topology has emerged as a promising
solution for building exascale HPC systems owing to its low
network diameter and large bisection bandwidth. Dragonfly
includes local links that form groups and global links that connect
these groups via high bandwidth optical links. Many aspects of
the dragonfly network design are yet to be explored, such as the
performance impact of the connectivity of the global links, i.e.,
global link arrangements, the bandwidth of the local and global
links, or the job allocation algorithm.
This paper first introduces a packet-level simulation framework to model the performance of HPC applications in detail.
The proposed framework is able to simulate known MPI (message
passing interface) routines as well as applications with customdefined communication patterns for a given job placement algorithm and network topology. Using this simulation framework,
we investigate the coupling between global link bandwidth
and arrangements, communication pattern and intensity, job
allocation and task mapping algorithms, and routing mechanisms
in dragonfly topologies. We demonstrate that by choosing the
right combination of system settings and workload allocation
algorithms, communication overhead can be decreased by up
to 44%. We also show that circulant arrangement provides
up to 15% higher bisection bandwidth compared to the other
arrangements; but for realistic workloads, the performance
impact of link arrangements is less than 3%.
",17.631426480028413,17.570332005312085,253
CCGrid_17_037.txt,15.918949766007302,14.273216499761567,CCGrid,6944,"
Many servers use technologies such as virtualization
or containerization to improve server utilization. These technologies pose challenges for power monitoring since it is not
possible to directly measure the power use of an abstraction
such as a virtual machine. Much work has been done in
modeling the power use of CPUs, virtual machines and entire
servers; however, there is a scarcity of work in building
lightweight power monitoring middleware that can be deployed
across a range of systems. In this paper, we present cWatts+ as
a prototype lightweight software-based virtual power meter.
Utilizing a simple but powerful application-agnostic power
model, it offers comparable performance to existing “more
complex and heavier-weight” power models. It uses a small
number of widely available CPU event counters and the
Performance Application Programming Interface Library to
estimate power usage on a per-thread basis. It has minimal
overhead and is portable across a variety of systems. It can
be used in containerized or virtualized environments. We
evaluate the estimation performance of cWatts+ for a variety
of real-world benchmarks that are relevant to large distributed
systems. Also, we examine the importance of including CPU
core temperature data in the power model. We demonstrate
that our power model has an average error of less than 5%.
This result compares favorably with existing state-of-the-art
power models and is achieved using a relatively simple power
model that exhibits minimal power consumption (overhead).
Consequently, our power monitoring middleware is viable for
use in real-world applications such as power estimation for
energy-aware scheduling.

",16.47975071879961,14.733460076045628,262
CCGrid_17_038.txt,15.541843079616342,14.09529192135605,CCGrid,7953,"
UCX is an open-source communication framework
with a two-level API design targeted at addressing the needs of
large supercomputing systems. The lower-level interface, UCT,
adds minimal overhead to data transfer but requires considerable
effort from the user. The higher-level interface, UCP, is easier to
use, but adds some overhead to the communication. This work
focuses on charting the performance of UCX over InfiniBand,
motivated by the usage of UCX as middleware for high-level
communication libraries. We analyze performance shortcomings
that stem from the two-level design and the sources of these
performance losses. In particular, we target basic functions of
UCP, evaluate their performance over InfiniBand, and analyze
sources of overheads compared with UCT and Verbs. We propose
and evaluate some fixes to minimize these overheads, in order to
enhance UCP performance and scalability.

Index Terms—communication software, communication middleware, UCX, InfiniBand, performance.

",16.061879428645646,14.587162162162162,149
CCGrid_17_039.txt,17.020237162474015,16.024337299852167,CCGrid,6720,"
Auto-scaling, a key property of cloud computing,
allows application owners to acquire and release resources
on demand. However, the shared environment, along with the
exponentially large configuration space of available parameters,
makes configuration of auto-scaling policies a challenging task. In
particular, it is difficult to quantify, a priori, the impact of a policy
on Quality of Service (QoS) provision. To address this problem,
we propose a novel approach based on performance modelling
and formal verification to produce performance guarantees
on particular rule-based auto-scaling policies. We demonstrate
the usefulness and efficiency of our model through a detailed
validation process on the Amazon EC2 cloud, using two types
of load patterns. Our experimental results show that it can be
very effective in helping a cloud application owner configure an
auto-scaling policy in order to minimise the QoS violations.

",17.879347455551382,16.67287234042553,142
CCGrid_17_040.txt,15.916085618671868,14.16324707751674,CCGrid,6348,"
In order to boost the performance of data-intensive
computing on HPC systems, in-memory computing frameworks,
such as Apache Spark and Flink, use local DRAM for data storage.
Optimizing the memory allocation to data storage is critical to
delivering performance to traditional HPC compute jobs and
throughput to data-intensive applications sharing the HPC
resources. Current practices that statically configure in-memory
storage may leave inadequate space for compute jobs or miss the
opportunity to utilize available space for data-intensive
applications. In this paper, we explore techniques to dynamically
adjust in-memory storage allocation and provide optimum
memory to compute jobs. We have developed a dynamic inmemory storage controller, DynIMS, which monitors memory
demands of compute tasks in real time and employs a feedbackbased control mechanism to adapt the allocation of in-memory
storage. We test DynIMS using HPCC and Spark workloads on a
HPC cluster. Experimental results show that DynIMS can achieve
up to 5X performance improvement compared to systems with
static memory allocations.

Keywords-component; in-memory storage; parallel file system;
dynamic control; HPC;

",17.410965686947208,15.891994382022471,179
CCGrid_17_041.txt,14.767927462751231,12.437703479576403,CCGrid,6605,"
Non-volatile memory express (NVMe) based SSDs
and the NUMA platform are widely adopted in servers to
achieve faster storage speed and more powerful processing
capability. As of now, very little research has been conducted
to investigate the performance and energy efficiency of the stateof-the-art NUMA architecture integrated with NVMe SSDs, an
emerging technology used to host parallel I/O threads. As this
technology continues to be widely developed and adopted, we
need to understand the runtime behaviors of such systems in
order to design software runtime systems that deliver optimal
performance while consuming only the necessary amount of

energy.

This paper characterizes the runtime behaviors of a Linuxbased NUMA system employing multiple NVMe SSDs. Our
comprehensive performance and energy-efficiency study using
massive numbers of parallel I/O threads shows that the penalty
due to CPU contention is much smaller than that due to remote
access of NVMe SSDs. Based on this insight, we develop a
dynamic “lesser evil’ algorithm called ESN, to minimize the
impact of these two types of penalties. ESN is an energyefficient profiling-based I/O thread scheduler for managing I/O
threads accessing NVMe SSDs on NUMA systems. Our empirical
evaluation shows that ESN can achieve optimal I/O throughput
and latency while consuming up to 50% less energy and using
fewer CPUs.

",18.7741,16.754790723981902,222
CCGrid_17_042.txt,17.715470667786573,16.648016678996836,CCGrid,8132,"
Service-based access models coupled with emerging
application deployment technologies are enabling opportunities
for realizing highly customized software-defined environments,
which can support dynamic and data-driven applications.
However, this requires rethinking traditional resource federation models to support dynamic resource compositions, which
can adapt to evolving application needs and the dynamic
state of underlying resources. In this paper, we present a
programmable approach that leverages software-defined techniques to create a dynamic space-time infrastructure service
composition. We propose the use of Constraint Programming
as a formal language to allow users, applications, and service
providers to define the desired state of the execution environment. The resulting distributed software-defined environment
continually adapts to meet objectives/constraints set by the
users, applications, and/or resource providers. We present
the design and prototype implementation of such distributed
software-defined environment. We use a cancer informatics
workflow to demonstrate the operation of our framework
using resources from five different cloud providers, which are
aggregated on-demand based on dynamic user and resource
provider constraints.

Index Terms—distributed software-defined environments; programmable infrastructure; programmable service composition

",18.7741,18.827554945054953,183
CCGrid_17_043.txt,16.110794974339047,14.8142474133294,CCGrid,4698,"
The management of complex network services requires flexible and efficient service provisioning as well as
optimized handling of continuous changes in the workload
of the services. To adapt to changes in the demand, service
components need to be replicated (scaling) and allocated to
physical resources (placement) dynamically. In this paper, we
propose a fully automated approach to the joint optimization
problem of scaling and placement, enabling quick reaction to
changes. We formalize the problem, analyze its complexity, and
develop two algorithms to solve it. Empirical results show the
applicability and effectiveness of the proposed approach.

",16.404322709996244,15.420000000000002,96
CCGrid_17_044.txt,15.002056648759961,12.854787597541485,CCGrid,6730,"
Finding a good partition of a computational
directed acyclic graph associated with an algorithm can help
find an execution pattern improving data locality, conduct
an analysis of data movement, and expose parallel steps. The
partition is required to be acyclic, i.e., the inter-part edges
between the vertices from different parts should preserve an
acyclic dependency structure among the parts. In this work,
we adopt the multilevel approach with coarsening, initial
partitioning, and refinement phases for acyclic partitioning of
directed acyclic graphs and develop a direct k-way partitioning
scheme. To the best of our knowledge, no such scheme exists
in the literature. To ensure the acyclicity of the partition
at all times, we propose novel and efficient coarsening and
refinement heuristics. The quality of the computed acyclic
partitions is assessed by computing the edge cut, the total
volume of communication between the parts, and the critical
path latencies. We use the solution returned by well-known
undirected graph partitioners as a baseline to evaluate our
acyclic partitioner, knowing that the space of solution is more
restricted in our problem. The experiments are run on large
graphs arising from linear algebra applications.

Keywords-directed graph; acyclic partitioning; multilevel
partitioning;

",18.12316971661352,15.862479061976547,201
CCGrid_17_045.txt,15.197691213927534,13.111297712872815,CCGrid,6436,"
Energy consumption has become one of the most
critical issues in the evolution of High Performance Computing
systems (HPC). Controlling the energy consumption of HPC
platforms is not only a way to control the cost but also a step
forward on the road towards exaflops. Powercapping is a widely
studied technique that guarantees that the platform will not
exceed a certain power threshold instantaneously but it gives
no flexibility to adapt job scheduling to a longer term energy
budget control.

We propose a job scheduling mechanism that extends the
backfilling algorithm to become energy-aware. Simultaneously,
we adapt resource management with a node shutdown technique to minimize energy consumption whenever needed. This
combination enables an efficient energy consumption budget
control on a cluster during a period of time. The technique is
experimented, validated and compared with various alternatives
through extensive simulations. Experimentation results show high
system utilization and limited bounded slowdown along with
interesting outcomes in energy efficiency while respecting an
energy budget during a particular time period.

Index Terms—HPC; Resource Management; Scheduling; Energy Budget;

",16.860833078287435,15.703712121212124,177
CCGrid_17_046.txt,14.976913770495841,13.327235963836852,CCGrid,7236,"
Consumers can realize significant cost savings by
procuring resources from computational spot markets such as
Amazon Elastic Compute Cloud (EC2) Spot Instances. They
can take advantage of the price differentials across time slots,
regions, and instance types to minimize the total cost of running
their applications on the cloud. However, Spot markets are
inherently volatile and dynamic, as a consequence of which
Spot prices change continuously. As such, prospective bidders
can benefit from intelligent insights into the Spot market
dynamics that can help them make more informed bidding
decisions. To enable this, we propose a descriptive statistics
approach for the analysis of Amazon EC2 Spot markets to
detect typical pricing patterns including the presence of seasonal components, extremes and trends. We use three statistical
measures — the Gini coefficient, the Theil index, and the
exponential weighted moving average. We also devise a model
for estimating minimum bids such that the Spot instances will
run for specified durations with a probability greater than a
set value based on different look back periods. Experimental
results show that our estimation yields on average a bidding
strategy that can reliably secure an instance at least 80% of
the time at minimum target guarantee between 50% and 95%.

Keywords-Spot market, descriptive statistics, Gini coefficient,
Thiel Index, bid price estimation

",17.37919286519448,15.353426791277261,215
CCGrid_17_047.txt,14.396600825163954,12.676485268725532,CCGrid,7229,"
Community networks (CNs) have gained momentum
in the last few years with the increasing number of spontaneously
deployed WiFi hotspots and home networks. These networks,
owned and managed by volunteers, offer various services to their
members and to the public. To reduce the complexity of service
deployment, community micro-clouds have recently emerged as a
promising enabler for the delivery of cloud services to community
users. By putting services closer to consumers, micro-clouds
pursue not only a better service performance, but also a low
entry barrier for the deployment of mainstream Internet services
within the CN. Unfortunately, the provisioning of the services
is not so simple. Due to the large and irregular topology, high
software and hardware diversity of CNs, it requires of a ""careful""
placement of micro-clouds and services over the network.
To achieve this, this paper proposes to leverage state information about the network to inform service placement decisions,
and to do so through a fast heuristic algorithm, which is vital to
quickly react to changing conditions. To evaluate its performance,
we compare our heuristic with one based on random placement
in Guifi.net, the biggest CN worldwide. Our experimental
results show that our heuristic consistently outperforms random
placement by 211% in terms of bandwidth gain. We quantify the
benefits of our heuristic on a real live video-streaming service,
and demonstrate that video chunk losses decrease significantly,
attaining a 37% decrease in the loss packet rate. Further, using a
popular Web 2.0 service, we demonstrate that the client response
times decrease up to an order of magnitude when using our
heuristic.
",16.24694786949722,15.04545454545455,268
CCGrid_17_048.txt,15.920980227554413,14.778821843215749,CCGrid,7104,"
The blockchain technology has emerged as an attractive solution to address performance and security issues in
distributed systems. Blockchain’s public and distributed peer-topeer ledger capability benefits cloud computing services which require functions such as, assured data provenance, auditing, management of digital assets, and distributed consensus. Blockchain’s
underlying consensus mechanism allows to build a tamper-proof
environment, where transactions on any digital assets are verified
by set of authentic participants or miners. With use of strong
cryptographic methods, blocks of transactions are chained together to enable immutability on the records. However, achieving
consensus demands computational power from the miners in
exchange of handsome reward. Therefore, greedy miners always
try to exploit the system by augmenting their mining power. In
this paper, we first discuss blockchain’s capability in providing
assured data provenance in cloud and present vulnerabilities
in blockchain cloud. We model the block withholding (BWH)
attack in a blockchain cloud considering distinct pool reward
mechanisms. BWH attack provides rogue miner ample resources
in the blockchain cloud for disrupting honest miners’ mining
efforts, which was verified through simulations.

Index Terms—Blockchain, cloud computing, block mining,
data provenance, proof-of-work, block withholding, distributed
ledger, pool mining, blockchain security and vulnerability

",16.404322709996244,15.889592964824121,203
CCGrid_17_049.txt,14.405118337218344,12.705507114599534,CCGrid,5945,"
Cloud data provenance is metadata that records
the history of the creation and operations performed on
a cloud data object. Secure data provenance is crucial for
data accountability, forensics and privacy. In this paper, we
propose a decentralized and trusted cloud data provenance
architecture using blockchain technology. Blockchain-based
data provenance can provide tamper-proof records, enable the
transparency of data accountability in the cloud, and help to
enhance the privacy and availability of the provenance data.
We make use of the cloud storage scenario and choose the
cloud file as a data unit to detect user operations for collecting
provenance data. We design and implement ProvChain, an
architecture to collect and verify cloud data provenance, by
embedding the provenance data into blockchain transactions.
ProvChain operates mainly in three phases: (1) provenance
data collection, (2) provenance data storage, and (3) provenance
data validation. Results from performance evaluation demonstrate that ProvChain provides security features including
tamper-proof provenance, user privacy and reliability with low
overhead for the cloud storage applications.

Keywords-Data provenance, Blockchain, Cloud Computing,
Privacy, Reliability, Blockchain Cloud.

",16.728156217252725,16.16219739292365,180
CCGrid_17_050.txt,14.788054655959716,12.815307687874299,CCGrid,6981,"
Nowadays, the vulnerability of cloud environment
exposed in security places Virtual Machine Introspection(V MI)
at risk: once attackers subvert any layers of cloud environment,
such as host, virtual machine manager(VMM) or qemu, VMI
will be exposed undoubtedly to those attackers too. Nearly all
existing VMI techniques implicitly assume that both VMM by
which VMI accesses specific VM data and host which VMI is
running on, are nonmalicious and immutable. Unfortunately,
this assumption can be potentially violated with the growing
shortage of security in cloud environment. Once VMM or
host is exploited, attackers can tamper the code or hijack the
data of VMI, then, falsify VM information and certifications
to Cloud system’s administrators who try to make sure the
security of specific VM in certain compute node. This paper
proposes a new trusted VMI monitor frame: T-VMI, which can
avoid the malicious subversion of the routine of VMI. T-VMI
guarantees the integrity of VMI code using isolation and the
correctness of VMI data using high privilege level instruction
and appropriate trap mechanism. This model is evaluated on a
simulation environment by using ARM Foundation Model 8.0
and has been presented on a real development ARMv8 JUNOr0 board. We finished the comprehensive experiments including
effectiveness and performance, and the result and analysis show
T-VMI has achieved the aim of expected effectiveness with
acceptable performance cost.

Keywords-Trusted VMI; TrustZone; Semantic Gap

",18.12316971661352,16.48589743589743,237
CCGrid_17_051.txt,14.819047319743085,12.554139874106813,CCGrid,6454,"
This paper introduces an approach that handles
with the trustworthy cloud service selection issue in Cloud
computing environments. Despite the fact that most of the
existing trust systems consider several QoS attributes for trust
computing, none of them did consider the correlation that may
exist among these attributes. However, we demonstrate in this
paper that the integration of correlation between QoS attributes
in trust computing significantly helps to solve many issues such
as predicting missing assessment, detecting malicious feedbacks
and improving the accuracy of trust values. The main goal of
this paper is to show the significance of correlation between
those attributes for trust computing. To do that, we propose
combining both the popular Naive Bayes model and the ngram Markov model to design a more efficient trust model for
cloud environments. Our proposed trust model also takes into
account the user’s requirements, aggregates both the qualitative
and quantitative assessment, and considers several sources when
computing cloud services’ trust values. Experimental results show
that our proposed approach outperforms the traditional Naive
Bayes trust models.

Index Terms—Cloud service selection; Naive Bayes; n-gram;
trust attributes

",17.122413403193683,15.115236486486491,187
CCGrid_17_052.txt,13.812355796187774,12.364875362650785,CCGrid,3499,"
Thanks to their high availability, scalability, and
usability, cloud databases have become one of the dominant
cloud services. However, since cloud users do not physically
possess their data, data integrity may be at risk. In this
paper, we present a novel protocol that utilizes crowdsourcing
paradigm to provide practical data integrity assurance in keyvalue cloud databases. The main advantage of our protocol
over previous work is its high applicability - as opposed to
existing approaches, our scheme does not require any system
changes on the cloud side and thus can be applied directly
to any existing system. We demonstrate the feasibility of our
scheme by a prototype implementation and its evaluation.

Keywords-cloud; NoSQL; key-value stores, data integrity;
secure storage;

",16.52667757954773,15.318333333333335,121
CCGrid_17_053.txt,15.978110994213203,14.924124408384046,CCGrid,7813,"
The vast attack surface of clouds presents a challenge in deploying scalable and effective defenses. Traditional
security mechanisms, which work from inside the VM fail to
provide strong protection as attackers can bypass them easily.
The only available option is to provide security from the layer
below the VM i.e., the hypervisor. Previous works that attempt
to secure VMs from “outside” either incur substantial space
or compute overheads making them slow and impractical or
require modifications to the OS or the application codebase. To
address these issues, we propose an anomaly detection fabric
for clouds based on system call monitoring, which compresses
the stream of system calls at their source making the system
scalable and near real-time. Our system requires no modifications
to the guest OS or the application making it ideal for the data
center setting. Additionally, for robust and early detection of
threats, we leverage the notion of VM/container communities that
share information about attacks in their early stages to provide
immunity to the entire deployment. We make certain aspects
of the system flexible so that vendors can tune metrics to offer
customized protection to clients based on their workload types.
Detailed evaluation on a prototype implementation on KVM
substantiates our claims.

",15.903189008614273,14.817249190938515,208
CCGrid_17_054.txt,16.36785224902676,15.338577968199115,CCGrid,7555,"
Today’s large-scale supercomputers are producing
a huge amount of log data. Exploring various potential correlations of fatal events is crucial for understanding their causality
and improving the working efficiency for system administrators. To this end, we developed a toolkit, named LogAider,
that can reveal three types of potential correlations: acrossfield, spatial, and temporal. Across-field correlation refers to
the statistical correlation across fields within a log or across
multiple logs based on probabilistic analysis. For analyzing
the spatial correlation of events, we developed a generic, easyto-use visualizer that can view any events queried by users
on a system machine graph. LogAider can also mine spatial
correlations by an optimized K-meaning clustering algorithm
over a Torus network topology. It is also able to disclose the
temporal correlations (or error propagations) over a certain
period inside a log or across multiple logs, based on an effective
similarity analysis strategy. We assessed LogAider using the
one-year reliability-availability-serviceability (RAS) log of Mira
system (one of the world’s most powerful supercomputers), as
well as its job log. We find that LogAider very helpful for
revealing the potential correlations of fatal system events and
job events, with an accurate mining of across-field correlation
with both precision and recall of 99.9-100%, as well as precise
detection of temporal-correlation with a high similarity (up to
95%) to the ground-truth.

",18.001758247042904,17.30484848484849,235
CCGrid_17_055.txt,15.548197230057237,13.771455223880597,CCGrid,4200,"
Fail-stop errors and Silent Data Corruptions
(SDCs) are the most common failure modes for High Performance Computing (HPC) applications. There are studies
that address fail-stop errors and studies that address SDCs.
However few studies address both types of errors together. In
this paper we propose a software-based selective replication
technique for HPC applications for both fail-stop errors and
SDCs. Since complete replication of applications can be costly
in terms of resources, we develop a runtime-based technique for
selective replication. Selective replication provides an opportunity to meet HPC reliability targets while decreasing resource
costs. Our technique is low-overhead, automatic and completely
transparent to the user.

",13.925175675911131,11.99311688311688,111
CCGrid_17_056.txt,14.113603088378259,11.726897511814798,CCGrid,8378,"
Recently, there has been a growing interest in
enabling fast data analytics by leveraging system capabilities
from large-scale high-performance computing (HPC) systems.
OpenSHMEM is a popular run-time system on HPC systems
that has been used for large-scale compute-intensive scientific
applications. In this paper, we propose to leverage OpenSHMEM
to design a distributed in-memory key-value store for fast data
analytics. Accordingly, we have developed SHMEMCache on
top of OpenSHMEM to leverage its symmetric global memory,
efficient one-sided communication operations and general portability. We have also evaluated SHMEMCache through extensive
experimental studies. Our results show that SHMEMCache has
accomplished significant performance improvements over hte
original Memcached in terms of latency and throughput. Our
evaluation on the Titan supercomputer has also demonstrated
that SHMEMCache can scale to 1024 nodes.

",17.77360955136429,16.00407249466951,135
CCGrid_17_057.txt,15.805434021129557,13.899858601278506,CCGrid,4691,"
Processing In Memory (PIM), the concept of integrating
processing directly with memory, has been attracting a
lot of attention since PIM can assist in overcoming the
throughput limitation caused by data movement between
CPU and memory. The challenge, however, is that it requires
the programmers to have a deep understanding of the PIM
architecture to maximize the benefits such as data locality
and parallel thread execution on multiple PIM devices.
In this study, we present AnalyzeThat, a programmable
shared-memory system for parallel data processing with
PIM devices. Thematic to AnalyzeThat is a rich PIMAware Data Structure (PADS), which is an encapsulation
that integrally ties together the data, the analysis tasks
and the runtime needed to interface with the PIM device
array. The PADS abstraction provides (i) a key-value data
container that allows programmers to easily store data on
multiple PIMs, (ii) a suite of parallel operations with which
users can easily implement data analysis applications, and
(iii) a runtime, hidden to programmers, which provides the
mechanisms needed to overlay both the data and the tasks
on the PIM device array in an intelligent fashion, based
on PIM-specific information collected from the hardware.
We have developed a PIM emulation framework called AnalyzeThat. Our experimental evaluation with representative
data analytics applications suggests that the proposed system
can significantly reduce the PIM programming effort without
losing its technology benefits.
",21.19438992294339,20.304815970056143,230
CCGrid_17_058.txt,14.824211006770057,13.266369099758226,CCGrid,5255,"
Clusters equipped with accelerators such as
graphics processing unit (GPU) and Many Integrated Core
(MIC) are widely used. For such clusters, programmers write
programs for their applications by combining MPI with one of
the available accelerator programming models. In particular,
OpenACC enables programmers to develop their applications
easily, but with lower productivity owing to complex MPI
programming. XcalableACC (XACC) is a new programming
model, which is an “orthogonal” integration of a partitioned
global address space (PGAS) language XcalableMP (XMP) and
OpenACC. While XMP enables distributed-memory programming on both global-view and local-view models, OpenACC
allows operations to be offloaded to a set of accelerators. In the
local-view model, programmers can describe communication
with the coarray features adopted from Fortran 2008, and we
extend them to communication between accelerators. We have
designed and implemented an XACC compiler for NVIDIA
GPU and evaluated its performance and productivity by using
two benchmarks, Himeno benchmark and NAS Parallel Benchmarks CG (NPB-CG). The performance of the XACC version
with the Himeno benchmark and NPB-CG are over 85%
and 97% in the local-view model against the MPI+OpenACC
version, respectively. Moreover, using non-blocking communication makes the performance of local-view version over
89% with the Himeno benchmark. From the viewpoint of
productivity, the local-view model provides an intuitive form
of array assignment statement for communication.

Keywords-Accelerator; GPU; Cluster; PGAS; Coarray; OpenACC;

",16.35954948731386,15.186924564796907,235
CCGrid_17_059.txt,16.19383178737843,14.380323542375411,CCGrid,7313,"
This paper studies the feasibility of efficiently
combining both a software component model and a taskbased model. Task based models are known to enable efficient
executions on recent HPC computing nodes while component
models ease the separation of concerns of application and
thus improve their modularity and adaptability. This paper
describes a prototype version of the COMET programming
model combining concepts of task-based and component models, and a preliminary version of the COMET runtime built
on top of StarPU and L2C. Evaluations of the approach have
been conducted on a real-world use-case analysis of a subpart of the production application GYSELA. Results show that
the approach is feasible and that it enables easy composition
of independent software codes without introducing overheads.
Performance results are equivalent to those obtained with a
plain OpenMP based implementation.

Keywords-HPC; Software Component Model; Task-Based
Model; Task scheduling; Multi-cores; Shared-memory;

",16.439396014739867,15.094380321665088,152
CCGrid_17_060.txt,15.012157951323378,13.402625504006327,CCGrid,7206,"
Supercomputers have batch queues to which parallel jobs with specific requirements are submitted. Commercial
schedulers come with various configurable parameters for the
queues which can be adjusted based on the requirements of
the system. The employed configuration affects both system
utilization and job response times. Often times, choosing an
optimal configuration with good performance is not straightforward and requires good knowledge of the system behavior to
various kinds of workloads. In this paper, we propose a dynamic
scheme for setting queue configurations, namely, the number of
queues, partitioning of the processor space and the mapping of
the queues to the processor partitions, and the processor size
and execution time limits corresponding to the queues based
on the historical workload patterns. We use a novel non-linear
programming formulation for partitioning and mapping of nodes
to the queues for homogeneous HPC systems. We also propose
a novel hybrid partitioned-nonpartitioned scheme for allocating
processors to the jobs submitted to the queues. Our simulation
results for a supercomputer system with 35,000+ CPU cores show
that our hybrid scheme gives up to 74% reduction in queue
waiting times and up to 12% higher utilizations than static queue
configurations.

",16.975882523387877,15.337448979591837,197
CCGrid_17_061.txt,16.870004714999915,15.141060057016592,CCGrid,7489,"
This work addresses the problem of scheduling
user-defined analytic applications, which we define as high-level
compositions of frameworks, their components, and the logic
necessary to carry out work. The key idea in our application
definition, is to distinguish classes of components, including rigid
and elastic types: the first being required for an application to
make progress, the latter contributing to reduced execution times.
We show that the problem of scheduling such applications poses
new challenges, which existing approaches address inefficiently.

Thus, we present the design and evaluation of a novel, flexible
heuristic to schedule analytic applications, that aims at high
system responsiveness, by allocating resources efficiently. Our
algorithm is evaluated using trace-driven simulations, with largescale real system traces: our flexible scheduler outperforms a
baseline approach across a variety of metrics, including application turnaround times, and resource allocation efficiency.

We also present the design and evaluation of a full-fledged
system, which we have called Zoe, that incorporates the ideas
presented in this paper, and report concrete improvements in
terms of efficiency and performance, with respect to prior
generations of our system.

",20.267338824336647,20.162391304347825,185
CCGrid_17_062.txt,16.379716159258408,14.360951963993454,CCGrid,8421,"
Consolidating applications of conflicting service level
objectives (SLOs) to share virtualized resources in cloud datacenters requires efficient resource management to ensure overall high
Quality-of-Service (QoS). Applications of different performance
targets often exhibit different resource demands. Thus, it is not
trivial to translate individual application SLOs to corresponding
resource shares in a shared virtualized environment to meet
performance targets. In this paper, we present CéiriCloud, a
performance-aware resource controlling system, that adaptively
allocates resources, with a resource-share controller and an allocation optimization model. The controller automatically adapts
resource demands based on performance deviations, while the
optimization model resolves conflicts in resource demands from
multiple co-located applications based on their ongoing performance achieved. We implement a proof-of-concept prototype of
CtrlCloud in Python on top of Xen hypervisor. Our experimental
results indicate that CtrlCloud can optimize allocations of CPU
resources across multiple applications to maintain the 95th
percentile latency within predefined SLO targets. CtriCloud
also provides QoS differentiation and yet fulfilling of CPU
share demands from applications is maximized given resource
availability. We further compare CtriCloud against two other
resource allocation methods commonly used in current clouds.
CtrlCloud improves resource utilization by allocating resource
shares optimal to ‘actual needs’ as it employs share-performance
online modeling.

",18.35118385865746,17.717142857142864,211
CCGrid_17_063.txt,16.98317299012971,14.71305233211493,CCGrid,6480,"
Virtualization of computing and communication infrastructures were disseminated as possible solutions for networks evolution and deployment of new services on cloud
data centers. Although promising, their effective application
faces obstacles, mainly caused by rigidity on the management
of communication resources. Currently, the Software-Defined
Networks (SDN) paradigm has been popularizing customization and flexibility in network management due to separation
of control and data planes. However, benefits introduced by
SDN are not trivially applied to Virtual Infrastructures (VIs)
provisioning on SDN-based cloud providers. An allocation
mechanism needs joint information of control and data planes
in order to deliver Quality-of-Service (QoS)-aware mappings
while achieving provider objectives. In this work, we formulate
the online VI allocation on SDN-based cloud data centers as a
Mixed Integer Program (MIP). Following, integer constraints
are relaxed to obtain a linear program, and rounding techniques are applied. The mechanism performs VI allocation considering latency, bandwidth, and virtual machine requirements.
The results indicate that the VIs mean internal latency can be
reduced while simultaneously enforcing other QoS constraints.

",18.12316971661352,16.770689655172415,175
CCGrid_17_064.txt,15.23199770223221,13.611565378604976,CCGrid,5698,"
In cloud-based stream processing services, the
maximum sustainable throughput (MST) is defined as the
maximum throughput that a system composed of a fixed
number of virtual machines (VMs) can ingest indefinitely. If
the incoming data rate exceeds the system’s MST, unprocessed
data accumulates, eventually making the system inoperable.
Thus, it is important for the service provider to keep the
MST always larger than the incoming data rate by dynamically
changing the number of VMs used by the system. In this paper,
we identify a common data processing environment used by
modern data stream processing systems, and we propose MST
prediction models for this environment. We train the models
using linear regression with samples obtained from a few VMs
and predict MST for a larger number of VMs. To minimize
the time and cost for model training, we statistically determine
a set of training samples using Intel’s Storm benchmarks with
representative resource usage patterns. Using typical use-case
benchmarks on Amazon’s EC2 public cloud, our experiments
show that, training with up to 8 VMs, we can predict MST for
streaming applications with less than 4% average prediction
error for 12 VMs, 9% for 16 VMs, and 32% for 24 VMs.
Further, we evaluate our prediction models with simulationbased elastic VM scheduling on a realistic workload. These
simulation results show that with 10% over-provisioning, our
proposed models’ cost efficiency is on par with the cost of
an optimal scaling policy without incurring any service level
agreement violations.

Keywords-cloud computing; performance prediction; resource management; auto-scaling

",17.693802365651003,18.047031250000007,260
CCGrid_17_065.txt,16.7966357510951,15.806847338192274,CCGrid,6613,"
The design and the deployment of energy-efficient
distributed systems is a challenging task, which requires software
engineers to consider all the layers of a system, from hardware to software. In particular, monitoring and analyzing the
power consumption of a distributed system spanning several—
potentially heterogeneous—nodes becomes particularly tedious
when aiming at a finer granularity than observing the power
consumption of hosting nodes. While the state-of-the-art in
software-defined power meters fails to deliver adaptive solutions
to offer such service-level perspective and to cope with the
diversity of hardware CPU architectures, this paper proposes
to automatically learn the power models of the nodes supporting
a distributed system, and then to use these inferred power models
to better understand how the power consumption of the system’s
processes is distributed across nodes at runtime.

Our solution, named WATTSKIT, offers a modular toolkit to
build software-defined power meters “a la carte”, thus dealing
with the diversity of user and hardware requirements. Beyond
the demonstrated capability of covering a wide diversity of CPU
architectures with high accuracy, we illustrate the benefits of
adopting software-defined power meters to analyze the power
consumption of complex layered and distributed systems. In
particular, we illustrate the capability of our approach to monitor
the power consumption of a system composed of Docker SWARM,
WEAVE, ELASTICSEARCH, and Apache ZOOKEEPER. Thanks
to WATTSKIT, developers and administrators are now able to
identify potential power leaks in their software infrastructure.

",20.803037591832787,21.072318501170958,246
CCGrid_17_066.txt,15.957343224061752,13.997013186435584,CCGrid,6321,"
Cloud computing enables end users to execute
high-performance computing applications by renting the required computing power. This pay-for-use approach enables
small enterprises and startups to run HPC-related businesses
with a significant saving in capital investment and a short time
to market.

When deploying an application in the cloud, the users may
a) fail to understand the interactions of the application with the
software layers implementing the cloud system, &) be unaware
of some hardware details of the cloud system, and c) fail
to understand how sharing part of the cloud system with
other users might degrade application performance. These
misunderstandings may lead the users to select suboptimal
cloud configurations in terms of cost or performance.

To aid the users in selecting the optimal cloud configuration
for their applications, we suggest that the cloud provider
generate a prediction model for the provided system. We
propose applying machine-learning techniques to generate this
prediction model. First, the cloud provider profiles a set of
training applications by means of a hardware-independent
profiler and then executes these applications on a set of training
cloud configurations to collect actual performance values. The
prediction model is trained to learn the dependencies of
actual performance data on the application profile and cloud
configuration parameters. The advantage of using a hardwareindependent profiler is that the cloud users and the cloud
provider can analyze applications on different machines and
interface with the same prediction model.

We validate the proposed methodology for a cloud system
implemented with OpenStack. We apply the prediction model
to the NAS parallel benchmarks. The resulting relative error is
below 15% and the Pareto optimal cloud configurations finally
found when maximizing application speed and minimizing
execution cost on the prediction model are also at most 15%
away from the actual optimal solutions.

",18.33330706580912,16.265333333333334,301
CCGrid_17_067.txt,15.486410719143525,14.174791213627422,CCGrid,6467,"
A challenging problem for users of Infrastructureas-a-Service (IaaS) clouds is selecting cloud providers, regions,
and instance types cost-optimally for a given desired service
level. Issues such as hardware heterogeneity, contention, and
virtual machine (VM) placement can result in considerably
differing performance across supposedly equivalent cloud resources. Existing research on cloud benchmarking helps, but
often the focus is on providing low-level microbenchmarks (e.g.,
CPU or network speed), which are hard to map to concrete
business metrics of enterprise cloud applications, such as request
throughput of a multi-tier Web application. In this paper, we
propose OKta, a general approach for fairly and comprehensively
benchmarking the performance and cost of a multi-tier Web
application hosted in an IaaS cloud. We exemplify our approach
for a case study based on the two-tier AcmeAir application,
which we evaluate for 11 real-life deployment configurations on
Amazon EC2 and Google Compute Engine. Our results show
that for this application, choosing compute-optimized instance
types in the Web layer and small bursting instances for the
database tier leads to the overall most cost-effective deployments.
This result held true for both cloud providers. The least costeffective configuration in our study provides only about 67%
of throughput per US dollar spent. Our case study can serve
as a blueprint for future industrial or academic application
benchmarking projects.

",16.32212239822248,15.714837758112097,228
CCGrid_17_068.txt,15.933035051655704,14.158189834077405,CCGrid,3475,"
By massively adopting OpenStack for operating small to large private and public clouds, the industry
has made it one of the largest running software project,
overgrowing the Linux kernel. However, with success comes
increased complexity; facing technical and scientific challenges, developers are in great difficulty when testing the
impact of individual changes on the performance of such
a large codebase, which will likely slow down the evolution
of OpenStack. Thus, we claim it is now time for the
scientific community to join the effort and get involved
in the development of OpenStack, like it has been once
done for Linux.

In this spirit, we developed Enos, an integrated framework that relies on container technologies for deploying
and evaluating OpenStack on any testbed. Enos allows
researchers to easily express different configurations, enabling fine-grained investigations of OpenStack services.
Enos collects performance metrics at runtime and stores
them for post-mortem analysis and sharing. The relevance
of the Enos approach to reproducible research is illustrated by evaluating different OpenStack scenarios on the
Grid’5000 testbed.

",17.93193317476759,16.69365813377374,174
CCGrid_17_069.txt,15.543001133461285,13.391706427959864,CCGrid,5354,"
This paper addresses energy efficient VNF placement and chaining over NFV enabled infrastructures. VNF
placement and chaining are formulated as a decision tree search
to overcome this NP-Hard problem complexity. The proposed
approach is an extension of the Monte Carlo Tree Search
(MCTS) method to achieve energy savings using physical
resource consolidation and sharing VNFs between multiple
tenants. A real cloud testbed and extensive simulations are
used to assess performance and ability to scale with problem
size. Evaluation results show significant reduction in energy
consumption of the proposed placement solution compared to
related work. The polynomial complexity of our proposal is
highlighted by the simulation results.

Keywords-Energy efficiency, VNF placement and chaining,
Green, Decision tree search, SFC, MCTS.

",15.532846611407376,13.909047619047623,121
CCGrid_17_070.txt,16.4370048719891,14.97873344356849,CCGrid,6999,"
Applications hosted in the cloud have become
indispensable in several contexts, with their performance often
being key to business operation and their running costs needing
to be minimized. To minimize running costs, most modern
virtualization technologies such as Linux Containers, Xen, and
KVM offer powerful resource control primitives for individual
provisioning — that enable adding or removing of fraction of
cores and/or megabytes of memory for as short as few seconds.
Despite the technology being ready, there is a lack of proper
techniques for fine-grained resource allocation, because there
is an inherent challenge in determining the correct composition
of resources an application needs, with varying workload, to
ensure deterministic performance.

This paper presents a control-based approach for the management of multiple resources, accounting for the resource consumption, together with the application performance, enabling
fine-grained vertical elasticity. The control strategy ensures
that the application meets the target performance indicators,
consuming as less resources as possible. We carried out an
extensive set of experiments using different applications —interactive with response-time requirements, as well as noninteractive with throughput desires — by varying the workload
mixes of each application over time. The results demonstrate
that our solution precisely provides guaranteed performance
while at the same time avoiding both resource over- and underprovisioning.

",20.267338824336647,19.597941773865944,212
CCGrid_17_071.txt,14.516946672751843,12.549933482996593,CCGrid,6715,"
As a promising alternative to centralized scheduling,
sample-based scheduling is especially suitable for high fan-out
workloads that contain a large number of interactive jobs. Compared to centralized schedulers, existing sample-based schedulers
do not hold a global view of the cluster’s resource status. Instead,
the scheduling decisions are made solely based on the status
of a small set of randomly sampled workers. Although this
simple approach is highly efficient in large clusters, the lack
of global knowledge of the cluster can lead to sub-optimal task
placement decisions and difficulties in enforcing global scheduling
policies. In this paper, we address these challenges in existing
sample-based scheduling approaches by allowing the scheduler
to maintain an approximate version of the global resource
status through caching the worker node’s status extracted from
reply messages. More specifically, we introduce the privatecluster-state technique (PCS) for the scheduler to obtain such
global information. We show that the scheduler can make better
scheduling decisions by utilizing PCS and the scheduler can
become more capable in enforcing global scheduling policies.
The use of PCS is of low cost since it does not initiate new
communication in sample-based scheduling. Our approach is
implemented in PSCSampler, a full distribute sample-based
scheduler, which gains global knowledge from PCS. Experiment
results from both simulation runs and Amazon cluster runs show
that compared to Sparrow, PCSsampler can significantly reduce
both 50°” percentile and 90°"" percentile runtime. The firsttime success rate of PCSsampler in gang scheduling is closer to
an omniscient centralized scheduler than baseline sample based
scheduler.

",17.015998829145012,15.047182512144346,263
CCGrid_17_072.txt,15.43378486999972,13.730342498505681,CCGrid,6553,"
Concerns about protecting personal data and intellectual property are major obstacles to the adoption of cloud
services. To ensure that a cloud tenant’s data cannot be accessed
by malicious code from another tenant, critical software components of different tenants are traditionally deployed on separate
physical machines. However, such physical separation limits
hardware utilization, leading to cost overheads due to inefficient
resource usage. Secure hardware enclaves offer mechanisms to
protect code and data from potentially malicious code deployed
on the same physical machine, thereby offering an alternative
to physical separation. We show how secure hardware enclaves
can be employed to address data protection concerns of cloud
tenants, while optimizing hardware utilization. We provide a
model, formalization and experimental evaluation of an efficient
algorithmic approach to compute an optimized deployment of
software components and virtual machines, taking into account
data protection concerns and the availability of secure hardware
enclaves. Our experimental results suggest that even if only a
small percentage of the physical machines offer secure hardware
enclaves, significant cost savings can be achieved.

Index Terms—virtual machine placement; cloud deployment;
data protection; privacy; secure computing

",17.693802365651003,18.134347826086955,186
CCS_17_001.txt,14.980071078311752,13.424238010098673,CCS,10570,"
Cut-and-choose (C&C) is the standard approach to making Yao’s
garbled circuit two-party computation (2PC) protocol secure against
malicious adversaries. Traditional cut-and-choose operates at the
level of entire circuits, whereas the LEGO paradigm (Nielsen & Orlandi, TCC 2009) achieves asymptotic improvements by performing
cut-and-choose at the level of individual gates. In this work we
propose a unified approach called DUPLO that spans the entire continuum between these two extremes. The cut-and-choose step in
our protocol operates on the level of arbitrary circuit “components,”
which can range in size from a single gate to the entire circuit itself.

With this entire continuum of parameter values at our disposal,
we find that the best way to scale 2PC to computations of realistic
size is to use C&C components of intermediate size, and not at
the extremes. On computations requiring several millions of gates
or more, our more general approach to C&C gives between 4-7x
improvement over existing approaches.

In addition to our technical contributions of modifying and optimizing previous protocol techniques to work with general C&C
components, we also provide an extension of the recent Frigate circuit compiler (Mood et al, EuroS&P 2016) to effectively express any
C-style program in terms of components which can be processed
efficiently using our protocol.
",18.548980349730343,17.682716207559263,225
CCS_17_002.txt,13.073714805831528,11.357535631493807,CCS,10340,"
We propose a simple and efficient framework for obtaining efficient
constant-round protocols for maliciously secure two-party computation. Our framework uses a function-independent preprocessing
phase to generate authenticated information for the two parties;
this information is then used to construct a single “authenticated”
garbled circuit which is transmitted and evaluated. We also show
how to efficiently instantiate the preprocessing phase with a new,
highly optimized version of the TinyOT protocol by Nielsen et al.
Our protocol outperforms existing work in both the singleexecution and amortized settings, with or without preprocessing:

e In the single-execution setting, our protocol evaluates an
AES circuit with malicious security in 37 ms with an online
time of 1 ms. Previous work with the best overall time requires 62 ms (with 14 ms online time); previous work with
the best online time (also 1 ms) requires 124 ms overall.

e If we amortize over 1024 executions, each AES computation
requires just 6.7 ms with roughly the same online time as
above. The best previous work in the amortized setting has
roughly the same total time but does not support functionindependent preprocessing.

Our work shows that the performance penalty for maliciously secure two-party computation (as compared to semi-honest security)
is much smaller than previously believed.
",18.548980349730343,18.104372904091218,215
CCS_17_003.txt,12.674822457086357,10.697283726199892,CCS,10528,"
We propose a new, constant-round protocol for multi-party computation of boolean circuits that is secure against an arbitrary number
of malicious corruptions. At a high level, we extend and generalize recent work of Wang et al. in the two-party setting. Namely,
we design an efficient preprocessing phase that allows the parties
to generate authenticated information; we then show how to use
this information to distributively construct a single “authenticated”
garbled circuit that is evaluated by one party.
Our resulting protocol improves upon the state-of-the-art both
asymptotically and concretely. We validate these claims via several
experiments demonstrating both the efficiency and scalability of
our protocol:
• Efficiency: For three-party computation over a LAN, our
protocol requires only 95 ms to evaluate AES. This is roughly
a 700× improvement over the best prior work, and only 2.5×
slower than the best known result in the two-party setting.
In general, for n-party computation our protocol improves
upon prior work (which was never implemented) by a factor of more than 230n, e.g., an improvement of 3 orders of
magnitude for 5-party computation.
• Scalability: We successfully executed our protocol with a
large number of parties located all over the world, computing (for example) AES with 128 parties across 5 continents
in under 3 minutes. Our work represents the largest-scale
demonstration of secure computation to date.
",16.728156217252725,16.232298850574715,233
CCS_17_004.txt,15.461224318218658,13.901207162018721,CCS,10328,"

Voice biometrics is drawing increasing attention as it is a promising
alternative to legacy passwords for mobile authentication. Recently,
a growing body of work shows that voice biometrics is vulnerable
to spoofing through replay attacks, where an adversary tries to
spoof voice authentication systems by using a pre-recorded voice
sample collected from a genuine user. In this work, we propose
VoiceGesture, a liveness detection system for replay attack detection on smartphones. It detects a live user by leveraging both the
unique articulatory gesture of the user when speaking a passphrase
and the mobile audio hardware advances. Specifically, our system
re-uses the smartphone as a Doppler radar, which transmits a high
frequency acoustic sound from the built-in speaker and listens to
the reflections at the microphone when a user speaks a passphrase.
The signal reflections due to user’s articulatory gesture result in
Doppler shifts, which are then analyzed for live user detection.
VoiceGesture is practical as it requires neither cumbersome operations nor additional hardware but a speaker and a microphone that
are commonly available on smartphones. Our experimental evaluation with 21 participants and different types of phones shows that
it achieves over 99% detection accuracy at around 1% Equal Error
Rate (EER). Results also show that it is robust to different phone
placements and is able to work with different sampling frequencies.
",17.251386760058843,15.97827380952381,226
CCS_17_005.txt,16.85962996503518,16.24317960524094,CCS,11474,"

The goal of this work is to enable user authentication via finger inputs on ubiquitous surfaces leveraging low-cost physical vibration.
We propose VibWrite that extends finger-input authentication beyond touch screens to any solid surface for smart access systems
(e.g., access to apartments, vehicles or smart appliances). It integrates passcode, behavioral and physiological characteristics, and
surface dependency together to provide a low-cost, tangible and
enhanced security solution. VibWrite builds upon a touch sensing technique with vibration signals that can operate on surfaces
constructed from a broad range of materials. It is significantly different from traditional password-based approaches, which only authenticate the password itself rather than the legitimate user, and
the behavioral biometrics-based solutions, which usually involve
specific or expensive hardware (e.g., touch screen or fingerprint
reader), incurring privacy concerns and suffering from smudge attacks. VibWrite is based on new algorithms to discriminate finegrained finger inputs and supports three independent passcode
secrets including PIN number, lock pattern, and simple gestures
by extracting unique features in the frequency domain to capture
both behavioral and physiological characteristics such as contacting area, touching force, and etc. VibWrite is implemented using
a single pair of low-cost vibration motor and receiver that can be
easily attached to any surface (e.g., a door panel, a desk or an appliance). Our extensive experiments demonstrate that VibWrite can
authenticate users with high accuracy (e.g., over 95% within two
trials), low false positive rate (e.g., less 3%) and is robust to various
types of attacks.
",20.385943968408593,19.867099999999997,256
CCS_17_006.txt,13.903220869374024,11.50018740966318,CCS,10062,"
Many popular modern processors include an important hardware
security feature in the form of a DRTM (Dynamic Root of Trust
for Measurement) that helps bootstrap trust and resists software
attacks. However, despite substantial body of prior research on trust
establishment, security of DRTM was treated without involvement
of the human user, who represents a vital missing link. The basic
challenge is: how can a human user determine whether an expected
DRTM is currently active on her device?

In this paper, we define the notion of “presence attestation”,
which is based on mandatory, though minimal, user participation.
We present three concrete presence attestation schemes: sightbased, location-based and scene-based. They vary in terms of security and usability features, and are suitable for different application
contexts. After analyzing their security, we assess their usability
and performance based on prototype implementations.
",15.532846611407376,14.396012332990754,140
CCS_17_007.txt,14.844415565062292,12.913360919197817,CCS,10490,"

Speech recognition (SR) systems such as Siri or Google Now have become an increasingly popular human-computer interaction method,
and have turned various systems into voice controllable systems
(VCS). Prior work on attacking VCS shows that the hidden voice
commands that are incomprehensible to people can control the
systems. Hidden voice commands, though ‘hidden’, are nonetheless audible. In this work, we design a completely inaudible attack,
DolphinAttack, that modulates voice commands on ultrasonic
carriers (e.g., f > 20 kHz) to achieve inaudibility. By leveraging
the nonlinearity of the microphone circuits, the modulated lowfrequency audio commands can be successfully demodulated, recovered, and more importantly interpreted by the speech recognition
systems. We validate DolphinAttack on popular speech recognition systems, including Siri, Google Now, Samsung S Voice, Huawei
HiVoice, Cortana and Alexa. By injecting a sequence of inaudible
voice commands, we show a few proof-of-concept attacks, which
include activating Siri to initiate a FaceTime call on iPhone, activating Google Now to switch the phone to the airplane mode, and even
manipulating the navigation system in an Audi automobile. We propose hardware and software defense solutions. We validate that it
is feasible to detect DolphinAttack by classifying the audios using
supported vector machine (SVM), and suggest to re-design voice
controllable systems to be resilient to inaudible voice command
attacks.
",17.37919286519448,16.915382262996946,219
CCS_17_008.txt,15.317412602280477,13.575478302185623,CCS,9400,"

Learning-based systems have been shown to be vulnerable to evasion through adversarial data manipulation. These attacks have
been studied under assumptions that the adversary has certain
knowledge of either the target model internals, its training dataset
or at least classification scores it assigns to input samples. In this
paper, we investigate a much more constrained and realistic attack scenario wherein the target classifier is minimally exposed to
the adversary, revealing only its final classification decision (e.g.,
reject or accept an input sample). Moreover, the adversary can
only manipulate malicious samples using a blackbox morpher. That
is, the adversary has to evade the targeted classifier by morphing malicious samples “in the dark”. We present a scoring mechanism that can assign a real-value score which reflects evasion
progress to each sample based on the limited information available.
Leveraging on such scoring mechanism, we propose an evasion
method —- EvadeHC- and evaluate it against two PDF malware detectors, namely PDFRATE and Hidost. The experimental evaluation
demonstrates that the proposed evasion attacks are effective, attaining 100% evasion rate on the evaluation dataset. Interestingly,
EvadeHC outperforms the known classifier evasion techniques that
operate based on classification scores output by the classifiers. Although our evaluations are conducted on PDF malware classifiers,
the proposed approaches are domain-agnostic and are of wider
application to other learning-based systems.

",17.353723509956247,16.72115315315315,224
CCS_17_009.txt,15.184664126161827,13.14214636591479,CCS,9443,"
Deep learning has shown impressive performance on hard perceptual problems. However, researchers found deep learning systems
to be vulnerable to small, specially crafted perturbations that are
imperceptible to humans. Such perturbations cause deep learning
systems to mis-classify adversarial examples, with potentially disastrous consequences where safety or security is crucial. Prior defenses against adversarial examples either targeted specific attacks
or were shown to be ineffective.
We propose MagNet, a framework for defending neural network
classifiers against adversarial examples. MagNet neither modifies
the protected classifier nor requires knowledge of the process for
generating adversarial examples. MagNet includes one or more
separate detector networks and a reformer network. The detector
networks learn to differentiate between normal and adversarial examples by approximating the manifold of normal examples. Since
they assume no specific process for generating adversarial examples, they generalize well. The reformer network moves adversarial examples towards the manifold of normal examples, which is
effective for correctly classifying adversarial examples with small
perturbation. We discuss the intrinsic difficulties in defending against
whitebox attack and propose a mechanism to defend against graybox attack. Inspired by the use of randomness in cryptography,
we use diversity to strengthen MagNet. We show empirically that
MagNet is effective against the most advanced state-of-the-art attacks in blackbox and graybox scenarios without sacrificing false
positive rate on normal examples.
",16.850672712058472,16.05198198198198,223
CCS_17_010.txt,15.656946666163705,15.170019424667682,CCS,8873,"

Much of recent research on mobile security has focused on malicious applications. Although mobile devices have powerful browsers
that are commonly used by users and are vulnerable to at least as
many attacks as their desktop counterparts, mobile web security
has not received the attention that it deserves from the community. In particular, there is no longitudinal study that investigates
the evolution of mobile browser vulnerabilities over the diverse
set of browsers that are available out there. In this paper, we undertake the first such study, focusing on UI vulnerabilities among
mobile browsers. We investigate and quantify vulnerabilities to 27
Ul-related attacks—compiled from previous work and augmented
with new variations of our own—across 128 browser families and
2,324 individual browser versions spanning a period of more than
5 years. In the process, we collect an extensive dataset of browser
versions, old and new, from multiple sources. We also design and
implement a browser-agnostic testing framework, called Hindsight,
to automatically expose browsers to attacks and evaluate their vulnerabilities. We use Hindsight to conduct the tens of thousands
of individual attacks that were needed for this study. We discover
that 98.6% of the tested browsers are vulnerable to at least one of
our attacks and that the average mobile web browser is becoming
less secure with each passing year. Overall, our findings support
the conclusion that mobile web security has been ignored by the
community and must receive more attention.
",15.6451,15.302545454545456,245
CCS_17_011.txt,14.963715590366053,13.14722231680419,CCS,12271,"

Timing attacks have been a continuous threat to users’ privacy in
modern browsers. To mitigate such attacks, existing approaches,
such as Tor Browser and Fermata, add jitters to the browser clock so
that an attacker cannot accurately measure an event. However, such
defenses only raise the bar for an attacker but do not fundamentally
mitigate timing attacks, i.e., it just takes longer than previous to
launch a timing attack.

In this paper, we propose a novel approach, called deterministic
browser, which can provably prevent timing attacks in modern
browsers. Borrowing from Physics, we introduce several concepts,
such as an observer and a reference frame. Specifically, a snippet
of JavaScript, i.e., an observer in JavaScript reference frame, will
always obtain the same, fixed timing information so that timing
attacks are prevented; at contrast, a user, i.e., an oracle observer,
will perceive the JavaScript differently and do not experience the
performance slowdown. We have implemented a prototype called
DETERFox and our evaluation shows that the prototype can defend
against browser-related timing attacks.
",16.785175570968402,15.466967418546371,175
CCS_17_012.txt,16.389779367025817,15.615492480674309,CCS,9893,"
Modern web browsers have accrued an incredibly broad set of
features since being invented for hypermedia dissemination in
1990. Many of these features benefit users by enabling new types
of web applications. However, some features also bring risk to
users’ privacy and security, whether through implementation error,
unexpected composition, or unintended use. Currently there is
no general methodology for weighing these costs and benefits.
Restricting access to only the features which are necessary for
delivering desired functionality on a given website would allow
users to enforce the principle of lease privilege on use of the myriad
APIs present in the modern web browser.

However, security benefits gained by increasing restrictions must
be balanced against the risk of breaking existing websites. This
work addresses this problem with a methodology for weighing
the costs and benefits of giving websites default access to each
browser feature. We model the benefit as the number of websites
that require the feature for some user-visible benefit, and the cost
as the number of CVEs, lines of code, and academic attacks related
to the functionality. We then apply this methodology to 74 Web
API standards implemented in modern browsers. Wefi nd that allowing websites default access to large parts of the Web API poses
significant security and privacy risks, with little corresponding
benefit.

We also introduce a configurable browser extension that allows
users to selectively restrict access to low-benefit, high-risk features
on a per site basis. We evaluated our extension with two hardened
browser configurations, and found that blocking 15 of the 74 standards avoids 52.0% of code paths related to previous CVEs, and
50.0% of implementation code identified by our metric, without
affecting the functionality of 94.7% of measured websites.
",16.52667757954773,15.73401567944251,291
CCS_17_013.txt,13.657382501245657,12.052258797795457,CCS,7118,"

In the Bitcoin system, participants are rewarded for solving cryptographic puzzles. In order to receive more consistent rewards over
time, some participants organize mining pools and split the rewards
from the pool in proportion to each participant’s contribution. However, several attacks threaten the ability to participate in pools. The
block withholding (BWH) attack makes the pool reward system unfair by letting malicious participants receive unearned wages while
only pretending to contribute work. When two pools launch BWH
attacks against each other, they encounter the miner’s dilemma:
in a Nash equilibrium, the revenue of both pools is diminished. In
another attack called selfish mining, an attacker can unfairly earn
extra rewards by deliberately generating forks.

In this paper, we propose a novel attack called a fork after withholding (FAW) attack. FAW is not just another attack. The reward
for an FAW attacker is always equal to or greater than that for a BWH
attacker, and it is usable up to four times more often per pool than
in BWH attack. When considering multiple pools — the current
state of the Bitcoin network — the extra reward for an FAW attack
is about 56% more than that for a BWH attack. Furthermore, when
two pools execute FAW attacks on each other, the miner’s dilemma
may not hold: under certain circumstances, the larger pool can
consistently win. More importantly, an FAW attack, while using
intentional forks, does not suffer from practicality issues, unlike
selfish mining. We also discuss partial countermeasures against
the FAW attack, but finding a cheap and efficient countermeasure
remains an open problem. As a result, we expect to see FAW attacks
among mining pools.
",13.484332010920856,12.116554621848742,276
CCS_17_014.txt,13.534559785071288,11.230870746478569,CCS,12362,"

Cloud computing has become an irreversible trend. Together comes
the pressing need for verifiability, to assure the client the correctness of computation outsourced to the cloud. Existing verifiable
computation techniques all have a high overhead, thus if being
deployed in the clouds, would render cloud computing more expensive than the on-premises counterpart. To achieve verifiability at
a reasonable cost, we leverage game theory and propose a smart
contract based solution. In a nutshell, a client lets two clouds compute the same task, and uses smart contracts to stimulate tension,
betrayal and distrust between the clouds, so that rational clouds
will not collude and cheat. In the absence of collusion, verification
of correctness can be done easily by crosschecking the results from
the two clouds. We provide a formal analysis of the games induced
by the contracts, and prove that the contracts will be effective under
certain reasonable assumptions. By resorting to game theory and
smart contracts, we are able to avoid heavy cryptographic protocols.
The client only needs to pay two clouds to compute in the clear,
and a small transaction fee to use the smart contracts. We also conducted a feasibility study that involves implementing the contracts
in Solidity and running them on the official Ethereum network.

",14.554592549557764,12.716980861244021,210
CCS_17_015.txt,13.939797006859795,12.489338932349039,CCS,11587,"

Zero Knowledge Contingent Payment (ZKCP) protocols allow fair
exchange of sold goods and payments over the Bitcoin network. In
this paper we point out two main shortcomings of current proposals
for ZKCP, and propose ways to address them.

First we show an attack that allows a buyer to learn partial
information about the digital good being sold, without paying for
it. This break in the zero-knowledge condition of ZKCP is due to
the fact that in the protocols we attack, the buyer is allowed to
choose common parameters that normally should be selected by
a trusted third party. We implemented and tested this attack: we
present code that learns, without paying, the value of a Sudoku
cell in the “Pay-to-Sudoku” ZKCP implementation [18]. We also
present ways to fix this attack that do not require a trusted third
party.

Second, we show that ZKCP are not suited for the purchase of
digital services rather than goods. Current constructions of ZKCP
do not allow a seller to receive payments after proving that a certain
service has been rendered, but only for the sale of a specific digital
good. We define the notion of Zero-Knowledge Contingent Service
Payment (ZKCSP) protocols and construct two new protocols, for
either public or private verification. We implemented our ZKCSP
protocols for Proofs of Retrievability, where a client pays the server
for providing a proof that the client’s data is correctly stored by
the server.We also implement a secure ZKCP protocol for “Pay-toSudoku"" via our ZKCSP protocol, which does not require a trusted
third party.

A side product of our implementation effort is a new optimized
circuit for SHA256 with less than @ quarter than the number of AND
gates of the best previously publicly available one. Our new SHA256
circuit may be of independent use for circuit-based MPC and FHE
protocols that require SHA256 circuits.

",14.06817628641468,13.630286624203823,317
CCS_17_016.txt,15.366769127193571,14.234252094041505,CCS,8521,"
 Following this exciting trend, we consider the problem of run
This paper considers the problem of running a long-term on-demand
service for executing actively-secure computations. We examined
state-of-the-art tools and implementations for actively-secure computation and identified a set of key features indispensable to offer
meaningful service like this. Since no satisfactory tools exist for the
purpose, we developed Poot, a new tool for building and executing
actively-secure computation protocols at extreme scales with nearly
zero offline delay. With PooL, we are able to obliviously execute, for
the first time, reactive computations like ORAM in the malicious
threat model. Many technical benefits of Poot can be attributed to
the concept of pool-based cut-and-choose. We show with experiments that this idea has significantly improved the scalability and
usability of JIMU [38], a state-of-the-art LEGO protocol.
",16.728156217252725,15.15683673469388,148
CCS_17_017.txt,13.741817390476989,12.031764673709109,CCS,17083,"

Protocols for secure multiparty computation enable a set of parties
to compute a function of their inputs without revealing anything
but the output. The security properties of the protocol must be
preserved in the presence of adversarial behavior. The two classic
adversary models considered are semi-honest (where the adversary
follows the protocol specification but tries to learn more than allowed by examining the protocol transcript) and malicious (where
the adversary may follow any arbitrary attack strategy). Protocols
for semi-honest adversaries are often far more efficient, but in many
cases the security guarantees are not strong enough.

In this paper, we present a new efficient method for “compiling”
a large class of protocols that are secure in the presence of semihonest adversaries into protocols that are secure in the presence
of malicious adversaries. Our method assumes an honest majority
(Le., that t < n/2 where t is the number of corrupted parties and n
is the number of parties overall), and is applicable to many semihonest protocols based on secret-sharing. In order to achieve high
efficiency, our protocol is secure with abort and does not achieve
fairness, meaning that the adversary may receive output while the
honest parties do not.

We present a number of instantiations of our compiler, and obtain protocol variants that are very efficient for both a small and
large number of parties. We implemented our protocol variants and
ran extensive experiments to compare them with each other. Our
results show that secure computation with an honest majority can
be practical, even with security in the presence of malicious adversaries. For example, we securely compute a large arithmetic circuit
of depth 20 with 1,000,000 multiplication gates, in approximately
0.5 seconds with three parties, and approximately 29 seconds with
50 parties, and just under 1 minute with 90 parties.
",17.122413403193683,16.653313397129192,307
CCS_17_018.txt,14.536069717788706,13.073201079990223,CCS,10793,"

While the feasibility of constant-round and actively secure MPC has
been known for over two decades, the last few years have witnessed
a flurry of designs and implementations that make its deployment
a palpable reality. To our knowledge, however, existing concretely
efficient MPC constructions are only for up to three parties.

In this paper we design and implement a new actively secure 5PC
protocol tolerating two corruptions that requires 8 rounds of interaction, only uses fast symmetric-key operations, and incurs 60% less
communication than the passively secure state-of-the-art solution
from the work of Ben-Efraim, Lindell, and Omri [CCS 2016]. For
example, securely evaluating the AES circuit when the parties are
in different regions of the U.S. and Europe only takes 1.8s which is
2.6x faster than the passively secure 5PC in the same environment.

Instrumental for our efficiency gains (less interaction, only symmetric key primitives) is a new 4-party primitive we call Attested OT,
which in addition to Sender and Receiver involves two additional
“assistant parties” who will attest to the respective inputs of both
parties, and which might be of broader applicability in practically
relevant MPC scenarios. Finally, we also show how to generalize our
construction to n parties with similar efficiency properties where
the corruption threshold is t ~ -Ym, and propose a combinatorial
problem which, if solved optimally, can yield even better corruption
thresholds for the same cost.

",20.581828153500815,20.951428571428576,241
CCS_17_019.txt,15.14892219577218,14.486985742874833,CCS,10721,"

Text passwords—a frequent vector for account compromise, yet
still ubiquitous—have been studied for decades by researchers attempting to determine how to coerce users to create passwords that
are hard for attackers to guess but still easy for users to type and
memorize. Most studies examine one password or a small number
of passwords per user, and studies often rely on passwords created
solely for the purpose of the study or on passwords protecting
low-value accounts. These limitations severely constrain our understanding of password security in practice, including the extent and
nature of password reuse, password behaviors specific to categories
of accounts (e.g., financial websites), and the effect of password
managers and other privacy tools.

In this paper we report on an in situ study of 154 participants
over an average of 147 days each. Participants’ computers were
instrumented—with careful attention to privacy—to record detailed
information about password characteristics and usage, as well as
many other computing behaviors such as use of security and privacy
web browser extensions. This data allows a more accurate analysis
of password characteristics and behaviors across the full range of
participants’ web-based accounts. Examples of our findings are
that the use of symbols and digits in passwords predicts increased
likelihood of reuse, while increased password strength predicts
decreased likelihood of reuse; that password reuse is more prevalent
than previously believed, especially when partial reuse is taken
into account; and that password managers may have no impact
on password reuse or strength. We also observe that users can be
grouped into a handful of behavioral clusters, representative of
various password management strategies. Our findings suggest
that once a user needs to manage a larger number of passwords,
they cope by partially and exactly reusing passwords across most
of their accounts.

",17.37919286519448,18.40144927536232,301
CCS_17_020.txt,14.591963740842818,13.190611018649957,CCS,10511,"

Passwords are still a mainstay of various security systems, as well
as the cause of many usability issues. For end-users, many of these
issues have been studied extensively, highlighting problems and
informing design decisions for better policies and motivating research into alternatives. However, end-users are not the only ones
who have usability problems with passwords! Developers who are
tasked with writing the code by which passwords are stored must
do so securely. Yet history has shown that this complex task often fails due to human error with catastrophic results. While an
end-user who selects a bad password can have dire consequences,
the consequences of a developer who forgets to hash and salt a
password database can lead to far larger problems. In this paper we
present a first qualitative usability study with 20 computer science
students to discover how developers deal with password storage
and to inform research into aiding developers in the creation of
secure password systems.

",14.348710955821954,13.828447204968949,162
CCS_17_021.txt,14.31333418477993,13.457399130859223,CCS,17417,"

Password checking systems traditionally allow login only if the correct password is submitted. Recent work on typo-tolerant password
checking suggests that usability can be improved, with negligible
security loss, by allowing a small number of typographical errors.
Existing systems, however, can only correct a handful of errors,
such as accidentally leaving caps lock on or incorrect capitalization
of the first letter in a password. This leaves out numerous kinds of
typos made by users, such as transposition errors, substitutions, or
capitalization errors elsewhere in a password. Some users therefore
receive no benefit from existing typo-tolerance mechanisms.

We introduce personalized typo-tolerant password checking. In
our approach, the authentication system learns over time the typos
made by a specific user. In experiments using Mechanical Turk, we
show that 45% of users would benefit from personalization. Therefore, we design a system, called TypTop, that securely implements
personalized typo-tolerance. Underlying TypTop is a new stateful
password-based encryption scheme that can be used to store recent
failed login attempts. Our formal analysis shows that security in
the face of an attacker that obtains the state of the system reduces
to the difficulty of a brute-force dictionary attack against the real
password. We implement TypTop for Linux and Mac OS login and
report on a proof-of-concept deployment.


",14.191785591663535,13.564942922374431,220
CCS_17_022.txt,16.091570246840366,14.845942243629285,CCS,10343,"
 discuss the significant improvements that non-expert human assis
Software permeates every aspect of our world, from our homes to
the infrastructure that provides mission-critical services.

As the size and complexity of software systems increase, the
number and sophistication of software security flaws increase as
well. The analysis of these flaws began as a manual approach, but it
soon became apparent that a manual approach alone cannot scale,
and that tools were necessary to assist human experts in this task,
resulting in a number of techniques and approaches that automated
certain aspects of the vulnerability analysis process.

Recently, DARPA carried out the Cyber Grand Challenge, a
competition among autonomous vulnerability analysis systems
designed to push the tool-assisted human-centered paradigm into
the territory of complete automation, with the hope that, by removing the human factor, the analysis would be able to scale to
new heights. However, when the autonomous systems were pitted
against human experts it became clear that certain tasks, albeit
simple, could not be carried out by an autonomous system, as they
require an understanding of the logic of the application under analysis.

Based on this observation, we propose a shift in the vulnerability analysis paradigm, from tool-assisted human-centered to
human-assisted tool-centered. In this paradigm, the automated system orchestrates the vulnerability analysis process, and leverages
humans (with different levels of expertise) to perform well-defined
sub-tasks, whose results are integrated in the analysis. As a result, it
is possible to scale the analysis to a larger number of programs, and,
at the same time, optimize the use of expensive human resources.

In this paper, we detail our design for a human-assisted automated vulnerability analysis system, describe its implementation
atop an open-sourced autonomous vulnerability analysis system
that participated in the Cyber Grand Challenge, and evaluate and
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.

",18.243605946275583,18.333266832917705,404
CCS_17_023.txt,15.316350971307013,13.534171022692338,CCS,6416,"

The problem of cross-platform binary code similarity detection
aims at detecting whether two binary functions coming from different platforms are similar or not. It has many security applications, including plagiarism detection, malware detection, vulnerability search, etc. Existing approaches rely on approximate graphmatching algorithms, which are inevitably slow and sometimes
inaccurate, and hard to adapt to a new task. To address these issues,
in this work, we propose a novel neural network-based approach to
compute the embedding, i.e., a numeric vector, based on the control
flow graph of each binary function, then the similarity detection
can be done efficiently by measuring the distance between the
embeddings for two functions. We implement a prototype called
Gemini. Our extensive evaluation shows that Gemini outperforms
the state-of-the-art approaches by large margins with respect to
similarity detection accuracy. Further, Gemini can speed up prior
art’s embedding generation time by 3 to 4 orders of magnitude and
reduce the required training time from more than 1 week down to
30 minutes to 10 hours. Our real world case studies demonstrate
that Gemini can identify significantly more vulnerable firmware
images than the state-of-the-art, ie., Genius. Our research showcases a successful application of deep learning on computer security
problems.


",17.631426480028413,15.881020733652317,212
CCS_17_024.txt,14.649097819767235,12.979661661434115,CCS,6115,"
As modern attacks become more stealthy and persistent, detecting
or preventing them at their early stages becomes virtually impossible. Instead, an attack investigation or provenance system aims
to continuously monitor and log interesting system events with
minimal overhead. Later, if the system observes any anomalous
behavior, it analyzes the log to identify who initiated the attack and
which resources were affected by the attack and then assess and
recover from any damage incurred. However, because of a fundamental tradeoff between log granularity and system performance,
existing systems typically record system-call events without detailed program-level activities (e.g., memory operation) required
for accurately reconstructing attack causality or demand that every monitored program be instrumented to provide program-level
information.

To address this issue, we propose RAIN, a Refinable Attack Investigation system based on a record-replay technology that records
system-call events during runtime and performs instruction-level
dynamic information flow tracking (DIFT) during on-demand process replay. Instead of replaying every process with DIFT, RAIN conducts system-call-level reachability analysis to filter out unrelated
processes and to minimize the number of processes to be replayed,
making inter-process DIFT feasible. Evaluation results show that
Raw effectively prunes out unrelated processes and determines
attack causality with negligible false positive rates. In addition, the
runtime overhead of Rarn is similar to existing system-call level
provenance systems and its analysis overhead is much smaller than
full-system DIFT.

",19.906493383657665,20.306617647058825,240
CCS_17_025.txt,14.900814624272563,12.497047630548952,CCS,9577,"
Existing probabilistic privacy enforcement approaches permit the
execution of a program that processes sensitive data only if the
information it leaks is within the bounds specified by a given policy.
Thus, to extract any information, users must manually design a
program that satisfies the policy.
In this work, we present a novel synthesis approach that automatically transforms a program into one that complies with a given
policy. Our approach consists of two ingredients. First, we phrase
the problem of determining the amount of leaked information as
Bayesian inference, which enables us to leverage existing probabilistic programming engines. Second, we present two synthesis
procedures that add uncertainty to the program’s outputs as a way
of reducing the amount of leaked information: an optimal one based
on SMT solving and a greedy one with quadratic running time.
We implemented and evaluated our approach on 10 representative programs from multiple application domains. We show that
our system can successfully synthesize a permissive enforcement
mechanism for all examples.
",16.678067442207542,15.213536585365858,166
CCS_17_026.txt,14.606275170588493,12.292379054064266,CCS,8408,"

Mature push button tools have emerged for checking trace properties (e.g. secrecy or authentication) of security protocols. The case
of indistinguishability-based privacy properties (e.g. ballot privacy
or anonymity) is more complex and constitutes an active research
topic with several recent propositions of techniques and tools.

We explore a novel approach based on type systems and provide
a (sound) type system for proving equivalence of protocols, for
a bounded or an unbounded number of sessions. The resulting
prototype implementation has been tested on various protocols
of the literature. It provides a significant speed-up (by orders of
magnitude) compared to tools for a bounded number of sessions
and complements in terms of expressiveness other state-of-the-art
tools, such as ProVerif and Tamarin: e.g., we show that our analysis
technique is the first one to handle a faithful encoding of the Helios
e-voting protocol in the context of an untrusted ballot box.
",17.80541091248751,17.49342857142857,158
CCS_17_027.txt,14.922992755353778,13.331113971482107,CCS,7576,"

A large amount of valuable information resides in decentralized
social graphs, where no entity has access to the complete graph
structure. Instead, each user maintains locally a limited view of
the graph. For example, in a phone network, each user keeps a
contact list locally in her phone, and does not have access to other
users’ contacts. The contact lists of all users form an implicit social
graph that could be very useful to study the interaction patterns
among different populations. However, due to privacy concerns,
one could not simply collect the unfettered local views from users
and reconstruct a decentralized social network.

In this paper, we investigate techniques to ensure local differential privacy of individuals while collecting structural information
and generating representative synthetic social graphs. We show
that existing local differential privacy and synthetic graph generation techniques are insufficient for preserving important graph
properties, due to excessive noise injection, inability to retain important graph structure, or both. Motivated by this, we propose
LDPGen, a novel multi-phase technique that incrementally clusters users based on their connections to different partitions of the
whole population. Every time a user reports information, LDPGen
carefully injects noise to ensure local differential privacy. We derive
optimal parameters in this process to cluster structurally-similar
users together. Once a good clustering of users is obtained, LDPGen adapts existing social graph generation models to construct a
synthetic social graph.

We conduct comprehensive experiments over four real datasets
to evaluate the quality of the obtained synthetic graphs, using a

",16.52667757954773,15.076190476190472,253
CCS_17_028.txt,15.324821527616795,14.110844105290873,CCS,10716,"

Scaling the transaction throughput of decentralized blockchain
ledgers such as Bitcoin and Ethereum has been an ongoing challenge. Two-party duplex payment channels have been designed
and used as building blocks to construct linked payment networks,
which allow atomic and trust-free payments between parties without exhausting the resources of the blockchain.

Once a payment channel, however, is depleted (e.g., because
transactions were mostly unidirectional) the channel would need
to be closed and re-funded to allow for new transactions. Users are
envisioned to entertain multiple payment channels with different
entities, and as such, instead of refunding a channel (which incurs
costly on-chain transactions), a user should be able to leverage his
existing channels to rebalance a poorly funded channel.

To the best of our knowledge, we present the first solution that
allows an arbitrary set of users in a payment channel network to
securely rebalance their channels, according to the preferences
of the channel owners. Except in the case of disputes (similar to
conventional payment channels), our solution does not require onchain transactions and therefore increases the scalability of existing
blockchains. In our security analysis, we show that an honest participant cannot lose any of its funds while rebalancing. We finally
provide a proof of concept implementation and evaluation for the
Ethereum network.
",16.52667757954773,16.241017441860468,217
CCS_17_029.txt,14.309306554095187,13.189925916554056,CCS,8712,"

Permissionless blockchains protocols such as Bitcoin are inherently
limited in transaction throughput and latency. Current efforts to
address this key issue focus on off-chain payment channels that
can be combined in a Payment-Channel Network (PCN) to enable
an unlimited number of payments without requiring to access the
blockchain other than to register the initial and final capacity of
each channel. While this approach paves the way for low latency
and high throughput of payments, its deployment in practice raises
several privacy concerns as well as technical challenges related to
the inherently concurrent nature of payments that have not been
sufficiently studied so far.

In this work, we lay the foundations for privacy and concurrency in PCNs, presenting a formal definition in the Universal
Composability framework as well as practical and provably secure solutions. In particular, we present Fulgor and Rayo. Fulgor
is the first payment protocol for PCNs that provides provable privacy guarantees for PCNs and is fully compatible with the Bitcoin
scripting system. However, Fulgor is a blocking protocol and therefore prone to deadlocks of concurrent payments as in currently
available PCNs. Instead, Rayo is the first protocol for PCNs that
enforces non-blocking progress (i.e., at least one of the concurrent
payments terminates). We show through a new impossibility result that non-blocking progress necessarily comes at the cost of
weaker privacy. At the core of Fulgor and Rayo is Multi-Hop HTLC,
a new smart contract, compatible with the Bitcoin scripting system,
that provides conditional payments while reducing running time
and communication overhead with respect to previous approaches.
Our performance evaluation of Fulgor and Rayo shows that a payment with 10 intermediate users takes as few as 5 seconds, thereby
demonstrating their feasibility to be deployed in practice.

",16.691746362846608,15.90125349053677,295
CCS_17_030.txt,14.360436864307953,12.845214221484401,CCS,11596,"

Bitcoin owes its success to the fact that transactions are transparently recorded in the blockchain, a global public ledger that removes
the need for trusted parties. Unfortunately, recording every transaction in the blockchain causes privacy, latency, and scalability issues.
Building on recent proposals for “micropayment channels” — two
party associations that use the ledger only for dispute resolution
— we introduce techniques for constructing anonymous payment
channels. Our proposals allow for secure, instantaneous and private
payments that substantially reduce the storage burden on the payment network. Specifically, we introduce three channel proposals,
including a technique that allows payments via untrusted intermediaries. We build a concrete implementation of our scheme and
show that it can be deployed via a soft fork to existing anonymous
currencies such as ZCash.

",15.903189008614273,15.7574,126
CCS_17_031.txt,13.766350427937976,12.747665251198182,CCS,9095,"

Oblivious Random Access Machine (ORAM) enables a client to
access her data without leaking her access patterns. Existing clientefficient ORAMs either achieve O(log N) client-server communication blowup without heavy computation, or O(1) blowup but
with expensive homomorphic encryptions. It has been shown that
O(log N) bandwidth blowup might not be practical for certain applications, while schemes with O(1) communication blowup incur
even more delay due to costly homomorphic operations.

In this paper, we propose a new distributed ORAM scheme referred to as Shamir Secret Sharing ORAM (S?ORAM), which achieves
O(1) client-server bandwidth blowup and O(1) blocks of client storage without relying on costly partial homomorphic encryptions.
S3ORAM harnesses Shamir Secret Sharing, tree-based ORAM structure and a secure multi-party multiplication protocol to eliminate
costly homomorphic operations and, therefore, achieves O(1) clientserver bandwidth blowup with a high computational efficiency. We
conducted comprehensive experiments to assess the performance
of S3ORAM and its counterparts on actual cloud environments,
and showed that S?7ORAM achieves three orders of magnitude lower
end-to-end delay compared to alternatives with O(1) client communication blowup (Onion-ORAM), while it is one order of magnitude
faster than Path-ORAM for a network with a moderate bandwidth
quality. We have released the implementation of SSORAM for further improvement and adaptation.

",15.616094167265928,16.297548579970105,224
CCS_17_032.txt,14.516946672751843,12.65254373540083,CCS,10325,"

Write-Only Oblivious RAM (WoORAM) protocols provide privacy
by encrypting the contents of data and also hiding the pattern of
write operations over that data. WoOORAMs provide better privacy
than plain encryption and better performance than more general
ORAM schemes (which hide both writing and reading access patterns), and the write-oblivious setting has been applied to important
applications of cloud storage synchronization and encrypted hidden
volumes. In this paper, we introduce an entirely new technique for
Write-Only ORAM, called DetWoORAM. Unlike previous solutions,
DetWoORAM uses a deterministic, sequential writing pattern without the need for any “stashing” of blocks in local state when writes
fail. Our protocol, while conceptually simple, provides substantial
improvement over prior solutions, both asymptotically and experimentally. In particular, under typical settings the Det-WoORAM
writes only 2 blocks (sequentially) to backend memory for each
block written to the device, which is optimal. We have implemented
our solution using the BUSE (block device in user-space) module
and tested DetWoORAM against both an encryption only baseline
of dm-crypt and prior, randomized WoORAM solutions, measuring
only a 3x-14x slowdown compared to an encryption-only baseline
and around 6x-19x speedup compared to prior work.

",17.122413403193683,17.49613783201723,200
CCS_17_033.txt,15.435032591254913,14.573482861348776,CCS,8845,"

We design and implement a Distributed Oblivious Random Access
Memory (DORAM) data structure that is optimized for use in twoparty secure computation protocols. We improve upon the access
time of previous constructions by a factor of up to ten, their memory
overhead by a factor of one hundred or more, and their initialization
time by a factor of thousands. We are able to instantiate ORAMs
that hold 234 bytes, and perform operations on them in seconds,
which was not previously feasible with any implemented scheme.

Unlike prior ORAM constructions based on hierarchical hashing [19], permutation [19], or trees [39], our Distributed ORAM is
derived from the new Function Secret Sharing scheme introduced
by Boyle, Gilboa and Ishai [11, 12]. This significantly reduces the
amount of secure computation required to implement an ORAM
access, albeit at the cost of O(n) efficient local memory operations.

We implement our construction and find that, despite its poor
O(n) asymptotic complexity, it still outperforms the fastest previously known constructions, Circuit ORAM [42] and Square-root
ORAM [55], for datasets that are 32 KiB or larger, and outperforms
prior work on applications such as stable matching [16] or binary
search [23] by factors of two to ten.

",18.422482065455632,18.472980295566504,204
CCS_17_034.txt,15.131180601913751,13.450228515922444,CCS,6377,"
 scammers set up phishing websites on domains resembling well
Domain names have been exploited for illicit online activities for
decades. In the past, miscreants mostly registered new domains
for their attacks. However, the domains registered for malicious
purposes can be deterred by existing reputation and blacklisting
systems. In response to the arms race, miscreants have recently
adopted a new strategy, called domain shadowing, to build their
attack infrastructures. Specifically, instead of registering new domains, miscreants are beginning to compromise legitimate ones
and spawn malicious subdomains under them. This has rendered
almost all existing countermeasures ineffective and fragile because
subdomains inherit the trust of their apex domains, and attackers
can virtually spawn an infinite number of shadowed domains.

In this paper, we conduct the first study to understand and detect
this emerging threat. Bootstrapped with a set of manually confirmed
shadowed domains, we identify a set of novel features that uniquely
characterize domain shadowing by analyzing the deviation from
their apex domains and the correlation among different apex domains. Building upon these features, we train a classifier and apply
it to detect shadowed domains on the daily feeds of VirusTotal, a
large open security scanning service. Our study highlights domain
shadowing as an increasingly rampant threat. Moreover, while previously confirmed domain shadowing campaigns are exclusively
involved in exploit kits, we reveal that they are also widely exploited
for phishing attacks. Finally, we observe that instead of algorithmically generating subdomain names, several domain shadowing
cases exploit the wildcard DNS records.

",16.32212239822248,15.165967741935486,249
CCS_17_035.txt,15.511032622021236,14.204020275384014,CCS,11767,"

Hosting providers play a key role in fighting web compromise,
but their ability to prevent abuse is constrained by the security
practices of their own customers. Shared hosting, offers a unique
perspective since customers operate under restricted privileges and
providers retain more control over configurations. We present the
first empirical analysis of the distribution of web security features
and software patching practices in shared hosting providers, the
influence of providers on these security practices, and their impact
on web compromise rates. We construct provider-level features on
the global market for shared hosting — containing 1,259 providers —
by gathering indicators from 442,684 domains. Exploratory factor
analysis of 15 indicators identifies four main latent factors that
capture security efforts: content security, webmaster security, web
infrastructure security and web application security. We confirm,
via a fixed-effect regression model, that providers exert significant
influence over the latter two factors, which are both related to the
software stack in their hosting environment. Finally, by means of
GLM regression analysis of these factors on phishing and malware
abuse, we show that the four security and software patching factors
explain between 10% and 19% of the variance in abuse at providers,
after controlling for size. For web-application security for instance,
we found that when a provider moves from the bottom 10% to
the best-performing 10%, it would experience 4 times fewer phishing incidents. We show that providers have influence over patch
levels—even higher in the stack, where CMSes can run as client-side
software—and that this influence is tied to a substantial reduction
in abuse levels.

",18.481644305966572,17.66212121212121,267
CCS_17_036.txt,16.38353143222482,15.335130071568653,CCS,8820,"

Domain squatting is a common adversarial practice where attackers register domain names that are purposefully similar to popular
domains. In this work, we study a specific type of domain squatting
called “combosquatting,” in which attackers register domains that
combine a popular trademark with one or more phrases (e.g., betterfacebook[.]com, youtube-live[.]com). We perform the first largescale, empirical study of combosquatting by analyzing more than
468 billion DNS records—collected from passive and active DNS data
sources over almost six years. We find that almost 60% of abusive
combosquatting domains live for more than 1,000 days, and even
worse, we observe increased activity associated with combosquatting year over year. Moreover, we show that combosquatting is
used to perform a spectrum of different types of abuse including
phishing, social engineering, affiliate abuse, trademark abuse, and
even advanced persistent threats. Our results suggest that combosquatting is a real problem that requires increased scrutiny by
the security community.


",17.122413403193683,16.788461538461537,159
CCS_17_037.txt,14.476065897035785,12.881686727078105,CCS,10856,"
 Modern ML models, especially artificial neural networks, have

Machine learning (ML) is becoming a commodity. Numerous ML
frameworks and services are available to data holders who are not
ML experts but want to train predictive models on their data. It is
important that ML models trained on sensitive inputs (e.g., personal
images or documents) not leak too much information about the
training data.

We consider a malicious ML provider who supplies model-training
code to the data holder, does not observe the training, but then obtains white- or black-box access to the resulting model. In this
setting, we design and implement practical algorithms, some of
them very similar to standard ML techniques such as regularization
and data augmentation, that “memorize” information about the
training dataset in the model—yet the model is as accurate and
predictive as a conventionally trained model. We then explain how
the adversary can extract memorized information from the model.

We evaluate our techniques on standard ML tasks for image
classification (CIFAR10), face recognition (LFW and FaceScrub),
and text analysis (20 Newsgroups and IMDB). In all cases, we show
how our algorithms create models that have high predictive power
yet allow accurate extraction of subsets of their training data.

",16.218646115125612,15.409901960784314,206
CCS_17_038.txt,15.270465596971455,14.109823164474005,CCS,8023,"

Deep Learning has recently become hugely popular in machine
learning for its ability to solve end-to-end learning systems, in
which the features and the classifiers are learned simultaneously,
providing significant improvements in classification accuracy in
the presence of highly-structured and large databases.

Its success is due to a combination of recent algorithmic breakthroughs, increasingly powerful computers, and access to significant amounts of data.

Researchers have also considered privacy implications of deep
learning. Models are typically trained in a centralized manner with
all the data being processed by the same training algorithm. If the
data is a collection of users’ private data, including habits, personal
pictures, geographical positions, interests, and more, the centralized server will have access to sensitive information that could
potentially be mishandled. To tackle this problem, collaborative
deep learning models have recently been proposed where parties
locally train their deep learning structures and only share a subset
of the parameters in the attempt to keep their respective training
sets private. Parameters can also be obfuscated via differential privacy (DP) to make information extraction even more challenging,
as proposed by Shokri and Shmatikov at CCS’15.

Unfortunately, we show that any privacy-preserving collaborative deep learning is susceptible to a powerful attack that we devise
in this paper. In particular, we show that a distributed, federated,
or decentralized deep learning approach is fundamentally broken
and does not protect the training sets of honest participants. The
attack we developed exploits the real-time nature of the learning
process that allows the adversary to train a Generative Adversarial
Network (GAN) that generates prototypical samples of the targeted
training set that was meant to be private (the samples generated by
the GAN are intended to come from the same distribution as the
training data). Interestingly, we show that record-level differential
privacy applied to the shared parameters of the model, as suggested
in previous work, is ineffective (i.e., record-level DP is not designed
to address our attack).

",18.243605946275583,18.383360044211106,331
CCS_17_039.txt,14.192606844446054,12.649055678773362,CCS,8333,"

Machine learning models hosted in a cloud service are increasingly
popular but risk privacy: clients sending prediction requests to the
service need to disclose potentially sensitive information. In this
paper, we explore the problem of privacy-preserving predictions:
after each prediction, the server learns nothing about clients’ input
and clients learn nothing about the model.

We present MiniONN, the first approach for transforming an
existing neural network to an oblivious neural network supporting
privacy-preserving predictions with reasonable efficiency. Unlike
prior work, MiniONN requires no change to how models are trained.
To this end, we design oblivious protocols for commonly used operations in neural network prediction models. We show that MiniONN
outperforms existing work in terms of response latency and message sizes. We demonstrate the wide applicability of MiniONN by
transforming several typical neural network models trained from
standard datasets.

",15.903189008614273,14.798571428571432,141
CCS_17_040.txt,13.723691483368896,11.378560305014378,CCS,10648,"

We consider the automatic verification of information flow security
policies of web-based workflows, such as conference submission
systems like EasyChair. Our workflow description language allows
for loops, non-deterministic choice, and an unbounded number of
participating agents. The information flow policies are specified in
a temporal logic for hyperproperties. We show that the verification
problem can be reduced to the satisfiability of a formula of firstorder linear-time temporal logic, and provide decidability results
for relevant classes of workflows and specifications. We report
on experimental results obtained with an implementation of our
approach on a series of benchmarks.

",16.887214914478655,16.685535353535354,100
CCS_17_041.txt,14.49007767728569,12.51809187927584,CCS,7213,"

Attribute-based encryption (ABE) is a cryptographic primitive which
supports fine-grained access control on encrypted data, making
it an appealing building block for many applications. In this paper, we propose, implement, and evaluate fully automated methods
for proving security of ABE in the Generic Bilinear Group Model
(Boneh, Boyen, and Goh, 2005, Boyen, 2008), an idealized model
which admits simpler and more efficient constructions, and can
also be used to find attacks. Our method is applicable to RationalFraction Induced ABE, a large class of ABE that contains most of the
schemes from the literature, and relies on a Master Theorem, which
reduces security in the GGM to a (new) notion of symbolic security,
which is amenable to automated verification using constraint-based
techniques. We relate our notion of symbolic security for RationalFraction Induced ABE to prior notions for Pair Encodings. Finally,
we present several applications, including automated proofs for
new schemes.

",18.458006810337128,18.080473684210528,153
CCS_17_042.txt,12.914172483223897,10.161933629405176,CCS,9507,"

Time and again, attribute-based encryption has been shown to
be the natural cryptographic tool for building various types of
conditional access systems with far-reaching applications, but the
deployment of such systems has been very slow. A central issue
is the lack of an encryption scheme that can operate on sensitive
data very efficiently and, at the same time, provides features that
are important in practice.

This paper proposes the first fully secure ciphertext-policy and
key-policy ABE schemes based on a standard assumption on TypeII pairing groups, which do not put any restriction on policy type
or attributes. We implement our schemes along with several other
prominent ones using the Charm library, and demonstrate that they
perform better on almost all parameters of interest.

",18.243605946275583,16.86179133858268,128
CCS_17_043.txt,14.507384702028538,12.003356587322937,CCS,10744,"

Certification of keys and attributes is in practice typically realized
by a hierarchy of issuers. Revealing the full chain of issuers for
certificate verification, however, can be a privacy issue since it
can leak sensitive information about the issuer’s organizational
structure or about the certificate owner. Delegatable anonymous
credentials solve this problem and allow one to hide the full delegation (issuance) chain, providing privacy during both delegation
and presentation of certificates. However, the existing delegatable
credentials schemes are not efficient enough for practical use.

In this paper, we present the first hierarchical (or delegatable)
anonymous credential system that is practical. To this end, we
provide a surprisingly simple ideal functionality for delegatable
credentials and present a generic construction that we prove secure
in the UC model. We then give a concrete instantiation using a
recent pairing-based signature scheme by Groth and describe a
number of optimizations and efficiency improvements that can be
made when implementing our concrete scheme. The latter might be
of independent interest for other pairing-based schemes as well. Finally, we report on an implementation of our scheme in the context
of transaction authentication for blockchain, and provide concrete
performance figures.

",17.5058628484301,16.21794871794872,197
CCS_17_044.txt,14.695235318326294,12.755734850735688,CCS,10025,"
Blockchains and more general distributed ledgers are becoming increasingly popular as efficient, reliable, and persistent records of data and transactions. Unfortunately, they ensure reliability and cor- rectness by making all data public, raising confidentiality concerns that eliminate many potential uses.
In this paper we present Solidus, a protocol for confidential trans- actions on public blockchains, such as those required for asset transfers with on-chain settlement. Solidus operates in a frame- work based on real-world financial institutions: a modest number of banks each maintain a large number of user accounts. Within this framework, Solidus hides both transaction values and the trans- action graph (i.e., the identities of transacting entities) while main- taining the public verifiability that makes blockchains so appealing. To achieve strong confidentiality of this kind, we introduce the concept of a Publicly-Verifiable Oblivious RAM Machine (PVORM). We present a set of formal security definitions for both PVORM and Solidus and show that our constructions are secure. Finally, we implement Solidus and present a set of benchmarks indicating that the system is efficient in practice.
",16.52667757954773,15.626825842696629,180
CCS_17_045.txt,14.05909896654886,12.400502239910228,CCS,11304,"
Secure multiparty computation allows mutually distrusting parties
to compute a function on their private inputs such that nothing but
the function output is revealed. Achieving fairness — that all parties
learn the output or no one does — is a long studied problem with
known impossibility results in the standard model if a majority of
parties are dishonest.

We present a new model for achieving fairness in MPC against
dishonest majority by using public bulletin boards implemented
via existing infrastructure such as blockchains or Google’s certificate transparency logs. We present both theoretical and practical
constructions using either witness encryption or trusted hardware
(such as Intel SGX).

Unlike previous works that either penalize an aborting party or
achieve weaker notions such as A-fairness, we achieve complete
fairness using existing infrastructure.

",16.15616582465906,16.703375000000005,130
CCS_17_046.txt,14.921404403822045,13.430315590924735,CCS,10940,"
 One of the most prominent applications of program obfusca
Program obfuscation is a powerful security primitive with many
applications. White-box cryptography studies a particular subset
of program obfuscation targeting keyed pseudorandom functions
(PRFs), a core component of systems such as mobile payment and
digital rights management. Although the white-box obfuscators
currently used in practice do not come with security proofs and
are thus routinely broken, recent years have seen an explosion of
cryptographic techniques for obfuscation, with the goal of avoiding
this build-and-break cycle.

In this work, we explore in detail cryptographic program obfuscation and the related primitive of multi-input functional encryption
(MIFE). In particular, we extend the 5Gen framework (CCS 2016) to
support circuit-based MIFE and program obfuscation, implementing both existing and new constructions. We then evaluate and
compare the efficiency of these constructions in the context of PRF
obfuscation.

As part of this work we (1) introduce a novel instantiation of
MIFE that works directly on functions represented as arithmetic
circuits, (2) use a known transformation from MIFE to obfuscation
to give us an obfuscator that performs better than all prior constructions, and (3) develop a compiler for generating circuits optimized
for our schemes. Finally, we provide detailed experiments, demonstrating, among other things, the ability to obfuscate a PRF with
a 64-bit key and 12 bits of input (containing 62k gates) in under
4 hours, with evaluation taking around 1 hour. This is by far the
most complex function obfuscated to date.

",18.001758247042904,16.91216467463479,252
CCS_17_047.txt,13.50385178698312,11.41955577587986,CCS,14984,"

Functional encryption (FE) is an extremely powerful cryptographic
mechanism that lets an authorized entity compute on encrypted
data, and learn the results in the clear. However, all current cryptographic instantiations for general FE are too impractical to be
implemented. We construct IRon, a provably secure, and practical
FE system using Intel’s recent Software Guard Extensions (SGX).
We show that Iron can be applied to complex functionalities, and
even for simple functions, outperforms the best known cryptographic schemes. We argue security by modeling FE in the context
of hardware elements, and prove that IRON satisfies the security
model.

",16.15616582465906,14.237855670103094,99
CCS_17_048.txt,14.287388258865,12.743123190608348,CCS,11929,"

We implemented (a simplified version of) the branching-program
obfuscator due to Gentry et al. (GGH15), which is itself a variation of
the first obfuscation candidate by Garg et al. (GGHRSW13). To keep
within the realm of feasibility, we had to give up on some aspects of
the construction, specifically the “multiplicative bundling” factors
that protect against mixed-input attacks. Hence our implementation
can only support read-once branching programs.

To be able to handle anything more than just toy problems,
we developed a host of algorithmic and code-level optimizations.
These include new variants of discrete Gaussian sampler and lattice
trapdoor sampler, efficient matrix-manipulation routines, and many
tradeoffs. We expect that these optimizations will find other uses
in lattice-based cryptography beyond just obfuscation.

Our implementation is the first obfuscation attempt using the
GGH15 graded encoding scheme, offering performance advantages
over other graded encoding methods when obfuscating finite-state
machines with many states. In out most demanding setting, we
were able to obfuscate programs with input length of 20 nibbles (80
bits) and over 100 states, which seems out of reach for prior implementations. Although further optimizations are surely possible, we
do not expect any implementation of current schemes to be able to
handle much larger parameters.

",13.747043046817236,13.38746411483254,210
CCS_17_049.txt,15.412103410684427,14.757431011890297,CCS,10791,"

When accessing online private resources (e.g., user profiles, photos,
shopping carts) from a client (e.g., a desktop web-browser or a
mobile app), the service providers must implement proper access
control, which typically involves both authentication and authorization. However, not all of the service providers follow the best
practice, resulting in various access control vulnerabilities. To understand such a threat in a large scale, and identify the vulnerable
access control implementations in online services, this paper introduces AUTHSCOPE, a tool that is able to automatically execute a
mobile app and pinpoint the vulnerable access control implementations, particularly the vulnerable authorizations, in the corresponding online service. The key idea is to use differential traffic
analysis to recognize the protocol fields and then automatically
substitute the fields and observe the server response. One of the
key challenges for a large scale study lies in how to obtain the postauthentication request-and-response messages for a given app. We
have thus developed a targeted dynamic activity explorer to perform an in-context analysis and drive the app execution to automatically log in the service. We have tested AUTHSCOPE with
4,838 popular mobile apps from Google Play, and identified 597
0-day vulnerable authorizations that map to 306 apps.
",19.620377997778096,20.183780487804878,209
CCS_17_050.txt,14.963260582780222,13.895937607864628,CCS,11862,"

Monitoring network behaviors of mobile applications, controlling
their resource access and detecting potentially harmful apps are
becoming increasingly important for the security protection within
today’s organizational, ISP and carriers. For this purpose, apps
need to be identified from their communication, based upon their
individual traffic signatures (called imprints in our research). Creating imprints for a large number of apps is nontrivial, due to the
challenges in comprehensively analyzing their network activities
at a large scale, for millions of apps on today’s rapidly-growing
app marketplaces. Prior research relies on automatic exploration
of an app’s user interfaces (UIs) to trigger its network activities,
which is less likely to scale given the cost of the operation (at least 5
minutes per app) and its effectiveness (limited coverage of an app’s
behaviors).

In this paper, we present Tiger (Traffic Imprint Generator), a
novel technique that makes comprehensive app imprint generation
possible in a massive scale. At the center of Tiger is a unique instantiated slicing technique, which aggressively prunes the program
slice extracted from the app’s network-related code by evaluating each variable’s impact on possible network invariants, and
removing those unlikely to contribute through assigning them concrete values. In this way, Tiger avoids exploring a large number of
program paths unrelated to the app’s identifiable traffic, thereby
reducing the cost of the code analysis by more than one order of
magnitude, in comparison with the conventional slicing and execution approach. Our experiments show that Tiger is capable of recovering an app’s full network activities within 18 seconds, achieving
over 98% coverage of its identifiable packets and 0.742% false detection rate on app identification. Further running the technique on
over 200,000 real-world Android apps (including 78.23% potentially
harmful apps) leads to the discovery of surprising new types of


",19.287186520377343,19.60085585585586,308
CCS_17_051.txt,15.369862099200798,14.506331569664905,CCS,11369,"

As a critical feature for enhancing user experience, cross-app URL
invocation has been reported to cause unauthorized execution of
app components. Although protection has already been put in place,
little has been done to understand the security risks of navigating
an app’s WebView through an URL, a legitimate need for displaying
the app’s UI during cross-app interactions. In our research, we found
that the current design of such cross-WebView navigation actually
opens the door to a cross-app remote infection, allowing a remote
adversary to spread malicious web content across different apps’
WebView instances and acquire stealthy and persistent control
of these apps. This new threat, dubbed Cross-App WebView Infection (XAWI), enables a series of multi-app, colluding attacks never
thought before, with significant real world impacts. Particularly,
we found that the remote adversary can collectively utilize multiple
infected apps’ individual capabilities to escalate his privileges on a
mobile device or orchestrate a highly realistic remote Phishing attack (e.g., running a malicious script in Chrome to stealthily change
Twitter’s WebView to fake Twitter’s own login UI). We show that
the adversary can easily find such attack “building blocks” (popular
apps whose WebViews can be redirected by another app) through
an automatic fuzz, and discovered about 7.4% of the most popular apps subject to the XAWI attacks, including Facebook, Twitter,
Amazon and others. Our study reveals the contention between the
demand for convenient cross-WebView communication and the
need for security control on the channel, and makes the first step
toward building OS-level protection to safeguard this fast-growing
technology.

",20.130776976110326,20.52841991341992,271
CCS_17_052.txt,15.152290612950349,13.381627464016319,CCS,10729,"

In recent years, applications increasingly adopt security primitives designed with better countermeasures against side channel attacks. A concrete example is Libgcrypt’s implementation of ECDH
encryption with Curve25519. The implementation employs the
Montgomery ladder scalar-by-point multiplication, uses the unified, branchless Montgomery double-and-add formula and implements a constant-time argument swap within the ladder. However,
Libgerypt’s field arithmetic operations are not implemented in a
constant-time side-channel-resistant fashion.

Based on the secure design of Curve25519, users of the curve are
advised that there is no need to perform validation of input points.
In this work we demonstrate that when this recommendation is
followed, the mathematical structure of Curve25519 facilitates the
exploitation of side-channel weaknesses.

We demonstrate the effect of this vulnerability on three software applications—encrypted git, email and messaging—that use
Libgcrypt. In each case, we show how to craft malicious OpenPGP
files that use the Curve25519 point of order 4 as a chosen ciphertext to the ECDH encryption scheme. We find that the resulting
interactions of the point at infinity, order-2, and order-4 elements
in the Montgomery ladder scalar-by-point multiplication routine
create side channel leakage that allows us to recover the private
key in as few as 11 attempts to access such malicious files.

",15.903189008614273,14.966666666666672,218
CCS_17_053.txt,15.462709721950375,14.432137362212746,CCS,11517,"

Intel Software Guard Extension (SGX) offers software applications
a shielded execution environment, dubbed enclave, to protect their
confidentiality and integrity from malicious operating systems.
As processors with this extended feature become commercially
available, many new software applications are developed to enrich
to the SGX-enabled software ecosystem. One important primitive
for these applications is a secure communication channel between
the enclave and a remote trusted party. The SSL/TLS protocol, which
is the de facto standard for protecting transport-layer network
communications, has been broadly regarded a natural choice for
such purposes. However, in this paper, we show that the marriage
between SGX and SSL may not be a smooth sailing.

Particularly, we consider a category of side-channel attacks
against SSL/TLS implementations in secure enclaves, which we call
the control-flow inference attacks. In these attacks, the malicious
operating system kernel may perform a powerful man-in-the-kernel
attack to collect execution traces of the enclave programs at the
page level, the cacheline level, or the branch level, while positioning itself in the middle of the two communicating parties. At the
center of our work is a differential analysis framework, dubbed
StTacco, to dynamically analyze the SSL/TLS implementations and
detect vulnerabilities—discernible execution traces—that can be
exploited as decryption oracles. Surprisingly, in spite of the prevailing constant-time programming paradigm adopted by many
cryptographic libraries, we found exploitable vulnerabilities in the
latest versions of all the SSL/TLS libraries we have examined.

To validate the detected vulnerabilities, we developed a man-inthe-kernel adversary to demonstrate Bleichenbacher attacks against
the latest OpenSSL library running in the SGX enclave (with the
help of Graphene) and completely broke the PreMasterSecret
encrypted by a 4096-bit RSA public key with only 57,286 queries.
We also conducted CBC padding oracle attacks against the latest GnuTLS running in Graphene-SGX and an open-source SGXimplementation of mbedTLS (i.e, mbedTLS-SGX) that runs directly
inside the enclave, and showed that it only needs 48,388 and 25,717

",18.43863989570496,18.58398914518318,340
CCS_17_054.txt,15.449600592656878,14.521359437270842,CCS,9609,"

This paper presents THEMIS, an end-to-end static analysis tool for
finding resource-usage side-channel vulnerabilities in Java applications. We introduce the notion of e-bounded non-interference,
a variant and relaxation of Goguen and Meseguer’s well-known
non-interference principle. We then present Quantitative Cartesian
Hoare Logic (QCHL), a program logic for verifying e-bounded noninterference. Our tool, THEMIS, combines automated reasoning in
CHL with lightweight static taint analysis to improve scalability.
We evaluate THEMIs on well known Java applications and demonstrate that THEMIs can find unknown side-channel vulnerabilities
in widely-used programs. We also show that THEMIs can verify
the absence of vulnerabilities in repaired versions of vulnerable
programs and that THEMIs compares favorably against BLAZER, a
state-of-the-art static analysis tool for finding timing side channels
in Java applications.

",15.470042427545799,16.225948905109487,139
CCS_17_055.txt,11.676650992834915,8.734814046100372,CCS,13240,"

The MD transform that underlies the MD and SHA families iterates
a compression function h to get a hash function H. The question we
ask is, what property X of h guarantees collision resistance (CR) of
H? The classical answer is that X itself be CR. We show that weaker
conditions X, in particular forms of what we call constrained-CR,
suffice. This reduces demands on compression functions, to the
benefit of security, and also, forensically, explains why collisionfinding attacks on compression functions have not, historically,
lead to immediate breaks of the corresponding hash functions. We
obtain our results via a definitional framework called RS security,
and a parameterized treatment of MD, that also serve to unify prior
work and variants of the transform.

",15.021129683784007,13.405483870967739,125
CCS_17_056.txt,15.90844475416009,14.353252796843911,CCS,10548,"

Notable recent security incidents have generated intense interest
in adversaries which attempt to subvert—perhaps covertly—cryptographic algorithms. In this paper we develop (IND-CPA) Semantically Secure encryption in this challenging setting.

This fundamental encryption primitive has been previously studied in the “kleptographic setting, though existing results must
relax the model by introducing trusted components or otherwise
constraining the subversion power of the adversary: designing a
Public Key System that is kletographically semantically secure (with
minimal trust) has remained elusive to date.

In this work, we finally achieve such systems, even when all
relevant cryptographic algorithms are subject to adversarial (kleptographic) subversion. To this end we exploit novel inter-component
randomized cryptographic checking techniques (with an offline
checking component), combined with common and simple software engineering modular programming techniques (applied to the
system’s black box specification level). Moreover, our methodology
yields a strong generic technique for the preservation of any semantically secure cryptosystem when incorporated into the strong
kleptographic adversary setting.

",20.267338824336647,20.453881987577635,163
CCS_17_057.txt,11.440099696516501,8.499550641947486,CCS,11618,"

Towards advancing the use of big keys as a practical defense against
key exfiltration, this paper provides efficiency improvements for
cryptographic schemes in the bounded retrieval model (BRM). We
identify probe complexity (the number of scheme accesses to the
slow storage medium storing the big key) as the dominant cost.
Our main technical contribution is what we call the large-alphabet
subkey prediction lemma. It gives good bounds on the predictability
under leakage of a random sequence of blocks of the big key, as a
function of the block size. We use it to significantly reduce the probe
complexity required to attain a given level of security. Together
with other techniques, this yields security-preserving performance
improvements for BRM symmetric encryption schemes and BRM
public-key identification schemes.

",15.688483145680365,13.748750000000001,129
CCS_17_058.txt,16.42090897004627,15.593396989054952,CCS,11637,"

The recent unprecedented delegation of new generic top-level domains (gTLDs) has exacerbated an existing, but fallow, problem
called name collisions. One concrete exploit of such problem was
discovered recently, which targets internal namespaces and enables Man in the Middle (MitM) attacks against end-user devices
from anywhere on the Internet. Analysis of the underlying problem shows that it is not specific to any single service protocol, but
little attention has been paid to understand the vulnerability status
and the defense solution space at the service level. In this paper,
we perform the first systematic study of the robustness of internal
network services under name collision attacks.

We first perform a measure study and uncover a wide spectrum
of services affected by the name collision problem. We then collect
their client implementations and systematically analyze their vulnerability status under name collision attacks using dynamic analysis. Out of the 48 identified exposed services, we find that nearly
all (45) of them expose vulnerabilities in popular clients. To demonstrate the severity, we construct exploits and find a set of new name
collision attacks with severe security implications including MitM
attacks, internal or personal document leakage, malicious code injection, and credential theft. We analyze the causes, and find that
the name collision problem broadly breaks common security assumptions made in today’s service client software. Leveraging the
insights from our analysis, we propose multiple service software
level solutions, which enables the victim services to actively defend against name collision attacks.

",17.80541091248751,16.740585365853658,248
CCS_17_059.txt,15.581551931889642,14.083499015560154,CCS,8226,"

The functionality and security of all domain names are contingent
upon their nameservers. When these nameservers, or requests to
them, are compromised, all domains that rely on them are affected.
In this paper, we study the exploitation of configuration issues (typosquatting and outdated WHOIS records) and hardware errors
(bitsquatting) to seize control over nameservers’ requests to hijack
domains. We perform a large-scale analysis of 10,000 popular nameserver domains, in which we map out existing abuse and vulnerable
entities. We confirm the capabilities of these attacks through realworld measurements. Overall, we find that over 12,000 domains are
susceptible to near-immediate compromise, while 52.8M domains
are being targeted by nameserver bitsquatters that respond with
rogue IP addresses. Additionally, we determine that 1.28M domains
are at risk of a denial-of-service attack by relying on an outdated
nameserver.

",15.903189008614273,14.330455486542444,143
CCS_17_060.txt,15.345792523736208,14.126863433614883,CCS,7990,"

Recent work in OS fingerprinting [41], [42] has focused on overcoming random distortion in network and user features during
Internet-scale SYN scans. These classification techniques work under an assumption that all parameters of the profiled network are
known a-priori — the likelihood of packet loss, the popularity of
each OS, the distribution of network delay, and the probability of
user modification to each default TCP/IP header value. However,
it is currently unclear how to obtain realistic versions of these parameters for the public Internet and/or customize them to a particular network being analyzed. To address this issue, we derive a
non-parametric Expectation-Maximization (EM) estimator, which
we call Faulds, for the unknown distributions involved in singleprobe OS fingerprinting and demonstrate its significantly higher
robustness to noise compared to methods in prior work. We apply
Faulds to a new scan of 67M webservers and discuss its findings.

",17.58133193835471,17.822,151
CCS_17_061.txt,13.538350280480032,12.071601347972699,CCS,10511,"
 reason is that challenge-response requires two-way communication

Time-based one-time password (TOTP) systems in use today require storing secrets on both the client and the server. As a result, an
attack on the server can expose all second factors for all users in the
system. We present T/Key, a time-based one-time password system
that requires no secrets on the server. Our work modernizes the
classic S/Key system and addresses the challenges in making such
a system secure and practical. At the heart of our construction is a
new lower bound analyzing the hardness of inverting hash chains
composed of independent random functions, which formalizes the
security of this widely used primitive. Additionally, we develop
a near-optimal algorithm for quickly generating the required elements in a hash chain with little memory on the client. We report on
our implementation of T/Key as an Android application. T/Key can
be used as a replacement for current TOTP systems, and it remains
secure in the event of a server-side compromise. The cost, as with
S/Key, is that one-time passwords are longer than the standard six
characters used in TOTP.

",13.205437297517054,12.001111111111111,199
CCS_17_062.txt,13.68668884186376,11.564792299898684,CCS,12314,"
A memory-hard function (MHF) f, with parameter n can be computed in sequential time and space n. Simultaneously, a high amortized parallel area-time complexity (aAT) is incurred per evaluation.
In practice, MHFs are used to limit the rate at which an adversary
(using a custom computational device) can evaluate a security sensitive function that still occasionally needs to be evaluated by honest
users (using an off-the-shelf general purpose device). The most
prevalent examples of such sensitive functions are Key Derivation
Functions (KDFs) and password hashing algorithms where rate
limits help mitigate off-line dictionary attacks. As the honest users’
inputs to these functions are often (low-entropy) passwords special
attention is given to a class of side-channel resistant MHFs called
iMHFs.

Essentially all iMHFs can be viewed as some mode of operation
(making n calls to some round function) given by a directed acyclic
graph (DAG) with very low indegree. Recently, a combinatorial
property of a DAG has been identified (called “depth-robustness”)
which results in good provable security for an iMHF based on
that DAG. Depth-robust DAGs have also proven useful in other
cryptographic applications. Unfortunately, up till now, all known
very depth-robust DAGs are impractically complicated and little
is known about their exact (i.e. non-asymptotic) depth-robustness
both in theory and in practice.

In this work we build and analyze (both formally and empirically)
several exceedingly simple and efficient to navigate practical DAGs
for use in iMHFs and other applications. For each DAG we:

e Prove that their depth-robustness is asymptotically maximal.

e Prove bounds of at least 3 orders of magnitude better on
their exact depth-robustness compared to known bounds
for other practical iMHF.

e Implement and empirically evaluate their depth-robustness
and aAT against a variety of state-of-the art (and several


new) depth-reduction and low aAT attacks. We find that,
against all attacks, the new DAGs perform significantly
better in practice than Argon2i, the most widely deployed
iMHF in practice.
Along the way we also improve the best known empirical attacks on
the aAT of Argon2i by implementing and testing several heuristic
versions of a (hitherto purely theoretical) depth-reduction attack.
Finally, we demonstrate practicality of our constructions by modifying the Argon2i code base to use one of the new high aAT DAGs.
Experimental benchmarks on a standard off-the-shelf CPU show
that the new modifications do not adversely affect the impressive
throughput of Argon2i (despite seemingly enjoying significantly
higher aAT).

",17.93193317476759,17.03488748721446,421
CCS_17_063.txt,13.697767674832651,11.453211164230883,CCS,14176,"

Block cipher modes of operation provide a way to securely encrypt using a block cipher. The main factors in analyzing modes
of operation are the level of security achieved (chosen-plaintext
security, authenticated encryption, nonce-misuse resistance, and
so on) and performance. When measuring the security level of a
mode of operation, it does not suffice to consider asymptotics, and
a concrete analysis is necessary. This is especially the case today,
when encryption rates can be very high, and so birthday bounds
may be approached or even reached.

In this paper, we show that key-derivation at every encryption significantly improves the security bounds in many cases. We
present a new key-derivation method that utilizes a truncated block
cipher, and show that this is far better than standard block-cipher
based key derivation. We prove that by using our key derivation
method, we obtain greatly improved bounds for many modes of
operation, with a result that the lifetime of a key can be significantly
extended. We demonstrate this for AES-CTR (CPA-security), AESGCM (authenticated encryption) and AES-GCM-SIV (nonce-misuse
resistance). Finally, we demonstrate that when using modern hardware with AES instructions (AES-NI), the performance penalty of
deriving keys at each encryption is insignificant for most uses.

",15.903189008614273,14.35712480252765,212
CCS_17_064.txt,16.721075219002653,15.323091408102592,CCS,7586,"

Third-party libraries are commonly used by app developers for
alleviating the development efforts and for monetizing their apps.
On Android, the host app and its third-party libraries reside in
the same sandbox and share all privileges awarded to the host
app by the user, putting the users’ privacy at risk of intrusions by
third-party libraries. In this paper, we introduce a new privilege
separation approach for third-party libraries on stock Android.
Our solution partitions Android applications at compile-time into
isolated, privilege-separated compartments for the host app and the
included third-party libraries. A particular benefit of our approach is
that it leverages compiler-based instrumentation available on stock
Android versions and thus abstains from modification of the SDK,
the app bytecode, or the device firmware. A particular challenge
for separating libraries from their host apps is the reconstruction of
the communication channels and the preservation of visual fidelity
between the now separated app and its libraries. We solve this
challenge through new IPC-based protocols to synchronize layout
and lifecycle management between different sandboxes. Finally,
we demonstrate the efficiency and effectiveness of our solution
by applying it to real world apps from the Google Play Store that
contain advertisements.

",18.377959752453624,16.449603658536585,206
CCS_17_065.txt,17.126813124542206,16.085808131809724,CCS,6566,"

The services in Android applications can be invoked either explicitly or implicitly before Android 5.0. However, since the implicit
service invocations suffer service hijacking attacks and thus lead
to sensitive information leakage, they have been forbidden since
Android 5.0. Thereafter since the Android system will simply throw
an exception and crash the application that still invokes services
implicitly, it was expected that application developers will be forced
to convert the implicit service invocations to explicit ones by specifying the package name of the service to be called.

In this paper, we revisit the service invocations by analyzing
two sets of the same 1390 applications downloaded from Google
Play Store before and after the the implicit service forbidden policy
is enforced. We develop a static analysis framework called ISA to
perform our study. Our analysis results show that the forbidden
policy effectively reduces the number of vulnerable service invocations from 643 to 112, namely, 82.58% reduction. However, after
a detailed analysis of the remaining 112 vulnerable invocations,
we discover that the forbidden policy fails to resolve the service
hijacking attacks. Among the 1390 applications downloaded in May
2017, we find 36 popular applications still vulnerable to service hijacking attacks, which can lead to the loss of user bank account and
VPN login credentials, etc. Moreover, we find that the forbidden
policy introduces a new type of denial of service attacks. Finally, we
discuss the root challenges on resolving service hijacking attacks
and propose countermeasures to help mitigate the service hijacking
attacks.

",18.243605946275583,16.532799999999998,254
CCS_17_066.txt,16.64633679573992,15.739358283002591,CCS,3584,"

Despite security advice in the official documentation and an extensive body of security research about vulnerabilities and exploits,
many developers still fail to write secure Android applications.
Frequently, Android developers fail to adhere to security best practices, leaving applications vulnerable to a multitude of attacks. We
point out the advantage of a low-time-cost tool both to teach better
secure coding and to improve app security. Using the FixDroid™
IDE plug-in, we show that professional and hobby app developers
can work with and learn from an in-environment tool without it
impacting their normal work; and by performing studies with both
students and professional developers, we identify key UI requirements and demonstrate that code delivered with such a tool by
developers previously inexperienced in security contains significantly less security problems. Perfecting and adding such tools
to the Android development environment is an essential step in
getting both security and privacy for the next generation of apps.

",19.48791578843652,19.224578616352208,159
CCS_17_067.txt,15.229241607997558,14.441525094687055,CCS,11757,"

The power capacity of multi-tenant data centers is typically oversubscribed in order to increase the utilization of expensive power
infrastructure. This practice can create dangerous situations and
compromise data center availability if the designed power capacity
is exceeded. This paper demonstrates that current safeguards are
vulnerable to well-timed power attacks launched by malicious tenants (i.e., attackers). Further, we demonstrate that there is a physical
side channel — a thermal side channel due to hot air recirculation —
that contains information about the benign tenants’ runtime power
usage and can enable a malicious tenant to time power attacks
effectively. In particular, we design a state-augmented Kalman filter
to extract this information from the side channel and guide an attacker to use its maximum power at moments that coincide with the
benign tenants’ high power demand, thus overloading the shared
power capacity. Our experimental results show that an attacker can
capture 54% of all attack opportunities, significantly compromising
the data center availability. Finally, we discuss a set of possible defense strategies to safeguard the data center infrastructure against
power attacks.

",17.122413403193683,17.65324820430966,181
CCS_17_068.txt,14.947901283484786,13.682523858601666,CCS,5916,"
Trustworthy operation of industrial control systems depends on secure and real-time code execution on the embedded programmable
logic controllers (PLCs). The controllers monitor and control the
critical infrastructures, such as electric power grids and healthcare platforms, and continuously report back the system status
to human operators. We present ZEUs, a contactless embedded
controller security monitor to ensure its execution control flow
integrity. Zeus leverages the electromagnetic emission by the PLC
circuitry during the execution of the controller programs. ZEUS’s
contactless execution tracking enables non-intrusive monitoring of
security-critical controllers with tight real-time constraints. Those
devices often cannot tolerate the cost and performance overhead
that comes with additional traditional hardware or software monitoring modules. Furthermore, ZEus provides an air-gap between
the monitor (trusted computing base) and the target (potentially
compromised) PLC. This eliminates the possibility of the monitor
infection by the same attack vectors.

ZEvUs monitors for control flow integrity of the PLC program
execution. ZEUS monitors the communications between the humanmachine interface and the PLC, and captures the control logic binary
uploads to the PLC. Zeus exercises its feasible execution paths,
and fingerprints their emissions using an external electromagnetic
sensor. ZEUS trains a neural network for legitimate PLC executions,
and uses it at runtime to identify the control flow based on PLC’s
electromagnetic emissions. We implemented Zeus on a commercial
Allen Bradley PLC, which is widely used in industry, and evaluated
it on real-world control program executions. ZEUs was able to
distinguish between different legitimate and malicious executions
with 98.9% accuracy and with zero overhead on PLC execution by
design.

",17.205460502629933,15.83742587601078,269
CCS_17_069.txt,15.280647541789783,13.524044186091427,CCS,11614,"

Various defense schemes — which determine the presence of an attack on the in-vehicle network — have recently been proposed. However, they fail to identify which Electronic Control Unit (ECU) actually mounted the attack. Clearly, pinpointing the attacker ECU is
essential for fast/efficient forensic, isolation, security patch, etc. To
meet this need, we propose a novel scheme, called Viden (Voltagebased attacker identification), which can identify the attacker ECU
by measuring and utilizing voltages on the in-vehicle network. The
first phase of Viden, called ACK learning, determines whether or
not the measured voltage signals really originate from the genuine
message transmitter. Viden then exploits the voltage measurements
to construct and update the transmitter ECUs’ voltage profiles as
their fingerprints. It finally uses the voltage profiles to identify
the attacker ECU. Since Viden adapts its profiles to changes inside/outside of the vehicle, it can pinpoint the attacker ECU under
various conditions. Moreover, its efficiency and design-compliance
with modern in-vehicle network implementations make Viden practical and easily deployable. Our extensive experimental evaluations
on both a CAN bus prototype and two real vehicles have shown
that Viden can accurately fingerprint ECUs based solely on voltage
measurements and thus identify the attacker ECU with a low false
identification rate of 0.2%.

",17.00531248756302,15.694285714285716,212
CCS_17_070.txt,15.475482561126114,13.518539175658603,CCS,10025,"

Graph modeling allows numerous security problems to be tackled
in a general way, however, little work has been done to understand their ability to withstand adversarial attacks. We design and
evaluate two novel graph attacks against a state-of-the-art networklevel, graph-based detection system. Our work highlights areas in
adversarial machine learning that have not yet been addressed,
specifically: graph-based clustering techniques, and a global feature
space where realistic attackers without perfect knowledge must
be accounted for (by the defenders) in order to be practical. Even
though less informed attackers can evade graph clustering with
low cost, we show that some practical defenses are possible.

",17.410965686947208,16.1362962962963,109
CCS_17_071.txt,14.368308544479174,12.58623627700868,CCS,10814,"
Malicious crowdsourcing forums are gaining traction as sources
of spreading misinformation online, but are limited by the costs of
hiring and managing human workers. In this paper, we identify a
new class of attacks that leverage deep learning language models
(Recurrent Neural Networks or RNNs) to automate the generation
of fake online reviews for products and services. Not only are these
attacks cheap and therefore more scalable, but they can control rate
of content output to eliminate the signature burstiness that makes
crowdsourced campaigns easy to detect.

Using Yelp reviews as an example platform, we show how a two
phased review generation and customization attack can produce
reviews that are indistinguishable by state-of-the-art statistical
detectors. We conduct a survey-based user study to show these
reviews not only evade human detection, but also score high on
“usefulness” metrics by users. Finally, we develop novel automated
defenses against these attacks, by leveraging the lossy transformation introduced by the RNN training and generation cycle. We
consider countermeasures against our mechanisms, show that they
produce unattractive cost-benefit tradeoffs for attackers, and that
they can be further curtailed by simple constraints imposed by
online service providers.

",16.613394197324528,16.70244897959184,197
CCS_17_072.txt,14.996090038679775,13.117727215508562,CCS,7902,"

Cybercriminals have found in online social networks a propitious
medium to spread spam and malicious content. Existing techniques
for detecting spam include predicting the trustworthiness of accounts and analyzing the content of these messages. However,
advanced attackers can still successfully evade these defenses.

Online social networks bring people who have personal connections or share common interests to form communities. In this
paper, we first show that users within a networked community
share some topics of interest. Moreover, content shared on these
social network tend to propagate according to the interests of people. Dissemination paths may emerge where some communities
post similar messages, based on the interests of those communities. Spam and other malicious content, on the other hand, follow
different spreading patterns.

In this paper, we follow this insight and present POISED, a system that leverages the differences in propagation between benign
and malicious messages on social networks to identify spam and
other unwanted content. We test our system on a dataset of 1.3M
tweets collected from 64K users, and we show that our approach
is effective in detecting malicious messages, reaching 91% precision and 93% recall. We also show that POISED’s detection is more
comprehensive than previous systems, by comparing it to three
state-of-the-art spam detection systems that have been proposed
by the research community in the past. POISED significantly outperforms each of these systems. Moreover, through simulations,
we show how POISED is effective in the early detection of spam
messages and how it is resilient against two well-known adversarial
machine learning attacks.

",15.195765016291148,13.188759689922481,261
CCS_17_073.txt,15.427742586118104,14.740051465696059,CCS,8824,"

We design a novel, communication-efficient, failure-robust protocol for secure aggregation of high-dimensional data. Our protocol
allows a server to compute the sum of large, user-held data vectors from mobile devices in a secure manner (i.e. without learning
each user’s individual contribution), and can be used, for example,
in a federated learning setting, to aggregate user-provided model
updates for a deep neural network. We prove the security of our
protocol in the honest-but-curious and active adversary settings,
and show that security is maintained even if an arbitrarily chosen
subset of users drop out at any time. We evaluate the efficiency
of our protocol and show, by complexity analysis and a concrete
implementation, that its runtime and communication overhead remain low even on large data sets and client pools. For 16-bit input
values, our protocol offers 1.73x communication expansion for 21°
users and 27°-dimensional vectors, and 1.98x expansion for 214
users and 2?4-dimensional vectors over sending data in the clear.

",16.728156217252725,17.46885964912281,174
CCS_17_074.txt,16.575812256461916,15.144432444469707,CCS,9778,"

This paper presents an approach to formalizing and enforcing a
class of use privacy properties in data-driven systems. In contrast
to prior work, we focus on use restrictions on proxies (i.e. strong
predictors) of protected information types. Our definition relates
proxy use to intermediate computations that occur in a program,
and identify two essential properties that characterize this behavior:
1) its result is strongly associated with the protected information
type in question, and 2) it is likely to causally affect the final output of the program. For a specific instantiation of this definition,
we present a program analysis technique that detects instances of
proxy use in a model, and provides a witness that identifies which
parts of the corresponding program exhibit the behavior. Recognizing that not all instances of proxy use of a protected information
type are inappropriate, we make use of a normative judgment oracle that makes this inappropriateness determination for a given
witness. Our repair algorithm uses the witness of an inappropriate
proxy use to transform the model into one that provably does not
exhibit proxy use, while avoiding changes that unduly affect classification accuracy. Using a corpus of social datasets, our evaluation
shows that these algorithms are able to detect proxy use instances
that would be difficult to find using existing techniques, and subsequently remove them while maintaining acceptable classification
performance.

",19.430816780756558,19.811492063492064,227
CCS_17_075.txt,15.289775535252314,13.971955131834893,CCS,8177,"

Recently, using secure processors for trusted computing in cloud
has attracted a lot of attention. Over the past few years, efficient
and secure data analytic tools (e.g., map-reduce framework, machine learning models, and SQL querying) that can be executed over
encrypted data using the trusted hardware have been developed.
However, these prior efforts do not provide a simple, secure and
high level language based framework that is suitable for enabling
generic data analytics for non-security experts who do not have
concepts such as “oblivious execution”. In this paper, we thus provide such a framework that allows data scientists to perform the
data analytic tasks with secure processors using a Python/Matlablike high level language. Our framework automatically compiles
programs written in our language to optimal execution code by
managing issues such as optimal data block sizes for I/O, vectorized
computations to simplify much of the data processing, and optimal ordering of operations for certain tasks. Furthermore, many
language constructs such as if-statements are removed so that a
non-expert user is less likely to create a piece of code that may reveal sensitive information while allowing oblivious data processing
(ie., hiding access patterns). Using these design choices, we provide
guarantees for efficient and secure data analytics. We show that
our framework can be used to run the existing big data benchmark
queries over encrypted data using the Intel SGX efficiently. Our
empirical results indicate that our proposed framework is orders of
magnitude faster than the general oblivious execution alternatives.

",17.122413403193683,17.387562582345193,255
CCS_17_076.txt,13.379282071553657,11.17358932080171,CCS,10490,"

Private set intersection (PSI) allows two parties, who each hold
a set of items, to compute the intersection of those sets without
revealing anything about other items. Recent advances in PSI have
significantly improved its performance for the case of semi-honest
security, making semi-honest PSI a practical alternative to insecure
methods for computing intersections. However, the semi-honest
security model is not always a good fit for real-world problems.

In this work we introduce a new PSI protocol that is secure in the
presence of malicious adversaries. Our protocol is based entirely on
fast symmetric-key primitives and inherits important techniques
from state-of-the-art protocols in the semi-honest setting. Our novel
technique to strengthen the protocol for malicious adversaries is
inspired by the dual execution technique of Mohassel & Franklin
(PKC 2006). Our protocol is optimized for the random-oracle model,
but can also be realized (with a performance penalty) in the standard
model.

We demonstrate our protocol’s practicality with a prototype
implementation. To securely compute the intersection of two sets of
size 279 requires only 13 seconds with our protocol, which is ~ 12x
faster than the previous best malicious-secure protocol (Rindal &
Rosulek, Eurocrypt 2017), and only 3x slower than the best semihonest protocol (Kolesnikov et al., CCS 2016).

",17.37919286519448,15.676481481481485,217
CCS_17_077.txt,13.906960027928047,11.971393526802732,CCS,9793,"

Private Set Intersection (PSI) is a cryptographic technique that allows two parties to compute the intersection of their sets without
revealing anything except the intersection. We use fully homomorphic encryption to construct a fast PSI protocol with a small
communication overhead that works particularly well when one of
the two sets is much smaller than the other, and is secure against
semi-honest adversaries.

The most computationally efficient PSI protocols have been constructed using tools such as hash functions and oblivious transfer,
but a potential limitation with these approaches is the communication complexity, which scales linearly with the size of the larger
set. This is of particular concern when performing PSI between
a constrained device (cellphone) holding a small set, and a large
service provider (e.g. WhatsApp), such as in the Private Contact
Discovery application.

Our protocol has communication complexity linear in the size
of the smaller set, and logarithmic in the larger set. More precisely,
if the set sizes are Ny < Nx, we achieve a communication overhead of O(N, log Nx). Our running-time-optimized benchmarks
show that it takes 36 seconds of online-computation, 71 seconds
of non-interactive (receiver-independent) pre-processing, and only
12.5MB of round trip communication to intersect five thousand
32-bit strings with 16 million 32-bit strings. Compared to prior
works, this is roughly a 38-115x reduction in communication with
minimal difference in computational overhead.

",16.728156217252725,15.736666666666668,238
CCS_17_078.txt,13.136431948490202,11.295178998978706,CCS,10462,"
We present a new paradigm for multi-party private set intersection
(PSI) that allows n parties to compute the intersection of their
datasets without revealing any additional information. We explore
a variety of instantiations of this paradigm. Our protocols avoid
computationally expensive public-key operations and are secure
in the presence of any number of semi-honest participants (i.e.,
without an honest majority).

We demonstrate the practicality of our protocols with an implementation. To the best of our knowledge, this is the first implementation of a multi-party PSI protocol. For 5 parties with data-sets of
279 items each, our protocol requires only 72 seconds. In an optimization achieving a slightly weaker variant of security (augmented
semi-honest model), the same task requires only 22 seconds.

The technical core of our protocol is oblivious evaluation of a
programmable pseudorandom function (OPPRF), which we instantiate in three different ways. We believe our new OPPRF abstraction
and constructions may be of independent interest.

",15.470042427545799,14.592962962962964,164
CCS_17_079.txt,14.554592549557764,12.844967937915222,CCS,4153,"

Many network intrusion detection systems use byte sequences to
detect lateral movements that exploit remote vulnerabilities. Attackers bypass such detection by stealing valid credentials and using
them to transmit from one computer to another without creating
abnormal network traffic. We call this method Credential-based
Lateral Movement. To detect this type of lateral movement, we
develop the concept of a Network Login Structure that specifies normal logins within a given network. Our method models a network
login structure by automatically extracting a collection of login
patterns by using a variation of the market-basket analysis algorithm. We then employ an anomaly detection approach to detect
malicious logins that are inconsistent with the enterprise network’s
login structure. Evaluations show that the proposed method is able
to detect malicious logins in a real setting. In a simulated attack,
our system was able to detect 82% of malicious logins, with a 0.3%
false positive rate. We used a real dataset of millions of logins over
the course of five months within a global financial company for
evaluation of this work.
",14.712192995108573,14.013333333333332,180
CCS_17_080.txt,14.206725079350207,12.797846085497806,CCS,6464,"

Anomaly detection is a critical step towards building a secure and
trustworthy system. The primary purpose of a system log is to
record system states and significant events at various critical points
to help debug system failures and perform root cause analysis. Such
log data is universally available in nearly all computer systems.
Log data is an important and valuable resource for understanding
system status and performance issues; therefore, the various system logs are naturally excellent source of information for online
monitoring and anomaly detection. We propose DeepLog, a deep
neural network model utilizing Long Short-Term Memory (LSTM),
to model a system log as a natural language sequence. This allows
DeepLog to automatically learn log patterns from normal execution,
and detect anomalies when log patterns deviate from the model
trained from log data under normal execution. In addition, we
demonstrate how to incrementally update the DeepLog model in
an online fashion so that it can adapt to new log patterns over time.
Furthermore, DeepLog constructs workflows from the underlying
system log so that once an anomaly is detected, users can diagnose
the detected anomaly and perform root cause analysis effectively.
Extensive experimental evaluations over large log data have shown
that DeepLog has outperformed other existing log-based anomaly
detection methods based on traditional data mining methodologies.
",16.860833078287435,16.488909370199696,218
CCS_17_081.txt,17.972493832021325,17.532568424711098,CCS,6740,"

The current evolution of the cyber-threat ecosystem shows that no
system can be considered invulnerable. It is therefore important
to quantify the risk level within a system and devise risk prediction methods such that proactive measures can be taken to reduce
the damage of cyber attacks. We present RiskTeller, a system that
analyzes binary file appearance logs of machines to predict which
machines are at risk of infection months in advance. Risk prediction
models are built by creating, for each machine, a comprehensive
profile capturing its usage patterns, and then associating each profile to a risk level through both fully and semi-supervised learning
methods. We evaluate RiskTeller on a year-long dataset containing
information about all the binaries appearing on machines of 18
enterprises. We show that RiskTeller can use the machine profile
computed for a given machine to predict subsequent infections
with the highest prediction precision achieved to date.

",16.52667757954773,15.250526315789475,153
CCS_17_082.txt,13.462698494232296,12.199777002557546,CCS,12469,"

We introduce the key reinstallation attack. This attack abuses design
or implementation flaws in cryptographic protocols to reinstall an
already-in-use key. This resets the key’s associated parameters such
as transmit nonces and receive replay counters. Several types of
cryptographic Wi-Fi handshakes are affected by the attack.

All protected Wi-Fi networks use the 4-way handshake to generate a fresh session key. So far, this 14-year-old handshake has
remained free from attacks, and is even proven secure. However,
we show that the 4-way handshake is vulnerable to a key reinstallation attack. Here, the adversary tricks a victim into reinstalling an
already-in-use key. This is achieved by manipulating and replaying
handshake messages. When reinstalling the key, associated parameters such as the incremental transmit packet number (nonce) and
receive packet number (replay counter) are reset to their initial
value. Our key reinstallation attack also breaks the PeerKey, group
key, and Fast BSS Transition (FT) handshake. The impact depends
on the handshake being attacked, and the data-confidentiality protocol in use. Simplified, against AES-CCMP an adversary can replay
and decrypt (but not forge) packets. This makes it possible to hijack
TCP streams and inject malicious data into them. Against WPATKIP and GCMP the impact is catastrophic: packets can be replayed,
decrypted, and forged. Because GCMP uses the same authentication
key in both communication directions, it is especially affected.

Finally, we confirmed our findings in practice, and found that
every Wi-Fi device is vulnerable to some variant of our attacks.
Notably, our attack is exceptionally devastating against Android 6.0:
it forces the client into using a predictable all-zero encryption key.

",12.931820455737444,11.455709987966312,280
CCS_17_083.txt,15.665141646143574,14.902615074411909,CCS,8251,"

Crypto Phones aim to establish end-to-end secure voice (and text)
communications based on human-centric (usually) short checksum
validation. They require end users to perform: (1) checksum comparison to detect traditional data-based man-in-the-middle (data
MITM) attacks, and, optionally, (2) speaker verification to detect
sophisticated voice-based man-in-the-middle (voice MITM) attacks.
However, research shows that both tasks are prone to human errors making Crypto Phones highly vulnerable to MITM attacks,
especially to data MITM given the prominence of these attacks.
Further, human errors under benign settings undermine usability
since legitimate calls would often need to be rejected.

We introduce Closed Captioning Crypto Phones (CCCP), that remove the human user from the loop of checksum comparison by
utilizing speech transcription. CCCP simply requires the user to announce the checksum to the other party—the system automatically
transcribes the spoken checksum and performs the comparison. Automating checksum comparisons offers many key advantages over
traditional designs: (1) the chances of data MITM due to human errors and “click-through” could be highly reduced (even eliminated);
(2) longer checksums can be utilized, which increases the protocol
security against data MITM; (3) users’ cognitive burden is reduced
due to the need to perform only a single task, thereby lowering the
potential of human errors.

As a main component of CCCP, we first design and implement
an automated checksum comparison tool based on standard Speech
to Text engines. To evaluate the security and usability benefits of
CCCP, we then design and conduct an online user study that mimics
a realistic VoIP scenario, and collect and transcribe a comprehensive
data set spoken by a wide variety of speakers in real-life conditions.
Our study results demonstrate that, by using our automated checksum comparison, CCCP can completely resist data MITM, while
significantly reducing human errors in the benign case compared
to the traditional approach. They also show that CCCP may help
reduce the likelihood of voice MITM. Finally, we discuss how CCCP
can be improved by designing specialized transcribers and carefully
selected checksum dictionaries, and how it can be integrated with
existing Crypto Phones to bolster their security and usability.

",16.92669308720184,17.645,361
CCS_17_084.txt,16.41995770472593,14.601163320615711,CCS,12431,"

An essential cornerstone of the definition of security for key exchange protocols is the notion of partnering. The de-facto standard
definition of partnering is that of (partial) matching conversations
(MC), which essentially states that two processes are partnered if
every message sent by the first is actually received by the second
and vice versa. We show that proving security under MC-based definitions is error-prone. To this end, we introduce no-match attacks,
a new class of attacks that renders many existing security proofs
invalid. We show that no-match attacks are often hard to avoid in
MC-based security definitions without a) modifications of the original protocol or b) resorting to the use of cryptographic primitives
with special properties. Finally, we show several ways to thwart
no-match attacks. Most notably and as one of our major contributions, we provide a conceptually new definition of partnering that
circumvents the problems of a MC-based partnering notion while
preserving all its advantages. Our new notion of partnering not only
makes security definitions for key exchange model practice much
more closely. In contrast to many other security notions of key exchange it also adheres to the high standards of good cryptographic
definitions: it is general, supports cryptographic intuition, allows
for efficient falsification, and provides a fundamental composition
property that MC-based notions lack.

",17.122413403193683,15.767559523809524,225
CCS_17_085.txt,16.889821358551387,17.221954907685497,CCS,11488,"

We propose indexes of queries, a novel mechanism for supporting
efficient, expressive, and information-theoretically private singleround queries over multi-server PIR databases. Our approach decouples the way that users construct their requests for data from
the physical layout of the remote data store, thereby enabling users
to fetch data using “contextual” queries that specify which data
they seek, as opposed to “positional” queries that specify where
those data happen to reside. For example, an open-access eprint
repository could employ indexes of queries to let researchers fetch
academic articles via PIR queries such as for “this year’s 5 most
cited papers about PIR” or “the 3 most recently posted papers about
PIR”. Our basic approach is compatible with any PIR protocol in
the ubiquitous “vector-matrix” model for PIR, though the most
sophisticated and useful of our constructions rely on some nice
algebraic properties of Goldberg’s IT-PIR protocol (Oakland 2007).
We have implemented our techniques as an extension to Percy++,
an open-source implementation of Goldberg’s IT-PIR protocol. Our
experiments indicate that the new techniques can greatly improve
not only utility for private information retrievers but also efficiency
for private information retrievers and servers alike.
",18.422482065455632,19.771000000000004,202
CCS_17_086.txt,13.968273953766033,12.135289379045798,CCS,7925,"

Individuals are continually observed by an ever-increasing number of sensors that make up the Internet of Things. The resulting
streams of data, which are analyzed in real time, can reveal sensitive
personal information about individuals. Hence, there is an urgent
need for stream processing solutions that can analyze these data in
real time with provable guarantees of privacy and low error.

We present PeGaSus, a new algorithm for differentially private
stream processing. Unlike prior work that has focused on answering individual queries over streams, our algorithm is the first that
can simultaneously support a variety of stream processing tasks
— counts, sliding windows, event monitoring - over multiple resolutions of the stream. PeGaSus uses a Perturber to release noisy
counts, a data-adaptive Perturber to identify stable uniform regions
in the stream, and a query specific Smoother, which combines the
outputs of the Perturber and Grouper to answer queries with low
error. In a comprehensive study using a WiFi access point dataset,
we empirically show that PeGaSus can answer continuous queries
with lower error than the previous state-of-the-art algorithms, even
those specialized to particular query types.

",17.77360955136429,17.04066869300912,189
CCS_17_087.txt,13.750147254761593,11.864445824753286,CCS,10350,"

Private record linkage (PRL) is the problem of identifying pairs
of records that are similar as per an input matching rule from
databases held by two parties that do not trust one another. We
identify three key desiderata that a PRL solution must ensure: (1)
perfect precision and high recall of matching pairs, (2) a proof
of end-to-end privacy, and (3) communication and computational
costs that scale subquadratically in the number of input records.
We show that all of the existing solutions for PRL— including secure
2-party computation (S2PC), and their variants that use non-private
or differentially private (DP) blocking to ensure subquadratic cost
— violate at least one of the three desiderata. In particular, S2PC
techniques guarantee end-to-end privacy but have either low recall
or quadratic cost. In contrast, no end-to-end privacy guarantee has
been formalized for solutions that achieve subquadratic cost. This is
true even for solutions that compose DP and S2PC: DP does not permit the release of any exact information about the databases, while
S2PC algorithms for PRL allow the release of matching records.

In light of this deficiency, we propose a novel privacy model,
called output constrained differential privacy, that shares the strong
privacy protection of DP, but allows for the truthful release of the
output of a certain function applied to the data. We apply this to
PRL, and show that protocols satisfying this privacy model permit
the disclosure of the true matching records, but their execution is
insensitive to the presence or absence of a single non-matching
record. We find that prior work that combine DP and S2PC techniques even fail to satisfy this end-to-end privacy model. Hence,
we develop novel protocols that provably achieve this end-to-end
privacy guarantee, together with the other two desiderata of PRL.
Our empirical evaluation also shows that our protocols obtain high
recall, scale near linearly in the size of the input databases and
the output set of matching pairs, and have communication and
computational costs that are at least 2 orders of magnitude smaller
than S2PC baselines.

",16.800685031470465,16.90306818181819,353
CCS_17_088.txt,13.323905999466326,12.176761085184562,CCS,7090,"
HTTPS error warnings are supposed to alert browser users to net- work attacks. Unfortunately, a wide range of non-attack circum- stances trigger hundreds of millions of spurious browser warnings per month. Spurious warnings frustrate users, hinder the wide- spread adoption of HTTPS, and undermine trust in browser warn- ings. We investigate the root causes of HTTPS error warnings in the field, with the goal of resolving benign errors.
We study a sample of over 300 million errors that Google Chrome users encountered in the course of normal browsing. After manually reviewing more than 2,000 error reports, we developed automated rules to classify the top causes of HTTPS error warnings. We are able to automatically diagnose the root causes of two-thirds of error reports. To our surprise, we find that more than half of er- rors are caused by client-side or network issues instead of server misconfigurations. Based on these findings, we implemented more actionable warnings and other browser changes to address client- side error causes. We further propose solutions for other classes of root causes.


",11.003577315987393,10.775595505617979,180
CCS_17_089.txt,15.794586918744532,14.919556761967979,CCS,9063,"

In this paper, we present the first longitudinal measurement study
of the underground ecosystem fueling credential theft and assess
the risk it poses to millions of users. Over the course of March,
2016-March, 2017, we identify 788,000 potential victims of off-theshelf keyloggers; 12.4 million potential victims of phishing kits; and
1.9 billion usernames and passwords exposed via data breaches and
traded on blackmarket forums. Using this dataset, we explore to
what degree the stolen passwords—which originate from thousands
of online services—enable an attacker to obtain a victim’s valid
email credentials—and thus complete control of their online identity due to transitive trust. Drawing upon Google as a case study,
we find 7-25% of exposed passwords match a victim’s Google account. For these accounts, we show how hardening authentication
mechanisms to include additional risk signals such as a user’s historical geolocations and device profiles helps to mitigate the risk of
hijacking. Beyond these risk metrics, we delve into the global reach
of the miscreants involved in credential theft and the blackhat tools
they rely on. We observe a remarkable lack of external pressure on
bad actors, with phishing kit playbooks and keylogger capabilities
remaining largely unchanged since the mid-2000s.

",15.719379583869454,16.25218487394958,211
CCS_17_090.txt,14.653201853947056,12.952055639745797,CCS,10336,"

Digitally signed malware can bypass system protection mechanisms
that install or launch only programs with valid signatures. It can
also evade anti-virus programs, which often forego scanning signed
binaries. Known from advanced threats such as Stuxnet and Flame,
this type of abuse has not been measured systematically in the
broader malware landscape. In particular, the methods, effectiveness window, and security implications of code-signing PKI abuse
are not well understood. We propose a threat model that highlights
three types of weaknesses in the code-signing PKI. We overcome
challenges specific to code-signing measurements by introducing
techniques for prioritizing the collection of code-signing certificates
that are likely abusive. We also introduce an algorithm for distinguishing among different types of threats. These techniques allow
us to study threats that breach the trust encoded in the Windows
code-signing PKI. The threats include stealing the private keys associated with benign certificates and using them to sign malware or by
impersonating legitimate companies that do not develop software
and, hence, do not own code-signing certificates. Finally, we discuss
the actionable implications of our findings and propose concrete
steps for improving the security of the code-signing ecosystem.

",14.410869940926823,13.356959390862944,198
CCS_17_091.txt,15.002689894826304,13.607592520862383,CCS,10273,"

The recently proposed file-injection type attacks are highlighting
the importance of forward security in dynamic searchable symmetric encryption (DSSE). Forward security enables to thwart those
attacks by hiding the information about the newly added files matching a previous search query. However, there are still only a few
DSSE schemes that provide forward security, and they have factors that hinder efficiency. In particular, all of these schemes do
not support actual data deletion, which increments both storage
space and computational complexity. In this paper, we design and
implement a forward secure DSSE scheme with optimal search and
update complexity, for both computation and communication point
of view. As a starting point, we propose a new, simple, theoretical data structure, called dual dictionary that can take advantage
of both the inverted and the forward indexes at the same time.
This data structure allows to delete data explicitly and in real time,
which greatly improves efficiency compared to previous works. In
addition, our scheme provides forward security by encrypting the
newly inserted data with fresh keys not related with the previous
search tokens. We implemented our scheme for Enron email and
Wikipedia datasets and measured its performance. The comparison
with Sophos shows that our scheme is very efficient in practice, for
both searches and updates in dynamic environments.

",15.774802946060372,14.303444444444448,217
CCS_17_092.txt,13.705759319618895,11.774187380978933,CCS,10620,"

Using dynamic Searchable Symmetric Encryption, a user with limited storage resources can securely outsource a database to an untrusted server, in such a way that the database can still be searched
and updated efficiently. For these schemes, it would be desirable
that updates do not reveal any information a priori about the modifications they carry out, and that deleted results remain inaccessible
to the server a posieriori. If the first property, called forward privacy,
has been the main motivation of recent works, the second one,
backward privacy, has been overlooked.

In this paper, we study for the first time the notion of backward privacy for searchable encryption. After giving formal definitions for different flavors of backward privacy, we present several
schemes achieving both forward and backward privacy, with various efficiency trade-offs.

Our constructions crucially rely on primitives such as constrained
pseudo-random functions and puncturable encryption schemes.
Using these advanced cryptographic primitives allows for a finegrained control of the power of the adversary, preventing her
from evaluating functions on selected inputs, or decrypting specific ciphertexts. In turn, this high degree of control allows our
SSE constructions to achieve the stronger forms of privacy outlined above. As an example, we present a framework to construct
forward-private schemes from range-constrained pseudo-random
functions.

Finally, we provide experimental results for implementations of
our schemes, and study their practical efficiency.

",16.887214914478655,15.601262008733627,230
CCS_17_093.txt,16.077521257181708,14.999999072276651,CCS,8678,"

Cybercrime markets support the development and diffusion of new
attack technologies, vulnerability exploits, and malware. Whereas
the revenue streams of cyber attackers have been studied multiple times in the literature, no quantitative account currently exists
on the economics of attack acquisition and deployment. Yet, this
understanding is critical to characterize the production of (traded)
exploits, the economy that drives it, and its effects on the overall
attack scenario. In this paper we provide an empirical investigation of the economics of vulnerability exploitation, and the effects
of market factors on likelihood of exploit. Our data is collected
first-handedly from a prominent Russian cybercrime market where
the trading of the most active attack tools reported by the security
industry happens. Our findings reveal that exploits in the underground are priced similarly or above vulnerabilities in legitimate
bug-hunting programs, and that the refresh cycle of exploits is
slower than currently often assumed. On the other hand, cybercriminals are becoming faster at introducing selected vulnerabilities, and the market is in clear expansion both in terms of players,
traded exploits, and exploit pricing. We then evaluate the effects
of these market variables on likelihood of attack realization, and
find strong evidence of the correlation between market activity
and exploit deployment. We discuss implications on vulnerability
metrics, economics, and exploit measurement.

",17.5058628484301,17.467711213517664,218
CCS_17_094.txt,17.77360955136429,16.98876631942466,CCS,7037,"

Product vendors and vulnerability researchers work with the same
underlying artifacts, but can be motivated by goals that are distinct
and, at times, disjoint. This potential for conflict, coupled with the
legal instruments available to product vendors (e.g., EULAs, DMCA,
CFAA, etc.) drive a broad concern that there are “chilling effects”
that dissuade vulnerability researchers from vigorously evaluating
product security. Indeed, there are well-known examples of legal
action taken against individual researchers. However, these are
inherently anecdotal in nature and skeptics of the chilling-effects
hypothesis argue that there is no systematic evidence to justify
such concerns. This paper is motivated by precisely this tussle. We
present some of the first work to address this issue on a quantitative
and empirical footing, illuminating the sentiments of both product
vendors and vulnerability researchers. First, we canvas a range
of product companies for explicit permission to conduct security
assessments and thus characterize the degree to which the broad
software vendor community is supportive of vulnerability research
activities and how this varies based on the nature of the researcher.
Second, we conduct an online sentiment survey of vulnerability
researchers to understand the extent to which they have abstract
concerns or concrete experience with legal threats and the extent
to which this mindset shapes their choices.
",17.122413403193683,17.119571596244132,215
CCS_17_095.txt,11.632502320430469,8.669629953198399,CCS,14793,"
We introduce identity-based format-preserving encryption (IB-FPE)
as a way to localize and limit the damage to format-preserving encryption (FPE) from key exposure. We give definitions, relations between them, generic attacks and two transforms of FPE schemes to
IB-FPE schemes. As a special case, we introduce and cover identitybased tweakable blockciphers. We apply all this to analyze DFF, an
FPE scheme proposed to NIST for standardization.
",14.90622815163357,11.800714285714289,71
CCS_17_096.txt,13.441788958396426,11.550338696631812,CCS,4497,"

We provide an analysis of IEEE standard P1735, which describes
methods for encrypting electronic-design intellectual property (IP),
as well as the management of access rights for such IP. We find a
surprising number of cryptographic mistakes in the standard. In
the most egregious cases, these mistakes enable attack vectors that
allow us to recover the entire underlying plaintext IP. Some of these
attack vectors are well-known, e.g. padding-oracle attacks. Others
are new, and are made possible by the need to support the typical
uses of the underlying IP; in particular, the need for commercial
system-on-chip (SoC) tools to synthesize multiple pieces of IP into a
fully specified chip design and to provide syntax errors. We exploit
these mistakes in a variety of ways, leveraging a commercial SoC
tool as a black-box oracle.

In addition to being able to recover entire plaintext IP, we show
how to produce standard-compliant ciphertexts of IP that have been
modified to include targeted hardware Trojans. For example, IP that
correctly implements the AES block cipher on all but one (arbitrary)
plaintext that induces the block cipher to return the secret key.

We outline a number of other attacks that the standard allows,
including on the cryptographic mechanism for IP licensing. Unfortunately, we show that obvious “quick fixes” to the standard
(and the tools that support it) do not stop all of our attacks. This
suggests that the standard requires a significant overhaul, and that
IP-authors using P1735 encryption should consider themselves at
risk.

",15.308715981407026,13.905894886363637,258
CCS_17_097.txt,14.139725605858839,11.8013661537863,CCS,11081,"

Bilinear groups form the algebraic setting for a multitude of important cryptographic protocols including anonymous credentials,
e-cash, e-voting, e-coupon, and loyalty systems. It is typical of such
crypto protocols that participating parties need to repeatedly verify that certain equations over bilinear groups are satisfied, e.g.,
to check that computed signatures are valid, commitments can
be opened, or non-interactive zero-knowledge proofs verify correctly. Depending on the form and number of equations this part
can quickly become a performance bottleneck due to the costly
evaluation of the bilinear map.

To ease this burden on the verifier, batch verification techniques
have been proposed that allow to combine and check multiple
equations probabilistically using less operations than checking each
equation individually. In this work, we revisit the batch verification
problem and existing standard techniques. We introduce a new
technique which, in contrast to previous work, enables us to fully
exploit the structure of certain systems of equations. Equations of
the appropriate form naturally appear in many protocols, e.g., due
to the use of Groth-Sahai proofs.


The beauty of our technique is that the underlying idea is pretty
simple: we observe that many systems of equations can alternatively
be viewed as a single equation of products of polynomials for which
probabilistic polynomial identity testing following Schwartz—Zippel
can be applied. Comparisons show that our approach can lead
to significant improvements in terms of the number of pairing
evaluations. Indeed, for the BeleniosRF voting system presented at
CCS 2016, we can reduce the number of pairings (required for ballot
verification) from 4k + 140, as originally reported by Chaidos et al.
[19], tok +7. As our implementation and benchmarks demonstrate,
this may reduce the verification runtime to only 5% to 13% of the
original runtime.

",18.14513906131404,17.189004914004915,297
CCS_17_098.txt,14.903485989738751,13.912025245359235,CCS,10230,"

We propose a lattice-based electronic voting scheme, EVOLVE (Electronic Voting from Lattices with Verification), which is conjectured
to resist attacks by quantum computers. Our protocol involves a
number of voting authorities so that vote privacy is maintained as
long as at least one of the authorities is honest, while the integrity
of the result is guaranteed even when all authorities collude. Furthermore, the result of the vote can be independently computed by
any observer.

At the core of the protocol is the utilization of a homomorphic
commitment scheme with strategically orchestrated zero-knowledge
proofs: voters use approximate but efficient “Fiat-Shamir with
Aborts” proofs to show the validity of their vote, while the authorities use amortized exact proofs to show that the commitments
are well-formed. We also present a novel efficient zero-knowledge
proof that one of two lattice-based statements is true (so-called OR
proof) and a new mechanism to control the size of the randomness
when applying the homomorphism to commitments.

We give concrete parameter choices to securely instantiate and
evaluate the efficiency of our scheme. Our prototype implementation shows that the voters require 8 milliseconds to submit a vote
of size about 20KB to each authority and it takes each authority
0.15 seconds per voter to create a proof that his vote was valid. The
size of the vote share that each authority produces is approximately
15KB per voter, which we believe is well within the practical bounds
for a large-scale election.

",17.553077303434723,17.02103915662651,251
CCS_17_099.txt,15.645734412145273,14.121057660605242,CCS,9178,"

The semiconductor industry is fully globalized and integrated circuits (ICs) are commonly defined, designed and fabricated in different premises across the world. This reduces production costs, but
also exposes ICs to supply chain attacks, where insiders introduce
malicious circuitry into the final products. Additionally, despite
extensive post-fabrication testing, it is not uncommon for ICs with
subtle fabrication errors to make it into production systems. While
many systems may be able to tolerate a few byzantine components,
this is not the case for cryptographic hardware, storing and computing on confidential data. For this reason, many error and backdoor
detection techniques have been proposed over the years. So far
all attempts have been either quickly circumvented, or come with
unrealistically high manufacturing costs and complexity.

This paper proposes Myst, a practical high-assurance architecture, that uses commercial off-the-shelf (COTS) hardware, and provides strong security guarantees, even in the presence of multiple malicious or faulty components. The key idea is to combine
protective-redundancy with modern threshold cryptographic techniques to build a system tolerant to hardware trojans and errors.
To evaluate our design, we build a Hardware Security Module that
provides the highest level of assurance possible with COTS components. Specifically, we employ more than a hundred COTS secure
cryptocoprocessors, verified to FIPS140-2 Level 4 tamper-resistance
standards, and use them to realize high-confidentiality random
number generation, key derivation, public key decryption and signing. Our experiments show a reasonable computational overhead
(less than 1% for both Decryption and Signing) and an exponential
increase in backdoor-tolerance as more ICs are added.

",17.332850977054683,16.482073971655726,264
CCS_17_100.txt,14.210050734458086,12.614274857643455,CCS,11897,"
Logic locking has been conceived as a promising proactive defense
strategy against intellectual property (IP) piracy, counterfeiting,
hardware Trojans, reverse engineering, and overbuilding attacks.
Yet, various attacks that use a working chip as an oracle have been
launched on logic locking to successfully retrieve its secret key,
undermining the defense of all existing locking techniques. In
this paper, we propose stripped-functionality logic locking (SFLL),
which strips some of the functionality of the design and hides it in
the form of a secret key(s), thereby rendering on-chip implementation functionally different from the original one. When loaded onto
an on-chip memory, the secret keys restore the original functionality of the design. Through security-aware synthesis that creates a
controllable mismatch between the reverse-engineered netlist and
original design, SFLL provides a quantifiable and provable resilience
trade-off between all known and anticipated attacks. We demonstrate the application of SFLL to large designs (>100K gates) using
a computer-aided design (CAD) framework that ensures attaining
the desired security level at minimal implementation cost, 8%, 5%,
and 0.5% for area, power, and delay, respectively. In addition to
theoretical proofs and simulation confirmation of SFLL’s security,
we also report results from the silicon implementation of SFLL on
an ARM Cortex-M0 microprocessor in 65nm technology.
",18.699421769314853,19.110431893687714,217
CCS_17_101.txt,15.755184324747162,14.041409744686103,CCS,13626,"

We report on our discovery of an algorithmic flaw in the construction of primes for RSA key generation in a widely-used library
of a major manufacturer of cryptographic hardware. The primes
generated by the library suffer from a significant loss of entropy.
We propose a practical factorization method for various key lengths
including 1024 and 2048 bits. Our method requires no additional
information except for the value of the public modulus and does
not depend on a weak or a faulty random number generator. We
devised an extension of Coppersmith’s factorization attack utilizing
an alternative form of the primes in question. The library in question is found in NIST FIPS 140-2 and CC EAL 5+ certified devices
used for a wide range of real-world applications, including identity
cards, passports, Trusted Platform Modules, PGP and tokens for
authentication or software signing. As the relevant library code was
introduced in 2012 at the latest (and probably earlier), the impacted
devices are now widespread. Tens of thousands of such keys were
directly identified, many with significant impacts, especially for
electronic identity documents, software signing, Trusted Computing and PGP. We estimate the number of affected devices to be in
the order of at least tens of millions.

The worst cases for the factorization of 1024 and 2048-bit keys
are less than 3 CPU-months and 100 CPU-years on single core of
common recent CPUs, respectively, while the expected time is half
of that of the worst case. The attack can be parallelized on multiple
CPUs. Worse still, all susceptible keys contain a strong fingerprint
that is verifiable in microseconds on an ordinary laptop — meaning
that all vulnerable keys can be quickly identified, even in very large
datasets.

",16.424793381693185,14.544368512110726,290
CCS_17_102.txt,14.83607586356922,12.855259572555546,CCS,8820,"

The goal of an algorithm substitution attack (ASA), also called a
subversion attack (SA), is to replace an honest implementation of
a cryptographic tool by a subverted one which allows to leak private information while generating output indistinguishable from
the honest output. Bellare, Paterson, and Rogaway provided at
CRYPTO *14 a formal security model to capture this kind of attacks
and constructed practically implementable ASAs against a large
class of symmetric encryption schemes, At CCS’15, Ateniese, Magri,
and Venturi extended this model to allow the attackers to work in
a fully-adaptive and continuous fashion and proposed subversion
attacks against digital signature schemes. Both papers also showed
the impossibility of ASAs in cases where the cryptographic tools are
deterministic. Also at CCS’15, Bellare, Jaeger, and Kane strengthened the original model and proposed a universal ASA against
sufficiently random encryption schemes. In this paper we analyze
ASAs from the perspective of steganography - the well known
concept of hiding the presence of secret messages in legal communications. While a close connection between ASAs and steganography is known, this lacks a rigorous treatment. We consider the
common computational model for secret-key steganography and
prove that successful ASAs correspond to secure stegosystems on
certain channels and vice versa. This formal proof allows us to
conclude that ASAs are stegosystems and to “rediscover” several
results concerning ASAs known in the steganographic literature.

",18.511140095513987,18.65815217391305,231
CCS_17_103.txt,14.783125029745705,13.103403514190607,CCS,4411,"

Modern Integrated Circuits (ICs) employ several classes of countermeasures to mitigate physical attacks. Recently, a powerful semiinvasive attack relying on optical contactless probing has been
introduced, which can assist the attacker in circumventing the integrated countermeasures and probe the secret data on a chip. This
attack can be mounted using IC debug tools from the backside of
the chip. The first published attack based on this technique was
conducted against a proof-of-concept hardware implementation
on a Field Programmable Gate Array (FPGA). Therefore, the success of optical probing techniques against a real commercial device
without any knowledge of the hardware implementation is still
questionable. The aim of this work is to assess the threat of optical contactless probing in a real attack scenario. To this end, we
conduct an optical probing attack against the bitstream encryption
feature of a common FPGA. We demonstrate that the adversary is
able to extract the plaintext data containing sensitive design information and intellectual property (IP). In contrast to previous optical
attacks from the IC backside, our attack does not require any device
preparation or silicon polishing, which makes it a non-invasive
attack. Additionally, we debunk the myth that small technology
sizes are unsusceptible to optical attacks, as we use an optical resolution of about 1 jm to successfully attack a 28 nm device. Based
on our time measurements, an attacker needs less than 10 working days to conduct the optical analysis and reverse-engineer the
security-related parts of the hardware. Finally, we propose and
discuss potential countermeasures, which could make the attack
more challenging.

",15.903189008614273,14.583355513307989,264
CCS_17_104.txt,14.987086067671061,14.031979534469379,CCS,10642,"

In 2007, Shacham published a seminal paper on Return-Oriented
Programming (ROP), the first systematic formulation of code reuse.
The paper has been highly influential, profoundly shaping the way
we still think about code reuse today: an attacker analyzes the “geometry” of victim binary code to locate gadgets and chains these
to craft an exploit. This model has spurred much research, with a
rapid progression of increasingly sophisticated code reuse attacks
and defenses over time. After ten years, the common perception
is that state-of-the-art code reuse defenses are effective in significantly raising the bar and making attacks exceedingly hard.

In this paper, we challenge this perception and show that an attacker going beyond “geometry” (static analysis) and considering
the “dynamics” (dynamic analysis) of a victim program can easily
find function call gadgets even in the presence of state-of-the-art
code-reuse defenses. To support our claims, we present NEWTON, a
run-time gadget-discovery framework based on constraint-driven
dynamic taint analysis. NEWTON can model a broad range of defenses by mapping their properties into simple, stackable, reusable
constraints, and automatically generate gadgets that comply with
these constraints. Using NEWTON, we systematically map and compare state-of-the-art defenses, demonstrating that even simple interactions with popular server programs are adequate for finding
gadgets for all state-of-the-art code-reuse defenses. We conclude
with an nginx case study, which shows that a NEwTon-enabled
attacker can craft attacks which comply with the restrictions of
advanced defenses, such as CPI and context-sensitive CFI.

",16.728156217252725,16.553589743589743,261
CCS_17_105.txt,16.20431710577834,14.748103539446173,CCS,10946,"

Defending against malware involves analysing large amounts of
suspicious samples. To deal with such quantities we rely heavily on
automatic approaches to determine whether a sample is malicious
or not. Unfortunately, complete and precise automatic analysis
of malware is far from an easy task. This is because malware is
often designed to contain several techniques and countermeasures
specifically to hinder analysis. One of these techniques is for the
malware to propagate through the operating system so as to execute
in the context of benign processes. The malware does this by writing
memory to a given process and then proceeds to have this memory
execute. In some cases these propagations are trivial to capture
because they rely on well-known techniques. However, in the cases
where malware deploys novel code injection techniques, rely on
code-reuse attacks and potentially deploy dynamically generated
code, the problem of capturing a complete and precise view of the
malware execution is non-trivial.

In this paper we present a unified approach to tracing malware
propagations inside the host in the context of code injections and
code-reuse attacks. We also present, to the knowledge of the authors, the first approach to identifying dynamically generated code
based on information-flow analysis. We implement our techniques
in a system called Tartarus and match Tartarus with both synthetic
applications and real-world malware. We compare Tartarus to previous works and show that our techniques substantially improve
the precision for collecting malware execution traces, and that
our approach can capture intrinsic characteristics of novel code
injection techniques.


",15.359359093739592,14.336860465116278,259
CCS_17_106.txt,16.037222667969925,14.486988283651325,CCS,10924,"

Cross-Site Scripting (XSS) is an unremitting problem for the Web.
Since its initial public documentation in 2000 until now, XSS has
been continuously on top of the vulnerability statistics. Even though
there has been a considerable amount of research [15, 18, 21] and
developer education to address XSS on the source code level, the
overall number of discovered XSS problems remains high. Because
of this, various approaches to mitigate XSS [14, 19, 24, 28, 30] have
been proposed as a second line of defense, with HTML sanitizers, Web Application Firewalls, browser-based XSS filters, and the
Content Security Policy being some prominent examples. Most of
these mechanisms focus on script tags and event handlers, either
by removing them from user-provided content or by preventing
their script code from executing.

In this paper, we demonstrate that this approach is no longer
sufficient for modern applications: We describe a novel Web attack
that can circumvent all of theses currently existing XSS mitigation techniques. In this attack, the attacker abuses so called script
gadgets (legitimate JavaScript fragments within an application’s
legitimate code base) to execute JavaScript. In most cases, these
gadgets utilize DOM selectors to interact with elements in the Web
document. Through an initial injection point, the attacker can inject
benign-looking HTML elements which are ignored by these mitigation techniques but match the selector of the gadget. This way, the
attacker can hijack the input of a gadget and cause processing of his
input, which in turn leads to code execution of attacker-controlled
values. We demonstrate that these gadgets are omnipresent in almost all modern JavaScript frameworks and present an empirical
study showing the prevalence of script gadgets in productive code.
As a result, we assume most mitigation techniques in web applications written today can be bypassed.

",16.52667757954773,14.964958193979935,301
CCS_17_107.txt,15.975528389648634,15.557059757681806,CCS,10980,"
As the extension of Distributed Denial-of-Service (DDoS) attacks
to application layer in recent years, researchers pay much interest
in these new variants due to a low-volume and intermittent pattern
with a higher level of stealthiness, invaliding the state-of-the-art
DDoS detection/defense mechanisms. We describe a new type of
low-volume application layer DDoS attack—-Tail Attacks on Web
Applications. Such attack exploits a newly identified system vulnerability of n-tier web applications (millibottlenecks with sub-second
duration and resource contention with strong dependencies among
distributed nodes) with the goal of causing the long-tail latency
problem of the target web application (e.g., 95th percentile response
time > 1 second) and damaging the long-term business of the service provider, while all the system resources are far from saturation,
making it difficult to trace the cause of performance degradation.

We present a modified queueing network model to analyze the
impact of our attacks in n-tier architecture systems, and numerically solve the optimal attack parameters. We adopt a feedback
control-theoretic (e.g., Kalman filter) framework that allows attackers to fit the dynamics of background requests or system state by
dynamically adjusting attack parameters. To evaluate the practicality of such attacks, we conduct extensive validation through
not only analytical, numerical, and simulation results but also real
cloud production setting experiments via a representative benchmark website equipped with state-of-the-art DDoS defense tools.
We further proposed a solution to detect and defense the proposed
attacks, involving three stages: fine-grained monitoring, identifying
bursts, and blocking bots.


",19.5731925562951,20.78821018062398,263
CCS_17_108.txt,15.234391540308398,14.569474548440066,CCS,9933,"
The Internet Archive’s Wayback Machine is the largest modern web
archive, preserving web content since 1996. We discover and analyze several vulnerabilities in how the Wayback Machine archives
data, and then leverage these vulnerabilities to create what are to
our knowledge the first attacks against a user’s view of the archived
web. Our vulnerabilities are enabled by the unique interaction between the Wayback Machine’s archives, other websites, and a user’s
browser, and attackers do not need to compromise the archives in
order to compromise users’ views of a stored page. We demonstrate
the effectiveness of our attacks through proof-of-concept implementations. Then, we conduct a measurement study to quantify
the prevalence of vulnerabilities in the archive. Finally, we explore
defenses which might be deployed by archives, website publishers,
and the users of archives, and present the prototype of a defense
for clients of the Wayback Machine, ArchiveWatcher.
",15.247664890283005,15.317837837837839,153
CCS_17_109.txt,14.082944154712731,12.233733579212526,CCS,10036,"

Cross-Site Request Forgery (CSRF) vulnerabilities are a severe class
of web vulnerabilities that have received only marginal attention
from the research and security testing communities. While much
effort has been spent on countermeasures and detection of XSS and
SQLi, to date, the detection of CSRF vulnerabilities is still performed
predominantly manually.

In this paper, we present Deemon, to the best of our knowledge the
first automated security testing framework to discover CSRF vulnerabilities. Our approach is based on a new modeling paradigm which
captures multiple aspects of web applications, including execution
traces, data flows, and architecture tiers in a unified, comprehensive
property graph. We present the paradigm and show how a concrete
model can be built automatically using dynamic traces. Then, using
graph traversals, we mine for potentially vulnerable operations.
Using the information captured in the model, our approach then
automatically creates and conducts security tests, to practically
validate the found CSRF issues. We evaluate the effectiveness of
Deemon with 10 popular open source web applications. Our experiments uncovered 14 previously unknown CSRF vulnerabilities that
can be exploited, for instance, to take over user accounts or entire
websites.

",16.45884130781739,15.887830687830693,190
CCS_17_110.txt,15.540602993579505,14.136657583569036,CCS,9711,"

The TLS protocol is intended to enable secure end-to-end communication over insecure networks, including the Internet. Unfortunately, this goal has been thwarted a number of times throughout
the protocol’s tumultuous lifetime, resulting in the need for a new
version of the protocol, namely TLS 1.3. Over the past three years, in
an unprecedented joint design effort with the academic community,
the TLS Working Group has been working tirelessly to enhance
the security of TLS.

We further this effort by constructing the most comprehensive,
faithful, and modular symbolic model of the TLS 1.3 draft 21 release
candidate, and use the Tamarin prover to verify the claimed TLS 1.3
security requirements, as laid out in draft 21 of the specification. In
particular, our model covers all handshake modes of TLS 1.3.

Our analysis reveals an unexpected behaviour, which we expect
will inhibit strong authentication guarantees in some implementations of the protocol. In contrast to previous models, we provide
a novel way of making the relation between the TLS specification
and our model explicit: we provide a fully annotated version of
the specification that clarifies what protocol elements we modelled,
and precisely how we modelled these elements. We anticipate this
model artifact to be of great benefit to the academic community
and the TLS Working Group alike.

",17.971250198000288,16.518703703703704,222
CCS_17_111.txt,15.706480438100755,14.006560060424487,CCS,11567,"

HACL/* is a verified portable C cryptographic library that implements modern cryptographic primitives such as the ChaCha20 and
Salsa20 encryption algorithms, Poly1305 and HMAC message authentication, SHA-256 and SHA-512 hash functions, the Curve25519
elliptic curve, and Ed25519 signatures.

HACL* is written in the F* programming language and then
compiled to readable C code. The F* source code for each cryptographic primitive is verified for memory safety, mitigations against
timing side-channels, and functional correctness with respect to a
succinct high-level specification of the primitive derived from its
published standard. The translation from F* to C preserves these
properties and the generated C code can itself be compiled via
the CompCert verified C compiler or mainstream compilers like
GCC or CLANG. When compiled with GCC on 64-bit platforms,
our primitives are as fast as the fastest pure C implementations in
OpenSSL and Libsodium, significantly faster than the reference C
code in TweetNaCl, and between 1.1x-5.7x slower than the fastest
hand-optimized vectorized assembly code in SUPERCOP.

HACL* implements the NaCl cryptographic API and can be used
as a drop-in replacement for NaCl libraries like Libsodium and
TweetNaCl. HACL* provides the cryptographic components for
a new mandatory ciphersuite in TLS 1.3 and is being developed
as the main cryptographic provider for the miTLS verified implementation. Primitives from HACL* are also being integrated within
Mozilla’s NSS cryptographic library. Our results show that writing
fast, verified, and usable C cryptographic libraries is now practical.

",19.16045447334379,17.795995934959354,251
CCS_17_112.txt,16.28108645931811,14.687488983170908,CCS,11313,"

Jasmin is a framework for developing high-speed and high-assurance
cryptographic software. The framework is structured around the

Jasmin programming language and its compiler. The language is

designed for enhancing portability of programs and for simplifying

verification tasks. The compiler is designed to achieve predictability

and efficiency of the output code (currently limited to x64 platforms),
and is formally verified in the Coq proof assistant. Using the suPERcop framework, we evaluate the Jasmin compiler on representative

cryptographic routines and conclude that the code generated by the

compiler is as efficient as fast, hand-crafted, implementations. Moreover, the framework includes highly automated tools for proving

memory safety and constant-time security (for protecting against

cache-based timing attacks). We also demonstrate the effectiveness

of the verification tools on a large set of cryptographic routines.

",15.903189008614273,14.771236673773988,135
CCS_17_113.txt,13.623346047003903,11.373621172703626,CCS,10383,"

We propose a new class of posi-quanitum digital signature schemes
that: (a) derive their security entirely from the security of symmetric-key primitives, believed to be quantum-secure, and (b) have
extremely small keypairs, and, (c) are highly parameterizable.

In our signature constructions, the public key is an image y =
f(x) of a one-way function f and secret key x. A signature is a
non-interactive zero-knowledge proof of x, that incorporates a
message to be signed. For this proof, we leverage recent progress of
Giacomelli et al. (USENIX’ 16) in constructing an efficient =-protocol
for statements over general circuits. We improve this -protocol to
reduce proof sizes by a factor of two, at no additional computational
cost. While this is of independent interest as it yields more compact
proofs for any circuit, it also decreases our signature sizes.

We consider two possibilities to make the proof non-interactive:
the Fiat-Shamir transform and Unruh’s transform (EUROCRYPT’ 12,
°15,16). The former has smaller signatures, while the latter has a
security analysis in the quantum-accessible random oracle model.
By customizing Unruh’s transform to our application, the overhead
is reduced to 1.6x when compared to the Fiat-Shamir transform,
which does not have a rigorous post-quantum security analysis.

We implement and benchmark both approaches and explore the
possible choice of f, taking advantage of the recent trend to strive
for practical symmetric ciphers with a particularly low number of
multiplications and end up using LowMC (EUROCRYPT’ 15).

",15.668782140382113,14.181596277738013,256
CCS_17_114.txt,13.281909814198613,11.06789658291536,CCS,9442,"

In the search for post-quantum secure alternatives to RSA and ECC,
lattice-based cryptography appears to be an attractive and efficient
option. A particularly interesting lattice-based signature scheme is
BLISS, offering key and signature sizes in the range of RSA moduli.
A range of works on efficient implementations of BLISS is available,
and the scheme has seen a first real-world adoption in strongSwan,
an IPsec-based VPN suite. In contrast, the implementation-security
aspects of BLISS, and lattice-based cryptography in general, are
still largely unexplored.

At CHES 2016, Groot Bruinderink et al. presented the first sidechannel attack on BLISS, thus proving that this topic cannot be
neglected. Nevertheless, their attack has some limitations. First, the
technique is demonstrated via a proof-of-concept experiment that
was not performed under realistic attack settings. Furthermore, the
attack does not apply to BLISS-B, an improved variant of BLISS and
also the default option in strongSwan. This problem also applies to
later works on implementation security of BLISS.

In this work, we solve both of the above problems. We present a
new side-channel key-recovery algorithm against both the original
BLISS and the BLISS-B variant. Our key-recovery algorithm draws
on a wide array of techniques, including learning-parity with noise,
integer programs, maximimum likelihood tests, and a lattice-basis
reduction. With each application of a technique, we reveal additional information on the secret key culminating in a complete key
recovery.

Finally, we show that cache attacks on post-quantum cryptography are not only possible, but also practical. We mount an asynchronous cache attack on the production-grade BLISS-B implementation
of strongSwan. The attack recovers the secret signing key after observing roughly 6 000 signature generations.

",14.191785591663535,12.108405172413796,291
CCS_17_115.txt,15.730595870801139,14.248050966047185,CCS,11414,"

In this paper, we investigate the security of the BLISS lattice-based
signature scheme, one of the most promising candidates for postquantum-secure signatures, against side-channel attacks. Several
works have been devoted to its efficient implementation on various
platforms, from desktop CPUs to microcontrollers and FPGAs, and
more recent papers have also considered its security against certain
types of physical attacks, notably fault injection and cache attacks.
We turn to more traditional side-channel analysis, and describe
several attacks that can yield a full key recovery.

We first identify a serious source of leakage in the rejection
sampling algorithm used during signature generation. Existing
implementations of that rejection sampling step, which is essential
for security, actually leak the “relative norm” of the secret key. We
show how an extension of an algorithm due to Howgrave-Graham
and Szydlo can be used to recover the key from that relative norm,
at least when the absolute norm is easy to factor (which happens for
a significant fraction of secret keys). We describe how this leakage
can be exploited in practice both on an embedded device (an 8-bit
AVR microcontroller) using electromagnetic analysis (EMA), and a
desktop computer (recent Intel CPU running Linux) using branch
tracing. The latter attack has been mounted against the open source
VPN software strongSwan.

We also show that other parts of the BLISS signing algorithm
can leak secrets not just for a subset of secret keys, but for 100%
of them. The BLISS Gaussian sampling algorithm in strongSwan
is intrinsically variable time. This would be hard to exploit using
a noisy source of leakage like EMA, but branch tracing allows to
recover the entire randomness and hence the key: we show that a
single execution of the strongSwan signature algorithm is actually
sufficient for full key recovery. We also describe a more traditional
side-channel attack on the sparse polynomial multiplications carried out in BLISS: classically, multiplications can be attacked using

",16.728156217252725,15.813115384615386,326
CCS_17_116.txt,15.452298432132022,13.936771278945255,CCS,10234,"

Noninterference is a popular semantic security condition because it
offers strong end-to-end guarantees, it is inherently compositional,
and it can be enforced using a simple security type system. Unfortunately, it is too restrictive for real systems. Mechanisms for
downgrading information are needed to capture real-world security
requirements, but downgrading eliminates the strong compositional
security guarantees of noninterference.

We introduce nonmalleable information flow, a new formal security condition that generalizes noninterference to permit controlled downgrading of both confidentiality and integrity. While
previous work on robust declassification prevents adversaries from
exploiting the downgrading of confidentiality, our key insight is
transparent endorsement, a mechanism for downgrading integrity
while defending against adversarial exploitation. Robust declassification appeared to break the duality of confidentiality and integrity
by making confidentiality depend on integrity, but transparent endorsement makes integrity depend on confidentiality, restoring this
duality. We show how to extend a security-typed programming
language with transparent endorsement and prove that this static
type system enforces nonmalleable information flow, a new security property that subsumes robust declassification and transparent
endorsement. Finally, we describe an implementation of this type
system in the context of Flame, a flow-limited authorization plugin
for the Glasgow Haskell Compiler.

",20.736966565827903,21.269038944723622,200
CCS_17_117.txt,15.611065946517414,14.216776012490424,CCS,11119,"

We present CLio, an information flow control (IFC) system that
transparently incorporates cryptography to enforce confidentiality
and integrity policies on untrusted storage. Cuio insulates developers from explicitly manipulating keys and cryptographic primitives
by leveraging the policy language of the IFC system to automatically use the appropriate keys and correct cryptographic operations.
We prove that Cio is secure with a novel proof technique that is
based on a proof style from cryptography together with standard
programming languages results. We present a prototype Cuio implementation and a case study that demonstrates CL1o’s practicality.


",18.511140095513987,18.04953296703297,93
CCS_17_118.txt,16.805413229789746,15.982101661607242,CCS,10317,"

Object flow integrity (OFI) augments control-flow integrity (CFI)
and software fault isolation (SFI) protections with secure, first-class
support for binary object exchange across inter-module trust boundaries. This extends both source-aware and source-free CFI and SFI
technologies to a large class of previously unsupported software:
those containing immutable system modules with large, objectoriented APIs—which are particularly common in component-based,
event-driven consumer software. It also helps to protect these intermodule object exchanges against confused deputy-assisted vtable
corruption and counterfeit object-oriented programming attacks.

A prototype implementation for Microsoft Component Object
Model demonstrates that OFI is scalable to large interfaces on the
order of tens of thousands of methods, and exhibits low overheads of
under 1% for some common-case applications. Significant elements
of the implementation are synthesized automatically through a
principled design inspired by type-based contracts.

",19.287186520377343,19.160055555555562,145
CCS_17_119.txt,14.189823948667058,12.074489366459257,CCS,15296,"

Black-box accumulation (BBA) has recently been introduced as a
building-block for a variety of user-centric protocols such as loyalty,
refund, and incentive systems. Loosely speaking, this building block
may be viewed as a cryptographic “piggy bank” that allows a user
to collect points (aka incentives, coins, etc.) in an anonymous and
unlinkable way. A piggy bank may be “robbed” at some point by a
user, letting her spend the collected points, thereby only revealing
the total amount inside the piggy bank and its unique serial number.

In this paper we present BBA+, a definitional framework extending the BBA model in multiple ways: (1) We support offline
systems in the sense that there does not need to be a permanent
connection to a serial number database to check whether a presented piggy bank has already been robbed. (2) We enforce the
collection of “negative points” which users may not voluntarily
collect, as this is, for example, needed in pre-payment or reputation
systems. (3) The security property formalized for BBA+ schemes is
stronger and more natural than for BBA: Essentially, we demand
that the amount claimed to be inside a piggy bank must be exactly
the amount legitimately collected with this piggy bank. As piggy
bank transactions need to be unlinkable at the same time, defining
",16.785175570968402,16.244415584415584,219
CCS_17_120.txt,16.52887588583532,15.17983484680801,CCS,6596,"

The development of positioning technologies has resulted in an
increasing amount of mobility data being available. While bringing
a lot of convenience to people’s life, such availability also raises
serious concerns about privacy. In this paper, we concentrate on
one of the most sensitive information that can be inferred from mobility data, namely social relationships. We propose a novel social
relation inference attack that relies on an advanced feature learning technique to automatically summarize users’ mobility features.
Compared to existing approaches, our attack is able to predict any
two individuals’ social relation, and it does not require the adversary to have any prior knowledge on existing social relations. These
advantages significantly increase the applicability of our attack and
the scope of the privacy assessment. Extensive experiments conducted on a large dataset demonstrate that our inference attack is
effective, and achieves between 13% to 20% improvement over the
best state-of-the-art scheme. We propose three defense mechanisms
— hiding, replacement and generalization — and evaluate their effectiveness for mitigating the social link privacy risks stemming from
mobility data sharing. Our experimental results show that both
hiding and replacement mechanisms outperform generalization.
Moreover, hiding and replacement achieve a comparable trade-off
between utility and privacy, the former preserving better utility
and the latter providing better privacy.


",17.58133193835471,17.183102803738326,216
CCS_17_121.txt,16.704223774357654,14.776055867977721,CCS,8813,"
 to the service provider. The privacy evaluation of these proposals

In the last years we have witnessed the appearance of a variety
of strategies to design optimal location privacy-preserving mechanisms, in terms of maximizing the adversary’s expected error with
respect to the users’ whereabouts. In this work, we take a closer
look at the defenses created by these strategies and show that, even
though they are indeed optimal in terms of adversary’s correctness,
not all of them offer the same protection when looking at other
dimensions of privacy. To avoid “bad” choices, we argue that the
search for optimal mechanisms must be guided by complementary
criteria. We provide two example auxiliary metrics that help in
this regard: the conditional entropy, that captures an informationtheoretic aspect of the problem; and the worst-case quality loss,
that ensures that the output of the mechanism always provides a
minimum utility to the users. We describe a new mechanism that
maximizes the conditional entropy and is optimal in terms of average adversary error, and compare its performance with previously
proposed optimal mechanisms using two real datasets. Our empirical results confirm that no mechanism fares well on every privacy
criteria simultaneously, making apparent the need for considering
multiple privacy dimensions to have a good understanding of the
privacy protection a mechanism provides.

",20.537248953866403,19.181402359108784,221
CCS_17_122.txt,14.56802640787176,12.404629497089477,CCS,8475,"
Mathematical constructs are necessary for computation on the
underlying algebraic structures of cryptosystems. They are often
written in assembly language and optimized manually for efficiency.
We develop a certified technique to verify low-level mathematical
constructs in X25519, the default elliptic curve Diffie-Hellman key
exchange protocol used in OPENSSH. Our technique translates an
algebraic specification of mathematical constructs into an algebraic
problem. The algebraic problem in turn is solved by the computer
algebra system SINGULAR. The proof assistant Cog certifies the
translation and solution to algebraic problems. Specifications about
output ranges and potential program overflows are translated to
SMT problems and verified by SMT solvers. We report our case
studies on verifying arithmetic computation over a large finite field
and the Montgomery Ladderstep, a crucial loop in X25519.
",15.742502247213078,15.030532945736436,130
CCS_17_123.txt,16.709601799973107,15.477822263931081,CCS,11748,"

We present a high-assurance software stack for secure function
evaluation (SFE). Our stack consists of three components: i. a verified compiler (CircGen) that translates C programs into Boolean
circuits; ii. a verified implementation of Yao’s SFE protocol based on
garbled circuits and oblivious transfer; and iii. transparent application integration and communications via FRESCO, an open-source
framework for secure multiparty computation (MPC). CircGen is a
general purpose tool that builds on CompCert, a verified optimizing
compiler for C. It can be used in arbitrary Boolean circuit-based
cryptography deployments. The security of our SFE protocol implementation is formally verified using EasyCrypt, a tool-assisted
framework for building high-confidence cryptographic proofs, and
it leverages a new formalization of garbled circuits based on the
framework of Bellare, Hoang, and Rogaway (CCS 2012). We conduct
a practical evaluation of our approach, and conclude that it is competitive with state-of-the-art (unverified) approaches. Our work
provides concrete evidence of the feasibility of building efficient,
verified, implementations of higher-level cryptographic systems.
All our development is publicly available.

",17.693802365651003,16.705416666666668,179
CCS_17_124.txt,15.131025362782136,13.48890700827431,CCS,10695,"

We have formalized the functional specification of HMAC-DRBG
(NIST 800-90A), and we have proved its cryptographic security—
that its output is pseudorandom—using a hybrid game-based proof.
We have also proved that the mbedTLS implementation (C program)
correctly implements this functional specification. That proof composes with an existing C compiler correctness proof to guarantee,
end-to-end, that the machine language program gives strong pseudorandomness. All proofs (hybrid games, C program verification,
compiler, and their composition) are machine-checked in the Coq
proof assistant. Our proofs are modular: the hybrid game proof
holds on any implementation of HMAC-DRBG that satisfies our
functional specification. Therefore, our functional specification can
serve as a high-assurance reference.

",15.903189008614273,13.580000000000002,119
CCS_17_125.txt,15.397135866805556,14.101375833432048,CCS,11063,"

Recent studies have shown that Tor onion (hidden) service websites
are particularly vulnerable to website fingerprinting attacks due to
their limited number and sensitive nature. In this work we present
a multi-level feature analysis of onion site fingerprintability, considering three state-of-the-art website fingerprinting methods and
482 Tor onion services, making this the largest analysis of this kind
completed on onion services to date.

Prior studies typically report average performance results for
a given website fingerprinting method or countermeasure. We investigate which sites are more or less vulnerable to fingerprinting
and which features make them so. We find that there is a high
variability in the rate at which sites are classified (and misclassified)
by these attacks, implying that average performance figures may
not be informative of the risks that website fingerprinting attacks
pose to particular sites.

We analyze the features exploited by the different website fingerprinting methods and discuss what makes onion service sites more
or less easily identifiable, both in terms of their traffic traces as well
as their webpage design. We study misclassifications to understand
how onion services sites can be redesigned to be less vulnerable to
website fingerprinting attacks. Our results also inform the design
of website fingerprinting countermeasures and their evaluation
considering disparate impact across sites.

",17.122413403193683,17.00885514018692,215
CCS_17_126.txt,15.698219973984465,15.576668569576071,CCS,7289,"

Decoy routing is an emerging approach for censorship circumvention in which circumvention is implemented with help from a
number of volunteer Internet autonomous systems, called decoy
ASes. Recent studies on decoy routing consider all decoy routing
systems to be susceptible to a fundamental attack —regardless of
their specific designs—in which the censors re-route traffic around
decoy ASes, thereby preventing censored users from using such
systems. In this paper, we propose a new architecture for decoy
routing that, by design, is significantly stronger to rerouting attacks
compared to all previous designs. Unlike previous designs, our new
architecture operates decoy routers only on the downstream traffic
of the censored users; therefore we call it downstream-only decoy
routing. As we demonstrate through Internet-scale BGP simulations, downstream-only decoy routing offers significantly stronger
resistance to rerouting attacks, which is intuitively because a (censoring) ISP has much less control on the downstream BGP routes
of its traffic.

Designing a downstream-only decoy routing system is a challenging engineering problem since decoy routers do not intercept the
upstream traffic of censored users. We design the first downstreamonly decoy routing system, called Waterfall, by devising unique
covert communication mechanisms. We also use various techniques
to make our Waterfall implementation resistant to traffic analysis
attacks.

We believe that downstream-only decoy routing is a significant
step towards making decoy routing systems practical. This is because a downstream-only decoy routing system can be deployed
using a significantly smaller number of volunteer ASes, given a
target resistance to rerouting attacks. For instance, we show that a
Waterfall implementation with only a single decoy AS is as resistant
to routing attacks (against China) as a traditional decoy system
(e.g., Telex) with 53 decoy ASes.

",15.668782140382113,16.909797979797982,290
CCS_17_127.txt,16.879581411996533,15.77141405793304,CCS,8459,"

Traffic analysis is the practice of inferring sensitive information
from communication patterns, particularly packet timings and
packet sizes. Traffic analysis is increasingly becoming relevant
to security and privacy with the growing use of encryption and
other evasion techniques that render content-based analysis of
network traffic impossible. The literature has investigated traffic
analysis for various application scenarios, from tracking stepping
stone cybercriminals to compromising anonymity systems.

The major challenge to existing traffic analysis mechanisms is
scaling to today’s exploding volumes of network traffic, i.e., they
impose high storage, communications, and computation overheads.
In this paper, we aim at addressing this scalability issue by introducing a new direction for traffic analysis, which we call compressive
traffic analysis. The core idea of compressive traffic analysis is to
compress traffic features, and perform traffic analysis operations on
such compressed features instead of on raw traffic features (therefore, improving the storage, communications, and computation
overheads of traffic analysis due to using smaller numbers of features). To compress traffic features, compressive traffic analysis
leverages linear projection algorithms from compressed sensing,
an active area within signal processing. We show that these algorithms offer unique properties that enable compressing network
traffic features while preserving the performance of traffic analysis
compared to traditional mechanisms.

We introduce the idea of compressive traffic analysis as a new
generic framework for scalable traffic analysis. We then apply compressive traffic analysis to two widely studied classes of traffic
analysis, namely, flow correlation and website fingerprinting. We
show that the compressive versions of state-of-the-art flow correlation and website fingerprinting schemes—significantly—outperform
their non-compressive (traditional) alternatives, e.g., the compressive version of Houmansadr et al. [44]’s flow correlation is two
orders of magnitude faster, and the compressive version of Wang et
al. [77] fingerprinting system runs about 13 times faster. We believe
that our study is a major step towards scaling traffic analysis.

",19.032712561301913,19.605445859872614,318
CCS_17_128.txt,14.051718531453783,12.290696852306898,CCS,9970,"

Systems for verifiable outsourcing incur costs for a prover, a verifier,
and precomputation; outsourcing makes sense when the combination of these costs is cheaper than not outsourcing. Yet, when
prior works impose quantitative thresholds to analyze whether outsourcing is justified, they generally ignore prover costs. Verifiable
ASICs (VA)—in which the prover is a custom chip—is the other way
around: its cost calculations ignore precomputation.

This paper describes a new VA system, called Giraffe; charges
Giraffe for all three costs; and identifies regimes where outsourcing
is worthwhile. Giraffe’s base is an interactive proof geared to dataparallel computation. Giraffe makes this protocol asymptotically
optimal for the prover and improves the verifier’s main bottleneck
by almost 3x, both of which are of independent interest. Giraffe
also develops a design template that produces hardware designs
automatically for a wide range of parameters, introduces hardware
primitives molded to the protocol’s data flows, and incorporates
program analyses that expand applicability. Giraffe wins even when
outsourcing several tens of sub-computations, scales to 500x larger
computations than prior work, and can profitably outsource parts
of programs that are not worthwhile to outsource in full.

",16.373557378465907,15.85020502645503,193
CCS_17_129.txt,14.255636829865225,12.196080613923375,CCS,13453,"

We design and implement a simple zero-knowledge argument protocol for NP whose communication complexity is proportional to
the square-root of the verification circuit size. The protocol can
be based on any collision-resistant hash function. Alternatively, it
can be made non-interactive in the random oracle model, yielding
concretely efficient zk-SNARKs that do not require a trusted setup
or public-key cryptography.

Our protocol is attractive not only for very large verification
circuits but also for moderately large circuits that arise in applications. For instance, for verifying a SHA-256 preimage in zeroknowledge with 2~* soundness error, the communication complexity is roughly 44KB (or less than 34KB under a plausible conjecture),
the prover running time is 140 ms, and the verifier running time is
62 ms. This proof is roughly 4 times shorter than a similar proof
of ZKB++ (Chase et al., CCS 2017), an optimized variant of ZKBoo
(Giacomelli et al., USENIX 2016).

The communication complexity of our protocol is independent of
the circuit structure and depends only on the number of gates. For
24° soundness error, the communication becomes smaller than the
circuit size for circuits containing roughly 3 million gates or more.
Our efficiency advantages become even bigger in an amortized
setting, where several instances need to be proven simultaneously.

Our zero-knowledge protocol is obtained by applying an optimized version of the general transformation of Ishai et al. (STOC
2007) to a variant of the protocol for secure multiparty computation of Damgard and Ishai (Crypto 2006). It can be viewed as
a simple zero-knowledge interactive PCP based on “interleaved”
Reed-Solomon codes.

",15.903189008614273,14.632518248175185,271
CCS_17_130.txt,14.885834840832597,13.338074632366766,CCS,15621,"

We continue the study of Homomorphic Secret Sharing (HSS), recently introduced by Boyle et al. (Crypto 2016, Eurocrypt 2017).
A (2-party) HSS scheme splits an input x into shares (x°, x!) such
that (1) each share computationally hides x, and (2) there exists
an efficient homomorphic evaluation algorithm Eval such that for
any function (or “program”) P from a given class it holds that
Eval(x°, P) +Eval(x!, P) = P(x). Boyle et al. show how to construct
an HSS scheme for branching programs, with an inverse polynomial
error, using discrete-log type assumptions such as DDH.

We make two types of contributions.

OPTIMIZATIONS. We introduce new optimizations that speed up the
previous optimized implementation of Boyle et al. by more than a
factor of 30, significantly reduce the share size, and reduce the rate
of leakage induced by selective failure.

APPLICATIONS. Our optimizations are motivated by the observation
that there are natural application scenarios in which HSS is useful
even when applied to simple computations on short inputs. We
demonstrate the practical feasibility of our HSS implementation in
the context of such applications.

",13.172668634160413,11.454784688995218,187
CCS_17_131.txt,15.38836006692635,14.248448864251817,CCS,10651,"

Device drivers are an essential part in modern Unix-like systems to
handle operations on physical devices, from hard disks and printers to digital cameras and Bluetooth speakers. The surge of new
hardware, particularly on mobile devices, introduces an explosive
growth of device drivers in system kernels. Many such drivers are
provided by third-party developers, which are susceptible to security vulnerabilities and lack proper vetting. Unfortunately, the
complex input data structures for device drivers render traditional
analysis tools, such as fuzz testing, less effective, and so far, research
on kernel driver security is comparatively sparse.

In this paper, we present DIFUZE, an interface-aware fuzzing
tool to automatically generate valid inputs and trigger the execution of the kernel drivers. We leverage static analysis to compose
correctly-structured input in the userspace to explore kernel drivers. DIFUZE is fully automatic, ranging from identifying driver
handlers, to mapping to device file names, to constructing complex
argument instances. We evaluate our approach on seven modern
Android smartphones. The results show that DIFUZE can effectively
identify kernel driver bugs, and reports 32 previously unknown vulnerabilities, including flaws that lead to arbitrary code execution.

",16.04434344847333,16.243333333333336,191
CCS_17_132.txt,14.673726220247122,13.461186403975734,CCS,6847,"

Patches and related information about software vulnerabilities are
often made available to the public, aiming to facilitate timely fixes.
Unfortunately, the slow paces of system updates (30 days on average) often present to the attackers enough time to recover hidden
bugs for attacking the unpatched systems. Making things worse is
the potential to automatically generate exploits on input-validation
flaws through reverse-engineering patches, even though such vulnerabilities are relatively rare (e.g., 5% among all Linux kernel
vulnerabilities in last few years). Less understood, however, are the
implications of other bug-related information (e.g., bug descriptions
in CVE), particularly whether utilization of such information can
facilitate exploit generation, even on other vulnerability types that
have never been automatically attacked.

In this paper, we seek to use such information to generate proofof-concept (PoC) exploits for the vulnerability types never automatically attacked. Unlike an input validation flaw that is often patched
by adding missing sanitization checks, fixing other vulnerability
types is more complicated, usually involving replacement of the
whole chunk of code. Without understanding of the code changed,
automatic exploit becomes less likely. To address this challenge,
we present SemFuzz, a novel technique leveraging vulnerabilityrelated text (e.g., CVE reports and Linux git logs) to guide automatic
generation of PoC exploits. Such an end-to-end approach is made
possible by natural-language processing (NLP) based information
extraction and a semantics-based fuzzing process guided by such
information, Running over 112 Linux kernel flaws reported in the
past five years, SemFuzz successfully triggered 18 of them, and further discovered one zero-day and one undisclosed vulnerabilities.
These flaws include use-after-free, memory corruption, information leak, etc., indicating that more complicated flaws can also be
automatically attacked. This finding calls into question the way
vulnerability-related information is shared today.

",17.64278748958908,18.41706528370958,302
CCS_17_133.txt,17.440661773868,16.65034317327225,CCS,5393,"

Algorithmic complexity vulnerabilities occur when the worst-case
time/space complexity of an application is significantly higher than
the respective average case for particular user-controlled inputs.
When such conditions are met, an attacker can launch Denial-ofService attacks against a vulnerable application by providing inputs
that trigger the worst-case behavior. Such attacks have been known
to have serious effects on production systems, take down entire
websites, or lead to bypasses of Web Application Firewalls.

Unfortunately, existing detection mechanisms for algorithmic
complexity vulnerabilities are domain-specific and often require
significant manual effort. In this paper, we design, implement, and
evaluate SLowFuzz, a domain-independent framework for automatically finding algorithmic complexity vulnerabilities. SLowFuzz
automatically finds inputs that trigger worst-case algorithmic behavior in the tested binary. SLowFuzz uses resource-usage-guided
evolutionary search techniques to automatically find inputs that
maximize computational resource utilization for a given application.

We demonstrate that SLowFuzz successfully generates inputs
that match the theoretical worst-case performance for several wellknown algorithms. SLowFuzz was also able to generate a large
number of inputs that trigger different algorithmic complexity vulnerabilities in real-world applications, including various zip parsers
used in antivirus software, regular expression libraries used in Web
Application Firewalls, as well as hash table implementations used
in Web applications. In particular, SLowFuzz generated inputs that
achieve 300-times slowdown in the decompression routine of the
bzip2 utility, discovered regular expressions that exhibit matching
times exponential in the input size, and also managed to automatically produce inputs that trigger a high number of collisions in
PHP’s default hashtable implementation.

",19.97928068152386,20.359114942528738,263
CCS_17_134.txt,15.477542241320727,14.194543754525743,CCS,10775,"

With millions of apps available to users, the mobile app market is
rapidly becoming very crowded. Given the intense competition, the
time to market is a critical factor for the success and profitability
of an app. In order to shorten the development cycle, developers
often focus their efforts on the unique features and workflows of
their apps and rely on third-party Open Source Software (OSS) for
the common features. Unfortunately, despite their benefits, careless use of OSS can introduce significant legal and security risks,
which if ignored can not only jeopardize security and privacy of
end users, but can also cause app developers high financial loss.
However, tracking OSS components, their versions, and interdependencies can be very tedious and error-prone, particularly if an
OSS is imported with little to no knowledge of its provenance.

We therefore propose OSSPoLIcE, a scalable and fully-automated
tool for mobile app developers to quickly analyze their apps and
identify free software license violations as well as usage of known
vulnerable versions of OSS. OSSPoLIcE introduces a novel hierarchical indexing scheme to achieve both high scalability and accuracy,
and is capable of efficiently comparing similarities of app binaries
against a database of hundreds of thousands of OSS sources (billions
of lines of code). We populated OSSPoLicE with 60K C/C++ and
77K Java OSS sources and analyzed 1.6M free Google Play Store
apps. Our results show that 1) over 40K apps potentially violate
GPL/AGPL licensing terms, and 2) over 100K of apps use known
vulnerable versions of OSS. Further analysis shows that developers
violate GPL/AGPL licensing terms due to lack of alternatives, and
use vulnerable versions of OSS despite efforts from companies like
Google to improve app security. OSSPoLIcE is available on GitHub.

KEYWORDS

Application Security; License Violation; Code Clone Detection

",17.122413403193683,16.72455749548465,302
CCS_17_135.txt,16.34741371454545,14.672846681106833,CCS,7516,"

Third-party libraries in Android apps have repeatedly been shown
to be hazards to the users’ privacy and an amplification of their
host apps’ attack surface. A particularly aggravating factor to this
situation is that the libraries’ version included in apps are very
often outdated.

This paper makes the first contribution towards solving the
problem of library outdatedness on Android. First, we conduct a
survey with 203 app developers from Google Play to retrieve firsthand information about their usage of libraries and requirements
for more effective library updates. With a subsequent study of library providers’ semantic versioning practices, we uncover that
those providers are likely a contributing factor to the app developers’ abstinence from library updates in order to avoid ostensible
re-integration efforts and version incompatibilities. Further, we
conduct a large-scale library updatability analysis of 1,264,118 apps
to show that, based on the library API usage, 85.6% of the libraries
could be upgraded by at least one version without modifying the
app code, 48.2% even to the latest version. Particularly alarming are
our findings that 97.8% out of 16,837 actively used library versions
with a known security vulnerability could be easily fixed through a
drop-in replacement of the vulnerable library with the fixed version.
Based on these results, we conclude with a thorough discussion
of solutions and actionable items for different actors in the app
ecosystem to effectively remedy this situation.

",18.7741,17.85425213675214,241
CCS_17_136.txt,16.615936245456705,16.072190131861554,CCS,4502,"

Given how the “patching treadmill” plays a central role for enabling
sites to counter emergent security concerns, it behooves the security community to understand the patch development process and
characteristics of the resulting fixes. Iumination of the nature of security patch development can inform us of shortcomings in existing
remediation processes and provide insights for improving current
practices. In this work we conduct a large-scale empirical study of
security patches, investigating more than 4,000 bug fixes for over
3,000 vulnerabilities that affected a diverse set of 682 open-source
software projects. For our analysis we draw upon the National
Vulnerability Database, information scraped from relevant external
references, affected software repositories, and their associated security fixes. Leveraging this diverse set of information, we conduct
an analysis of various aspects of the patch development life cycle,
including investigation into the duration of impact a vulnerability
has on a code base, the timeliness of patch development, and the
degree to which developers produce safe and reliable fixes. We then
characterize the nature of security fixes in comparison to other
non-security bug fixes, exploring the complexity of different types
of patches and their impact on code bases.

Among our findings we identify that: security patches have a
lower footprint in code bases than non-security bug patches; a third
of all security issues were introduced more than 3 years prior to
remediation; attackers who monitor open-source repositories can
often get a jump of weeks to months on targeting not-yet-patched
systems prior to any public disclosure and patch distribution; nearly
5% of security fixes negatively impacted the associated software;
and 7% failed to completely remedy the security hole they targeted.

",21.828451593036586,23.272857142857145,283
CCS_17_137.txt,14.538579237484427,13.43648405703745,CCS,8025,"

Mobile devices today have been increasingly used to store and
process sensitive information. To protect sensitive data, mobile
operating systems usually incorporate a certain level of encryption
to protect sensitive data. However, conventional encryption cannot
defend against a coercive attacker who can capture the device
owner, and force the owner to disclose keys used for decrypting
sensitive information. To defend against such a coercive adversary,
Plausibly Deniable Encryption (PDE) was introduced to allow the
device owner to deny the very existence of sensitive data stored
on his/her device. The existing PDE systems, built on flash storage
devices, are problematic, since they either neglect the special nature
of the underlying storage medium (which is usually NAND flash),
or suffer from deniability compromises.

In this paper, we propose DEFTL, a Deniability Enabling Flash
Translation Layer for devices which use flash-based block devices
as storage media. DEFTL is the first PDE design which incorporates
deniability to Flash Translation Layer (FTL), a pervasively deployed
“translation layer” which stays between NAND flash and the file
system in literally all the computing devices. A salient advantage
of DEFTL lies in its capability of achieving deniability while being
able to accommodate the special nature of NAND flash as well as
eliminate deniability compromises from it. We implement DEFTL
using an open-source NAND flash controller. The experimental
results show that, compared to conventional encryption which
does not provide deniability, our DEFTL design only incurs a small
overhead.

",17.58133193835471,16.862876033057855,243
CCS_17_138.txt,15.304515389462328,13.690970959026902,CCS,7473,"

Encryption ransomware is a malicious software that stealthily encrypts user files and demands a ransom to provide access to these
files. Several prior studies have developed systems to detect ransomware by monitoring the activities that typically occur during a
ransomware attack. Unfortunately, by the time the ransomware is
detected, some files already undergo encryption and the user is still
required to pay a ransom to access those files. Furthermore, ransomware variants can obtain kernel privilege, which allows them
to terminate software-based defense systems, such as anti-virus.
While periodic backups have been explored as a means to mitigate
ransomware, such backups incur storage overheads and are still
vulnerable as ransomware can obtain kernel privilege to stop or destroy backups. Ideally, we would like to defend against ransomware
without relying on software-based solutions and without incurring
the storage overheads of backups.

To that end, this paper proposes FlashGuard, a ransomwaretolerant Solid State Drive (SSD) which has a firmware-level recovery system that allows quick and effective recovery from encryption ransomware without relying on explicit backups. FlashGuard
leverages the observation that the existing SSD already performs
out-of-place writes in order to mitigate the long erase latency of
flash memories. Therefore, when a page is updated or deleted, the
older copy of that page is anyway present in the SSD. FlashGuard
slightly modifies the garbage collection mechanism of the SSD to
retain the copies of the data encrypted by ransomware and ensure
effective data recovery. Our experiments with 1,447 manually labeled ransomware samples show that FlashGuard can efficiently
restore files encrypted by ransomware. In addition, we demonstrate
that FlashGuard has a negligible impact on the performance and
lifetime of the SSD.

",17.219254097808864,15.752304964539011,284
CCS_17_139.txt,16.273934834768117,14.362943109062304,CCS,12110,"

The USB protocol has become ubiquitous, supporting devices from
high-powered computing devices to small embedded devices and
control systems. USB’s greatest feature, its openness and expandability, is also its weakness, and attacks such as BadUSB exploit the
unconstrained functionality afforded to these devices as a vector
for compromise. Fundamentally, it is virtually impossible to know
whether a USB device is benign or malicious. This work introduces
FiRMUSB, a USB-specific firmware analysis framework that uses
domain knowledge of the USB protocol to examine firmware images and determine the activity that they can produce. Embedded
USB devices use microcontrollers that have not been well studied
by the binary analysis community, and our work demonstrates how
lifters into popular intermediate representations for analysis can
be built, as well as the challenges of doing so. We develop targeting
algorithms and use domain knowledge to speed up these processes
by a factor of 7 compared to unconstrained fully symbolic execution. We also successfully find malicious activity in embedded
8051 firmwares without the use of source code. Finally, we provide insights into the challenges of symbolic analysis on embedded
architectures and provide guidance on improving tools to better
handle this important class of devices.

",19.032712561301913,17.288,202
CCS_17_140.txt,12.84336770197558,10.774167933539768,CCS,11599,"

We introduce a new approach to actively secure two-party computation based on so-called oblivious linear function evaluation (OLE),
a natural generalisation of oblivious transfer (OT) and a special
case of the notion of oblivious polynomial evaluation introduced by
Naor and Pinkas at STOC 1999. OLE works over a finite field F. In an
OLE the sender inputs two field elements a € F and b € F, and the
receiver inputs a field element x € F and learns only f(x) = ax +b.
Our protocol can evaluate an arithmetic circuit over a finite field
F given black-box access to OLE for F. The protocol is unconditionally secure and consumes only a constant number of OLEs per
multiplication gate. An OLE over a field F of size O(2*) can be
implemented with communication O(x). This gives a protocol with
communication complexity O(|C|x) for large enough fields, where
C is an arithmetic circuit computing the desired function.

This asymptotically matches the best previous protocols, but our
protocol at the same time obtains significantly smaller constants
hidden by the big-O notation, yielding a highly practical protocol.
Conceptually our techniques lift the techniques for basing practical
actively secure 2PC of Boolean circuits on OT introduced under the
name TinyOT by Nielsen, Nordholt, Orlandi and Burra at Crypto
2012 to the arithmetic setting. In doing so we develop several novel
techniques for generating various flavours of OLE and combining
these.

We believe that the efficiency of our protocols, both in asymptotic
and practical terms, establishes OLE and its variants as an important
foundation for efficient actively secure 2PC.

",17.879347455551382,17.40478260869566,270
CCS_17_141.txt,14.802544089921781,13.18526237769742,CCS,6116,"

This paper introduces a cryptographic protocol for efficiently aggregating a count of unique items across a set of data parties privately
— that is, without exposing any information other than the count.
Our protocol allows for more secure and useful statistics gathering
in privacy-preserving distributed systems such as anonymity networks; for example, it allows operators of anonymity networks such
as Tor to securely answer the questions: how many unique users are
using the distributed service? and how many hidden services are being
accessed? We formally prove the correctness and security of our protocol in the Universal Composability framework against an active
adversary that compromises all but one of the aggregation parties.
We also show that the protocol provides security against adaptive
corruption of the data parties, which prevents them from being
victims of targeted compromise. To ensure safe measurements, we
also show how the output can satisfy differential privacy.

We present a proof-of-concept implementation of the private
set-union cardinality protocol (PSC) and use it to demonstrate that
PSC operates with low computational overhead and reasonable
bandwidth. In particular, for reasonable deployment sizes, the protocol run at timescales smaller than the typical measurement period
would be and thus is suitable for distributed measurement.

",18.643177196211187,17.831067073170733,206
CCS_17_142.txt,12.547309697712283,10.060749526416085,CCS,12478,"

We provide efficient constructions for trace-and-revoke systems with public traceability in the black-box confirmation
model. Our constructions achieve adaptive security, are based
on standard assumptions and achieve significant efficiency
gains compared to previous constructions.

Our constructions rely on a generic transformation from
inner product functional encryption (IPFE) schemes to traceand-revoke systems. Our transformation requires the underlying IPFE scheme to only satisfy a very weak notion of security
— the attacker may only request a bounded number of random
keys — in contrast to the standard notion of security where
she may request an unbounded number of arbitrarily chosen
keys. We exploit the much weaker security model to provide a new construction for bounded collusion and random
key IPFE from the learning with errors assumption (LWE),
which enjoys improved efficiency compared to the scheme of
Agrawal et al. [CRYPTO’16].

Together with IPFE schemes from Agrawal et al., we obtain
trace and revoke from LWE, Decision Diffie Hellman and
Decision Composite Residuosity.

",16.439396014739867,15.788361086765995,164
CCS_17_143.txt,15.115912754022649,13.745548335098459,CCS,10197,"

Fuzzing is a software testing technique that finds bugs by repeatedly
injecting mutated inputs to a target program. Known to be a highly
practical approach, fuzzing is gaining more popularity than ever
before. Current research on fuzzing has focused on producing an
input that is more likely to trigger a vulnerability.

In this paper, we tackle another way to improve the performance
of fuzzing, which is to shorten the execution time of each iteration. We observe that AFL, a state-of-the-art fuzzer, slows down by
24x because of file system contention and the scalability of forkO
system call when it runs on 120 cores in parallel. Other fuzzers
are expected to suffer from the same scalability bottlenecks in that
they follow a similar design pattern. To improve the fuzzing performance, we design and implement three new operating primitives
specialized for fuzzing that solve these performance bottlenecks
and achieve scalable performance on multi-core machines. Our
experiment shows that the proposed primitives speed up AFL and
LibFuzzer by 6.1 to 28.9x and 1.1 to 735.7x, respectively, on the
overall number of executions per second when targeting Google’s
fuzzer test suite with 120 cores. In addition, the primitives improve
AFL’s throughput up to 7.7x with 30 cores, which is a more common
setting in data centers. Our fuzzer-agnostic primitives can be easily
applied to any fuzzer with fundamental performance improvement
and directly benefit large-scale fuzzing and cloud-based fuzzing
services.

",15.514038796780547,14.327338842975205,250
CCS_17_144.txt,14.699256369655409,13.117955757589733,CCS,11742,"

Existing Greybox Fuzzers (GF) cannot be effectively directed, for
instance, towards problematic changes or patches, towards critical
system calls or dangerous locations, or towards functions in the
stacktrace of a reported vulnerability that we wish to reproduce.
In this paper, we introduce Directed Greybox Fuzzing (DGF)
which generates inputs with the objective of reaching a given set
of target program locations efficiently. We develop and evaluate
a simulated annealing-based power schedule that gradually assigns more energy to seeds that are closer to the target locations
while reducing energy for seeds that are further away. Experiments
with our implementation AFLGo demonstrate that DGF outperforms both directed symbolic-execution-based whitebox fuzzing
and undirected greybox fuzzing. We show applications of DGF to
patch testing and crash reproduction, and discuss the integration of
AFLGo into Google’s continuous fuzzing platform OSS-Fuzz. Due
to its directedness, AFLGo could find 39 bugs in several well-fuzzed,
security-critical projects like LibXML2. 17 CVEs were assigned.

",17.28802050969988,16.0272049689441,163
CCS_17_145.txt,13.615776140434534,11.995039354584812,CCS,8428,"

Kernel vulnerabilities are critical in security because they naturally
allow attackers to gain unprivileged root access. Although there has
been much research on finding kernel vulnerabilities from source
code, there are relatively few research on kernel fuzzing, which is
a practical bug finding technique that does not require any source
code. Existing kernel fuzzing techniques involve feeding in random
input values to kernel API functions. However, such a simple approach does not reveal latent bugs deep in the kernel code, because
many API functions are dependent on each other, and they can
quickly reject arbitrary parameter values based on their calling
context. In this paper, we propose a novel fuzzing technique for
commodity OS kernels that leverages inferred dependence model
between API function calls to discover deep kernel bugs. We implement our technique on a fuzzing system, called IMF. IMF has
already found 32 previously unknown kernel vulnerabilities on the
latest macOS version 10.12.3 (16D32) at the time of this writing.

",15.021129683784007,16.202950310559007,164
CCS_17_146.txt,16.163147103737156,14.79062928838054,CCS,7103,"

Partitioning a security-sensitive application into least-privileged
components and putting each into a separate protection domain
have long been a goal of security practitioners and researchers.
However, a stumbling block to automatically partitioning C/C++
applications is the presence of pointers in these applications. Pointers make calculating data dependence, a key step in program partitioning, difficult and hard to scale; furthermore, C/C++ pointers
do not carry bounds information, making it impossible to automatically marshall and unmarshall pointer data when they are sent
across the boundary of partitions. In this paper, we propose a set of
techniques for supporting general pointers in automatic program
partitioning. Our system, called PtrSplit, constructs a Program Dependence Graph (PDG) for tracking data and control dependencies
in the input program and employs a parameter-tree approach for
representing data of pointer types; this approach is modular and
avoids global pointer analysis. Furthermore, it performs selective
pointer bounds tracking to enable automatic marshalling/unmarshalling of pointer data, even when there is circularity and arbitrary
aliasing. As a result, PtrSplit can automatically generate executable
partitions for C applications that contain arbitrary pointers.

",18.848423458724294,18.347293233082713,187
CCS_17_147.txt,15.955512809499833,14.266661549584526,CCS,5935,"

Type confusion, often combined with use-after-free, is the main
attack vector to compromise modern C++ software like browsers
or virtual machines.

Typecasting is a core principle that enables modularity in C++.
For performance, most typecasts are only checked statically, i.e.,
the check only tests if a cast is allowed for the given type hierarchy,
ignoring the actual runtime type of the object. Using an object of
an incompatible base type instead of a derived type results in type
confusion. Attackers abuse such type confusion issues to attack
popular software products including Adobe Flash, PHP, Google
Chrome, or Firefox.

We propose to make all type checks explicit, replacing static
checks with full runtime type checks. To minimize the performance
impact of our mechanism HexType, we develop both low-overhead
data structures and compiler optimizations. To maximize detection
coverage, we handle specific object allocation patterns, e.g., placement new or reinterpret_cast which are not handled by other
mechanisms.

Our prototype results show that, compared to prior work, HexType has at least 1.1 — 6.1 times higher coverage on Firefox benchmarks. For SPEC CPU2006 benchmarks with overhead, we show a 2
— 33.4 times reduction in overhead. In addition, HexType discovered
4 new type confusion bugs in Qt and Apache Xerces-C++.

",15.549919911452193,12.729983007646563,214
CCS_17_148.txt,14.450438210422526,12.863125065353355,CCS,10498,"

In spite of years of improvements to software security, heap-related
attacks still remain a severe threat. One reason is that many existing
memory allocators fall short in a variety of aspects. For instance,
performance-oriented allocators are designed with very limited
countermeasures against attacks, but secure allocators generally
suffer from significant performance overhead, e.g., running up to
10x slower. This paper, therefore, introduces FreeGuard, a secure
memory allocator that prevents or reduces a wide range of heaprelated attacks, such as heap overflows, heap over-reads, use-afterfrees, as well as double and invalid frees. FreeGuard has similar
performance to the default Linux allocator, with less than 2% overhead on average, but provides significant improvement to security
guarantees. FreeGuard also addresses multiple implementation issues of existing secure allocators, such as the issue of scalability.
Experimental results demonstrate that FreeGuard is very effective
in defending against a variety of heap-related attacks.

",17.77360955136429,16.57914853358562,153
CCS_17_149.txt,15.486477302889565,13.71243699907772,CCS,5714,"

Memory-corruption vulnerabilities pose a serious threat to modern computer security. Attackers exploit these vulnerabilities to
manipulate code and data of vulnerable applications to generate
malicious behavior by means of code-injection and code-reuse attacks. Researchers already demonstrated the power of data-only
attacks by disclosing secret data such as cryptographic keys in the
past. A large body of literature has investigated defenses against
code-injection, code-reuse, and data-only attacks. Unfortunately,
most of these defenses are tailored towards statically generated
code and their adaption to dynamic code comes with the price of
security or performance penalties. However, many common applications, like browsers and document viewers, embed just-in-time
compilers to generate dynamic code.

The contribution of this paper is twofold: first, we propose a
generic data-only attack against JIT compilers, dubbed DOJITA.
In contrast to previous data-only attacks that aimed at disclosing secret data, DOJITA enables arbitrary code-execution. Second,
we propose JITGuard, a novel defense to mitigate code-injection,
code-reuse, and data-only attacks against just-in-time compilers
(including DOJITA). JITGuard utilizes Intel’s Software Guard Extensions (SGX) to provide a secure environment for emitting the
dynamic code to a secret region, which is only known to the JIT
compiler, and hence, inaccessible to the attacker. Our proposal is
the first solution leveraging SGX to protect the security critical JIT
compiler operations, and tackles a number of difficult challenges.
As proof of concept we implemented JITGuard for Firefox’s JIT
compiler SpiderMonkey. Our evaluation shows reasonable overhead
of 9.8% for common benchmarks.

",17.122413403193683,15.59776061776062,263
CCS_17_150.txt,14.537941349236075,13.322082406539035,CCS,12020,"

Side-channel risks of Intel SGX have recently attracted great attention. Under the spotlight is the newly discovered page-fault attack,
in which an OS-level adversary induces page faults to observe the
page-level access patterns of a protected process running in an SGX
enclave. With almost all proposed defense focusing on this attack,
little is known about whether such efforts indeed raise the bar for
the adversary, whether a simple variation of the attack renders
all protection ineffective, not to mention an in-depth understanding of other attack surfaces in the SGX system. In the paper, we
report the first step toward systematic analyses of side-channel
threats that SGX faces, focusing on the risks associated with its
memory management. Our research identifies 8 potential attack
vectors, ranging from TLB to DRAM modules. More importantly, we
highlight the common misunderstandings about SGX memory side
channels, demonstrating that high frequent AEXs can be avoided
when recovering EdDSA secret key through a new page channel
and fine-grained monitoring of enclave programs (at the level of
64B) can be done through combining both cache and cross-enclave
DRAM channels. Our findings reveal the gap between the ongoing
security research on SGX and its side-channel weaknesses, redefine the side-channel threat model for secure enclaves, and can
provoke a discussion on when to use such a system and how to use
it securely.

",16.084390811093357,17.09185775597793,234
CCS_17_151.txt,15.494135306543829,14.252662605044716,CCS,11605,"

Recent proposals for trusted hardware platforms, such as Intel SGX
and the MIT Sanctum processor, offer compelling security features
but lack formal guarantees. We introduce a verification methodology based on a trusted abstract platform (TAP), a formalization
of idealized enclave platforms along with a parameterized adversary. We also formalize the notion of secure remote execution and
present machine-checked proofs showing that the TAP satisfies the
three key security properties that entail secure remote execution:
integrity, confidentiality and secure measurement. We then present
machine-checked proofs showing that SGX and Sanctum are refinements of the TAP under certain parameterizations of the adversary,
demonstrating that these systems implement secure enclaves for
the stated adversary models.

",18.243605946275583,19.74605263157895,115
CIDR_17_001.txt,14.618307451709668,13.470037964303522,CIDR,8757,"
In this paper, we argue that in many “Big Data” applica-
tions, getting data into the system correctly and at scale via
traditional ETL (Extract, Transform, and Load) processes is
a fundamental roadblock to being able to perform timely an-
alytics or make real-time decisions. The best way to address
this problem is to build a new architecture for ETL which
takes advantage of the push-based nature of a stream pro-
cessing system. We discuss the requirements for a streaming
ETL engine and describe a generic architecture which sat-
isfies those requirements. We also describe our implemen-
tation of streaming ETL using a scalable messaging system
(Apache Kafka), a transactional stream processing system
(S-Store), and a distributed polystore (Intel’s BigDAWG),
as well as propose a new time-series database optimized to
handle ingestion internally.
",17.410965686947208,18.094507299270074,139
CIDR_17_002.txt,15.052568118957907,13.650766621118603,CIDR,5582,"
The rising popularity of large-scale real-time analytics applications (real-time inventory/pricing, mobile apps that give
you suggestions, fraud detection, risk analysis, etc.) emphasize the need for distributed data management systems that
can handle fast transactions and analytics concurrently. Efficient processing of transactional and analytical requests,
however, require different optimizations and architectural
decisions in a system. This paper presents the Wildfire
system, which targets Hybrid Transactional and Analytical
Processing (HTAP). Wildfire leverages the Spark ecosystem
to enable large-scale data processing with different types of
complex analytical requests, and columnar data processing
to enable fast transactions and analytics concurrently.
",21.41881206717044,20.661460396039605,102
CIDR_17_003.txt,15.81880103939158,14.280266230791142,CIDR,4969,"
Many modern applications are a mixture of streaming, transactional
and analytical workloads. However, traditional data platforms are
each designed for supporting a specific type of workload. The
lack of a single platform to support all these workloads has forced
users to combine disparate products in custom ways. The common
practice of stitching heterogeneous environments has caused enormous production woes by increasing complexity and the total cost
of ownership.

To support this class of applications, we present SnappyData as
the first unified engine capable of delivering analytics, transactions,
and stream processing in a single integrated cluster. We build this
hybrid engine by carefully marrying a big data computational engine (Apache Spark) with a scale-out transactional store (Apache
GemFire). We study and address the challenges involved in building such a hybrid distributed system with two conflicting components designed on drastically different philosophies: one being a
lineage-based computational model designed for high-throughput
analytics, the other a consensus- and replication-based model designed for low-latency operations.

",17.613555460941566,16.689896729776247,167
CIDR_17_004.txt,17.543836403343956,16.803564608206717,CIDR,4558,"
While data volumes continue to rise, the capacity of human attention
remains limited. As a result, users need analytics engines that can
assist in prioritizing attention in this fast data that is too large for
manual inspection. We present a set of design principles for the
design of fast data analytics engines that leverage the relative scarcity
of human attention and overabundance of data: return fewer results,
prioritize iterative analysis, and filter fast to compute less. We report
on our early experiences employing these principles in the design
and deployment of MacroBase, an open source analysis engine for
prioritizing attention in fast data. By combining streaming operators
for feature transformation, classification, and data summarization,
MacroBase provides users with interpretable explanations of key
behaviors, acting as a search engine for fast data.

",18.458006810337128,18.498229007633586,132
CIDR_17_005.txt,14.742500664749322,13.257989423458724,CIDR,6214,"
In many organizations, it is often challenging for users to find relevant data for specific tasks, since the data is usually scattered across
the enterprise and often inconsistent. In fact, data scientists routinely
report that the majority of their effort is spent finding, cleaning, integrating, and accessing data of interest to a task at hand. In order
to decrease the “grunt work” needed to facilitate the analysis of
data “in the wild”, we present DATA CIVILIZER, an end-to-end big
data management system. DATA CIVILIZER has a linkage graph
computation module to build a linkage graph for the data and a data
discovery module which utilizes the linkage graph to help identify
data that is relevant to user tasks. It also uses the linkage graph
to discover possible join paths that can then be used in a query.
For the actual query execution, we use a polystore DBMS, which
federates query processing across disparate systems. In addition,
DATA CIVILIZER integrates data cleaning operations into query pro-
cessing. Because different users need to invoke the above tasks in
different orders, DATA CIVILIZER embeds a workflow engine which
enables the arbitrary composition of different modules, as well as
the handling of data updates. We have deployed our preliminary
DATA CIVILIZER system in two institutions, MIT and Merck and
describe initial positive experiences that show the system shortens
the time and effort required to find, prepare, and analyze data.


",16.728156217252725,16.38801687763713,238
CIDR_17_006.txt,15.641374445519485,14.47782942081935,CIDR,7707,"
Ground is an open-source data context service, a system to manage
all the information that informs the use of data. Data usage has
changed both philosophically and practically in the last decade,
creating an opportunity for new data context services to foster further
innovation. In this paper we frame the challenges of managing data
context with basic ABCs: Applications, Behavior, and Change. We
provide motivation and design guidelines, present our initial design
of a common metamodel and API, and explore the current state of
the storage solutions that could serve the needs of a data context
service. Along the way we highlight opportunities for new research
and engineering solutions.
",14.554592549557764,14.33727272727273,111
CIDR_17_007.txt,16.04614889245342,14.630781542827176,CIDR,6976,"
The rigid schemas of classical relational databases help users
in specifying queries and inform the storage organization of
data. However, the advantages of schemas come at a high
upfront cost through schema and ETL process design. In
this work, we propose a new paradigm where the database
system takes a more active role in schema development and
data integration. We refer to this approach as adaptive
schema databases (ASDs). An ASD ingests semi-structured
or unstructured data directly using a pluggable combination of extraction and data integration techniques. Over
time it discovers and adapts schemas for the ingested data
using information provided by data integration and information extraction techniques, as well as from queries and
user-feedback. In contrast to relational databases, ASDs
maintain multiple schema workspaces that represent individualized views over the data, which are fine-tuned to the
needs of a particular user or group of users. A novel aspect of ASDs is that probabilistic database techniques are
used to encode ambiguity in automatically generated data
extraction workflows and in generated schemas. ASDs can
provide users with context-dependent feedback on the quality of a schema, both in terms of its ability to satisfy a user’s
queries, and the quality of the resulting answers. We outline
our vision for ASDs, and present a proof of concept implementation as part of the Mimir probabilistic data curation
system.

",15.774802946060372,15.349368421052631,230
CIDR_17_008.txt,15.217838202452462,13.924095692140224,CIDR,4813,"
Modern database engines balance the demanding requirements of mixed, hybrid transactional and analytical processing (HTAP) workloads by relying on i) global shared
memory, ii) system-wide cache coherence, and iii) massive
parallelism. Thus, database engines are typically deployed
on multi-socket multi-cores, which have been the only platform to support all three aspects.

Two recent trends, however, indicate that these hardware assumptions will be invalidated in the near future.
First, hardware vendors have started exploring alternate
non-cache-coherent shared-memory multi-core designs due
to escalating complexity in maintaining coherence across

hundreds of cores. Second, as GPGPUs overcome programma
bility, performance, and interfacing limitations, they are
being increasingly adopted by emerging servers to expose
heterogeneous parallelism. It is thus necessary to revisit
database engine design because current engines can neither
deal with the lack of cache coherence nor exploit heterogeneous parallelism.

In this paper, we make the case for Heterogeneous-HTAP
(H? TAP), a new architecture explicitly targeted at emerging
hardware. H?TAP engines store data in shared memory to
maximize data freshness, pair workloads with ideal processor
types to exploit heterogeneity, and use message passing with
explicit processor cache management to circumvent the lack
of cache coherence. Using Caldera, a prototype H*TAP engine, we show that the H*TAP architecture can be realized
in practice and can offer performance competitive with specialized OLTP and OLAP engines.

",16.691746362846608,15.138458498023713,231
CIDR_17_009.txt,14.214724648442406,13.111391634221825,CIDR,5709,"
After four decades of research, today’s database systems still suf-
fer from poor query execution plans. Bad plans are usually caused
by poor cardinality estimates, which have been called the “Achilles
Heel” of modern query optimizers. In this work we propose index-
based join sampling, a novel cardinality estimation technique for
main-memory databases that relies on sampling and existing in-
dex structures to obtain accurate estimates. Results on a real-world
data set show that this approach significantly improves estimation
as well as overall plan quality. The additional sampling effort is
quite low and can be configured to match the desired application
profile. The technique can be easily integrated into most systems.
",14.554592549557764,12.953230088495577,115
CIDR_17_010.txt,13.984427724209894,11.907069479158174,CIDR,5593,"
Many  of  today’s  interactive  server  applications  are  implemented 
using actor-oriented programming frameworks. Such applications 
treat  actors  as  a  distributed  in-memory  object-oriented  database. 
However,  actor  programming  frameworks  offer  few  if  any  data-
base  system  features,  leaving  application  developers  to  fend  for 
themselves.  It  is  challenging  to  add  such  features  because  the 
design  space  is  different  than  traditional  database  systems.  The 
system must be scalable to a large number of servers, it must work 
well with a variety of cloud storage services, and it must integrate 
smoothly with the actor programming model.  

We  present  the  vision  of  an  actor-oriented  database.  We  then 
describe  one  component  of  such  a  system,  to  support  indexed 
actors, focusing especially on details of the fault tolerance design. 
We  implemented  the  indexing  component  in  the  Orleans  actor-
oriented programming framework and present the result of initial 
performance measurements.  
",15.414825405933506,13.964722222222221,146
CIDR_17_011.txt,16.244216065833722,14.993712683328994,CIDR,5331,"
Provenance is a way to answer “why” questions about computations. It has found a number of uses in the database community,
such as debugging query answers or tracing unexpected results to
database tuples. In fact, the ability to ask “why” can be useful
for a much broader range of applications. In this paper, we summarize our experiences over the past few years in adapting provenance
for diagnostic and forensic uses in networks and distributed systems. Our work draws inspirations from database provenance, yet
the deployment scale, use cases, and distributed nature of networks
require a significant re-design of traditional data provenance models. We review a number of use cases, ranging from investigating
intrusions to diagnosing (and even automatically fixing) softwaredefined networks, and present a unified system architecture that we
have designed and implemented for provenance in distributed systems. We conclude with a discussion of open issues in this space.
",16.084390811093357,15.18714285714286,151
CIDR_17_012.txt,16.13497784778672,15.069120390756677,CIDR,6484,"
In this paper, we present an overview of the Myria stack for
big data management and analytics that we developed in the
database group at the University of Washington and that
we have been operating as a cloud service aimed at domain
scientists around the UW campus. We highlight Myria’s key
design choices and innovations and report on our experience
with using Myria for various data science use-cases.

",17.122413403193683,18.04471014492754,71
CIDR_17_013.txt,14.504662991434447,13.428229433316094,CIDR,5350,"
In most Big Data applications, the data is heterogeneous.
As we have been arguing in a series of papers, storage engines should be well suited to the data they hold. Therefore,
a system supporting Big Data applications should be able to
expose multiple storage engines through a single interface.
We call such systems, polystore systems. Our reference implementation of the polystore concept is called BigDAWG
(short for the Big Data Analytics Working Group). In this
demonstration, we will show the BigDAWG system and a
number of polystore applications built to help ocean metagenomics researchers handle their heterogenous Big Data.

CCS Concepts

eInformation systems — DBMS engine architectures;
Federated databases; Data federation tools; eHumancentered computing — Visualization toolkits;
",14.348710955821954,14.314534161490688,116
CIDR_17_014.txt,14.504243895494952,13.009946774039332,CIDR,4257,"
In the last two decades, both researchers and vendors have built
advisory tools to assist database administrators (DBAs) in various
aspects of system tuning and physical design. Most of this previous
work, however, is incomplete because they still require humans to
make the final decisions about any changes to the database and are
reactionary measures that fix problems after they occur.

What is needed for a truly “self-driving” database management
system (DBMS) is a new architecture that is designed for autonomous
operation. This is different than earlier attempts because all aspects
of the system are controlled by an integrated planning component
that not only optimizes the system for the current workload, but also
predicts future workload trends so that the system can prepare itself
accordingly. With this, the DBMS can support all of the previous
tuning techniques without requiring a human to determine the right
way and proper time to deploy them. It also enables new optimizations that are important for modern high-performance DBMSs, but
which are not possible today because the complexity of managing
these systems has surpassed the abilities of human experts.

This paper presents the architecture of Peloton, the first selfdriving DBMS. Peloton’s autonomic capabilities are now possible
due to algorithmic advancements in deep learning, as well as improvements in hardware and adaptive database architectures.

",17.122413403193683,16.428636363636368,222
CIDR_17_015.txt,17.575254890526274,16.75758543695557,CIDR,6502,"
Data exploration and analysis, especially for non-programmers, remains a tedious and frustrating process of trial-and-error—data scientists spend many hours poring through visualizations in the hope
of finding those that match desired patterns. We demonstrate zenvisage, an interactive data exploration system tailored towards “fastforwarding” to desired trends, patterns, or insights, without much
effort from the user. zenvisage’s interface supports simple dragand-drop and sketch-based interactions as specification mechanisms
for the exploration need, as well as an intuitive data exploration
language called ZQL for more complex needs. zenvisage is being developed in collaboration with ad analysts, battery scientists,
and genomic data analysts, and will be demonstrated on similar
datasets.
",26.683356303267143,30.271428571428576,114
CIDR_17_016.txt,16.63657903940628,15.882453819577602,CIDR,7528,"
Interactive data visualizations have emerged as a prominent way
to bring data exploration and analysis capabilities to both technical and non-technical users. Despite their ubiquity and importance
across applications, multiple design- and performance-related challenges lurk beneath the visualization creation process. To meet
these challenges, application designers either use visualization systems (e.g., Endeca, Tableau, and Splunk) that are tailored to domainspecific analyses, or manually design, implement, and optimize
their own solutions. Unfortunately, both approaches typically slow
down the creation process. In this paper, we describe the status of
our progress towards an end-to-end relational approach in our data
visualization management system (DVMS). We introduce DeVIL,
a SQL-like language to express static as well as interactive visualizations as database views that combine user inputs modeled as
event streams and database relations, and we show that DeVIL can
express a range of interaction techniques across several taxonomies
of interactions. We then describe how this relational lens enables
a number of new functionalities and system design directions and
highlight several of these directions. These include (a) the use of
provenance queries to express and optimize interactions, (b) the application of concurrency control ideas to interactions, (c) a streaming framework to improve near-interactive visualizations, and (d)
techniques to synthesize interactive interfaces tailored to end-users.
",19.287186520377343,18.86777777777778,218
CIDR_17_017.txt,15.351688814469142,14.266597654306633,CIDR,5320,"
Enterprises increasingly employ a wide array of tools and
processes to make data-driven decisions. However, there are
large inefficiencies in the enterprise-wide workflow that stem
from the fact that business workflows are expressed in natural language but the actual computational workflow has
to be manually translated into computational programs. In
this paper, we present an initial approach to bridge this gap
by targeting the data science component of enterprise workflows. In many cases, this component is the slowest part of
the overall enterprise process, and focusing on it allows us to
take an initial step in solving the larger enterprise-wide productivity problem. In this initial approach, we propose using
a chatbot to allow a data scientist to assemble data analytics
pipelines. A crucial insight is that while precise interpretation of general natural language continues to be challenging,
controlled natural language methods are starting to become
practical as natural interfaces in complex decision-making
domains. In addition, we recognize that data science workflow components are often templatized. Putting these two
insights together, we develop a practical system, called Ava,
that uses (controlled) natural language to program data science workflows. We have an initial proof-of-concept that
demonstrates the potential of our approach.

",16.45884130781739,15.33918699186992,206
CIDR_17_018.txt,15.963586929337552,14.439619587494501,CIDR,5019,"
In this paper, we predict the rise of Dependency-Driven Analytics
(DDA), a new class of data analytics designed to cope with growing
volumes of unstructured data. DDA drastically reduces the cognitive burden of data analysis by systematically leveraging a compact
dependency graph derived from the raw data. The computational
cost associated with the analysis is also reduced substantially, as
the graph acts as an index for commonly accessed data items. We
built a system supporting DDA using off-the-shelf Big Data and
graph DB technologies, and deployed it in production at Microsoft
to support the analysis of the exhaust of our Big Data infrastructure
producing petabytes of system logs daily. The dependency graph in
this setting captures lineage information among jobs and files and
is used to guide the analysis of telemetry data. We qualitatively discuss the improvement over the brute-force analytics our users used
to perform by considering a series of practical applications, including: job auditing and compliance, automated SLO extraction of
recurring tasks, and global job ranking. We conclude by discussing
the shortcomings of our current implementation and by presenting
some of the open research challenges for Dependency-Driven Analytics that we plan to tackle next.
",18.08858127442927,17.682203269367452,202
CIDR_17_019.txt,15.175314602521407,13.315180732656152,CIDR,6591,"
The increasing use of databases in the storage of critical and
sensitive information in many organizations has lead to an
increase in the rate at which databases are exploited in computer crimes. While there are several techniques and tools
available for database forensics, they mostly assume apriori
database preparation, such as relying on tamper-detection
software to be in place or use of detailed logging. Investigators, alternatively, need forensic tools and techniques that
work on poorly-configured databases and make no assumptions about the extent of damage in a database.
In this paper, we present DBCarver, a tool for reconstructing database content from a database image without using
any log or system metadata. The tool uses page carving
to reconstruct both query-able data and non-queryable data
(deleted data). We describe how the two kinds of data can
be combined to enable a variety of forensic analysis questions
hitherto unavailable to forensic investigators. We show the
generality and efficiency of our tool across several databases
through a set of robust experiments.
",17.613555460941566,16.35797342192691,173
CIDR_17_020.txt,15.96632800443452,13.55802289856464,CIDR,5012,"
An increasing amount of information is being collected in
structured, evolving, curated databases, driving the question
of how information extracted from such datasets via queries
should be cited. Unlike traditional research products, such
books and journals, which have a fixed granularity, data citation is a challenge because the granularity varies. Different
portions of the database, with varying granularity, may have
different citations. Furthermore, there are an infinite number of queries over a database, each accessing and generating
different subsets of the database, so we cannot hope to explicitly attach a citation to every possible result set and/or
query. We present the novel problem of automatically generating citations for general queries over a relational database,
and explore a solution based on a set of citation views, each
of which attaches a citation to a view of the database. Citation views are then used to automatically construct citations
for general queries. Our approach draws inspiration from results in two areas, query rewriting using views and database
provenance and combines them in a robust model. We then
discuss open issues in developing a practical solution to this
challenging problem.

",17.971250198000288,16.24282754010695,188
CIDR_17_021.txt,15.459543963350765,13.58148405009294,CIDR,10700,"
Recent research on multi-core database architectures has made the
argument that, when possible, database systems should abandon the
use of latches in favor of latch-free algorithms. Latch-based algorithms are thought to scale poorly due to their use of synchronization based on mutual exclusion. In contrast, latch-free algorithms
make strong theoretical guarantees which ensure that the progress
of a thread is never impeded due to the delay or failure of other
threads. In this paper, we analyze the various factors that influence
the performance and scalability of latch-free and latch-based algorithms, and perform a microbenchmark evaluation of latch-free and
latch-based synchronization algorithms. Our findings indicate that
the argument for latch-free algorithms’ superior scalability is far
more nuanced than the current state-of-the-art in multi-core database architectures suggests.

",17.58133193835471,16.294289855072467,139
CIDR_17_022.txt,15.557550373405789,13.671233210812087,CIDR,5337,"
Use of transactional multicore main-memory databases is
growing due to dramatic increases in memory size and CPU
cores available for a single machine. To leverage these resources, recent concurrency control protocols have been proposed for main-memory databases, but are largely optimized
for specific workloads. Due to shifting and unknown access
patterns, workloads may change and one specific algorithm
cannot dynamically fit all varied workloads. Thus, it is desirable to choose the right concurrency control protocol for
a given workload. To address this issue we present adaptive concurrency control (ACC), that dynamically clusters
data and chooses the optimal concurrency control protocol
for each cluster. ACC addresses three key challenges: i) how
to cluster data to minimize cross-cluster access and maintain load-balancing, ii) how to model workloads and perform
protocol selection accordingly, and iii) how to support mixed
concurrency control protocols running simultaneously. In
this paper, we outline these challenges and present preliminary results
",17.28802050969988,16.247582417582418,157
CIDR_17_023.txt,13.068205034771267,10.818239694605179,CIDR,5255,"
It is expensive to maintain strong data consistency during concurrent execution. However, weak consistency levels, which are considered harmful, have been widely applied
in analytical jobs. Their success challenges our belief: data
consistency, which is believed to be an essential to precise
computing, does not always need to be preserved. In this
paper, we tackle one of the core questions related to the application of weak consistency: When does weak consistency
work well? We propose an effective explanation for the success of weak consistency. We name it bad things do not come
in threes, or BN3. It is based on the observation that the
volume of data is far larger than the number of workers. If all
workers are operating concurrently, the probability that two
workers access the same data at the same time is relatively
low. Although it is not small enough to be neglected, the
chance that three or more workers access the same data at
the same time is even lower. Based on the BN3 conjecture,
we analyze different consistency levels. We show that a weak
consistency level in transaction processing is equivalent to
snapshot isolation (SI) under reasonable assumptions. Although the BN3 is an oversimplification of real scenarios,
it explains why weak consistency often achieves results that
are accurate enough. It also serves as a quality promise for
the future wide application of weak consistency in analytical
tasks. We verify our results in experimental studies.

",13.484332010920856,11.656141661685595,240
CIDR_17_024.txt,16.45807714114678,15.980458942900498,CIDR,4565,"
Have you ever been in a sauna? If yes, according to our recent survey
conducted on Amazon Mechanical Turk, people who go to saunas
are more likely to know that Mike Stonebraker is not a character in
“The Simpsons”. While this result clearly makes no sense, recently
proposed tools to automatically suggest visualizations, correlations,
or perform visual data exploration, significantly increase the chance
that a user makes a false discovery like this one. In this paper, we
first show how current tools mislead users to consider random fluctuations as significant discoveries. We then describe our vision and
early results for QUDE, a new system for automatically controlling
the various risk factors during the data exploration process.

",15.112257680678326,14.82006896551724,117
CIDR_17_025.txt,16.90633138182658,15.834123869267213,CIDR,6765,"
Systems for declarative large-scale machine learning (ML)
algorithms aim at high-level algorithm specification and automatic optimization of runtime execution plans. State-ofthe-art compilers rely on algebraic rewrites and operator selection, including fused operators to avoid materialized intermediates, reduce memory bandwidth requirements, and
exploit sparsity across chains of operations. However, the
unlimited number of relevant patterns for rewrites and operators poses challenges in terms of development effort and
high performance impact. Query compilation has been studied extensively in the database literature, but ML programs
additionally require handling linear algebra and exploiting
algebraic properties, DAG structures, and sparsity. In this
paper, we introduce SPOOF, an architecture to automatically
(1) identify algebraic simplification rewrites, and (2) generate fused operators in a holistic framework. We describe a
snapshot of the overall system, including key techniques of
sum-product optimization and code generation. Preliminary
experiments show performance close to hand-coded fused
operators, significant improvements over a baseline without
fused operators, and moderate compilation overhead.

",20.130776976110326,20.05952673093778,164
CIDR_17_026.txt,15.079925798581503,13.799247300865158,CIDR,3882,"
Existing scale-up graph engines are tuned for either short,
navigational requests (e.g., Nearest-Neighbor) or longer, analytics requests (e.g., PageRank). However, they do not
have good performance for both workloads running concurrently. We present Janus, a scale-up graph engine architected for modern, many-core servers with large memory.
Janus has excellent scale-up performance on navigational requests, on analytics requests, and on a mixed workload running concurrently both navigational and analytics requests.

",14.554592549557764,13.789864864864864,77
CIDR_17_027.txt,15.304253766206372,13.909668967958932,CIDR,5628,"
Modern analytics applications combine multiple functions from
different libraries and frameworks to build increasingly complex
workflows. Even though each function may achieve high performance in isolation, the performance of the combined workflow is
often an order of magnitude below hardware limits due to extensive
data movement across the functions. To address this problem, we
propose Weld, a runtime for data-intensive applications that optimizes across disjoint libraries and functions. Weld uses a common
intermediate representation to capture the structure of diverse dataparallel workloads, including SQL, machine learning and graph
analytics. It then performs key data movement optimizations and
generates efficient parallel code for the whole workflow. Weld can
be integrated incrementally into existing frameworks like TensorFlow, Apache Spark, NumPy and Pandas without changing their
user-facing APIs. We show that Weld can speed up these frameworks, as well as applications that combine them, by up to 30x.

",16.32212239822248,16.441190476190474,148
CIDR_17_028.txt,14.650831435125447,12.644124910817194,CIDR,5195,"
Big Data comes with huge challenges. Its volume and velocity
makes handling, curating, and analytical processing a costly affair.
Even to simply “look at” the data within an a priori defined budget
and with a guaranteed interactive response time might be impossible to achieve. Commonly applied scale-out approaches will hit the
technology and monetary wall soon, if not done so already. Likewise, blindly rejecting data when the channels are full, or reducing
the data resolution at the source, might lead to loss of valuable observations.

An army of well-educated database administrators or full software stack architects might deal with these challenges albeit at
substantial cost. This calls for a mostly knobless DBMS with a
fundamental change in database management.

Data rotting has been proposed as a direction to find a solution [10, 11]. For the sake of storage management and responsiveness, it lets the DBMS semi-autonomously rot away data. Rotting
is based on the systems own unwillingness to keep old data as easily accessible as fresh data. This paper sheds more light on the
opportunities and potential impacts of this radical departure in data
management. Specifically, we study the case where a DBMS selectively forgets tuples (by marking them inactive) under various amnesia scenarios and with different implementation strategies. Our
ultimate goal is to use the findings of this study to morph an existing data management engine to serve demanding big data scientific
applications with well-chosen built-in data amnesia algorithms.

",15.299343439825492,13.000000000000004,246
CIDR_17_029.txt,15.35327999143593,14.167652716186723,CIDR,7578,"
The onset of cloud computing has brought about computing power that can be provisioned and released on-demand.
This capability has drastically increased the complexity of
workload and resource management for database applications. Existing solutions rely on query latency prediction
models, which are notoriously inaccurate in cloud environments. We argue for a substantial shift away from query
performance prediction models and towards machine learning techniques that directly model the monetary cost of
using cloud resources and processing query workloads on
them. Towards this end, we sketch the design of a learningbased service for IaaS-deployed data management applications that uses reinforcement learning to learn, over time,
low-cost policies for provisioning virtual machines and dispatching queries across them. Our service can effectively
handle dynamic workloads and changes in resource availability, leading to applications that are continuously adaptable,
cost effective, and performance aware. In this paper, we discuss several challenges involved in building such a service,
and we present results from a proof-of-concept implementation of our approach.

",17.122413403193683,16.808095238095238,169
CIDR_17_030.txt,14.470270418902462,13.078201487491551,CIDR,5211,"
RocksDB is an embedded, high-performance, persistent keyvalue storage engine developed at Facebook. Much of our
current focus in developing and configuring RocksDB is to
give priority to resource efficiency instead of giving priority
to the more standard performance metrics, such as response
time latency and throughput, as long as the latter remain
acceptable. In particular, we optimize space efficiency while
ensuring read and write latencies meet service-level requirements for the intended workloads. This choice is motivated
by the fact that storage space is most often the primary
bottleneck when using Flash SSDs under typical production
workloads at Facebook. RocksDB uses log-structured merge
trees to obtain significant space efficiency and better write
throughput while achieving acceptable read performance.

This paper describes methods we used to reduce storage
usage in RocksDB. We discuss how we are able to trade
off storage efficiency and CPU overhead, as well as read
and write amplification. Based on experimental evaluations
of MySQL with RocksDB as the embedded storage engine
(using TPC-C and LinkBench benchmarks) and based on
measurements taken from production databases, we show
that RocksDB uses less than half the storage that InnoDB
uses, yet performs well and in many cases even better than
the B-tree-based InnoDB storage engine. To the best of our
knowledge, this is the first time a Log-structured merge treebased storage engine has shown competitive performance
when running OLTP workloads at large scale.

",15.616094167265928,14.959972105997213,240
CIDR_17_031.txt,15.693517975766621,14.651702583723235,CIDR,5013,"
Deciding query equivalence is an important problem in data management with many practical applications. Solving the problem,
however, is not an easy task. While there has been a lot of work
done in the database research community in reasoning about the
semantic equivalence of SQL queries, prior work mainly focuses
on theoretical limitations. In this paper, we present COSETTE, a
fully automated prover that can determine the equivalence of SQL
queries. COSETTE leverages recent advances in both automated
constraint solving and interactive theorem proving, and returns a
counterexample (in terms of input relations) if two queries are not
equivalent, or a proof of equivalence otherwise. Although the problem of determining equivalence for arbitrary SQL queries is undecidable, our experiments show that COSETTE can determine the
equivalences of a wide range of queries that arise in practice, including conjunctive queries, correlated queries, queries with outer
joins, and queries with aggregates. Using COSETTE, we have also
proved the validity of magic set rewrites, and confirmed various
real-world query rewrite errors, including the famous COUNT bug.
We are unaware of any prior tool that can automatically determine the equivalences of a broad range of queries as COSETTE,
and believe that our tool represents a major step towards building
provably-correct query optimizers for real-world database systems.

",17.693802365651003,17.558226744186047,216
CIDR_17_032.txt,14.510013789832026,13.041173889030677,CIDR,6919,"
Modern database systems support one set of integrity constraints
per database. Imagine you could specify multiple sets of integrity
constraints per database, one for each type of application. This
paper argues why this might be a good idea and introduces a system
that implements this idea.

",15.021129683784007,11.937826086956523,47
CLOUD_17_001.txt,14.034603594593207,12.033563912019538,CLOUD,4604,"
Existing coflow scheduling frameworks effectively
shorten communication time and completion time of cluster applications. However, existing frameworks only consider available
bandwidth on hosts and overlook congestion in the network
when making scheduling decisions. Through extensive simulations using the realistic workload probability distribution from
Facebook, we observe the performance degradation of the stateof-the-art coflow scheduling framework, Varys, in the cloud
environment on a shared data center network (DCN) because
of the lack of network congestion information. We propose
Coflourish, the first coflow scheduling framework that exploits
the congestion feedback assistances from the software-definednetworking(SDN)-enabled switches in the networks for available
bandwidth estimation. Our simulation results demonstrate that
Coflourish outperforms Varys by up to 75.5% in terms
of average coflow completion time under various workload
conditions. The proposed work also reveals the potentials of
integration with traffic engineering mechanisms in lower levels
for further performance optimization.

Index Terms—Application-aware Networks; Coflow Scheduling; Data Center Networks; Software-defined Networking; Cloud
Computing

",18.699421769314853,18.539281332164766,165
CLOUD_17_002.txt,15.077610082524195,13.677800844583238,CLOUD,6362,"
Multi-tenant data centers for cloud computing
require the deployment of virtual private networks for tenants
in an on-demand manner, providing isolation and security
between tenants. To address these requirements, network
virtualization techniques such as encapsulation and tunneling
have been widely used. However, these approaches inherently
incur processing overhead on end-points (such as the host
hypervisor), reducing the effective throughput for the tenant
virtual network compared to the native network. This problem
is exacerbated with increases in line rates, now exceeding
10Gbps. In this paper, we introduce PARES (PAcket REwriting
on SDN), a novel technique which uses the packet rewriting
feature of SDN switches to provide multi-tenancy in data center
networks at edge switches, thereby reducing the load on endpoint hypervisors and improving the throughput, compared to
tunneling. Experiments in an SDN testbed show that our proposed data center arhictecture with PARES achieves near linerate multi-tenancy virtualization with 10Gbps links (compared
to 20% of line-rate for VXLAN tunneling), without incurring
processing overhead at end-point hypervisors or guest servers.
Additionally, the paper evaluates the scalability of PARES for
ARP protocol handling and with respect to number of SDN
flow entries.

Keywords-Software Defined Network(SDN), OpenFlow, Network Virtualization, Multi-Tenant Data Center, Cloud Computing

",17.833180683606166,17.464778708133974,210
CLOUD_17_003.txt,15.03834750478119,12.452104014304982,CLOUD,5441,"
Nowadays many companies and _ organizations
choose to deploy their applications in data centers to leverage
resource sharing. The increase in tasks of multiple applications,
however, makes it challenging for a data center provider to
maximize its revenue by intelligently scheduling tasks in softwaredefined networking (SDN)-enabled data centers. Existing SDN
controllers only reduce network latency while ignoring virtual
machine (VM) latency, thus may lead to revenue loss. In the
context of SDN-enabled data centers, this paper presents a
workload-aware revenue maximization (WARM) approach to
maximize the revenue from a data center provider’s perspective.
The core idea is to jointly consider the optimal combination of
VMs and routing paths for tasks of each application. Comparing
with state-of-the-art methods, the experimental results show that
WARM yields the best schedules that not only increase the
revenue but also reduce the round-trip time of tasks of all
applications.

Keywords-Cloud data center, revenue maximization, softwaredefined networking, task scheduling, metaheuristic

",18.243605946275583,16.760124223602485,164
CLOUD_17_004.txt,14.941436386092448,13.23996509158751,CLOUD,4032,"
In the age of Big Data, advances in distributed technologies and cloud storage services provide highly efficient and
cost-effective solutions to large scale data storage and management. Supporting self-emerging data using clouds is a challenging
problem. While straight-forward centralized approaches provide
a basic solution to the problem, unfortunately they are limited to
a single point of trust. Supporting attack-resilient timed release of
encrypted data stored in clouds requires new mechanisms for self
emergence of data encryption keys that enables encrypted data
to become accessible at a future point in time. Prior to the release
time, the encryption key remains undiscovered and unavailable in
a secure distributed system, making the private data unavailable.
In this paper, we propose Emerge, a self-emerging timed data
release protocol for securely hiding data encryption keys of
private encrypted data in a large-scale Distributed Hash Table
(DHT) network that makes the data available and accessible only
at the defined release time. We develop a suite of erasure-codingbased routing path construction schemes for securely storing
and routing encryption keys in DHT networks that protect an
adversary from inferring the encryption key prior to the release
time (release-ahead attack) or from destroying the key altogether
(drop attack). Through extensive experimental evaluation, we
demonstrate that the proposed schemes are resilient to both
release-ahead attack and drop attack as well as to attacks that
arise due to traditional churn issues in DHT networks.

",18.243605946275583,17.75956611570248,243
CLOUD_17_005.txt,15.89784308286912,14.78891013633805,CLOUD,6136,"
In large-scale data stream management systems,
sampling rate of different sensors can change quickly in response
to changed execution environment. However, such changes can
cause significant load imbalance on the back-end servers, leading
towards performance degradation and data loss. To address this
challenge, in this paper, we present a model-driven middleware
service (i.e., Arion) that uses a two-step approach to minimize
data loss. Specifically, Arion constructs models and algorithms for
overload prediction for heterogeneous systems (where different
streams can have different sampling rates and message sizes)
leveraging limited execution traces from homogeneous systems
(where each stream has the same sampling rate and message
size). Subsequently, when an overload condition is predicted
(or detected), Arion first leverages the a priori constructed
models to identify the streams (if any) that can be split into
multiple substreams to scale up the performance and minimize
data loss without allocating additional servers. If the software
based solution turns out to be inadequate, in the second stage,
the system allocates additional servers and redirects streams to
stabilize the system leveraging the models. Extensive evaluation
on a 6 node cluster using Apache Cassandra for various scenarios
shows that our approach can predict the potential overload
condition with high accuracy (81.9%) while minimizing data loss
and the number of additional servers significantly.

Keywords-Cassandra; Data Stream Management; Middleware

",18.643177196211187,17.876428571428573,227
CLOUD_17_006.txt,15.322241378113624,14.085610816687051,CLOUD,4402,"
Cloud storage services are associated with high
latency variance, and degraded throughput which is problematic when users are fetching and storing content for interactive
applications. This can be attributed to performance hotspots
created by slow nodes in a storage cluster, and performance
interference caused by multi-tenancy, and background tasks
such as data scrubbing, backfilling, recovery, etc. In this paper,
we present DLR, a system that improves the performance
of cloud storage services in the presence of hardware heterogeneity, and performance interference through a dynamic
load redistribution technique. We designed DLR to dynamically
adjust the load serving ratio of storage servers based on the
system-level performance measurements from the storage cluster. We implemented DLR using Ceph, a popular distributed
object storage system, and evaluated its performance on NSFCloud’s Chameleon testbed using Ceph’s Rados benchmark.
Experimental results show that DLR improves the average
throughput and latency of Ceph storage by up to 65%, and
41% respectively compared to the default case. Compared to
Ceph’s in-built load balancing technique, DLR improves the
throughput by up to 98%, and latency by 96%.

Keywords-Hardware Heterogeneity; Performance Interference; Ceph; Cloud Storage;

",16.373557378465907,15.787771164021166,193
CLOUD_17_007.txt,16.794926429455337,15.625735904560702,CLOUD,5894,"
 The increasing relevance of information assurance
in cloud computing has forced governments and stakeholders to
turn their attention to Information Technology (IT) security
certifications and standards. The introduction of new frameworks
such as FedRAMP in the US and C5 in Germany is aimed to raise
the level of protection against threats and vulnerabilities unique to
cloud computing. However, our in-depth and systematic analyses
reveals that these new standards do not bring a radical change in
the realm of certifications. Results also shows that the newly
developed standards share much of their basis with older, more
consolidated standards such as the ISO/IEC 27001 and hence the
need for determining the added value.

In this study, we provide an overview of ISO/IEC 27001, C5,
and FedRAMP while examining their completeness and adequacy
in addressing current threats to cloud assurance. We question the
level of protection they offer by comparing these three
certifications alongside each other. We identify weaknesses in the
three frameworks and highlight necessary improvements to meet
the security requirements indispensable in relation to the current
threat landscape.

Keywords— FedRAMP; ISO; C5; Certification; Standard;
Framework; Cloud; Privacy; Security.

",16.827784334635936,15.471447368421057,191
CLOUD_17_008.txt,15.718111361294202,14.192578787850874,CLOUD,5931,"
A service-oriented architecture (SOA)-based application is composed of a number of distributed and looselycoupled services which are interconnected to accomplish a more
complex functionality. The main security challenge in SOA is
that we cannot trust the participating services in a service
composition to behave as expected all the time. Moreover, the
chain of all services involved in an end-to-end invocation may
not be visible to the clients. As a result, any violation of the
client’s policies could remain undetected. To address these
challenges in SOA, we propose the following contributions.
First, we propose a new end-to-end security architecture for
SOA based on a dynamic composite trust model. To maintain
the dynamic trust, we designed a trusted-third party service
called trust manager component, which collects and processes
feedbacks from the actual execution of services. Second, we
developed an end-to-end inter-service policy monitoring and
enforcement framework (PME framework), which is able
to dynamically intercept the interactions between services
at runtime and react to the potentially malicious activities
according to the client’s policies. Third, we design an intraservice policy monitoring and enforcement framework based
on taint analysis mechanism to monitor the flow of information
within services and detect and prevent information disclosure
attacks. These two frameworks together can provide an end-toend visibility and security in SOA. Finally, we have extensively
studied the correctness and performance of the proposed
security frameworks based on a realistic SOA case study in
a cloud environment. All experimental studies validate that
the practicality and effectiveness of the presented solutions.

Keywords-End-to-End Security; Service-Oriented Architecture; Security Policy

",17.388976658221566,15.649411764705885,275
CLOUD_17_009.txt,15.815694218579477,14.25029294415964,CLOUD,5958,"
Cloud storage providers can reduce storage costs by
detecting identical files and storing only one instance of them.
While appealing to the storage providers, this deduplication
set-up raises various privacy concerns among clients. Various
techniques to retrofit content confidentiality in deduplication have
been studied in the literature. Nevertheless, data encryption alone
is insufficient to protect users’ privacy, for the ownership and
equality information of the outsourced data left unprotected may
have serious privacy implications. In this paper, we investigate a
three-tier architecture that saves bandwidth otherwise incurred
by server-side deduplication solutions, yet does not admit the
client-side deduplication’s leakage on file existence. Leveraging
trusted SGX-enabled processors, we construct the first privacypreserving data deduplication protocol that protects not only the
confidentiality, but also the ownership and equality information
of the outsourced data, offering better privacy guarantees in
comparison with existing works on secure data deduplication.
Our experiments show that the proposed protocol incurs low
performance overhead over conventional solutions that provide
weaker level of privacy protection.
",19.142268018852484,19.07789915966387,172
CLOUD_17_010.txt,15.241757717185116,13.365463073571782,CLOUD,4483,"
Mining big data often requires tremendous
computational resources. This has become a major obstacle to
broad applications of big data analytics. Cloud computing
allows data scientists to access computational resources ondemand for building their big data analytics solutions in the
cloud. However, the monetary cost of mining big data in the
cloud can still be unexpectedly high. For example, running 100
m4-xlarge Amazon EC2 instances for a month costs
approximately $17,495.00. On this ground, it is a critical issue
to analyze the cost effectiveness of big data mining in the cloud,
i.e, how to achieve a sufficiently satisfactory result at the
lowest possible computation cost. In certain big data mining
scenarios, 100% accuracy is unnecessary. Instead, it is often
more preferable to achieve a sufficient accuracy, e.g., 99%, ata
much lower cost, e.g., 10%, than the cost of achieving the
100% accuracy. In this paper, we explore and demonstrate the
cost effectiveness of big data mining with a case study using
well known k-means. With the case study, we find that
achieving 99% accuracy only needs 0.32%-46.17%
computation cost of 100% accuracy. This finding lays the
cornerstone for cost-effective big data mining in a variety of
domains.

Keywords-Cloud Computing; Data Mining; Cost-Effective; Big
Data; K-Means

",14.434950587195996,12.49494131455399,220
CLOUD_17_011.txt,15.127760945858537,14.210291871534682,CLOUD,3590,"
Data analytics is undergoing a revolution in many
scientific domains, demanding cost-effective parallel data analysis techniques. Traditional Java-based Big Data processing
tools like Hadoop MapReduce are designed for commodity
CPUs. In contrast, emerging manycore processors like Xeon
Phi has an order of magnitude of computation power and
memory bandwidth. To harness the computing capabilities, we
propose a Harp-DAAL framework. We show that enhanced
versions of MapReduce can be replaced by Harp, a Hadoop
plug-in, that offers useful data abstractions for both of highperformance iterative computation and MPI-quality communication, and it can drive Intel’s native library DAAL. We select
a subset of three machine learning algorithms and implement
them within Harp-DAAL. Our scalability benchmarks run on
Knights Landing (KNL) clusters and achieve up to 2.5 times
speedup of performance to the HPC solution in NOMAD and
15 to 40 times faster than Java-based solutions in Spark. We
further quantify the workloads on single node KNL with a
performance breakdown at micro-architecture level.

Keywords-HPC, Xeon Phi, BigData

",14.867677710551934,13.583333333333336,177
CLOUD_17_012.txt,17.408344920673752,16.324982137243136,CLOUD,2828,"
Nowadays, with the increasing burst of newly generated data everyday, as well as the vast expanding needs
for corresponding data analyses, grand challenges have been
brought to big data computing platforms. Computing resources
in a single cluster are often not able to fulfill the computing
capability needs. The requests of distributed computing resources
are dramatically arising. In addition, with increasing popularity
of cloud computing platforms, many organizations with data
security concerns are more favor to hybrid cloud, a multi-cluster
environment composed by both public cloud and private cloud in
purpose of keeping sensitive data local. All these scenarios show
great necessity of migrating big data computing to multi-cluster
environment.

In this paper, we present a hierarchical multi-cluster big data
computing framework built upon Apache Spark. Our framework supports combination of heterogeneous Spark computing
clusters. With an integrated controller within the framework,
it also facilitates ability for submitting, monitoring, executing
of Spark workflow. Our experimental results show that the
proposed framework not only enables possibility of distributing
Spark workflow throughout multiple clusters, but also provides
significant performance improvement compared to single cluster
environment by optimizing utilization of multi-cluster computing
resources.

Key Words: Hybrid Cloud, Hierarchical, Multi-cluster, Big
data, Spark

",16.887214914478655,16.62453694581281,204
CLOUD_17_013.txt,16.14261713058803,14.363094239134366,CLOUD,4480,"
Cloud computing resources that are equipped with
GPU devices are widely used for applications that require
extensive parallelism, such as deep learning. When the demand
of cloud computing instance is low, the surplus of resources is
provided at a lower price in the form of spot instance by AWS
EC2. This paper proposes DeepSpotCloud that utilizes GPUequipped spot instances to run deep learning tasks in a cost
efficient and fault-tolerant way. Thorough analysis about spot
instance price history logs reveals that GPU spot instances show
more dynamic price change pattern than other general types of
cloud computing resources. To deal with the price dynamicity
of the GPU spot instance, DeepSpotCloud utilizes instances in
different regions across continents as a single resource pool.
This paper also proposes a task migration heuristic by utilizing
a checkpointing mechanism of existing deep learning analysis
platform to conduct fast task migration when a running spot
instance is interrupted. Extensive experiments using real AWS
services prove that the proposed task migration method is
effective even in a WAN environment with limited network
bandwidth. Comprehensive simulations by replaying AWS EC2
price history logs reveal that DeepSpotCloud can achieve 13%
more cost gain than a state-of-the-art interrupt-driven scheduling
policy. The prototype of DeepSpotCloud is implemented using
various cloud computing services provided by AWS to serve real
deep learning tasks.

",18.12316971661352,15.40156342182891,227
CLOUD_17_014.txt,12.243015498526727,10.541479049211475,CLOUD,5367,"
Apache HBase is a widely used non-relational
database in the Hadoop ecosystem. However, it will be inefficient
if users perform multidimensional queries. Some of existing
approaches incur extra costs in write performance or consistency
maintenance, others are limited to specific applications. In this
paper, we propose a novel data model called CFIDM, short for
Column Family Indexed Data Model. In CFIDM, we convert
the queried column into multiple column families. Values in the
specific column are partitioned. Each partition is manifested by
a column family, turning column family into an index with no
additional cost. Then we provide guides to build this data model.
Finally, we evaluate the effectiveness and versatility of CFIDM
on the Bixi data set and the TPC-DS benchmark. Results show
that CFIDM can save 6.6% disk space for Bixi and 35% for
TPC-DS, maximally speeding up the queries by 5X and 5.5X
respectively.

Index Terms—HBase; Multiple Column Families; Data Model;
Dimension Extension.

",13.319297830178854,11.543977272727272,163
CLOUD_17_015.txt,15.498178455653242,13.740402018921369,CLOUD,5616,"
Many robotic tasks require heavy computation,
which can easily exceed the robot’s onboard computer capability.
A promising solution to address this challenge is outsourcing the
computation to the cloud. However, exploiting the potential of
cloud resources in robotic software is difficult, because it involves complex code modification and extensive (re)configuration
procedures. Moreover, quality of service (QoS) such as timeliness,
which is critical to robot’s behavior, have to be considered. In
this paper, we propose a transparent and QoS-aware software
framework called Cloudroid for cloud robotic applications. This
framework supports direct deployment of existing robotic software packages to the cloud, transparently transforming them
into Internet-accessible cloud services. And with the automatically generated service stubs, robotic applications can outsource
their computation to the cloud without any code modification.
Furthermore, the robot and the cloud can cooperate to maintain
the specific QoS property such as request response time, even
in a highly dynamic and resource-competitive environment. We
evaluated Cloudroid based on a group of typical robotic scenarios
and a set of software packages widely adopted in real-world
robot practices. Results show that robots capability can be
enhanced significantly without code modification and specific QoS
objectives can be guaranteed. In certain tasks, the ’cloud + robot”
setup shows improved performance in orders of magnitude
compared with the robot native setup.

Keywords-Cloud robotics; Platform as a service; Computation
outsourcing; Quality of service

",16.92669308720184,15.278637339055795,235
CLOUD_17_016.txt,15.745913395353224,14.442719374141156,CLOUD,5198,"
This paper presents a cloud-based positioning method
that leverages visualized signal images through visual analytics
and deep learning. At a mobile client, such as a smart phone, this
approach transforms multidimensional signals captured at a
known location and at a given time into a signal image and
transmits such visual signal images to the Cloud. By collecting and
storing many such visual signal images as fingerprints, we can
build a visual signal image cloud and produce a signal image map
for the geographical region of interest and utilize such signal image
map to serve the positioning requests of mobile clients on the move.
When a user Alice wants to know her current position, her mobile
client will generate a signal image from the multiple signals it
receives with timestamp and send this query image to a Cloud
server. The server searches the existing signal images stored in the
cloud to find those that are similar to this query signal image and
estimates the positioning of Alice based on the locations of those
similar signal images collected by the server. We evaluate our
visual signal images based positioning system on the entire two
floors of a large department store and on the street and shops
outside the department store, with the signal images collected over
30 minutes before serving positioning queries. The mean error of
up to 4 meters is observed. To further verify the applicability of
the proposed method, extensive experiments were conducted to
distinguish whether a user is indoor or outdoor by applying a deep
learning algorithm with 60% of signal images collected for
training and 40% signal images for testing. This experiment shows
that the proposed method is able to distinguish indoor and outdoor
with accuracy of about 95%.

Keywords—Indoor Positioning; Fingerprinting Localization; Data
Visualization; Cloud Computing; Deep Learning;

",17.238542476582836,17.17881518151815,304
CLOUD_17_017.txt,14.363310325635933,11.900036257413447,CLOUD,4647,"
Due to the centralized control, network-wide monitoring and flow-level scheduling of Software-Defined-Networking
(SDN), it can be utilized to achieve Quality of Service (QoS)
for cloud applications and services, such as voice over IP,
video conference and online games, etc. However, most existing
approaches stay at the QoS framework design and test level, while
few works focus on studying the basic QoS techniques supported
by SDN. In this paper, we enable SDN with QoS guaranteed
abilities, which could provide end-to-end QoS routing for each
cloud user service. First of all, we implement an application
identification technique on SDN controller to determine required
QoS levels for each application type. Then, we implement a queue
scheduling technique on SDN switch. It queues the application
flows into different queues and schedules the flows out of
the queues with different priorities. At last, we evaluate the
effectiveness of the proposed SDN-based QoS technique through
an experimental analysis. Results show that when the output
interface has sufficiently available bandwidth, the delay can be
reduced by 28% on average. In addition, for the application flow
with the highest priority, our methods can reduce 99.99% delay
and increase 90.17% throughput on average when the output
interface utilization approaches to the maximum bandwidth
limitation.

",17.251386760058843,15.137232704402518,215
CLOUD_17_018.txt,13.526145731211752,11.30777450816558,CLOUD,6584,"
We study a fundamental problem of how to
schedule complex workflows in the cloud for applications
such as data analytics. One of the main challenges is that
such workflow scheduling problems involve many constraints,
requirements and varied objectives and it is extremely difficult
to find high-quality solutions. To meet the challenge, we explore
using mixed integer programming (MIP) to formulate and solve
complex workflow scheduling problems. To illustrate the MIPbased method, we formulate three related workflow scheduling
problems in MIP. They are fairly generic, comprehensive and
are expected to be useful for a wide range of workflow scheduling scenarios. Using results from numerical experiments, we
demonstrate that, for problems up to certain size, the MIP
approach is entirely applicable and more advantageous over
heuristic algorithms.

Keywords-workflow scheduling; precedence constraints;
cloud computing; mixed integer programming

",16.439396014739867,14.198907563025212,137
CLOUD_17_019.txt,13.545545738263211,11.332288443401897,CLOUD,4793,"
Researches affirm that coflow scheduling/routing
substantially shortens the average application inner communication time in data center networks(DCNs). The commonly
desirable critical features of existing coflow scheduling/routing
framework includes (1) coflow scheduling, (2) coflow routing,
and (3) per-flow rate-limiting. However, to provide the 3 features,
existing frameworks require customized computing frameworks,
customized operating systems, or specific external commercial
monitoring frameworks on software-defined networking(SDN)
switches. These requirements defer or even prohibit the deployment of coflow scheduling/routing in production DCNs. In this
paper, we design a coflow scheduling and routing framework,
MinCOF which has minimal requirements on hosts and switches
for cloud storage area networks(SANs) based on OpenFlow SDN.
MinCOF accommodates all critical features of coflow scheduling/routing from previous works. The deployability in production
environment is especially taken into consideration. The OpenFlow architecture is capable of processing the traffic load in a
cloud SAN. Not necessary requirements for hosts from existing
frameworks are migrated to the mature commodity OpenFlow
1.3 Switch and our coflow scheduler. Transfer applications on
hosts only need slight enhancements on their existing connection
establishment and progress reporting functions. Evaluations reveal that MinCOF decreases the average coflow completion time
(CCT) by 12.94% compared to the latest OpenFlow-based coflow
scheduling and routing framework.

Index Terms—Application-aware Networks; Data Center
Networks; Software-defined Networking; OpenFlow; Coflow
Scheduling; Storage Area Networks; Cloud Computing

",17.410965686947208,16.10948275862069,235
CLOUD_17_020.txt,16.083477405317126,14.077659089237105,CLOUD,3831,"
Detecting anomalous behaviors of cloud platforms is
one of critical tasks for cloud providers. Every anomalous behavior
potentially causes incidents, especially some unaware and/or
unknown issues, which severely harm their SLA (Service Level
Agreement). Existing solutions generally monitor cloud platform
at different layers and then detect anomalies based on rules or
learning algorithms on monitoring metrics. However, complexity
of nowadays cloud platforms, high dynamics of cloud workloads
and thousands of various types of metrics make anomalous
behavior detection more challenging to be applied in production,
especially in large scale cloud production environments. In this
paper, we present a practical cloud anomalous behavior detection
system called Driftinsight. It firstly analyzes multi-denominational
metrics of each component and identifies a set of representative
steady components based on the convergences of their states.
Then it generates a state model and a state transit model for each
steady cloud component. Finally, it detects behavior anomalies of
these steady components in near real-time and meanwhile evolve
behavior models on the fly. The evaluation results of this
approach in a commercial large-scale PaaS (Platform-as-a-Service)
cloud are demonstrated its capability and efficiency.
",17.755912252390015,15.607085514834207,192
CLOUD_17_021.txt,15.164789057053063,13.430027202879227,CLOUD,5730,"
High-performance analysis of big data demands
more computing resources, forcing similar growth in computation
cost. So, the challenge to the HPC system designers is not providing high performance only but providing high performance at
lower cost. With this motivation, We propose a theoretical model
augmenting Amdahls second law for balanced system to optimize
cost/performance. We express the optimal balance among CPUspeed, I/O-bandwidth and DRAM-size (i.e., Amdahls I/O- and
memory-number) in terms of application characteristics and
hardware cost. Considering Xeon processor and recent hardware
prices, we showed that a system needs almost 0.17GBPS I/Obandwidth and 3GB DRAM per GHz CPU-speed to minimize the
cost/performance for data- and compute-intensive applications.

To substantiate our claim, we evaluate three different cluster
architectures: 1) SupermikelI, a traditional HPC cluster, 2)
SwatIII, a regular datacenter, and 3) CeresII, a MicroBrickbased novel hyperscale system. CeresII with 6-Xeon-D1541 cores
(2GHz/core), 1-NVMe SSD (2GBPS I/O-bandwidth) and 64GB
DRAM per node, closely resembles the optimum produced by
our model. Consequently, in terms of cost/performance CeresII
outperformed both Supermikell (by 65-85%) and SwatIII (by
40-50%) for data- and compute-intensive Hadoop benchmarks
(TeraSort and WordCount) and our own benchmark genome
assembler based on Hadoop and Giraph.

Index Terms—component; formatting; style; styling;

",16.860833078287435,15.34413273001508,224
CLOUD_17_022.txt,14.283834923507609,12.130089964560632,CLOUD,6443,"
We present Timestamp Order Preserving (TOP),
a replicated state machine (RSM) protocol that exploits the
synchrony of networks to provide high performance. TOP uses
physical timestamp of synchronized clock as a consistent total
order to achieve consensus. It keeps estimating the bounds of
network latency and offset of synchronized clock to deduce the
commit time for each operation. It adopts speculative processing and reconciliation techniques to improve performance. To
demonstrate its advantages, we implement a key-value data store
that uses TOP for data replication. Through evaluations in a
geo-deployed testbed, by comparing it with Primary-Copy and
Quorum-Replication protocols, we demonstrate that TOP has a
similar commit latency with a higher sustainable throughput.
In addition, it processes operations in the order of submission
timestamp, which provides a stricter form of consistency.

",16.439396014739867,14.507057569296382,135
CLOUD_17_023.txt,14.532932828563307,12.783264861714184,CLOUD,4468,"
Software-defined datacenters combine centralized
resource management, software-defined networking, and virtualized infrastructure to meet diverse requirements of cloud
computing. To fully realizing their capability in traffic engineering and flow-based bandwidth management, it is critical for the
switches to measure network traffic for both individual flows
between virtual machines and aggregate flows between clusters
of physical or virtual machines. This paper proposes a novel
hierarchical traffic measurement scheme for software-defined
datacenter networks. It measures both aggregate flows and
individual flows that are organized in a hierarchy with an arbitrary number of levels. The measurement is performed based
on a new concept of hierarchical virtual counter arrays, which
record each packet only once by updating a single counter,
yet the sizes of all flows that the packet belongs to will be
properly updated. We demonstrate that the new measurement
scheme not only supports hierarchical traffic measurement with
accuracy, but does so with memory efficiency, using a fewer
number of counters than the number of flows.

",17.879347455551382,17.449397590361446,167
CLOUD_17_024.txt,15.640414991413621,14.103741411267578,CLOUD,3348,"
With the growing energy demands from server
farms, it becomes necessary to understand the tradeoffs between
energy consumption and application performance. Typically,
server farms are provisioned for peak load even when they
are mostly operating at low utilization levels. This results in
wasteful energy consumption. At the same time, application
workloads have Quality of Service (QoS) constraints that need
to be satisfied. Optimizing server farm energy consumption with
QoS constraints is a challenging task since the workload can
have variabilities in job sizes, job arrival patterns and system
utilization levels.

In this paper, we present WASP, where we explore techniques
that make smart use of the processor and system low-power
states, and orchestrate their use with workload adaptivity for
more effective energy management. We perform an extensive
study of Energy-Latency tradeoffs with simulations, and evaluate
WASP on a testbed with a cluster of servers. Our experiments
on real systems show that WASP achieves up to 57% energy
reduction over a naive policy that uses a shallow processor sleep
state when there are no jobs to execute, and 39% over a delaytimer based approach while maintaining the 90"" percentile job
service latency to be under 2x job execution time.

Keywords-Cloud Computing; Data Center Energy Optimization; Energy-Latency tradeoffs; Processor and System low-power
states; Workload adaptivity

",16.860833078287435,15.399785932721713,219
CLOUD_17_025.txt,15.625642990384431,15.075853932216162,CLOUD,6359,"
In this paper we describe a new, efficient predictive scheduling methodology for implementing computing
infrastructure power savings using private clouds. Our approach, termed “QPRED,” estimates the quantiles on the
distribution of future machine usage so that unneeded machines
may be powered down to save power. A cloud administrator
sets a bound on the probability that all available machines
will be powered down when a cloud request arrives. This
target probability is the basis of a Service Level Agreement
between the cloud administrator and all cloud users covering
start-up delay resulting from power savings. Our results,
validated using activity traces from several private clouds used
in commercial production, indicate that QPRED successfully
reduces power consumption substantially while maintaining the
SLAs specified by the cloud administrator.

Keywords-cloud power optimization, cloud workload, performance evaluation

",16.92669308720184,16.38883458646617,134
CLOUD_17_026.txt,17.386203981419502,16.273593981171075,CLOUD,7087,"
 Providers of computing services such as data science clouds need to maintain large hardware infrastructures often
with thousands of nodes. Using commodity hardware leads to heterogeneous setups that differ significantly in individual nodes’
performance, which must be understood to allow for accounting,
strategic planning, and to identify problems and bottlenecks. Today’s method of choice are active benchmarks, but they disturb
normal operations and are too expensive to run continuously.
They also struggle to be representative of an ever changing workload. We therefore design a passive benchmarking technique,
which computes expressive and accurate performance metrics
based on actual workloads. We prove the quality and performance
benefits of our passive benchmark on a practical workload in one
of the world’s largest scientific computing infrastructures, the
CERN Computing Center. In fact, our approach allows continuous benchmarking of the active system, while avoiding costs in
terms of downtime and achieves prediction quality comparable to
the state-of-the-art approach of active benchmarking.

Keywords—benchmarking; performance; computing infrastructure management; cloud computing

",16.52667757954773,15.162028443113773,170
CLOUD_17_027.txt,15.240625351559292,13.538495738979215,CLOUD,6690,"
The efficiency of datacenters is important
consideration for cloud service providers to make their
datacenters always ready for fulfilling the increasing demand for
computing resources. Container-based virtualization is one
approach to improving efficiency by reducing the overhead of
virtualization. Resource overcommitment is another approach,
but cloud providers tend to make conservative allocations of
resources because there is no good understanding of the
relationship between physical resource overcommitment and its
impact on performance. This paper presents a quantitative study
of performance degradation of containerized workloads due to
memory overcommitment and a technique to mitigate it. We
focused on physical memory overcommitment, where the sum of
the working set memory is larger than the physical memory. We
drove a small fraction of Docker containers at a high load level
and the rest of them at a very low load level to emulate a common
usage pattern of cloud datacenters. Detailed measurements
revealed it is difficult to predict how many additional containers
can be launched before thrashing hurts performance. We show
that tuning the per-container swappiness of heavily loaded
containers is effective for launching a larger number of
containers and that it achieves an overcommitment of about
three times.
",18.243605946275583,16.649368686868687,199
CLOUD_17_028.txt,14.540443327349916,12.92453045822717,CLOUD,6782,"
Network function virtualization has emerged as a
promising technology to enable rapid network service composition/innovation, energy conservation and cost minimization for
network operators. To optimally operate a virtualized network
service, it is of key importance to optimally deploy a VNF
(virtualized network function) service chain within the provisioning infrastructure (e.g., servers and the network within a
cloud datacenter), and dynamically scale it in response to flow
traffic changes. Most of the existing work on VNF scaling assume
access to precise network bandwidth information for placement
decisions, while in reality, network bandwidth typically fluctuates
following an unknown pattern and an effective way to adapt
to it is to do trials. In this paper, we address dynamic VNF
service chain deployment and scaling by a novel combination
of an online provisioning algorithm and a multi-armed bandit
optimization framework, which exploits online learning of the
available bandwidths to enable optimal deployment of a scaled
service chain. Specifically, we adopt the online algorithm to
minimize the cost for provisioning VNF instances on the go,
and a bandit-based online learning algorithm to place the
VNF instances which minimizes the congestion in a datacenter
network. We demonstrate effectiveness of our algorithms using
solid theoretical analysis and trace-driven evaluation.

",20.267338824336647,21.228155339805827,208
CLOUD_17_029.txt,14.878705028822601,12.82111552972367,CLOUD,5566,"
Hadoop clusters have been transitioning from a
dedicated cluster environment to a shared cluster environment.
This trend has resulted in the YARN container abstraction
that isolates computing tasks from physical resources. With
YARN containers, Hadoop has expanded to support various
distributed frameworks. However, it has been reported that
Hadoop tasks suffer from a significant overhead of container
relaunch. In order to reduce the container overhead without
making significant changes to the existing YARN framework,
we propose leveraging the input split, which is the logical
representation of physical HDFS blocks. Our assorted block
coalescing scheme combines multiple HDFS blocks and creates
large input splits of various sizes, reducing the number of
containers and their initialization overhead. Our experimental
study shows the assorted block coalescing scheme reduces the
container overhead by a large margin while it achieves good
load balance and job scheduling fairness without impairing the
degree of overlap between map phase and reduce phase.

",17.613555460941566,15.51727272727273,155
Cluster_17_001.txt,14.406395711149063,12.775509581447288,Cluster,8053,"
Subgraph pattern matching is fundamental to graph
analytics and has wide applications. Unfortunately, high computational complexity limits the robustness guarantees of existing
algorithms: they do not scale for modern large graph datasets
and/or they have limitations in terms of accuracy or in terms of
the intricacy of the patterns supported. We present algorithms,
theory, and empirical evidence that iteratively eliminating vertices that do not meet local constraints dramatically reduces the
search space for pattern matching in real-world graphs, and
demonstrate a scalable implementation of our algorithms. We
additionally identify the characteristics of patterns for which
every non-eliminated vertex participates in a match. These
techniques are an essential step to enable scalable, practical
solutions for robust pattern matching in large-scale labeled
graphs. We demonstrate the advantages of the proposed approach
through strong and weak scaling experiments on massive-scale
real-world (up to 257 billion edges) and synthetic (up to 2.2
trillion edges) graphs and at scales (256 compute nodes with
6,144 processors) orders of magnitude larger than those used in
the past for similar problems.
",18.7741,18.06287709497207,182
Cluster_17_002.txt,14.197763806415818,12.048516071852791,Cluster,7183,"
Multi-/many-core CPU based architectures are seeing widespread adoption due to their unprecedented compute
performance in a small power envelope. With the increasingly
large number of cores on each node, applications spend a
significant portion of their execution time in intra-node communication. While shared memory is commonly used for intranode communication, it needs to copy each message once at the
sender and once at the receiver side. Kernel-assisted mechanisms
transfer a message using a single copy but suffer from significant contention with a large number of concurrent accesses.
Consequently, naively using Kernel-assisted copy techniques in
collectives can lead to severe performance degradation. In this
work, we analyze and propose a model to quantify the contention
and design collective algorithms to avoid this bottleneck. We
evaluate the proposed designs on three different architectures Xeon, Xeon Phi, and OpenPOWER and compare them against
state-of-the-art MPI libraries - MVAPICH2, Intel MPI, and Open
MPI. Our designs show up to 50x improvement for One-to-all
and All-to-one collectives (Scatter and Gather) and up to 5x
improvement for All-to-all collectives (Allgather and Alltoall).

Keywords-Collective Communication, Kernel-assisted, CrossMemory Attach, Multi-core, CMA, MPI, HPC

",16.32212239822248,14.547860696517414,202
Cluster_17_003.txt,15.236562217415912,13.942775862683273,Cluster,7985,"
We investigate efficient sensitivity analysis (SA) of
algorithms that segment and classify image features in a large
dataset of high-resolution images. Algorithm SA is the process
of evaluating variations of methods and parameter values to
quantify differences in the output. A SA can be very compute
demanding because it requires re-processing the input dataset
several times with different parameters to assess variations in
output. In this work, we introduce strategies to efficiently speed
up SA via runtime optimizations targeting distributed hybrid
systems and reuse of computations from runs with different
parameters. We evaluate our approach using a cancer image
analysis workflow on a hybrid cluster with 256 nodes, each
with an Intel Phi and a dual socket CPU. The SA attained a
parallel efficiency of over 90% on 256 nodes. The cooperative
execution using the CPUs and the Phi available in each node
with smart task assignment strategies resulted in an additional
speedup of about 2x. Finally, multi-level computation reuse
lead to an additional speedup of up to 2.46 on the parallel
version. The level of performance attained with the proposed
optimizations will allow the use of SA in large-scale studies.

",16.728156217252725,15.37076923076923,197
Cluster_17_004.txt,15.259470606565635,13.630490437312233,Cluster,5960,"
The availability and amount of sequenced
genomes have been rapidly growing in recent years because of
the adoption of next-generation sequencing (NGS) technologies
that enable high-throughput short-read generation at highly
competitive cost. Since this trend is expected to continue in the
foreseeable future, the design and implementation of efficient
and scalable NGS bioinformatics algorithms are important to
research and industrial applications. In this paper, we introduce
S-Aligner—a highly scalable read mapper designed for the
Sunway Taihu Light supercomputer and its fourth-generation
ShenWei many-core architecture (SW26010). S-Aligner employs a combination of optimization techniques to overcome
both the memory-bound and the compute-bound bottlenecks
in the read mapping algorithm. In order to make full use of
the compute power of Sunway Taihu Light, our design employs
three levels of parallelism: (1) internode parallelism using
MPI based on a task-grid pattern, (2) intranode parallelism
using multithreading and asynchronous data transfer to fully
utilize all 260 cores of the SW26010 many-core processor,
and (3) vectorization to exploit the available 256-bit SIMD
vector registers. Moreover, we have employed asynchronous
access patterns and data-sharing strategies during file I/O to
overcome bandwidth limitations of the network file system.
Our performance evaluation demonstrates that S-Aligner scales
almost linearly with approximately 95% efficiency for up to
13,312 nodes (concurrently harnessing more than 3 million
compute cores). Furthermore, our implementation on a single
node outperforms the established RazerS3 mapper running
on a platform with eight Intel Xeon E7-8860v3 CPUs while
achieving highly competitive alignment accuracy.

",19.906493383657665,20.05558908045977,263
Cluster_17_005.txt,14.871198956971423,13.294945217648799,Cluster,6691,"
Sparse tensors appear in many large-scale applications with multidimensional and sparse data. While multidimensional sparse data often need to be processed on manycore
processors, attempts to develop highly-optimized GPU-based implementations of sparse tensor operations are rare. The irregular
computation patterns and sparsity structures as well as the
large memory footprints of sparse tensor operations make such
implementations challenging. We leverage the fact that sparse
tensor operations share similar computation patterns to propose
a unified tensor representation called F-COO. Combined with
GPU-specific optimizations, F-COO provides highly-optimized
implementations of sparse tensor computations on GPUs. The
performance of the proposed unified approach is demonstrated
for tensor-based kernels such as the Sparse Matricized TensorTimes-Khatri-Rao Product (SpMTTKRP) and the Sparse TensorTimes-Matrix Multiply (SpTTM) that are used in tensor decomposition algorithms. Compared to state-of-the-art work we
improve the performance of SpTTM and SpMTTKRP up to
3.7 and 30.6 times respectively on NVIDIA Titan-X GPUs. We
implement the CANDECOMP/PARAFAC (CP) decomposition
and achieve up to 14.9 times speedup using the unified method
over state-of-the-art libraries on NVIDIA Titan-X GPUs.

",17.451712890111917,17.10922619047619,196
Cluster_17_006.txt,15.356536834767265,13.546836980552037,Cluster,7276,"

Scientific simulations on high performance computing
(HPC) platforms generate large quantities of data. To bridge
the widening gap between compute and I/O, and enable data
to be more efficiently stored and analyzed, simulation outputs
need to be refactored, reduced, and appropriately mapped to
storage tiers. However, a systematic solution to support these
steps has been lacking on the current HPC software ecosystem.
To that end, this paper develops Canopus, a progressive JPEGlike data management scheme for storing and analyzing big
scientific data. It co-designs the data decimation, compression
and data storage, taking the hardware characteristics of each
storage tier into considerations. With reasonably low overhead,
our approach refactors simulation data into a much smaller,
reduced-accuracy base dataset, and a series of deltas that is
used to augment the accuracy if needed. The base dataset and
deltas are compressed and written to multiple storage tiers.
Data saved on different tiers can then be selectively retrieved
to restore the level of accuracy that satisfies data analytics.
Thus, Canopus provides a paradigm shift towards elastic data
analytics and enables end users to make trade-offs between
analysis speed and accuracy on-the-fly. We evaluate the impact
of Canopus on unstructured triangular meshes, a pervasive
data model used by scientific modeling and simulations. In
particular, we demonstrate the progressive data exploration of
Canopus using the “blob detection” use case on the fusion
simulation data.

",16.908762484321528,15.449098679098679,235
Cluster_17_007.txt,14.085367993212849,12.20279557940555,Cluster,5227,"
Reading and writing data efficiently from storage
system is necessary for most scientific simulations to achieve
good performance at scale. Many software solutions have been
developed to decrease the I/O bottleneck. One well-known strategy, in the context of collective I/O operations, is the twophase I/O scheme. This strategy consists of selecting a subset
of processes to aggregate contiguous pieces of data before
performing reads/writes. In this paper, we present TAPIOCA, an
MPI-based library implementing an efficient topology-aware twophase I/O algorithm. We show how TAPIOCA can take advantage
of double-buffering and one-sided communication to reduce as
much as possible the idle time during data aggregation. We also
introduce our cost model leading to a topology-aware aggregator
placement optimizing the movements of data. We validate our
approach at large scale on two leadership-class supercomputers:
Mira (IBM BG/Q) and Theta (Cray XC40). We present the
results obtained with TAPIOCA on a micro-benchmark and the
TV/O kernel of a large-scale simulation. On both architectures, we
show a substantial improvement of I/O performance compared
with the default MPI I/O implementation. On BG/Q+GPFS, for
instance, our algorithm leads to a performance improvement by
a factor of twelve while on the Cray XC40 system associated
with a Lustre filesystem, we achieve an improvement of four.

",15.549919911452193,13.604200079396588,229
Cluster_17_008.txt,14.97696229002403,13.33225304107124,Cluster,3319,"
The increasingly growing data sets processed on
HPC platforms raise major challenges for the underlying storage
layer. A promising alternative to POSIX-IO- compliant file
systems are simpler blobs (binary large objects), or object storage
systems. They offer lower overhead and better performance
at the cost of largely unused features such as file hierarchies
or permissions. Similarly, blobs are increasingly considered for
replacing distributed file systems for big data analytics or as a
base for storage abstractions like key-value stores or time-series
databases. This growing interest in such object storage on HPC
and big data platforms raises the question: Are blobs the right
level of abstraction to enable storage-based convergence between
HPC and Big Data? In this paper we take a first step towards
answering the question by analyzing the applicability of blobs
for both platforms.

",15.247664890283005,14.243561151079138,140
Cluster_17_009.txt,15.87121376695405,14.446414277839033,Cluster,3640,"
Burst Buffer is an effective solution for reducing
the data transfer time and the I/O interference in HPC systems.
Extending Burst Buffers (BBs) to handle Big Data applications is
challenging because BBs must account for the large data inputs
of Big Data applications and the performance guarantees of HPC
applications - which are considered as first-class citizens in HPC
systems. Existing BBs focus on only intermediate data of Big
Data applications and incur a high performance degradation
of both Big Data and HPC applications. We present Eley, a
burst buffer solution that helps to accelerate the performance
of Big Data applications while guaranteeing the performance
of HPC applications. In order to improve the performance of
Big Data applications, Eley employs a prefetching technique that
fetches the input data of these applications to be stored close to
computing nodes thus reducing the latency of reading data inputs.
Moreover, Eley is equipped with a full delay operator to guarantee
the performance of HPC applications - as they are running
independently on a HPC system. The experimental results show
the effectiveness of Eley in obtaining shorter execution time of
Big Data applications (shorter map phase) while guaranteeing
the performance of HPC applications.

Keywords—HPC, MapReduce, Big Data, Parallel File Systems,
Burst Buffers, Interference, Prefetch.

",17.410965686947208,16.450752369668248,212
Cluster_17_010.txt,17.306823865275412,15.997404811924774,Cluster,6414,"
Monitoring and assessing the energy efficiency of
supercomputers and data centers is crucial in order to limit
and reduce their energy consumption. Applications from the
domain of High Performance Computing (HPC), such as MPI
applications, account for a significant fraction of the overall
energy consumed by HPC centers. Simulation is a popular
approach for studying the behavior of these applications in
a variety of scenarios, and it is therefore advantageous to
be able to study their energy consumption in a cost-efficient,
controllable, and also reproducible simulation environment. Alas,
simulators supporting HPC applications commonly lack the
capability of predicting the energy consumption, particularly
when target platforms consist of multi-core nodes. In this work,
we aim to accurately predict the energy consumption of MPI
applications via simulation. Firstly, we introduce the models
required for meaningful simulations: The computation model,
the communication model, and the energy model of the target
platform. Secondly, we demonstrate that by carefully calibrating
these models on a single node, the predicted energy consumption
of HPC applications at a larger scale is very close (within a
few percents) to real experiments. We further show how to
integrate such models into the SimGrid simulation toolkit. In
order to obtain good execution time predictions on multi-core
architectures, we also establish that it is vital to correctly account
for memory effects in simulation. The proposed simulator is
validated through an extensive set of experiments with wellknown HPC benchmarks. Lastly, we show the simulator can
be used to study applications at scale, which allows researchers
to save both time and resources compared to real experiments.

",18.243605946275583,16.826586620926246,266
Cluster_17_011.txt,16.107729718441814,14.757680600566111,Cluster,6307,"
The performance model of an application can provide understanding about its runtime behavior on particular
hardware. Such information can be analyzed by developers
for performance tuning. However, model building and analyzing is frequently ignored during software development until
performance problems arise because they require significant
expertise and can involve many time-consuming application
runs. In this paper, we propose a fast, accurate, flexible and
user-friendly tool, Mira, for generating performance models
by applying static program analysis, targeting scientific applications running on supercomputers. We parse both the
source code and binary to estimate performance attributes with
better accuracy than considering just source or just binary
code. Because our analysis is static, the target program does
not need to be executed on the target architecture, which
enables users to perform analysis on available machines instead
of conducting expensive experiments on potentially expensive
resources. Moreover, statically generated models enable performance prediction on nonexistent or unavailable architectures.
In addition to flexibility, because model generation time is significantly reduced compared to dynamic analysis approaches,
our method is suitable for rapid application performance
analysis and improvement. We present empirical validation
results to demonstrate the current capabilities of our approach
on small benchmarks and a mini application.

Keywords-performance, static analysis, polyhedral model

",19.083932058031824,18.819231884057974,208
Cluster_17_012.txt,15.249916365433002,14.297123172888934,Cluster,7891,"
Modern task parallel programming models provide sophisticated runtime task schedulers for handling the
scheduling of logical tasks on a large and varying number of
hardware parallel resources at runtime. The performance of
these programming models increasingly rely on how fast their
runtime schedulers do their job. The more delay a scheduler
incurs in matching a ready task to a free processor core at
any point in time, the more impact it causes to the program’s
parallel execution. We have developed a tool that is able to
detect these delayed intervals caused by the scheduler in a
parallel execution, and spot them specifically on two kinds
of visualizations: the logical task graph captured at runtime
(DAG visualizations) and time-series visualizations of threads
(timelines). By further analyzing positions of these delays on
those visualizations the tool could identify possible scheduling
issues in the scheduler that causes these delays, yielding
improvement insights for the development of task parallel
programming models. From an application programmer’s perspective, our tool is useful by being able to contrast differences
of various task parallel programming models executing the
same program, helping users choose the right model for their
application. We demonstrate that usefulness by using the tool
to analyze 10 applications in BOTS benchmark suite in our
case studies.

Keywords-task parallel; runtime scheduler; scheduling delay;
performance analysis; scalability factors;

",19.032712561301913,17.452550448430497,226
Cluster_17_013.txt,17.904757030726422,17.0796358314544,Cluster,6605,"
Multi-tiered memory systems, such as those based
on Intel® Xeon Phi™ processors, are equipped with several memory tiers with different characteristics including, among others,
capacity, access latency, bandwidth, energy consumption, and
volatility. The proper distribution of the application data objects
into the available memory layers is key to shorten the time—
to-solution, but the way developers and end-users determine
the most appropriate memory tier to place the application data
objects has not been properly addressed to date.

In this paper we present a novel methodology to build an
extensible framework to automatically identify and place the
application’s most relevant memory objects into the Intel Xeon
Phi fast on-package memory. Our proposal works on top of inproduction binaries by first exploring the application behavior
and then substituting the dynamic memory allocations. This
makes this proposal valuable even for end-users who do not
have the possibility of modifying the application source code. We
demonstrate the value of a framework based in our methodology
for several relevant HPC applications using different allocation
strategies to help end-users improve performance with minimal
intervention. The results of our evaluation reveal that our
proposal is able to identify the key objects to be promoted
into fast on-package memory in order to optimize performance,
leading to even surpassing hardware-based solutions.

Index Terms—heterogeneous memory, hybrid memory, highbandwidth memory, performance analysis, PEBS, sampling, instrumentation

",20.503739492662863,19.5666755319149,235
Cluster_17_014.txt,16.657142383477613,15.348263582966226,Cluster,7180,"
This paper addresses the shared resource contention problem associated with the auto-parallelization of
running queries in distributed stream processing engines. In
such platforms, analyzing a large amount of data often requires
to execute user-defined queries over continues raw-inputs in a
parallel fashion at each single host. However, previous studies
showed that the collocated applications can fiercely compete
for shared resources, resulting in a severe performance degradation among applications. This paper presents an advanced
resource allocation strategy for handling scenarios in which
the target applications have different quality of service (QoS)
requirements while shared-resource interference is considered
as a key performance-limiting parameter.

To properly allocate the best possible resource to each query,
the proposed controller predicts the performance degradation
of the running pane-level as well as the window-level queries
when co-running with other queries. This is addressed as an
optimization problem where a set of cost functions is defined to
achieve the following goals: a) reduce the sum of QoS violation
incidents over all machines; b) keep the CPU utilization
level within an accepted range; and c) avoid fierce shared
resource interference among collocated applications. Particle
swarm optimization is used to find an acceptable solution
at each round of the controlling period. The performance
of the proposed solution is benchmarked with Round-Robin
and best-effort strategies, and the experimental results clearly
demonstrate that the proposed controller has the following
advantages over its opponents: it increases the overall resource
utilization by 15% on average while can reduce the average
tuple latencies by 14%. It also achieves an average 123%
improvement in preventing QoS violation incidents.

Keywords-Stream Processing, Parallelization Factor, Shared
Resource Interference, Workload Consolidation

",19.1858809040293,18.50587234042553,283
Cluster_17_015.txt,14.699224794949302,13.083297210865535,Cluster,3341,"
 The increase in memory capacity is substantially behind the increase in computing power in today’s supercomputers.
In order to alleviate the effect of this gap, diverse options such as
NVM - non-volatile memory (less expensive but slow) and HBM high bandwidth memory (fast but expensive) are being explored.
In this paper, we present a common approach using parallel
runtime techniques for utilizing NVM and HBM as extensions of
the existing memory hierarchy. We evaluate our approach using
matrix-matrix multiplication kernel implemented in CHARM++
and show that applications with memory requirement four times
the HBM/DRAM capacity can be executed efficiently using
significantly less total resources.

",18.511140095513987,17.77518518518519,108
Cluster_17_016.txt,17.277507759449808,15.434021282541174,Cluster,8640,"
While the memory footprints of cloud and HPC applications
continue to increase, fundamental issues with DRAM scaling are
likely to prevent traditional main memory systems, composed
of monolithic DRAM, from greatly growing in capacity. Hybrid
memory systems can mitigate the scaling limitations of monolithic DRAM by pairing together multiple memory technologies
(e.g., different types of DRAM, or DRAM and non-volatile memory) at the same level of the memory hierarchy. The goal of a
hybrid main memory is to combine the different advantages of
the multiple memory types in a cost-effective manner while
avoiding the disadvantages of each technology. Memory pages are placed in and migrated between the different memories
within a hybrid memory system, based on the properties of each
page. It is important to make intelligent page management (i.e.,
placement and migration) decisions, as they can significantly
affect system performance.

In this paper, we propose utility-based hybrid memory management (UH-MEM), a new page management mechanism
for various hybrid memories, that systematically estimates the
utility (i.e. the system performance benefit) of migrating a page
between different memory types, and uses this information to
guide data placement. UH-MEM operates in two steps. First, it
estimates how much a single application would benefit from
migrating one of its pages to a different type of memory, by
comprehensively considering access frequency, row buffer locality, and memory-level parallelism. Second, it translates the
estimated benefit of a single application to an estimate of the
overall system performance benefit from such a migration.

We evaluate the effectiveness of UH-MEM with various types
of hybrid memories, and show that it significantly improves
system performance on each of these hybrid memories. For a
memory system with DRAM and non-volatile memory, UHMEM improves performance by 14% on average (and up to 26%)
compared to the best of three evaluated state-of-the-art mechanisms across a large number of data-intensive workloads.

",19.917542046945165,18.104304729538374,325
Cluster_17_017.txt,15.677902131507718,13.977142908867084,Cluster,8327,"
A heterogeneous memory system (HMS) consists of
multiple memory components with different properties. GPU is a
representative architecture with HMS. It is challenging to decide
optimal placement of data objects on HMS because of the large
exploration space and complicated memory hierarchy on HMS.
In this paper, we introduce performance modeling techniques
to predict performance of various data placements on GPU. In
essence, our models quantify and capture implicit performance
correlation between different data placements. Given the memory
access information and performance of a sample data placement,
our models predict performance for other data placements based
on the quantified correlation. We reveal critical performance
factors that cause performance variation across data placements.
Those factors include instruction replay, addressing mode, hardware queuing delay of memory requests, off-chip memory access
latency, and caching effects. Those factors, which are often not
sufficiently considered in the existing performance models, can
significantly impact modeling accuracy. We introduce a series
of techniques to model those factors. We extensively evaluate
our models with a variety of benchmarks with various data
placements. Our models are able to quantitatively predict the
benefit or performance loss of data placements.

",15.359359093739592,14.495526315789476,191
Cluster_17_018.txt,15.65548789932468,14.196482433235243,Cluster,6174,"
Designing low-latency network topologies of
switches is a key objective for next-generation large-scale
clusters. Low latency is preconditioned on low hop counts, but
existing network topologies have hop counts much larger than
theoretical lower bounds. To alleviate this problem, we propose
building network topologies based on uni-directional graphs that
are known to have hop counts close to theoretical lower bounds.
A practical difficulty with uni-directional topologies is switchby-switch flow control, which we resolve by using hot-potato
routing. Cycle-accurate network simulation experiments for
various traffic patterns on uni-directional topologies show that
hot-potato routing achieves performance comparable to that of
conventional deadlock-free routing. Similar experiments are used
to compare several uni-directional topologies to bi-directional
topologies, showing that the former achieve significantly
lower latency and higher throughput. We quantify end-to-end
application performance for parallel application benchmarks via
discrete-even simulation, showing that uni-directional topologies
can lead to large application performance improvements over
their bi-directional counterparts. Finally, we discuss practical
issues for uni-directional topologies such as cabling complexity
and cost, power consumption, and soft-error tolerance. Our
results make a compelling case for considering uni-directional
topologies for upcoming large-scale clusters.

Index Terms—HPC clusters, interconnection networks, unidirectional network topologies, hot-potato routing

",17.916177094544413,17.54044036697248,219
Cluster_17_019.txt,17.221728442212488,16.07098783454988,Cluster,3369,"
We present a multi-word atomic (1,N) register
for multi-core machines exploiting Read-Modify-Write (RMW)
instructions to coordinate the writer and the readers in a waitfree manner. Our proposal, called Anonymous Readers Counting (ARC), enables large-scale data sharing by admitting up
to 2°? — 2 concurrent readers on off-the-shelf 64-bit machines,
as opposed to the most advanced RMW-based approach which
is limited to 58 readers. Further, ARC avoids multiple copies
of the register content while accessing it—this affects classical
register’s algorithms based on atomic read/write operations
on single words. Thus, ARC allows for higher scalability with
respect to the register size.

",16.52667757954773,15.324391891891896,112
Cluster_17_020.txt,16.41646610009324,15.136953999595189,Cluster,7731,"
High-radix, low-diameter, hierarchical networks
based on the Dragonfly topology are common picks for building
next generation HPC systems. However, effective tools are lacking
for analyzing the network performance and exploring the design
choices for such emerging networks at scale. In this paper, we
present visual analytics methods that couple data aggregation
techniques with interactive visualizations for analyzing largescale Dragonfly networks. We create an interactive visual analytics system based on these techniques. To facilitate effective analysis and exploration of network behaviors, our system provides
intuitive, scalable visualizations that can be customized to show
various traffic characteristics and correlate between different
performance metrics. Using high-fidelity network simulation and
HPC applications communication traces, we demonstrate the
usefulness of our system with several case studies on exploring
network behaviors at scale with different workloads, routing
strategies, and job placement policies. Our simulations and
visualizations provide valuable insights for mitigating network
congestion and inter-job interference.

Index Terms—visualization, dragonfly networks, visual analytics, performance analysis

",18.7741,18.634777607361965,164
Cluster_17_021.txt,15.996773329384947,15.158164583296053,Cluster,6555,"
HPC systems have shifted to burst buffer storage
and high-radix interconnect topologies in order to meet the
challenges of large-scale, data-intensive scientific computing.
Both of these technologies have been studied in detail independently, but the interaction between them is not well understood.
V/O traffic and communication traffic from concurrently scheduled applications may interfere with each other in unexpected
ways, and this behavior may vary considerably depending on
resource allocation, scheduling, and routing policies.

In this work, we analyze I/O and network traffic interference
on burst-buffer-equipped dragonfly-based systems using the
high-resolution packet-level simulation provided by the CODES
storage and interconnect simulation framework. The analysis
is performed using realistic I/O workload sizes, a variety of
resource allocation and network routing strategies employed
in production environments, and a dragonfly network configuration modeled after current vendor options. We analyze the
impact of interference on both I/O and communication traffic.

We observe that although average network packet latency
is stable across a wide variety of configurations, the maximum
network packet latency in the presence of concurrent I/O traffic
is highly sensitive to subtle policy changes. Our simulations
reveal a worst-case single packet latency of 4,700 times the average latency for sub-optimal configurations. While a topologyaware mapping of compute nodes to burst buffer storage nodes
can minimize the variation in maximum packet latency, it can
slow down the I/O traffic by creating contention on the burst
buffer nodes. Overall, balancing I/O and network performance
requires careful selection of routing, data placement, and job
placement policies.

Keywords-burst buffer, dragonfly networks, discrete-event
simulation, checkpoint, I/O and communication traffic

",17.74463917768843,17.389415584415584,282
Cluster_17_022.txt,16.09822454788146,14.180776887698922,Cluster,7821,"
Data reduction through compression is emerging
as a promising approach to ease I/O costs for simulation codes
on supercomputers. Typically, this compression is achieved by
techniques that operate on individual time slices. However, as
simulation codes advance in time, outputting multiple time
slices as they go, the opportunity for compression incorporating
the time dimension has not been extensively explored. Moreover, recent supercomputers are increasingly equipped with
deeper memory hierarchies, including solid state drives and
burst buffers, which creates the opportunity to temporarily
store multiple time slices and then apply compression to
them all at once, i.e., spatiotemporal compression. This paper
explores the benefits of incorporating the time dimension into
existing wavelet compression, including studying its key parameters and demonstrating its benefits in three axes: storage,
accuracy, and temporal resolution. Our results demonstrate
that temporal compression can improve each of these axes,
and that the impact on performance for real systems, including
tradeoffs in memory usage and execution time, is acceptable.
We also demonstrate the benefits of spatiotemporal wavelet
compression with real-world visualization use cases and tailored evaluation metrics.

",19.854308518879517,18.563015873015875,182
Cluster_17_023.txt,14.684602338530102,12.634310603020484,Cluster,2878,"
As hardware vendors provision more cores and
faster storage devices, attaining fast data durability for concurrent file writes is demanding to high-performance storage systems
in cluster systems. We approach the challenge by proposing a
system that uses a small amount of fast persistent memory for
buffering concurrent file writes while preserving data durability.
The main issue in designing a durable file buffer is allowing
concurrent file writes to store data in a shared and limited
space of persistent memory without incurring lock or resource
contention. This paper addresses such issue and presents AUTOBAHN, a durable file buffer that expedites file I/O operations.

Keywords-write buffer; file system; concurrency control; multicore scalability;

",17.353723509956247,15.466477876106197,114
Cluster_17_024.txt,15.98023760975147,14.759382247271368,Cluster,6879,"
In this paper, we present Justice, a fair-share
deadline-aware resource allocator for big data cluster managers.
In resource constrained environments, where resource contention
introduces significant execution delays, Justice outperforms the
popular existing fair-share allocator that is implemented as part
of Mesos and YARN. Justice uses deadline information supplied
with each job and historical job execution logs to implement
admission control. It automatically adapts to changing workload
conditions to assign enough resources for each job to meet
its deadline “just in time.’ We use trace-based simulation of
production YARN workloads to evaluate Justice under different
deadline formulations. We compare Justice to the existing fairshare allocation policy deployed on cluster managers like YARN
and Mesos and find that in resource-constrained settings, Justice
improves fairness, satisfies significantly more deadlines, and
utilizes resources more efficiently.

Keywords—resource-constrained clusters; deadlines; admission
control; resource allocation; big data;

",16.26309291913925,15.95918367346939,148
Cluster_17_025.txt,14.701566558603659,13.59780040222353,Cluster,6271,"
Burst buffers tolerate I/O spikes in HighPerformance Computing environments by using a non-volatile
flash technology. Burst buffers are commonly located between
parallel file systems and compute nodes, handling bursty I/Os
in the middle. In this architecture, burst buffers are shared
resources. The performance of an SSD is significantly reduced
when it is used excessively because of garbage collection, and
we have observed that SSDs in a burst buffer become slow when
many users simultaneously use the burst buffer. To mitigate the
performance problem, we propose a new user-level I/O isolation
framework in a High-Performance Computing environment
using a multi-streamed SSD. The multi-streamed SSD allocates
the same flash block for I/Os in the same stream. We assign
a different stream to each user; thus, the user can use the
stream exclusively. To evaluate the performance, we have used
open-source supercomputing workloads and I/O traces from
real workloads in the Cori supercomputer at the National
Energy Research Scientific Computing Center. Via user-level
I/O isolation, we have obtained up to a 125% performance
improvement in terms of I/O throughput. In addition, our
approach reduces the write amplification in the SSDs, leading
to improved SSD endurance. This user-level I/O isolation
framework could be applied to deployed burst buffers without
having to make any user interface changes.

Keywords-Burst Buffer; SSD; Write Amplification; Multistreamed SSD; Supercomputing Workload

",14.191785591663535,12.819789915966389,239
Cluster_17_026.txt,13.547150406616833,11.618994478974624,Cluster,7490,"
It is common for real-world applications to analyze big graphs using distributed graph processing systems.
Popular in-memory systems require an enormous amount of
resources to handle big graphs. While several out-of-core
approaches have been proposed for processing big graphs on
disk, the high disk I/O overhead could significantly reduce
performance. In this paper, we propose GraphH to enable highperformance big graph analytics in small clusters. Specifically,
we design a two-stage graph partition scheme to evenly divide
the input graph into partitions, and propose a GAB (GatherApply-Broadcast) computation model to make each worker
process a partition in memory at a time. We use an edge
cache mechanism to reduce the disk I/O overhead, and design
a hybrid strategy to improve the communication performance.
GraphH can efficiently process big graphs in small clusters
or even a single commodity server. Extensive evaluations have
shown that GraphH could be up to 7.8x faster compared to
popular in-memory systems, such as Pregel+ and PowerGraph
when processing generic graphs, and more than 100x faster
than recently proposed out-of-core systems, such as GraphD
and Chaos when processing big graphs.

Keywords-Graph Processing, Distributed Computing System,
Network

",15.903189008614273,13.60855721393035,202
Cluster_17_027.txt,16.872065581661165,15.998894891784552,Cluster,5311,"
A proliferation of data from vast networks of remote
sensing platforms (satellites, unmanned aircraft systems (UAS),
airborne etc.), observational facilities (meteorological, eddy covariance etc.), state-of-the-art sensors, and simulation models
offer unprecedented opportunities for scientific discovery. Unsupervised classification is a widely applied data mining approach
to derive insights from such data. However, classification of very
large data sets is a complex computational problem that requires
efficient numerical algorithms and implementations on high performance computing (HPC) platforms. Additionally, increasing
power, space, cooling and efficiency requirements has led to the
deployment of hybrid supercomputing platforms with complex
architectures and memory hierarchies like the Titan system at
Oak Ridge National Laboratory. The advent of such accelerated
computing architectures offers new challenges and opportunities
for big data analytics in general and specifically, large scale
cluster analysis in our case. Although there is an existing body
of work on parallel cluster analysis, those approaches do not
fully meet the needs imposed by the nature and size of our large
data sets. Moreover, they had scaling limitations and were mostly
limited to traditional distributed memory computing platforms.
We present a parallel Multivariate Spatio-Temporal Clustering
(MSTC) technique based on k-means cluster analysis that can
target hybrid supercomputers like Titan. We developed a hybrid
MPI, CUDA and OpenACC implementation that can utilize both
CPU and GPU resources on computational nodes. We describe
performance results on Titan that demonstrate the scalability
and efficacy of our approach in processing large ecological data
sets.

",18.7741,19.076198380566805,248
Cluster_17_028.txt,16.14272403667108,14.63251019660326,Cluster,7761,"
Property-graphs are becoming popular for Intrusion
Detection Systems (IDSs) because they allow to leverage distributed graph processing platforms in order to identify malicious
network traffic patterns. However, a benchmark for studying
their performance when operating on big data has not yet
been reported. In general, benchmarking a system involves the
execution of workloads on datasets, where both of them must
be representative of the application of interest. However, few
datasets containing real network traffic are openly available due
to privacy concerns, which in turn could limit the scope and
results of the benchmark. In this work, we build two synthetic
data generators for benchmarking next generation IDSs by
introducing the support for property-graphs in two well-known
graph generation algorithms: Barabasi-Albert and Kronecker.
We run an extensive experimental evaluation using a publicly
available dataset as seed for the data generation, and we show
that the proposed approach is able to generate synthetic datasets
with high veracity, while also exhibiting linear performance
scalability.

",20.10790988173199,18.234393939393943,166
Cluster_17_029.txt,16.28003928494969,14.943210185462487,Cluster,7911,"
Platform as a Service (PaaS) clouds provide part
of the hardware/software stack and related services to tenant
applications. Increased load is handled elastically by scaling,
which either modifies the number of instances an application
has available on the cloud or increases their available resources.
However, because all these instances run inside isolated containers, experience gained by the first instance of an application
cannot be easily shared with subsequent scaled instances. This
results in both increased startup time and response timeout
errors for the scaled instances as well as increased performance
interference for any co-located applications; reacquiring this
experience is a time-consuming and resource-intensive process.
We propose a scalable and secure technique to share dynamically
compiled artifacts produced by the first execution instance of an
application and otherwise created for intra-OS sharing only with
subsequent scaled or restarted instances as a solution to these
problems. Our solution abides by the usual PaaS limitations and
uses a distributed and containerized cloud service, which we
experimentally show to be scalable on a Docker Swarm running
on top of a 6-VM cluster; also, we discuss the results of a usability
survey for the service’s GUI conducted with expert subjects. The
effectiveness of the DCAS technique was experimentally tested
on an isolated installation of the PaaS software Cloudy Foundry;
we measured significant reductions in both the startup time and
response errors of scaled out instances as well as performance
interference to co-located tenants during scaling.

",19.854308518879517,20.051590514748415,249
Cluster_17_030.txt,15.336099125613678,13.354662379421224,Cluster,5953,"
Nowadays, Graphics Processing Unit (GPU) is essential for general-purpose high-performance computing, because
of its dominant performance in parallel computing compare to
that of CPU. There have been many successful trials on the
use of GPU in virtualized environment. Especially, NVIDIA
Docker obtained a most practical way to bring GPU into the
container-based virtualized environment. However, most of these
trials did not consider sharing GPU among multiple containers.
Without the above consideration, a system will experience a
program failure or a deadlock situation in the worst case. In
this paper, we propose ConVGPU, a solution to share the GPU
in multiple containers. With ConVGPU, the system can guarantee
the required GPU memory which the container needs to execute.
To achieve it, we introduce four scheduling algorithms that
manage the GPU memory to be taken by the containers. These
algorithms can prevent the system from falling into deadlock
situations between containers during execution.

",15.616094167265928,13.074242424242424,155
Cluster_17_031.txt,16.761141444514248,15.69974434855089,Cluster,8195,"
While large-scale simulations have been the hallmark of the High Performance Computing (HPC) community
for decades, Large Scale Data Analytics (LSDA) workloads are
gaining attention within the scientific community not only as
a processing component to large HPC simulations, but also as
standalone scientific tools for knowledge discovery. With the path
towards Exascale, new HPC runtime systems are also emerging
in a way that differs from classical distributed computing models.
However, system software for such capabilities on the latest
extreme-scale DOE supercomputing needs to be enhanced to
more appropriately support these types of emerging software
ecosystems.

In this paper, we propose the use of Virtual Clusters on
advanced supercomputing resources to enable systems to support
not only HPC workloads, but also emerging big data stacks.
Specifically, we have deployed the KVM hypervisor within
Cray’s Compute Node Linux on a XC-series supercomputer
testbed. We also use libvirt and QEMU to manage and provision
VMs directly on compute nodes, leveraging Ethernet-over-Aries
network emulation. To our knowledge, this is the first known
use of KVM on a true MPP supercomputer. We investigate the
overhead our solution using HPC benchmarks, both evaluating
single-node performance as well as weak scaling of a 32-node
virtual cluster. Overall, we find single node performance of our
solution using KVM on a Cray is very efficient with near-native
performance. However overhead increases by up to 20% as
virtual cluster size increases, due to limitations of the Ethernetover-Aries bridged network. Furthermore, we deploy Apache
Spark with large data analysis workloads in a Virtual Cluster,
effectively demonstrating how diverse software ecosystems can
be supported by High Performance Virtual Clusters.

",17.332850977054683,16.297335083688875,279
Cluster_17_032.txt,15.532292216850532,13.923063777845595,Cluster,8280,"
We present EclipseMR, a novel MapReduce framework prototype that efficiently utilizes a large distributed memory in cluster environments. EclipseMR consists of double-layered
consistent hash rings - a decentralized DHT-based file system and
an in-memory key-value store that employs consistent hashing.
The in-memory key-value store in EclipseMR is designed not
only to cache local data but also remote data as well so that
globally popular data can be distributed across cluster servers
and found by consistent hashing.

In order to leverage large distributed memories and increase
the cache hit ratio, we propose a /ocality-aware fair (LAF) job
scheduler that works as the load balancer for the distributed inmemory caches. Based on hash keys, the LAF job scheduler
predicts which servers have reusable data, and assigns tasks
to the servers so that they can be reused. The LAF job
scheduler makes its best efforts to strike a balance between
data locality and load balance, which often conflict with each
other. We evaluate EclipseMR by quantifying the performance
effect of each component using several representative MapReduce
applications and show EclipseMR is faster than Hadoop and
Spark by a large margin for various applications.

Index Terms—MapReduce; Distributed Caching; Consistent
Hashing; Distributed Hash Table;

",17.122413403193683,16.047645631067965,207
Cluster_17_033.txt,15.358212811428274,14.117693250853069,Cluster,4515,"
The last few years saw the emergence of 64-bit
ARM SoCs targeted for mobile systems and servers. Mobileclass SoCs rely on the heterogeneous integration of a mix of CPU
cores, GPGPU cores, and accelerators, whereas server-class SoCs
instead rely on integrating a larger number of CPU cores with no
GPGPU support and a number of network accelerators. Previous
works, such as the Mont-Blanc project, built their prototype
ARM < cluster out of mobile-class SoCs and compared their work
against x86 solutions. These works mainly focused on the CPU
performance. In this paper, we propose a novel ARM-based
cluster organization that exploits faster network connectivity and
GPGPU acceleration to improve the performance and energy
efficiency of the cluster. Our custom cluster, based on Nvidia
Jetson TX1 boards, is equipped with 10Gb network interface
cards and enables us to study the characteristics, scalability
challenges, and programming models of GPGPU-accelerated
workloads. We also develop an extension to the Roofline model to
establish a visually intuitive performance model for the proposed
cluster organization. We compare the GPGPU performance of
our cluster with discrete GPGPUs. We demonstrate that our
cluster improves both the performance and energy efficiency
of workloads that scale well and can leverage the better CPUGPGPU balance of our cluster. We contrast the CPU performance
of our cluster with ARM-based servers that use many CPU cores.
Our results show the poor performance of the branch predictor
and L2 cache are the bottleneck of server-class ARM SoCs.
Furthermore, we elucidate the impact of using 10Gb connectivity
with mobile systems instead of traditional, 1Gb connectivity.

Index Terms—GPGPU acceleration, energy efficiency, ARM
computing, distributed systems.

",15.195765016291148,14.05791366906475,278
Cluster_17_034.txt,15.77662024907271,14.24196416171792,Cluster,2701,"
In high-performance computing (HPC), end-to-end
workflows are typically utilized to gain insights from scientific
simulations. An end-to-end workflow consists of scientific simulation and data analysis, and can be executed in-situ, in-transit,
and offline. Existing studies on end-to-end workflows have largely
focused on the high-performance execution approaches. However,
the emerging heterogeneous architectures and energy concerns
lead to the rethinking of workflow execution approaches. As
a guide to the rethinking, this paper evaluates how to run
end-to-end HPC workflows efficiently in terms of performance,
energy, and error resilience. The evaluation covers emerging
heterogeneous processor architectures, processor power capping
techniques, and heterogeneous-reliability memory.

",17.315433740611066,15.981548672566372,114
Cluster_17_035.txt,14.48495459318966,13.222851526891525,Cluster,3279,"
Various extensions of TCP/IP have been proposed
to reduce network latency; examples include Explicit Congestion
Notification CECN), Data Center TCP (DCTCP) and several
proposals for Active Queue Management (AQM). Combining
these techniques requires adjusting various parameters, and
recent studies have found that it is difficult to do so while obtaining both high performance and low latency. This is especially
true for mixed use data centres that host both latency-sensitive
applications and high-throughput workloads such as Hadoop.

This paper studies the difficulty in configuration, and characterises the problem as related to ACK packets. Such packets cannot be set as ECN Capable Transport (ECT), with
the consequence that a disproportionate number of them are
dropped. We explain how this behavior decreases throughput,
and propose a small change to the way that non-ECT-capable
packets are handled in the network switches. We demonstrate
robust performance for modified AQMs on a Hadoop cluster,
maintaining full throughput while reducing latency by 85%. We
also demonstrate that commodity switches with shallow buffers
are able to reach the same throughput as deeper buffer switches.
Finally, we explain how both TCP-ECN and DCTCP can achieve
the best performance using a simple marking scheme, in constrast
to the current preference for relying on AQMs to mark packets.

Keywords-Hadoop, ECN, DCTCP, Throughput, Latency

",15.381575749822971,13.318123287671234,220
Cluster_17_036.txt,16.504514491453406,15.178249043674949,Cluster,2730,"
Studying the interaction among applications, MPI
runtimes, and the fabric they run on is critical to understanding
application performance. There exists no high-performance and
scalable tool that enables understanding this interplay on modern
multi-petaflop systems. Designing such a tool is non-trivial and
involves multiple components including 1) data profiling/collection from network/MPI library, 2) storing and, 3) rendering
the data. Furthermore, achieving this with minimal overhead
and scalability is a challenging task. We take up this challenge
and propose a high-performance and scalable network-based
performance analysis tool for MPI libraries operating on modern
networks like InfiniBand and Omni-Path. Our designs facilitate
caching and pre-rendering, allowing a cluster with 6,541 nodes,
764 switches and, 16,893 network links renders in just 30 seconds
— a 44X speed up over non-prerendered solutions. The proposed
lock-free and optimized memory-backed storage design enables
the tool to handle over a quarter million inserts into the database
every 45 seconds (data from 27,504 switch ports and 104,656 MPI
processes). The tool has been successfully deployed and validated
on HPC systems at OSC and on Comet at SDSC.

",17.267425705330172,14.974605263157894,195
Cluster_17_037.txt,15.507568531215586,13.886723915256123,Cluster,8150,"
Scientific data sets, which grow rapidly in volume,
are often attached with plentiful metadata, such as their associated experiment or simulation information. Thus, it becomes
difficult for them to be utilized and their value is lost over
time. Ideally, metadata should be managed along with its corresponding data by a single storage system, and can be accessed
and updated directly. However, existing storage systems in highperformance computing (HPC) environments, such as Lustre
parallel file system, still use a static metadata structure composed
of non-extensible and fixed amount of information. The burden
of metadata management falls upon the end-users and require
ad-hoc metadata management software to be developed.

With the advent of “object-centric’”’ storage systems, there is an
opportunity to solve this issue. In this paper, we present SoMeta,
a scalable and decentralized metadata management approach
for object-centric storage in HPC systems. It provides a flat
namespace that is dynamically partitioned, a tagging approach to
manage metadata that can be efficiently searched and updated,
and a light-weight and fault tolerant management strategy. In
our experiments, SoMeta achieves up to 3.7X speedup over
Lustre in performing common metadata operations, and up to
16X faster than SciDB and MongoDB for advanced metadata
operations, such as adding and searching tags. Additionally, in
contrast to existing storage systems, SoMeta offers scalable userspace metadata management by allowing users with the capability
to specify the number of metadata servers depending on their
workload.

",17.58133193835471,16.716595041322318,244
Cluster_17_038.txt,15.381575749822971,13.82432816635161,Cluster,5261,"
In situ workflows contain tasks that exchange messages composed of several data fields. However, a consumer task
may not necessarily need all the data fields from its producer.
For example, a molecular dynamics simulation can produce atom
positions, velocities, and forces; but some analyses require only
atom positions. The user should decide whether to specialize
the output of a producer task for a particular consumer and
get better performance or to send more data than required by
the consumer. The first option limits task portability, while the
second wastes resources. In this paper, we introduce contracts
for in situ tasks. A contract specifies for a producer each data
field available for output and for a consumer the data fields
needed as input. Comparing a producer and consumer contract
allows automatic selection of the data fields a producer has to
send for that consumer. We integrated our contracts mechanism
within Decaf, a middleware for building and executing in situ
workflows. Contracts enable to automatically extract at the
producer the data the consumer needs. We evaluate the cost and
performance of message extraction at runtime with both synthetic
examples and a real scientific workflow coupling a molecular
dynamics simulation with three different data analytics codes.
Our contract-based automatic data extraction removes the need
to specialize producers while entailing small overheads.

",16.11434528070225,13.995993150684932,220
Cluster_17_039.txt,14.554592549557764,11.97624495081866,Cluster,3074,"
Stream processing applications continuously process
large amounts of online streaming data in real-time or near
real-time. They have strict latency constraints, but they are
also vulnerable to failures. Failure recoveries may slow down
the entire processing pipeline and break latency constraints.
Upstream backup is one of the most widely applied fault-tolerant
schemes for stream processing systems. It introduces complex
backup dependencies to tasks, and increases the difficulty of
controlling recovery latencies. Moreover, when dependent tasks
are located on the same processor, they fail at the same time
in processor-level failures, bringing extra recovery latencies that
increase the impacts of failures. This paper presents a correlated
failure effect model to describe the recovery latency of a stream
topology in processor-level failures for an allocation plan. We
introduce a Recovery-latency-aware Task Allocation Problem
(RTAP) that seeks task allocation plans for stream topologies that
will achieve guaranteed recovery latencies. We present a heuristic
algorithm with a computational complexity of O(nlog? n) to
solve the problem. Extensive experiments were conducted to
verify the correctness and effectiveness of our approach.

Keywords-Stream processing, task Allocation, fault-tolerance,
upstream backup, recovery latency

",15.359359093739592,13.098505154639177,195
Cluster_17_040.txt,14.430676047946264,12.281415686092306,Cluster,3503,"
Relational databases are well suited for vertical
scaling; however, specialized hardware can be expensive. Conversely, NewSQL and NoSQL data stores are designed to scale
horizontally. NewSQL databases provide ACID transaction support; however, joins are limited to the partition keys, resulting
in restricted query expressiveness. On the other hand, NoSQL
databases are designed to scale out on commodity hardware;
however, they are limited by slow join performance. Hence, we
consider if the NoSQL join performance can be improved while
ensuring ACID semantics and without drastically sacrificing write
performance, disk utilization and query expressiveness.

This paper presents the Synergy system that leverages schema
and workload driven mechanisms to identify materialized views,
and a specialized concurrency control system on top of a NoSQL
database to enable scalable data management with familiar
relational conventions. Synergy trades slight write performance
degradation and increased disk utilization for faster join performance (compared to standard NoSQL databases) and improved
query expressiveness (compared to NewSQL databases).

Keywords-Transaction processing, materialized views, NoSQL
databases, performance evaluation

",18.511140095513987,17.239849397590365,167
Cluster_17_041.txt,15.541070020829881,13.86378591773882,Cluster,6248,"
Efficiently programming shared-memory machines is a difficult challenge because mapping application
threads onto the memory hierarchy has a strong impact on
the performance. However, optimizing such thread placement
is difficult: architectures become increasingly complex and
application behavior changes with implementations and input
parameters, e.g problem size and number of threads. In this
work, we propose a fully automatic, abstracted and portable
affinity module. It produces and implements an optimized
affinity strategy that combines knowledge about application
characteristics and the platform topology. Implemented in the
back-end of our runtime system (ORWL), our approach was
used to enhance the performance and the scalability of several
unmodified ORWL-coded applications: matrix multiplication,
a 2D stencil (Livermore Kernel 23), and a video tracking real
world application. On two SMP machines with quite different
hardware characteristics, our tests show spectacular performance improvements for these unmodified application codes
due to a dramatic decrease of cache misses and pipeline stalls. A
comparison to reference implementations using OpenMP confirms this performance gain of almost one order of magnitude.

Keywords-Thread placement, Task based runtimes, Hardware affinity, Parallel programming.

",18.377959752453624,17.595686813186813,184
Cluster_17_042.txt,16.012561673072522,14.02392233707399,Cluster,6866,"
Most applications running on supercomputers
achieve only a fraction of a system’s peak performance. It has
been demonstrated that the co-scheduling of applications can
improve the overall system utilization. However, following this
approach, applications need to fulfill certain criteria such that
the mutual slowdown is kept at a minimum. In this paper,
we present an HPC scheduler that applies co-scheduling and
utilizes virtual machine migration for a re-orchestration of
applications at runtime based on their main memory bandwidth
requirements. Given a job queue consisting of main memorybound applications and compute-bound applications, we can see
a throughput increase of up to 35% while at the same time
reducing energy consumption by around 30 %.

",17.353723509956247,15.226965517241378,118
Cluster_17_043.txt,14.651420488808167,13.298152500313975,Cluster,4829,"
Resource usage data, collected using tools such as
TACC_Stats, capture the resource utilization by nodes within
a high performance computing system. We present methods
to analyze the resource usage data to understand the system
performance and identify performance anomalies. The core idea
is to model the data as a three-way tensor corresponding to the
compute nodes, usage metrics, and time. Using the reconstruction
error between the original tensor and the tensor reconstructed
from a low rank tensor decomposition, as a scalar performance
metric, enables us to monitor the performance of the system in
an online fashion. This error statistic is then used for anomaly
detection that relies on the assumption that the normal/routine
behavior of the system can be captured using a low rank approximation of the original tensor. We evaluate the performance of
the algorithm using information gathered from system logs and
show that the performance anomalies identified by the proposed
method correlates with critical errors reported in the system logs.
Results are shown for data collected for 2013 from the Lonestar4
system at the Texas Advanced Computing Center (TACC).

Index Terms—Tensor Analysis, Feature Extraction, HPC, performance monitoring, anomaly detection

",16.827784334635936,16.00770618556701,195
Cluster_17_044.txt,17.47673057361469,16.328684630345943,Cluster,6563,"
Almost all performance analysis tools in the HPC
space perform some form of aggregation to compute summary
information of a series of performance measurements, from summations to more complex operations like histograms. Aggregation
not only reduces data volumes and consequently storage space
requirements and overheads, but is also crucial to extract insights
from recorded measurement data. In current tools, however,
most aspects that control the aggregation, such as the data
dimensions to be reduced, are hard-coded in the tool for a set of
particular use cases identified by the tool developer and cannot
be extended or modified by the user. This limits their flexibility
and often results in users having to learn and use multiple tools
with different aggregation options for their performance analysis
needs.

We present a novel approach for performance data aggregation
based on a flexible key:value data model with user-defined
attributes, where users can define custom aggregation schemes
in a simple description language. This not only gives users the
control to deploy the particular data aggregation they need,
but also opens the door for aggregations along applicationspecific data dimensions that cannot be achieved with traditional
profiling tools. We show how our approach can be applied for
performance profiling at runtime, cross-process data aggregation,
and interactive data analysis and demonstrate its functionality
with several case studies driven by real world codes.
",18.99602597827317,19.13948166877371,228
Cluster_17_045.txt,14.369710893918644,12.300774285099653,Cluster,4992,"
Accelerated clusters, which are distributed memory systems equipped with accelerators, have been used in
various fields. For accelerated clusters, programmers often implement their applications by a combination of MPI and CUDA
(MPI+CUDA). However, the approach faces programming
complexity issues. This paper introduces the XcalableACC
(XACC) language, which is a hybrid model of XcalableMP
CXMP) and OpenACC. While XMP is a directive-based language for distributed memory systems, OpenACC is also
a directive-based language for accelerators. XACC enables
programmers to develop applications on accelerated clusters
with ease. To evaluate XACC performance and productivity
levels, we implemented a lattice quantum chromodynamics
(Lattice QCD) application using XACC on 64 compute nodes
and 256 GPUs and found its performance was almost the
same as that of MPI+CUDA. Moreover, we found that XACC
requires much less change from the serial Lattice QCD code
than MPI+CUDA to implement the parallel Lattice QCD code.

Keywords-Accelerated cluster; Parallel language; Compiler;

",15.616094167265928,13.837083333333332,158
Cluster_17_046.txt,14.33831761319674,12.986263947484883,Cluster,6678,"
Graph models of social information systems typically contain trillions of edges. Such big graphs cannot be
processed on a single machine. The graph object must be
partitioned and distributed among machines and processed
in parallel on a computer cluster. Programming such systems
is very challenging. In this work, we present DH-Falcon,
a graph DSL (domain-specific language) which can be used
to implement parallel algorithms for large-scale graphs, targeting Distributed Heterogeneous (CPU and GPU) clusters.
DH-Falcon compiler is built on top of the Falcon compiler,
which targets single node devices with CPU and multiple
GPUs. An important facility provided by DH-Falcon is that it
supports mutation of graph objects, which allows programmer
to write dynamic graph algorithms. Experimental evaluation
shows that DH-Falcon matches or outperforms state-of-theart frameworks and gains a speedup of up to 13x.

",14.37465228746014,12.107147887323944,143
Cluster_17_047.txt,15.625854183359206,13.661752077426971,Cluster,10046,"
Failure tolerant data encoding and storage is of
paramount importance for data centers, supercomputers, data
transfers, and many aspects of information technology. ReedSolomon failure erasure codes and their variants are the basis
for many applications in this field. Efficient implementation of
these codes is challenging because they require computations in
Galois fields, which are not supported by processors natively.
Improved variants such as the Cauchy-Reed-Solomon code enable
a better mapping of the required calculations to computer
instructions. However, this works best when the source code of
the application is tuned for fixed encoding parameters which
deteriorates the flexibility. We present an approach to overcoming
these limitations by just in time compiling optimized code for
arbitrary encoding settings. Our open source library is optimized
for x86 processors using SSE and AVX extensions and we present
prototypes for GPUs and FPGAs as well. For a significant range
of encoding parameters, our implementation encodes at the maximum bandwidth the processor can read the data from memory.
In more complicated settings with additional redundancy data to
compensate the failure of multiple data stores, the algorithm
becomes compute limited. The optimized JIT code leverages
the full potential of modern processors reaching an instruction
throughput of more than three SIMD-instructions per compute
cycle, and encodes up to 19 gigabytes of data per second on a
Skylake system.

Keywords-Failure Erasure Coding; Red Solomon Code; GPU;
FPGA; Just In Time Compilation; Assembler; Vectorization;
High Performance Computing;

",17.946242345024807,16.00487603305785,243
Cluster_17_048.txt,15.052016013431665,13.140933672096676,Cluster,4772,"
Checkpoint/restart has been widely used to cope
with fail-stop errors. The checkpointing frequency is most often optimized by assuming an exponential failure distribution.
However, field studies show that most often failures do not
follow a constant failure rate exponential distribution. Therefore,
the optimal checkpointing frequency should be computed and
tuned considering the different distributions that failures follow.
Moreover, due to operating system and input/output jitter and hybrid solutions that combine checkpointing with other techniques,
such as data compression, checkpointing time can no longer be
assumed constant. Thus, time varying checkpointing time should
be accounted for to realistically model the application execution.

In this study, we develop a mathematical theory and model
to optimize the checkpointing frequency with respect to arbitrary failure distributions while capturing time-dependent nonconstant checkpointing time. We show that we can provide
closed-form formulas for important failure distributions in most
cases. By instantiating our model, we study and analyze 10
important failure distributions to obtain the optimal checkpointing frequency for these distributions. Experimental evaluation
shows that our model is highly accurate and deviates from the
simulations less than 1% on average.

",16.647925096878797,14.96540425531915,189
Cluster_17_049.txt,14.974015388500995,13.117809870173804,Cluster,8759,"
Fault tolerance is one of the major design goals for
HPC. The emergence of non-volatile memories (NVM) provides
a solution to build fault tolerant HPC. Data in NVM-based main
memory are not lost when the system crashes because of the nonvolatility nature of NVM. However, because of volatile caches,
data must be logged and explicitly flushed from caches into NVM
to ensure consistence and correctness before crashes, which can
cause large runtime overhead.

In this paper, we introduce an algorithm-based method to
establish crash consistence in NVM for HPC applications. We
slightly extend application data structures or sparsely flush
cache blocks, which introduce ignorable runtime overhead. Such
extension or cache flushing allows us to use algorithm knowledge
to reason data consistence or correct inconsistent data when
the application crashes. We demonstrate the effectiveness of
our method for three algorithms, including an iterative solver,
dense matrix multiplication, and Monte-Carlo simulation. Based
on comprehensive performance evaluation on a variety of test
environments, we demonstrate that our approach has very small
runtime overhead (at most 8.2% and less than 3% in most cases),
much smaller than that of traditional checkpoint, while having
the same or less recomputation cost.

",17.93193317476759,16.96246949030869,201
Cluster_17_050.txt,14.449865385127875,11.904059005492595,Cluster,8224,"
We consider the problem of orchestrating the execution of workflow applications structured as Directed Acyclic
Graphs (DAGs) on parallel computing platforms that are subject
to fail-stop failures. The objective is to minimize expected overall
execution time, or makespan. A solution to this problem consists
of a schedule of the workflow tasks on the available processors
and of a decision of which application data to checkpoint to stable
storage, so as to mitigate the impact of processor failures. For
general DAGs this problem is hopelessly intractable. In fact, given
a solution, computing its expected makespan is still a difficult
problem. To address this challenge, we consider a restricted class
of graphs, Minimal Series-Parallel Graphs (M-SPG S). It turns
out that many real-world workflow applications are naturally
structured as M-SPG S. For this class of graphs, we propose
a recursive list-scheduling algorithm that exploits the M-SPG
structure to assign sub-graphs to individual processors, and uses
dynamic programming to decide which tasks in these sub-gaphs
should be checkpointed. Furthermore, it is possible to efficiently
compute the expected makespan for the solution produced by
this algorithm, using a first-order approximation of task weights
and existing evaluation algorithms for 2-state probabilistic DAGs.
We assess the performance of our algorithm for production
workflow configurations, comparing it to (i) an approach in
which all application data is checkpointed, which corresponds
to the standard way in which most production workflows are
executed today; and (ii) an approach in which no application
data is checkpointed. Our results demonstrate that our algorithm
strikes a good compromise between these two approaches, leading
to lower checkpointing overhead than the former and to better
resilience to failure than the latter.
",17.946242345024807,15.926464646464648,289
Cluster_17_051.txt,16.856722396238048,15.532210045877793,Cluster,6590,"
In this research we describe the development and
optimisation of a new Monte Carlo neutral particle transport
mini-app, neutral. In spite of the success of previous research
efforts to load balance the algorithm at scale, it is not clear how
to take advantage of the diverse architectures being installed
in the newest supercomputers. We explore different algorithmic
approaches, and perform extensive investigations into the performance of the application on modern hardware including Intel
Xeon and Xeon Phi CPUs, POWERS CPUs, and NVIDIA GPUs.

When applied to particle transport the Monte Carlo method
is not embarrassingly parallel, as might be expected, due to
dependencies on the computational mesh that expose random
memory access patterns. The algorithm requires the use of atomic
operations, and exhibits load imbalance at the node-level due
to the random branching of particle histories. The algorithmic
characteristics make it challenging to exploit the high memory
bandwidth and FLOPS of modern HPC architectures.

Both of the parallelisation schemes discussed in this paper are
dominated latency issues caused by poor data locality, and are
restricted by the use of atomic operations for tallying calculations.
We saw a significant improvement in performance through the
use of hyperthreading on all CPUs and best performance on
the NVIDIA P100 GPU. A key observation is that architectures
that are tolerant to latencies may be able to hide the negative
characteristics of the algorithms.

",17.5058628484301,16.283203463203467,232
Cluster_17_052.txt,14.256089693589889,12.20004357034891,Cluster,7136,"
Tasks coupled in an in situ workflow may not
process data at the same speed, potentially causing overflows
in the communication channel between them. To prevent this
problem, software infrastructures for in situ workflows usually
impose a strict FIFO policy that has the side-effect of slowing
down faster tasks to the speed of the slower ones. This may
not be the desired behavior; for example, a scientist may prefer
to drop older data in the communication channel in order to
visualize the latest snapshot of a simulation. In this paper,
we present Manala, a flexible flow control library designed to
manage the flow of messages between a producer and a consumer
in an in situ workflow. Manala intercepts messages from the
producer, stores them, and selects the message to forward to
the consumer depending on the flow control policy. The library
is designed to ease the creation of new flow control policies and
buffering mechanisms. We demonstrate with three examples how
changing the flow control policy between tasks can influence the
performance and results of scientific workflows. The first example
focuses on materials science with LAMMPS and a synthetic
diffraction analysis code. The second example is an interactive visualization scenario with Gromacs as the producer and
Damaris/Viz as consumer. Our third example studies different
strategies to perform an asynchronous checkpoint with Gromacs.

",16.404322709996244,14.322785714285718,225
Cluster_17_053.txt,13.306631327048002,11.53002527850165,Cluster,2829,"
Markov Chain Monte Carlo methods provide a
tool for tackling high dimensional problems. With many-core
systems readily available today, it is no surprise that leveraging
parallelism in these samplers has been a subject of recent
research. The focus has been on solutions for shared-memory
architectures, however these perform poorly in a distributedmemory environment. This paper introduces a fully decentralized version of an affine invariant sampler. By observing that a
pseudorandom number generator makes stochastic algorithms
deterministic, communication is both minimized and hidden by
computation. Two cases at opposite ends of the communicationto-computation ratio spectrum are used during evaluation
against the currently available master-slave solution, where a
more than tenfold reduction in execution time is measured.

Keywords-Markov Chain Monte Carlo, High Performance
Computing, Affine Invariant Sampling

",16.084390811093357,16.160549450549453,131
Cluster_17_054.txt,16.087661936208256,15.085346164346166,Cluster,2610,"
Stencil-based applications such as CFD have succeeded in obtaining high performance on GPU supercomputers.
The problem sizes of these applications are limited by the GPU
device memory capacity, which is typically smaller than the
host memory. On GPU supercomputers, a locality improvement technique using temporal blocking method with memory
swapping between host and device enables large computation
beyond the device memory capacity. However, because the loop
management of temporal blocking with data movement across
these memories increase programming difficulty, the applying
this methodology to the real stencil applications demands
substantially higher programming cost. Our high-productivity
stencil framework automatically applies temporal blocking to
boundary exchange required for stencil computation and supports automatic memory swapping provided by a MPI/CUDA
wrapper library. The framework-based application for the
airflow in an urban city maintains 80% performance even
with the twice larger than the GPU memory capacity and
have demonstrated good weak scalability on the TSUBAME
2.5 supercomputer.
",19.454632303725965,18.9665923566879,159
Cluster_17_055.txt,15.23294001863546,12.93323948681397,Cluster,6969,"
Job runtime estimates provided by users are widely
acknowledged to be overestimated and runtime overestimation
can greatly degrade job scheduling performance. Previous studies
focus on improving accuracy of job runtime estimates by reducing
runtime overestimation, but fail to address the underestimation
problem (i.e., the underestimation of job runtimes). Using an
underestimated runtime is catastrophic to a job as the job will
be killed by the scheduler before completion. We argue that
both the improvement of runtime accuracy and the reduction
of underestimation rate are equally important. To address this
problem, we propose an online runtime adjustment framework
called TRIP. TRIP explores the data censoring capability of the
Tobit model to improve prediction accuracy while keeping a low
underestimation rate of job runtimes. TRIP can be used as a
plugin to job scheduler for improving job runtime estimates and
hence boosting job scheduling performance. Preliminary results
demonstrate that TRIP is capable of achieving high accuracy of
80% and low underestimation rate of 5%. This is significant
as compared to other well-known machine learning methods
such as SVM, Random Forest, and Last-2 which result in a
high underestimation rate (20%-50%). Our experiments further
quantify the amount of scheduling performance gain achieved by
the use of TRIP.

Keywords-job scheduling; runtime prediction; Blue Gene systems

",16.581925556534415,14.974122621564486,217
Cluster_17_056.txt,14.795881454719854,13.204966369027982,Cluster,3325,"
The need for parallel task execution has been
steadily growing in recent years since manufacturers mainly improve processor performance by scaling the number of installed
cores instead of the frequency of processors. To make use of this
potential, an essential technique to increase the parallelism of a
program is to parallelize loops. However, a main restriction of
available tools for automatic loop parallelization is that the loops
often have to be ‘polyhedral’ and that it is, e.g., not allowed to
call functions from within the loops.

In this paper, we present a seemingly simple extension to
the C programming language which marks functions without
side-effects. These functions can then basically be ignored when
checking the parallelization opportunities for polyhedral loops.
We extended the GCC compiler toolchain accordingly and evaluated several real-world applications showing that our extension
helps to identify additional parallelization chances and, thus, to
significantly enhance the performance of applications.

",17.5058628484301,16.721013071895424,155
Cluster_17_057.txt,16.278726483062847,15.19537299248903,Cluster,6168,"
High performance computing systems will need to
operate with certain power budgets while maximizing performance in the exascale era. Such systems are built with power
aware components, whose collective peak power may exceed the
specified power budget. Cluster level power bounded computing
addresses this power challenge by coordinating power among
components within compute nodes and further adjusting the
number of participating nodes. It offers more space to increase
system performance by utilizing the available power budget more
efficiently within and across the nodes.

In this paper, we present the design of a hierarchical multidimensional power aware allocation framework, CLIP, for power
bounded parallel computing on multicore-based computer clusters. The framework satisfies the specified power bound by
managing the power distribution among nodes at the cluster
level, and among sockets, cores and NUMA memory modules
at the node level. The power allocation is enforced with multiple complementary power management techniques, including
memory power level setting, thread concurrency throttling, and
core-thread affinity. We present an application characterization
method based on applications’ scalability and an associated
performance model, which can accurately determine the optimal
number of participating compute nodes and components, and
their power distribution for given applications. Experimental
results on a Haswell-based computer cluster show that the
proposed scheduler outperforms compared methods by over 20%
on average for various power budgets.

Index Terms—Power-bounded computing, resource coordination, performance analysis, multicore computing, cluster.

",17.693802365651003,17.892410256410262,235
Cluster_17_058.txt,13.95855707691042,11.463011060536097,Cluster,2851,"
Virtual machine (VM) consolidation is necessary
for increasing the server utilization; however, it also leads to
VM performance degradation. This work presents a method
to predict the consolidated VMs performance from the critical
system events data. Experiments are designed to demonstrate
the effect of system events like interrupts, page faults, mutex
operations, and context switching on the consolidated VMs.
Results show that the host server counters are not reliable
for such predictions. On the other hand, the coupling of the
task execution time with the VM system events is an effective
way to predict the consolidation performance. Results further
show that the VM memory allocation plays an important part
in the consolidated tasks performance. The system event data
is also used to train an Artificial Neural Network (ANN) for
performance prediction on three hypervisors; ESXi, Xen, and
XenServer and similar results are observed in all three.

Keywords-Virtual machine; consolidation; performance; prediction;

",16.26309291913925,15.081203007518798,153
Cluster_17_059.txt,16.13650231786591,14.633643308201616,Cluster,6153,"
As the memory and storage hierarchy get deeper
and more complex, it is important to have new benchmarks and
evaluation tools that allow us to explore the emerging middleware
solutions to use this hierarchy. Skel is a tool aimed at automating
and refining this process of studying HPC I/O performance.
It works by generating application I/O kernel/benchmarks as
determined by a domain-specific model. This paper provides
some techniques for extending Skel to address new situations and
to answer new research questions. For example, we document use
cases as diverse as using Skel to troubleshoot I/O performance
issues for remote users, refining an I/O system model, and facilitating the development and testing of a mechanism for runtime
monitoring and performance analytics. We also discuss dataoriented extensions to Skel to support the study of compression
techniques for Exascale scientific data management.

",17.122413403193683,15.644655172413795,146
Cluster_17_060.txt,15.045722751030102,13.079872147818403,Cluster,7025,"
Traditional machine learning algorithms often require computations on centralized data, but modern datasets are
collected and stored in a distributed way. In addition to the cost of
moving data to centralized locations, increasing concerns about
privacy and security warrant distributed approaches. We propose
keybin, a distributed key-based binning clustering algorithm
for high-dimensional spaces. keybin locally generates a spatial
key for each data point across all dimensions without needing
knowledge of other data. Then, it performs a conceptual MapReduce procedure in the index space to form a global clustering
assignment. We present an implementation and a case study on
the capabilities and limitations of this approach, showing that
this algorithm can learn a global clustering structure with limited
communication and can scale with the dimensionality and size
of data sets.

",18.458006810337128,17.86769465648855,132
Cluster_17_061.txt,14.739383391665452,12.715025998263087,Cluster,2359,"
In-memory key-value store is a crucial building block
of large-scale web architecture. Given the growth of the data
volume and the need for low-latency responses, cost-effective
storage expansion and fast large-message processing are the
major challenges. In this paper, we explore the design of keyvalue middleware that takes advantage of modern NVMe SSDs
and RDMA interconnects to achieve high performance without
excessive DRAM deployment. We propose an all-in-userland
approach to improve the data plane efficiency. Both NVMe and
RDMaA are interfaced directly from the user-space for effective
data access and tailored data management. We present a lowlatency storage extension framework based on NVMe and a new
design of JVM-aware Memcache protocol based on RDMA. To
further accelerate large-message transfer, we provide a hybrid
communication protocol fusing Eager and Rendezvous schemas,
and a united I/O staging approach to achieve maximum latency
hiding through pipelining. As the benchmarking results indicate,
with the non-negligible JVM overhead taken into account, our
solution obtains comparable communication performance with
the RDMA-Memceached released by the OSU. For SSD-involved
operations, the latency decreases by up to 31% compared to the
kernel-based I/O processing.

Keywords-key-value middleware; NVMe; RDMA; JVM; userspace I/O
",15.6451,13.717622641509436,213
Cluster_17_062.txt,15.564610419143051,14.321816711590298,Cluster,2957,"Task mapping is an important problem in parallel
and distributed computing. The goal in task mapping is to find an
optimal layout of the processes of an application (or a task) onto a
given network topology. We target this problem in the context of
staging applications. A staging application consists of two or more
parallel applications (also referred to as staging tasks) which run
concurrently and exchange data over the course of computation.
Task mapping becomes a more challenging problem in staging
applications, because not only data is exchanged between the
staging tasks, but also the processes of a staging task may
exchange data with each other. We propose a novel method, called
Task Graph Embedding (TGE), that harnesses the observable
graph structures of parallel applications and network topologies.
TGE employs a machine learning based algorithm to find the
best representation of a graph, called an embedding, onto a
space in which the task-to-processor mapping problem can be
solved. We evaluate and demonstrate the effectiveness of TGE
experimentally with the communication patterns extracted from
runs of XGC, a large-scale fusion simulation code, on Titan.
",14.90622815163357,14.097372994652407,187
Cluster_17_063.txt,14.202520854346488,11.677836886350036,Cluster,7137,"
Scientific computing requires trust in results. In
high-performance computing, trust is impeded by silent data
corruption (SDC), in other words corruption that remains
unnoticed. Numerical integration solvers are especially sensitive
to SDCs because an SDC introduced in a certain step affects all
the following steps. SDCs can even cause the solver to become
unstable. Adaptive solvers can change the step size, by comparing
an estimation of the approximation error with an user-defined
tolerance. If the estimation exceeds the tolerance, the step is
rejected and recomputed. Adaptive solvers have an inherent
resilience, because some SDCs might have no consequences on
the accuracy of the results, and some SDCs might push the
approximation error beyond the tolerance. Our first contribution
shows that the rejection mechanism is not reliable enough
to reject all SDCs that affect the results’ accuracy, because
the estimation is also corrupted. We therefore provide another
protection mechanism: at the end of each step, a second error
estimation is employed to increase the redundancy. Because of
the complex dynamics, the choice of the second estimate is
difficult: two methods are explored. We evaluated them in HyPar
and PETSc, on a cluster of 4,096 cores. We injected SDCs
that are large enough to affect the trust or the convergence
of the solvers. The new approach can detect 99% of the SDCs,
reducing by more than 10 times the number of undetected SDCs.
Compared with replication, a classic SDC detector, our protection
mechanism reduces the memory overhead by more than 2 times
and the computational overhead by more than 20 times in our
experiments.

Index Terms—high-performance computing, resilience, fault
tolerance, silent data corruption, numerical integration solver

",15.381575749822971,13.030805755395686,280
Cluster_17_064.txt,16.496938191975143,14.670227837008913,Cluster,3341,"
Aggregating millions of hardware components to
construct an exascale computing platform will pose significant
resilience challenges. In addition to slowdowns associated with
detected errors, silent errors are likely to further degrade application performance. Moreover, silent data corruption (SDC) has
the potential to undermine the integrity of the results produced
by important scientific applications.

In this paper, we propose an application-independent mechanism to efficiently detect and correct SDC in read-mostly
memory, where SDC may be most likely to occur. We use
memory protection mechanisms to maintain compressed backups
of application memory. We detect SDC by identifying changes
in memory contents that occur without explicit write operations.
We demonstrate that, for several applications, our approach can
potentially protect a significant fraction of application memory
pages from SDC with modest overheads. Moreover, our proposed
technique can be straightforwardly combined with many other
approaches to provide a significant bulwark against SDC.

",16.827784334635936,16.303280201342286,150
Cluster_17_065.txt,15.112257680678326,12.786118587856063,Cluster,2909,"
In this paper, we present a non-parametric dataanalytic soft-error detector. Our detector uses the key properties of Gaussian process regression. First, because Gaussian
process regression provides confidence on the prediction, this
confidence can be used to automatize construction of the
detection range. Second, because the correlation model of a
Gaussian process captures the similarity among neighboring
point values, only one-time online training is needed. This leads
to very low online performance overheads. Finally, Gaussian
process regression localizes the detection range computation,
thereby avoiding communication costs. We compare our detector with the adaptive impact-driven (AID) and spatial supportvector-machine (SSD) detectors, two effective detectors based
on observation of the temporal and spatial evolution of data,
respectively. Experiments with five failure distributions and
six real-world high-performance computing applications reveal
that the Gaussian-process-based detector achieves low false
positive rate and high recall while incurring less than 0.1%
performance and memory overheads. Considering the detection
performance and overheads, our Gaussian process detector
provides the best trade-off.

",17.5058628484301,15.627017543859651,173
CoNEXT_17_001.txt,14.87534998438447,13.29819341063542,CoNEXT,10062,"

Internet communication today typically involves intermediary middleboxes like caches, compression proxies, or virus scanners. Unfortunately, as encryption becomes more widespread, these middleboxes become blind and we lose their security, functionality, and
performance benefits. Despite initial efforts in both industry and
academia, we remain unsure how to integrate middleboxes into
secure sessions—it is not even clear how to define “secure” in this
multi-entity context.

In this paper, we first describe a design space for secure multientity communication protocols, highlighting tradeoffs between
mutually incompatible properties. We then target real-world requirements unmet by existing protocols, like outsourcing middleboxes to untrusted infrastructure and supporting legacy clients. We
propose a security definition and present Middlebox TLS (mbTLS),
a protocol that provides it (in part by using Intel SGX to protect
middleboxes from untrusted hardware). We show that mbTLS is
deployable today and introduces little overhead, and we describe
our experience building a simple mbTLS HTTP proxy.

CCS CONCEPTS

- Security and privacy — Security protocols; Trusted computing; « Networks — Session protocols; Middle boxes / network
appliances;

KEYWORDS
TLS, middleboxes, trusted computing, SGX

ACM Reference format:

David Naylor, Richard Li, Christos Gkantsidis, Thomas Karagiannis, and
Peter Steenkiste. 2017. And Then There Were More: Secure Communication
for More Than Two Parties. In Proceedings of CoNEXT °17: The 13th International Conference on emerging Networking EXperiments and Technologies,
Incheon, Republic of Korea, December 12-15, 2017 (CoNEXT ’17), 13 pages.
DOT: 10.1145/3143361.3143383

Work done while David and Richard were interns at Microsoft Research.

 

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.

CoNEXT ’17, Incheon, Republic of Korea

© 2017 ACM. 978-1-4503-5422-6/17/12...$15.00

DOI: 10.1145/3143361.3143383

",15.6451,13.872768698060941,366
CoNEXT_17_002.txt,14.298821670099933,12.156750526958067,CoNEXT,8883,"

Owing to great potential in smart home and human-computer interactive applications, WiFi indoor localization has attracted extensive
attentions in the past several years. The state-of-the-art systems
have successfully achieved decimeter-level accuracies. However, the
high accuracy is acquired at the cost of dense access point (AP) deployment, employing large size of frequency bandwidths or specialpurpose radar signals which are not compatible with existing WiFi
protocol, limiting their practical deployments. This paper presents
the design and implementation of AWL, an accurate indoor localization system that enables a single WiFi AP to achieve decimeterlevel accuracy with only one channel hopping The key enabler of
the system is we novelly employ channel hopping to create virtual
antennas, without the need of adding more antennas or physically
move the antennas’ positions for a larger antenna array. We successfully utilize the widely known “bad” spatial aliasing to improve the
AoA estimation accuracy. A novel multipath suppression scheme
is also proposed to combat the severe multipath issue indoors. We
build a prototype of AWL on WARP software-defined radio platform. Comprehensive experiments manifest that AWL achieves a
median localization accuracy of 38 cm in a rich multipath indoor
environment with only a single AP equipped with 6 antennas. In a
small scale area, AWL is able to accurately track a moving device’s
trajectory, enabling applications such as writing/drawing in the air.

",18.481644305966572,17.514927536231884,232
CoNEXT_17_003.txt,14.072425296591003,11.774208850684499,CoNEXT,6220,"

We introduce the design and implementation of FreeRider,
the first system that enables backscatter communication with
multiple commodity radios, such as 802.11g/n WiFi, ZigBee,
and Bluetooth, while these radios are simultaneously used
for productive data communication. Furthermore, we are,
to our knowledge, the first to implement and evaluate a
multi-tag system. The key technique used by FreeRider is
codeword translation, where a tag can transform a codeword
present in the original excitation signal into another valid
codeword from the same codebook during backscattering.
In other words, the backscattered signal is still a valid WiFi,
ZigBee, or Bluetooth signal. Therefore, commodity radios decode the backscattered signal and extract the tag’s embedded
information. More importantly, FreeRider does codeword
translation regardless of the data transmitted by these radios.
Therefore, these radios can still do productive data communication. FreeRider accomplishes codeword translation by
modifying one or more of the three dimensions of a wireless
signal — amplitude, phase and frequency. A tag ensures that
the modified signal is still comprised of valid codewords that
come the same codebook as the original excitation signal. We
built a hardware prototype of FreeRider, and our empirical
evaluations show a data rate of ~60kbps in single tag mode,
15kbps in multi-tag mode, and a backscatter communication
distance up to 42m when operating on 802.11g/n WiFi.

CCS CONCEPTS

« Networks — Network architectures; Wireless access
networks;

 

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from permissions@acm.org.

CoNEXT ’17, Incheon, Republic of Korea

© 2017 ACM. 978-1-4503-5422-6/17/12...$15.00

DOI: 10.1145/3143361.3143374

389

KEYWORDS
Backscatter; WiFi; ZigBee; Bluetooth; Wireless

",15.832023613614833,13.457689393939393,358
CoNEXT_17_004.txt,16.763591458749374,15.532441659176403,CoNEXT,9528,"

Deploying LTE networks in unlicensed spectrum requires us to
move beyond coexistence mechanisms and understand the suitability of LTE’s synchronous operation in a spectrum that is governed
by asynchronous access principles. Our study reveals a fundamental conflict in LTE uplink access that arises between the scheduled
nature of LTE’s multi-user transmissions — critical for leveraging
the diversity (OFDMA) and multiplexing (multi-user MIMO) gains
— and the asynchronous nature of interference on the clients. The
result is a significant loss in spectrum utilization and throughput
that scales with the number of interfering terminals.

To tackle this critical challenge on the LTE uplink, we propose Biv. Btu transforms today’s LTE schedulers into speculative
schedulers that leverage interference diversity across clients to intelligently over-schedule clients on the same spectral resources to
prevent this utilization loss. BLu’s challenges lie in how to overschedule appropriate clients on the same resources without paying
the penalty of collisions, while containing the exponential overhead
incurred in measuring the required interference dependencies between clients. The under-pinning of BLu’s design includes a novel
mechanism to blue-print the very source of interference on LTE
clients along with their dependencies, which allows for a constant,
significantly reduced overhead. Btu can be realized in today’s LTE
base stations. Its realization in an enterprise environment with
SDRs (hosting LTE release 10) reveals appreciable gains of 1.5-2x
in both utilization and throughput over existing schemes for SISO
and MU-MIMO transmissions in unlicensed spectrum.

",18.001758247042904,17.811604938271604,251
CoNEXT_17_005.txt,14.823600612239588,12.861041442404815,CoNEXT,10296,"

Achieving consensus among a set of distributed entities (or participants) is a fundamental problem at the heart of many distributed systems. A critical problem with most consensus protocols
is that they do not scale well. As the number of participants trying to achieve consensus increases, increasing network traffic can
quickly overwhelm the network from topology-oblivious broadcasts, or a central coordinator for centralized consensus protocols.
Thus, either achieving strong consensus is restricted to a handful
of participants, or developers must resort to weaker models of consensus.

We propose Canopus, a highly-parallel consensus protocol that
is ‘plug-compatible’ with ZooKeeper, which exploits modern data
center network topology, parallelism, and consensus semantics to
achieve scalability with respect to the number of participants and
throughput (i.e., the number of key-value reads/writes per second). In our prototype implementation, compared to EPaxos and
ZooKeeper, Canopus increases throughput by more than 4x and
16x respectively for read-heavy workloads.

",18.946978176291534,17.847435897435897,158
CoNEXT_17_006.txt,15.763335937973839,14.162185023717779,CoNEXT,7830,"

Achieving data-rates of multiple Gbps in 60 GHz millimeter-wave
(mm-wave) communication systems requires efficient beam-steering
algorithms. To find the optimal steering direction on IEEE 802.11ad
compatible devices, state-of-the-art approaches sweep through all
predefined antenna sectors. Recently, much more efficient alternatives, such as compressive path tracking, have been proposed,
which scale well even with arrays with thousands of antenna elements. However, such have not yet been integrated into consumer
devices. In this work, we adapt compressive path tracking for sector selection in off-the-shelf IEEE 802.11ad devices. In contrast to
existing solutions, our compressive sector selection tolerates the
imperfections of low-cost hardware, tracks beam directions in 3D
and does not rely on pseudo-random beams. We implement our
protocol on a commodity router, the TP-Link Talon AD7200, by
modifying the sector sweep algorithm in the IEEE 802.11ad chip’s
firmware. In particular, we modify the firmware to obtain the signal
strength of received frames and to select custom sectors. Using
this extension, we precisely measure the device’s sector patterns.
We then select the best sector based on the measured patterns and
sweep only through a subset of probing sectors. Our results demonstrate, that our protocol outperforms the existing sector sweep,
increases stability, and speeds up the sector selection by factor 2.3.

",16.018793980428352,13.780938148609383,226
CoNEXT_17_007.txt,15.665551720507455,14.153899251543635,CoNEXT,6553,"
Currently, the attempt to choose the “best"" content replica
server for a client is carried out solely by CDNs. While CDNs
have a decent view of load distribution and content placement, they receive little input from the clients themselves.
We propose a hybrid solution, subnet assimilation, where the
client participates in the server selection process while still
leaving the final say to the CDN. Subnet assimilation allows
clients to declare their own “network location,"" different from
the actual one, which in turn drives a CDN towards making better decisions. To demonstrate, we introduce Drongo,
a client-side system, readily deployable on existing clients
without any changes to the CDNs, that employs subnet assimilation to dramatically improve replica server selection.
We implemented and extensively evaluated Drongo on a
set of 429 clients spread across 177 countries and 6 major
CDNs. We show that Drongo affects 69.93% of all clients,
prompting better CDN replica choices which reduce the latency of affected requests by up to an order of magnitude
and by 24.89% on average across six major providers, with
Google’s performance improving by 50% in the median case.
Our results indicate that client participation holds great opportunities for the advancement of CDN performance.

",15.742502247213078,15.16695895522388,205
CoNEXT_17_008.txt,14.448664920967587,13.371540351379917,CoNEXT,6813,"

Multi-Path TCP (MPTCP) is a new standardized transport protocol
that enables devices to utilize multiple network interfaces. The
default MPTCP path scheduler prioritizes paths with the smallest
round trip time (RTT). In this work, we examine whether the default
MPTCP path scheduler can provide applications the ideal aggregate
bandwidth, i.e., the sum of available bandwidths of every paths. Our
experimental results show that heterogeneous paths cause underutilization of the fast path, resulting in undesirable application
behaviors such as lower streaming quality in a video than can be
obtained using the available aggregate bandwidth. To solve this
problem, we propose and implement a new MPTCP path scheduler,
ECF (Earliest Completion First), that utilizes all relevant information
about a path, not just RTT. We compare ECF with both the default
and other MPTCP path schedulers, using both an experimental
testbed and in-the-wild measurements. Our results show that ECF
consistently utilizes all available paths more efficiently than other
approaches under path heterogeneity, particularly for streaming
video. In Web browsing workloads, ECF also does better in some
scenarios and never does worse.

",17.613555460941566,16.66003157063931,183
CoNEXT_17_009.txt,14.68453734165275,12.586202608265012,CoNEXT,6287,"

Visible light is gaining significant attention as a medium to achieve
accurate relative localization. Most of the studies in the area focus
on indoor positioning and rely on two important assumptions: (i)
lights are static, and (ii) the receiver has line-of-sight with multiple lights. These requirements limit the application of localization
methods in scenarios where nodes have a single light and are mobile,
such as motorbikes or swarms of robots. In general, this particular
type of scenarios (single lights moving on a plane) leads to underdetermined localization systems where no unique solution can be
found. We follow a holistic approach that includes theory, simulations, and experiments to overcome some of the limitations present
in such type of scenarios. Our theoretical and simulation results
show that if nodes are enhanced with sensors providing relative
directions (such as compasses), we can derive dependencies in the
system to obtain unique solutions. Our proof-of-concept implementation validates our model by showing that single lights can provide
relative localization with high accuracy: an average error below
53cm.

",17.451712890111917,16.344,176
CoNEXT_17_010.txt,13.607296013878685,11.913441217349124,CoNEXT,9791,"

Modern CDNs cache and deliver a highly-diverse set of traffic
classes, including web pages, images, videos and software downloads. It is economically advantageous for a CDN to cache and
deliver all traffic classes using a shared distributed cache server
infrastructure. However, such sharing of cache resources across
multiple traffic classes poses significant cache provisioning challenges that are the focus of this paper.

Managing a vast shared caching infrastructure requires careful
modeling of user request sequences for each traffic class. Using
extensive traces from Akamai’s CDN, we show how each traffic
class has drastically different object access patterns, object size
distributions, and cache resource requirements. We introduce the
notion of a footprint descriptor that is a succinct representation of
the cache requirements of a request sequence. Leveraging novel
connections to Fourier analysis, we develop a footprint descriptor
calculus that allows us to predict the cache requirements when
different traffic classes are added, subtracted and scaled to within a
prediction error of 2.5%. We integrated our footprint calculus in the
cache provisioning operations of the production CDN and show
how it is used to solve key challenges in cache sizing, traffic mixing,
and cache partitioning.

",17.122413403193683,15.581932989690724,197
CoNEXT_17_011.txt,15.75914335860628,14.37127672307162,CoNEXT,9923,"

SDN controllers demand tight performance guarantees over the
control plane actions performed by switches. For example, traffic
engineering techniques that frequently reconfigure the network
require guarantees on the speed of reconfiguring the network. Initial experiments show that poor performance of Ternary ContentAddressable Memory (TCAM) control actions (e.g., rule insertion)
can inflate application performance by a factor of 2x! Yet, modern
switches provide no guarantees for these important control plane
actions — inserting, modifying, or deleting rules.

In this paper, we present the design and evaluation of Hermes,
a practical and immediately deployable framework that offers a
novel method for partitioning and optimizing switch TCAM to
enable performance guarantees. Hermes builds on recent studies
on switch performance and provides guarantees by trading-off a
nominal amount of TCAM space for assured performance. We evaluated Hermes using large-scale simulations. Our evaluations show
that with less than 5% overheads, Hermes provides 5ms insertion
guarantees that translates into an improvement of application level
metrics by up to 80%. Hermes is more than 50% better than existing
state of the art techniques and provides significant improvement
for traditional networks running BGP.

",16.594172100314452,15.308709677419355,188
CoNEXT_17_012.txt,14.427497466443395,12.78415477431393,CoNEXT,7514,"

Bandwidth adaptation for real-time streaming applications is typically designed to be conservative, since pushing for higher bandwidth could be counterproductive if it means an increased latency.
However, such bandwidth adaptation operates based on the ""symptoms"" of congestion (e.g., increased delay) without knowing the
underlying cause (self-congestion vs. cross-traffic). In this paper,
we consider this problem in the context of Wi-Fi networks and
introduce a novel technique, Ping-Pair, to measure and attribute
congestion. We have integrated Ping-Pair into the popular Skype
audio-video conferencing application to enable improved bandwidth adaptation dubbed Kwikr, using which we have conducted
controlled experiments and also randomized A/B tests in a production setting.

",18.243605946275583,17.26184210526316,116
CoNEXT_17_013.txt,17.48273977299223,16.6593036146003,CoNEXT,11576,"
Location information is crucial for enabling many applications in
the era of Internet-of-Things (IoT). Localizing IoT devices using
existing ubiquitous WiFi infrastructure enables rapid and universal
deployment of IoT devices empowered with localization service.
However, current WiFi-based localization systems require the target, that needs to be localized, to be equipped with a WiFi radio
that actively transmits WiFi signals. Since a WiFi radio consumes
where is
here
lots of power for data transmission, such radio cannot be usedwhere
in is
my pill?
energy-impoverish IoT devices (for example, locating a pill bottlemyinpill?
an elder’s apartment). This paper presents WiTag, the first system
floor plan
that reuses existing WiFi infrastructures
for localizing low-power
backscatter tags. We build upon recent work on low power tags
that communicate via backscattering ambient WiFi signals and
introduce novel modeling of the backscattered signal received at
WiFi access points to localize the target IoT device. We prototype
WiTag using off-the-shelf WiFi chips and a customized backscatter
tag and show that it achieves sub-meter localization error. Thus,
WiTag achieves localization error comparable to that of state-ofthe-art WiFi based localization systems while consuming orders of
magnitude lower power at the target IoT devices.

",16.404322709996244,15.156196078431375,206
CoNEXT_17_014.txt,14.77626248513877,13.177010271054876,CoNEXT,7483,"
We present a smartphone-based application, called LocBLE, for enabling users to estimate the location of nearby Bluetooth low energy
(BLE) beacons. In contrast to existing BLE beacon-based proximity
applications that can only show coarse-grained (immediate, near,
and far) distance estimation, LocBLE’s fine-grained estimation can
enhance human-environment interactions.
LocBLE has three salient features in estimating location from
BLE beacon signals. First, it is adaptive to dynamic signal propagation environments by learning the environmental changes directly
from the received signal strength (RSS). Second, it performs sensorfusion for location estimation by utilizing motion sensor data and
RSS readings from a smartphone. Finally, LocBLE improves location
tracking accuracy with novel on-line calibration on a set of beacons nearby. We have built a prototype of LocBLE on smartphones
and evaluated it on commodity proximity-enabled beacons. Our
experimental results demonstrate that LocBLE achieves an average
of 1.8m and 1.2m accuracies in locating indoor and outdoor BLE
beacons, respectively.
",16.52667757954773,15.687036163522013,163
CoNEXT_17_015.txt,13.711029569250492,12.287795465297915,CoNEXT,9717,"

Bitcoin has not only attracted many users but also been considered
as a technical breakthrough by academia. However, the expanding
potential of Bitcoin is largely untapped due to its limited throughput.
The Bitcoin community is now facing its biggest crisis in history as
the community splits on how to increase the throughput. Among
various proposals, Bitcoin Unlimited recently became the most
popular candidate, as it allows miners to collectively decide the
block size limit according to the real network capacity. However,
the security of BU is heatedly debated and no consensus has been
reached as the issue is discussed in different miner incentive models. In this paper, we systematically evaluate BU’s security with
three incentive models via testing the two major arguments of BU
supporters: the block validity consensus is not necessary for BU’s
security; such consensus would emerge in BU out of economic
incentives. Our results invalidate both arguments and therefore
disprove BU’s security claims. Our paper further contributes to
the field by addressing the necessity of a prescribed block validity
consensus for cryptocurrencies.

",17.267425705330172,15.664678571428574,179
CoNEXT_17_016.txt,16.29186808918373,14.470045217596898,CoNEXT,9057,"
Virtual 360° cockpit

360◦
Panoramic or
video streaming has been supported by a
wide range of content providers and mobile devices. Yet existing work primarily focused on streaming on-demand 360◦
videos stored on servers. In this paper, we examine a more
challenging problem: Can we stream real-time interactive
360◦ videos across existing LTE cellular networks, so as to
trigger new applications such as ubiquitous 360◦ video chat
and panoramic outdoor experience sharing? To explore the
feasibility and challenges underlying this vision, we design
POI360, a portable interactive 360◦ video telephony system
that jointly investigates both panoramic video compression
and responsive video stream rate control. For the challenge
that the legacy spatial compression algorithms for 360◦ video
suffer from severe quality fluctuations as the user changes
her region-of-interest (ROI), we design an adaptive compression scheme, which dynamically adjusts the compression
strategy to stabilize the video quality within ROI under various user input and network condition. In addition, to meet
the responsiveness requirement of panoramic video telephony, we leverage the diagnostic statistics on commodity
phones to promptly detect cellular link congestion, hence
significantly boosting the rate control responsiveness. Extensive field tests for our real-time POI360 prototype validate
its effectiveness in enabling panoramic video telephony over
the highly dynamic cellular networks.
",21.703370914358928,20.26714285714286,214
CoNEXT_17_017.txt,15.209405710694153,13.90400001003771,CoNEXT,6313,"

Various trends are reshaping Internet video delivery: exponential
growth in video traffic, rising expectations of high video quality of
experience (QoE), and the proliferation of varied content delivery
network (CDN) deployments (e.g., cloud computing-based, content
provider-owned datacenters, and ISP-owned CDNs). More fundamentally though, content providers are shifting delivery from a
single CDN to multiple CDNs, through the use of a content broker.
Brokers have been shown to invalidate many traditional delivery
assumptions (e.g., shifting traffic invalidates short- and long-term
traffic prediction) by not communicating their decisions with CDNs.
In this work, we analyze these problems using data from a CDN and
a broker. We examine the design space of potential solutions, finding that a marketplace design (inspired by advertising exchanges)
potentially provides interesting tradeoffs. A marketplace allows all
CDNs to profit on video delivery through fine-grained pricing and
optimization, where CDNs learn risk-adverse bidding strategies to
aid in traffic prediction. We implement a marketplace-based system
(which we dub Video Delivery eXchange or VDX) in CDN and broker
data-driven simulation, finding significant improvements in cost
and data-path distance.

",18.699421769314853,17.357803379416286,189
CoNEXT_17_018.txt,12.86389467269791,10.610398967949706,CoNEXT,9864,"

Radio-frequency identification (RFID) systems, as major enablers
of automatic identification, are currently supplemented with various interesting sensing functions, e.g., motion tracking. All these
sensing applications forcedly require much higher reading rate (i.e.,
sampling rate) such that any fast movement of tagged objects can
be accurately captured in a timely manner through tag readings.
However, COTS RFID systems suffer from an extremely low individual reading rate when multiple tags are present, due to their intense channel contention in the link layer. In this work, we present
a holistic system, called Tagwatch, a rate-adaptive reading system
for COTS RFID devices. This work revisits the reading rate from a
distinctive perspective: mobility. We observe that the reading demands of mobile tags are considerably more urgent than those of
stationary tags because the states of the latter nearly remain unchanged; meanwhile, only a few tags (e.g., < 20%) are actually in
motion despite the existence of a massive amount of tags in practice. Thus, Tagwatch adaptively improves the reading rates for mobile tags by cutting down the readings of stationary tags. Our main
contribution is a two-phase reading design, wherein the mobile
tags are discriminated in the Phase I and exclusively read in the
Phase II. We built a prototype of Tagwatch with COTS RFID readers and tags. Results from our microbenchmark analysis demonstrate that the new design outperforms the reading rate by 3.2x
when 5% of tags are moving.

",14.554592549557764,14.072422594142264,243
CoNEXT_17_019.txt,14.574690177314679,13.413370634767507,CoNEXT,5132,"

CoFlow scheduling improves data-intensive application performance
by improving their networking performance. State-of-the-art CoFlow
schedulers in essence approximate the classic online Shortest-JobFirst (SJF) scheduling, designed for a single CPU, in a distributed
setting, with no coordination among how the flows of a CoFlow at
individual ports are scheduled, and as a result suffer two performance
drawbacks: (1) The flows of a CoFlow may suffer the out-of-sync
problem — they may be scheduled at different times and become
drifting apart, negatively affecting the CoFlow completion time
(CCT); (2) FIFO scheduling of flows at each port bears no notion of
SJF, leading to suboptimal CCT.

We propose SAATH, an online CoFlow scheduler that overcomes
the above drawbacks by explicitly exploiting the spatial dimension
of CoFlows. In SAATH, the global scheduler schedules the flows of a
CoFlow using an all-or-none policy which mitigates the out-of-sync
problem. To order the CoFlows within each queue, SAATH resorts to
a Least-Contention-First (LCoF) policy which we show extends the
gist of SJF to the spatial dimension, complemented with starvation
freedom. Our evaluation using an Azure testbed and simulations of
two production cluster traces show that compared to Aalo, SAATH
reduces the CCT in median (P90) cases by 1.53x (4.5x) and 1.42x
(7x), respectively.

",17.879347455551382,18.580437788018433,221
CoNEXT_17_020.txt,14.872409360456107,13.434524178724075,CoNEXT,7862,"

Internet eXchange Points (IXPs) play an ever-growing role in
Internet inter-connection. To facilitate the exchange of routes
amongst their members, IXPs provide Route Server (RS) services to dispatch the routes according to each member’s peering policies. Nowadays, to make use of RSes, these policies
must be disclosed to the IXP. This poses fundamental questions regarding the privacy guarantees of route-computation
on confidential business information. Indeed, as evidenced by
interaction with IXP administrators and a survey of network
operators, this state of affairs raises privacy concerns among
network administrators and even deters some networks from
subscribing to RS services. We design sIxPACK, an RS service
that leverages Secure Multi-Party Computation (SMPC) to
keep peering policies confidential, while extending, the functionalities of today’s RSes. As SMPC is notoriously heavy
in terms of communication and computation, our design
and implementation of srxpack aims at moving computation outside of the SMPC without compromising the privacy
guarantees. We assess the effectiveness and scalability of
our system by evaluating a prototype implementation using
traces of data from one of the largest IXPs in the world. Our
evaluation results indicate that srxpAck can scale to support
privacy-preserving route-computation, even at IXPs with
many hundreds of member networks.

",16.32212239822248,15.848317152103565,209
CoNEXT_17_021.txt,13.601032296331816,11.962165732010593,CoNEXT,9109,"

Visible Light Communication (VLC) based on LEDs has been a
hot topic investigated for over a decade. However, most of the
research efforts in this area assume the intensity of the light emitted
from LEDs is constant. This is not true any more when Smart
Lighting is introduced to VLC in recent years, which requires the
LEDs to adapt their brightness according to the intensity of the
natural ambient light. Smart lighting saves power consumption
and improves user comfort. However, intensity adaptation severely
affects the throughput performance of the data communication. In
this paper, we propose SmartVLC, a system that can maximize the
throughput (benefit communication) while still maintaining the
LEDs’ illumination function (benefit smart lighting). A new adaptive
multiple pulse position modulation scheme is proposed to support
fine-grained dimming levels to avoid flickering and at the same time,
maximize the throughput under each dimming level. SmartVLC is
implemented on low-cost commodity hardware and several real-life
challenges in both hardware and software are addressed to make
SmartVLC a robust realtime system. Comprehensive experiments
are carried out to evaluate the performance of SmartVLC under
multifaceted scenarios. The results demonstrate that SmartVLC
supports a communication distance up to 3.6m, and improves the
throughput achieved with two state-of-the-art approaches by 40%
and 12% on average, respectively, without bringing any flickering
to users.

",15.774802946060372,14.267666666666667,227
CoNEXT_17_022.txt,16.211794742703784,15.82276221592106,CoNEXT,10172,"
Transport Layer Security (TLS), has become the de-facto standard
for secure Internet communication. When used correctly, it provides secure data transfer, but used incorrectly, it can leave users
vulnerable to attacks while giving them a false sense of security.
Numerous efforts have studied the adoption of TLS (and its predecessor, SSL) and its use in the desktop ecosystem, attacks, and
vulnerabilities in both desktop clients and servers. However, there
is a dearth of knowledge of how TLS is used in mobile platforms. In
this paper we use data collected by Lumen, a mobile measurement
platform, to analyze how 7,258 Android apps use TLS in the wild.
We analyze and fingerprint handshake messages to characterize the
TLS APIs and libraries that apps use, and also evaluate weaknesses.
We see that about 84% of apps use default OS APIs for TLS. Many
apps use third-party TLS libraries; in some cases they are forced
to do so because of restricted Android capabilities. Our analysis
shows that both approaches have limitations, and that improving
TLS security in mobile is not straightforward. Apps that use their
own TLS configurations may have vulnerabilities due to developer
inexperience, but apps that use OS defaults are vulnerable to certain
attacks if the OS is out of date, even if the apps themselves are up to
date. We also study certificate verification, and see low prevalence
of security measures such as certificate pinning, even among highrisk apps such as those providing financial services, though we
did observe major third-party tracking and advertisement services
deploying certificate pinning.

",15.186304673781336,14.096643356643359,262
CoNEXT_17_023.txt,15.783802379968538,13.421801195533462,CoNEXT,6038,"

Existing network policy abstractions handle basic group
based reachability and access control list based security policies. However, QoS policies as well as dynamic policies are
also important and not representing them in the high level
policy abstraction poses serious limitations. At the same
time, efficiently configuring and composing group based QoS
and dynamic policies present significant technical challenges,
such as (a) maintaining group granularity during configuration, (b) dealing with network-bandwidth contention among
policies from distinct writers and (c) dealing with multiple
path changes corresponding to dynamically changing policies,
group membership and end-point mobility. In this paper we
propose JANUS, a system which makes two major contributions. First, we extend the prior policy graph abstraction
model to represent complex QoS and dynamic stateful/temporal policies. Second, we convert the policy configuration
problem into an optimization problem with the goal of mazimizing the number of satisfied and configured policies, and
minimizing the number of path changes under dynamic environments. To solve this, JANUS presents several novel heuristic
algorithms. We evaluate our system using a diverse set of
bandwidth policies and network topologies. Our experiments
demonstrate that JANUS can achieve near-optimal solutions
in a reasonable amount of time.

",18.36309006607702,17.06676767676768,199
CoNEXT_17_024.txt,12.497219330725958,10.581306617068133,CoNEXT,6295,"

Remote Direct Memory Access over Converged Ethernet (RoCE)
deployments are vulnerable to deadlocks induced by Priority Flow
Control (PFC). Prior solutions for deadlock prevention either require significant changes to routing protocols, or require excessive
buffers in the switches. In this paper, we propose Tagger, a scheme
for deadlock prevention. It does not require any changes to the routing protocol, and needs only modest buffers. Tagger is based on the
insight that given a set of expected lossless routes, a simple tagging
scheme can be developed to ensure that no deadlock will occur
under any failure conditions. Packets that do not travel on these
lossless routes may be dropped under extreme conditions. We design such a scheme, prove that it prevents deadlock and implement
it efficiently on commodity hardware.

",13.25671669890799,12.191428571428574,129
CoNEXT_17_025.txt,15.113640060296547,13.728368632496114,CoNEXT,9738,"

TCP does not work well in modern cellular networks because the

current congestion-window-based (cwnd-based) congestion control mechanism intimately couples congestion control and packet

dispatch, which provides TCP with only indirect control of the

effective data rate. The throughput degradation arising from the

cwnd-based mechanism is especially serious when the uplink is

congested. We describe PropRate, a new rate-based TCP algorithm

that directly regulates the packets in the bottleneck buffer to achieve
a trade-off in terms of delay and throughput along a more efficient

frontier than conventional cwnd-based TCP variants. To the best

of our knowledge, PropRate is the first TCP algorithm that allows

an application to set and achieve a target average latency, if the

network conditions allow for it. Also, unlike the cwnd-based TCP

mechanism, our new rate-based TCP mechanism is significantly

more resilient to saturated uplinks in cellular networks. PropRate

does not require modifications at the receiver and is amenable to

practical deployment in the base stations and proxies in mobile

cellular networks.

",18.243605946275583,17.01425287356322,175
CoNEXT_17_026.txt,14.551824410801316,12.531213202421807,CoNEXT,10853,"

We have recently seen an increasing number of attacks that are
distributed, and span an entire wide area network (WAN). Today,
typically, intrusion detection systems (IDSs) are deployed at enterprise scale and cannot handle attacks that cover a WAN. Moreover,
such IDSs are implemented at a single entity that expects to look
at all packets to determine an intrusion. Transferring copies of raw
packets to centralized engines for analysis in a WAN can significantly impact both network performance and detection accuracy. In
this paper, we propose Jaal, a framework for achieving accurate network intrusion detection at scale. The key idea in Jaal is to monitor
traffic and construct in-network packet summaries. The summaries
are then processed centrally to detect attacks with high accuracy.
The main challenges that we address are (a) creating summaries
that are concise, but sufficient to draw highly accurate inferences
and (b) transforming traditional IDS rules to handle summaries
instead of raw packets. We implement Jaal on a large scale SDN
testbed. We show that on average Jaal yields a detection accuracy
of about 98%, which is the highest reported for ISP scale network
intrusion detection. At the same time, the overhead associated with
transferring summaries to the central inference engine is only about
35% of what is consumed if raw packets are transferred.

",15.668782140382113,12.816155129274396,219
CoNEXT_17_027.txt,16.181248306881766,14.792481680329903,CoNEXT,9957,"

A common practice to increase the reliability of a cloud application
is to deploy redundant instances. Unfortunately, such redundancy
efforts can be undermined if the application’s instances share common dependencies. This paper presents RECLOUD, a novel system
that can efficiently find a reliable deployment plan for cloud applications. RECLOUD considers and avoids common dependencies shared
across application instances that may lead to correlated failures, and
works with applications that even have complex internal structures.
RECLOUD utilizes various pieces of available dependency information (e.g., hardware, software and/or network dependencies) about
the cloud infrastructure to quantitatively assess the reliability of
the application’s deployment plan with rigorous error bounds. This
assessment further enables RECLOuD to find a deployment plan that
balances between reliability and other criteria such as application
performance and resource utilization. We implemented a fully functional system. The experimental results show that, even in a large
cloud environment with more than 27K hosts, RECLoUD needs only
30 seconds to find a deployment plan that is one order of magnitude
more reliable than the common practice.

",17.267425705330172,16.87839285714286,179
CoNEXT_17_028.txt,14.63025889004032,12.853828534210514,CoNEXT,6065,"

In this paper we study network architecture for unlicensed cellular networking for outdoor coverage in TV white spaces. The
main technology proposed for TV white spaces is 802.11af, a Wi-Fi
variant adapted for TV frequencies. However, 802.11af is originally
designed for improved indoor propagation. We show that long links,
typical for outdoor use, exacerbate known Wi-Fi issues, such as
hidden and exposed terminal, and significantly reduce its efficiency.

Instead, we propose CellFi, an alternative architecture based
on LTE. LTE is designed for long-range coverage and throughput
efficiency, but it is also designed to operate in tightly controlled
and centrally managed networks. CellFi overcomes these problems
by designing an LTE-compatible spectrum database component,
mandatory for TV white space networking, and introducing an
interference management component for distributed coordination.
CellFi interference management is compatible with existing LTE
mechanisms, requires no explicit communication between base
stations, and is more efficient than CSMA for long links.

We evaluate our design through extensive real world evaluation
on off-the-shelf LTE equipment and simulations. We show that,
compared to 802.11af, it increases coverage by 40% and reduces
median flow completion times by 2.3x.

",16.52667757954773,14.576124999999998,197
CoNEXT_17_029.txt,15.970911625076639,15.226445878934722,CoNEXT,6907,"

Stream processing pipelines operated by current big data streaming frameworks present two problems. First, the pipelines are not
flexible, controllable, and programmable enough to accommodate
dynamic streaming application needs. Second, the application-level
data routing over the pipelines do not exhibit optimal performance
for increasingly common one-to-many communication. To address
these problems, we propose an SDN-based real-time big data streaming framework called Typhoon, that tightly integrates SDN functionality into a real-time stream framework. By partially offloading
application-layer data routing and control to the network layer
via SDN interfaces and protocols, Typhoon provides on-the-fly
programmability of both the application and network layers, and
achieve high-performance data routing. In addition, Typhoon SDN
controller exposes cross-layer information, from both the application and the network, to SDN control plane applications to extend
the framework’s functionality. We introduce several SDN control
plane applications to illustrate these benefits.

",17.28802050969988,16.866729323308274,154
CoNEXT_17_030.txt,15.161040728423977,13.887790602488547,CoNEXT,7601,"

Most datacenters still use Equal Cost Multi-Path (ECMP), which
performs congestion-oblivious hashing of flows over multiple paths,
leading to an uneven distribution of traffic. Alternatives to ECMP
come with deployment challenges, as they require either changing
the tenant VM network stacks (e.g., MPTCP) or replacing all of
the switches (e.g., CONGA). We argue that the hypervisor provides
a unique point for implementing load-balancing algorithms that
are easy to deploy, while still reacting quickly to congestion. We
propose Clove, a scalable load-balancer that (i) runs entirely in the
hypervisor, requiring no modifications to tenant VM networking
stacks or physical switches, and (ii) works on any topology and
adapts quickly to topology changes and traffic shifts. Clove relies
on standard ECMP in physical switches, discovers paths using a
novel traceroute mechanism, uses software-based flowlet-switching,
and continuously learns congestion (or path utilization) state using
standard switch features. It then manipulates packet-header fields
in the hypervisor switch to direct traffic over less congested paths.
Clove achieves 1.5 to 7 times smaller flow-completion times at 70%
network load than other load-balancing algorithms that work with
existing hardware. Clove also captures some 80% of the performance
gain of best-of-breed hardware-based load-balancing algorithms like
CONGA that require new equipment.

",16.827784334635936,15.966598837209308,219
CoNEXT_17_031.txt,13.094628845348051,10.92403590687739,CoNEXT,10643,"

Millimeter-wave devices must use highly directional antennas to
achieve GBit/s data rates over reasonable distances due to the high
path loss. As a consequence, it is important to precisely align the
antenna beams between sender and receiver. Even minor movement
or rotation of a device can result in beam misalignment and thus
a strong performance degradation. Existing work as well as standards such as IEEE 802.11ad tackle this issue by means of antenna
sector probing. This comes at the expense of a significant overhead,
which may significantly reduce the performance of millimeter-wave
communication, particularly in mobile scenarios. In this paper, we
present a mechanism that can track both movement and rotation of
60 GHz mobile devices with zero overhead. To this end, we transmit
part of the preamble of each packet using a multi-lobe beampattern.
Our approach does not require any additional control messages and
is backward compatible with 802.11ad. We implement our scheme
on a 60 GHz testbed using phased antenna arrays, and show that
we reduce the angle error to less than 5° in most cases. We also
perform simulations to validate our approach in a wide range of
scenarios, achieving up to 2x throughput gain.

",14.836745963215662,12.90433990147783,205
CoNEXT_17_032.txt,16.20010209045384,14.921388103584757,CoNEXT,8289,"

Large-scale mobile traffic analytics is becoming essential to digital
infrastructure provisioning, public transportation, events planning,
and other domains. Monitoring city-wide mobile traffic is however
a complex and costly process that relies on dedicated probes. Some
of these probes have limited precision or coverage, others gather
tens of gigabytes of logs daily, which independently offer limited insights. Extracting fine-grained patterns involves expensive spatial
aggregation of measurements, storage, and post-processing. In this
paper, we propose a mobile traffic super-resolution technique that
overcomes these problems by inferring narrowly localised traffic
consumption from coarse measurements. We draw inspiration from
image processing and design a deep-learning architecture tailored
to mobile networking, which combines Zipper Network (ZipNet)
and Generative Adversarial neural Network (GAN) models. This enables to uniquely capture spatio-temporal relations between traffic
volume snapshots routinely monitored over broad coverage areas
(‘low-resolution’) and the corresponding consumption at 0.05 km?
level (‘high-resolution’) usually obtained after intensive computation. Experiments we conduct with a real-world data set demonstrate that the proposed ZipNet(-GAN) infers traffic consumption
with remarkable accuracy and up to 100x higher granularity as
compared to standard probing, while outperforming existing data
interpolation techniques. To our knowledge, this is the first time
super-resolution concepts are applied to large-scale mobile traffic
analysis and our solution is the first to infer fine-grained urban
traffic patterns from coarse aggregates.

",17.353723509956247,17.058000000000003,234
EuroPar_17_001.txt,14.311876939941868,13.75343265141938,EuroPar,5955,"
. Radix trees belong to the class of trie data structures, used
for storing both sets and dictionaries in a way optimized for space and
lookup. In this work, we present an efficient non-blocking implementation
of radix tree data structure that can be configured for arbitrary alphabet
sizes. Our algorithm implements a linearizable set with contains, insert
and remove operations and uses single word compare-and-swap (CAS)
instruction for synchronization. We extend the idea of marking the child
edges instead of nodes to improve the parallel performance of the data
structure. Experimental evaluation indicates that our implementation
out-performs other known lock-free implementations of trie and binary
search tree data structures using CAS by more than 100% under heavy
contention.

",15.381575749822971,15.301409836065577,123
EuroPar_17_002.txt,15.324197758612073,13.540415051881485,EuroPar,5065,"
. The paper presents the first concurrency-optimal implementation of a binary search tree (BST). The implementation, based on a
standard sequential implementation of a partially-external tree, ensures
that every schedule, i.e., interleaving of steps of the sequential code, is
accepted unless linearizability is violated. To ensure this property, we use
a novel read-write locking protocol that protects tree edges in addition
to its nodes.

Our implementation performs comparably to the state-of-the-art
BSTs and even outperforms them on few workloads, which suggests that
optimizing the set of accepted schedules of the sequential code can be an
adequate design principle for efficient concurrent data structures.


",18.243605946275583,16.90111111111111,110
EuroPar_17_003.txt,13.865139601183568,11.927007905138343,EuroPar,3956,"
. In this paper, we present a fine-grained multi-stage metricbased triangular remeshing algorithm on manycore and NUMA architectures. It is motivated by the dynamically evolving data dependencies
and workload of such irregular algorithms, often resulting in poor performance and data locality at high number of cores. In this context, we
devise a multi-stage algorithm in which a task graph is built for each kernel. Parallelism is then extracted through fine-grained independent set,
maximal cardinality matching and graph coloring heuristics. In addition
to index ranges precalculation, a dual-step atomic-based synchronization
scheme is used for nodal data updates. Despite its intractable latencyboundness, a good overall scalability is achieved on a NUMA dual-socket
Intel Haswell and a dual-memory Intel KNL computing nodes (64 cores).
The relevance of our synchronization scheme is highlighted through a
comparison with the state-of-the-art.


",16.26309291913925,15.1120197044335,146
EuroPar_17_004.txt,15.334413061927382,13.549320942988764,EuroPar,5228,"
. Thread-Level Speculation (TLS) is a hardware/software
technique that enables the execution of multiple loop iterations in parallel, even in the presence of some loop-carried dependences. TLS requires
hardware mechanisms to support conflict detection, speculative storage,
in-order commit of transactions, and transaction roll-back. There is no
off-the-shelf processor that provides direct support for TLS. Speculative
execution is supported, however, in the form of Hardware Transactional
Memory (HTM)—available in recent processors such as the Intel Core
and the IBM POWERS. Earlier work has demonstrated that, in the
absence of specific TLS support in commodity processors, HTM support
can be used to implement TLS. This paper presents a careful evaluation
of the implementation of TLS on the HTM extensions available in such
machines. This evaluation provides evidence to support several important claims about the performance of TLS over HTM in the Intel Core
and the IBM POWERS8 architectures. Experimental results reveal that
by implementing TLS on top of HTM, speed-ups of up to 3.8x can be
obtained for some loops.

",16.218646115125612,14.377500000000001,178
EuroPar_17_005.txt,15.374232970151567,13.614735384128718,EuroPar,5216,"
. This study characterizes the NVIDIA Jetson TK1 and TX1
Platforms, both built on a NVIDIA Tegra System on Chip and combining a quad-core ARM CPU and an NVIDIA GPU. Their heterogeneous
nature, as well as their wide operating frequency range, make it hard
for application developers to reason about performance and determine
which optimizations are worth pursuing. This paper attempts to inform
developers’ choices by characterizing the platforms’ performance using
Roofline models obtained through an empirical measurement-based approach as well as through a case study of a heterogeneous application
(matrix multiplication). Our results highlight a difference of more than
an order of magnitude in compute performance between the CPU and
GPU on both platforms. Given that the CPU and GPU share the same
memory bus, their Roofline models’ balance poinis are also more than
an order of magnitude apart. We also explore the impact of frequency
scaling: build CPU and GPU Roofline profiles and characterize both platforms’ balance point variation, power consumption, and performance per
watt as frequency is scaled.

The characterization we provide can be used in two main ways. First,
given an application, it can inform the choice and number of processing
elements to use (i.e., CPU/GPU and number of cores) as well as the
optimizations likely to lead to high performance gains. Secondly, this
characterization indicates that developers can use frequency scaling to
tune the Jetson Platform to suit the requirements of their applications.
Third, given a required power/performance budget, application developers can identify the appropriate parameters to use to tune the Jetson
platforms to their specific workload requirements. We expect that this
optimization approach can lead to overall gains in performance and/or
power efficiency without requiring application changes.

",17.122413403193683,16.170820399113083,289
EuroPar_17_006.txt,15.539556780473472,13.935066117159607,EuroPar,4618,"
. Offering insights into the behavior of applications at higher
scale, performance models are useful for finding performance bugs and
tuning the system. Extra-P, a tool for automated performance modeling,
uses statistical methods to automatically generate, from a small number
of performance measurements, models that can be used to predict performance where no measurements are available. However, the current version requires the manual pre-configuration of a search space, which might
turn out to be unsuitable for the problem at hand. Furthermore, noise in
the data often leads to models that indicate a worse behavior than there
actually is. In this paper, we propose a new model-generation algorithm
that solves both of the above problems: The search space is built and
automatically refined on demand, and a scale-independent error metric
tells both when to stop the refinement process and whether a model
reflects faithfully enough the behavior the data exhibits. This makes
Extra-P easier to use, while also allowing it to produce more accurate
results. Using data from previous case studies, we show that the mean
relative prediction error decreases from 46% to 13%.

",16.785175570968402,15.336592818945764,188
EuroPar_17_007.txt,14.843891307770505,13.025526745509428,EuroPar,5650,"
. This paper describes a cache-line conflict profiling method
that advances the state of the art performance tuning workflow by accurately highlighting the sources of conflicts. The basic idea behind this is
the use of cache simulators as a diagnosis tool for cache-line conflicts.
We also propose a mechanism that enables to identify where line conflict misses are incurred and the reasons why the conflicts occur. We
evaluate our conflict simulator using some of the benchmark codes used
in the HPC field. From the results, we confirm that our simulator can
accurately model the cache behaviors that cause line conflicts and reveal
the sources of them during the execution. Finally, we demonstrate that
optimizations assisted by our mechanism contribute to improving performance for both of serial and parallel executions.


",15.470042427545799,13.918461538461539,131
EuroPar_17_008.txt,13.997688319796524,12.626863948408573,EuroPar,4172,"
. Large scale distributed systems are expected to consume
huge amounts of energy. To solve this issue, shutdown policies constitute an appealing approach able to dynamically adapt the resource set
to the actual workload. However, multiple constraints have to be taken
into account for such policies to be applied on real infrastructures, in particular the time and energy cost of shutting down and waking up nodes,
and power capping to avoid disruption of the system. In this paper, we
propose models translating these various constraints into different shutdown policies that can be combined. Our models are validated through
simulations on real workload traces and power measurements on real
testbeds.

",15.112257680678326,13.811777777777781,109
EuroPar_17_009.txt,15.562485235769564,13.323765733912516,EuroPar,4094,"
. For the parallel-in-time integration method Parareal,
pipelining can be used to hide some of the cost of the serial correction
step and improve its efficiency. The paper introduces a basic OpenMP
implementation of pipelined Parareal and compares it to a standard
MPI-based variant. Both versions yield almost identical runtimes, but,
depending on the compiler, the OpenMP variant consumes about 7%
less energy and has a significantly smaller memory footprint. However,
its higher implementation complexity might make it difficult to use in
legacy codes and in combination with spatial parallelisation.


",17.971250198000288,16.338695652173914,93
EuroPar_17_010.txt,15.450864572248289,14.2981717856668,EuroPar,4064,"
. Adaptive Mesh Refinement (AMR) is a well known method
for efficiently solving partial differential equations. A straightforward
AMR algorithm typically exhibits many synchronization points even
during a single time step, where costly communication often degrades
the performance. This problem will be even more pronounced on future
supercomputers containing billion way parallelism, which will raise the
communication cost further. Re-designing AMR algorithms to avoid synchronization is not a viable solution due to the large code size and complex control structures. We present a nonintrusive asynchronous approach to hiding the effects of communication in an AMR application.
Specifically, our approach reasons about data dependencies automatically using domain knowledge about AMR applications, allowing asynchrony to be discovered with only a modest amount of code modification.
Using this approach, we optimize the synchronous AMR algorithm in the
BoxLib software framework without severely affecting the productivity
of the application programmer. We observe around 27-31% performance
improvement for an advection solver on the Hazel Hen supercomputer
using 12288 cores.

",16.678067442207542,17.054962121212125,166
EuroPar_17_011.txt,14.138980108339055,12.220679945192472,EuroPar,4686,"
. We propose a novel parallel approach to compute the sparse
matrix-vector product (SPMV) on graphics processing units (GPUs),
optimized for matrices with an irregular row distribution of the non-zero
entries. Our algorithm relies on the standard CSR format to store the
sparse matrix, requires an inexpensive pre-processing step, and consumes
only a minor amount of additional memory compared with significantly
more expensive GPU-specific sparse matrix layouts. In addition, we propose a simple heuristic to determine whether our method or the standard
CSR SPMV algorithm should be used for a specific matrix. As a result,
our proposal, combined with the standard CSR SPMV, can be adopted
as the default choice for the implementation of SPMV in general-purpose
sparse linear algebra libraries for GPUs.

",17.971250198000288,17.976751968503937,128
EuroPar_17_012.txt,14.596316946051246,11.981310890061007,EuroPar,4011,"
. Heterogeneous systems are nowadays a common choice in
the path to Exascale. Through the use of accelerators they offer outstanding energy efficiency. The programming of these devices employs
the host-device model, which is suboptimal as CPU remains idle during
kernel executions, but still consumes energy. Making the CPU contribute
computing effort might improve the performance and energy consumption of the system. This paper analyses the advantages of this approach
and sets the limits of when its beneficial. The claims are supported by
a set of models that determine how to share a single data-parallel task
between the CPU and the accelerator for optimum performance, energy
consumption or efficiency. Interestingly, the models show that optimising performance does not always mean optimum energy or efficiency as
well. The paper experimentally validates the models, which represent an
invaluable tool for programmers when faced with the dilemma of whether
to distribute their workload in these systems.

",16.678067442207542,14.521396103896105,155
EuroPar_17_013.txt,14.970003680105606,12.41132,EuroPar,4979,"
. Processor speed is improving at a faster rate than the speed
of main memory, which makes memory accesses increasingly expensive.
One way to solve this problem is to reduce miss ratio of the processor’s
last level cache by improving its replacement policy. We approach the
problem by co-designing the runtime system and hardware and exploiting the semantics of the applications written in data-flow task-based
programming models to provide hardware with information about the
task types and task data-dependencies. We propose the Task-Type aware
Insertion Policy, TTIP, which uses the runtime system to dynamically
determine the best probability per task type for bimodal insertion in
the recency stack and the static Dependency-Type aware Insertion Policy, DTIP, that inserts cache lines in the optimal position taking into
account the dependency types of the current task. TTIP and DTIP perform similarly or better than state-of-the-art replacement policies, while
requiring less hardware.

",19.083932058031824,17.3247898089172,159
EuroPar_17_014.txt,14.625140746180303,12.610560446140227,EuroPar,4174,"
. Careful data layout design is crucial for achieving high performance. However exploring data layouts is time-consuming and errorprone, and assessing the impact of a layout transformation on performance is difficult without performing it. We propose to guide application programmers through data layout restructuring by providing a
comprehensive multidimensional description of the initial layout, built
from trace analysis, and then by giving a performance evaluation of the
transformations tested and an expression of each transformed layout.
The programmer can limit the exploration to layouts matching some
patterns. We apply this method to two multithreaded applications. The
performance prediction of multiple transformations matches within 5%
the performance of hand-transformed layout code.


",16.52667757954773,15.118693693693697,112
EuroPar_17_015.txt,13.940010088308938,11.147976110787742,EuroPar,5121,"
. Graphics Processing Units (GPUs) have become the accelerator of choice for data-parallel applications, enabling the execution of
thousands of threads in a Single Instruction - Multiple Thread (SIMT)
fashion. Using OpenCL terminology, GPUs offer a global memory space
shared by all the threads in the GPU, as well as a low-latency local
memory space shared by a subset of the threads. The latter is used as a
scratchpad to improve the performance of the applications.

We propose GPU-LocalTM, a hardware transactional memory (TM),
as an alternative to data locking mechanisms in local memory. GPULocalTM allocates transactional metadata in the existing memory
resources, minimizing the storage requirements for TM support. In addition, it ensures forward progress through an automatic serialization
mechanism. In our experiments, GPU-LocalTM provides up to 100X
speedup over serialized execution.

",15.903189008614273,14.482539682539684,136
EuroPar_17_016.txt,16.45203868358879,15.325696628796319,EuroPar,4412,"
. Although simulators provide approximate, faster and easier
simulation of an application execution in Clouds, still many researchers
argue that these results cannot be always generalized for complex application types, which consist of many dependencies among tasks and various scheduling possibilities, such as workflows. DynamicCloudSim, the
extension of the well known CloudSim simulator, offers users the capability to simulate the Cloud heterogeneity by introducing noisiness in
dozens parameters. Still, it is difficult, or sometimes even impossible to
determine appropriate values for all these parameters because they are
usually Cloud or application-dependent. In this paper, we propose a new
model that simplifies the simulation setup for a workflow and reduces
the bias between the behavior of simulated and real Cloud environments
based on one parameter only, the Cloud noisiness. It represents the noise
produced by the Cloud’s interference including the application’s (in our
case a workflow) noisiness too. Another novelty in our model is that it
does not use a normal distribution naively to create noised values, but
shifts the mean value of the task execution time by the cloud noisiness
and uses its deviation as a standard deviation. Besides our model reduces
the complexity of DynamicCloudSim’s heterogeneity model, evaluation
conducted in Amazon EC2 shows that it is also more accurate, with
better trueness (closeness to the real mean values) of up to 9.2% and
precision (closeness to the real deviation) of up to 8.39 times.


",19.99310930976973,20.299452887538,241
EuroPar_17_017.txt,14.82934308801249,12.639635559131133,EuroPar,4133,"
. In a Cloud computing data center and especially in a IaaS
(Infrastructure as a Service), performance predictability is one of the most
important challenges. For a given allocated virtual machine (VM) in one
laaS, a client expects his application to perform identically whatever is
the hosting physical server or its resource management strategy. However, performance predictability is very difficult to enforce in a heterogeneous hardware environment where machines do not have identical performance characteristics, and even more difficult when machines are internally heterogeneous as for Asymmetric Multicore Processor machines.
In this paper, we introduce a VM scheduler extension which takes into
account hardware performance heterogeneity of Asymmetric Multicore
Processor machines in the cloud. Based on our analysis of the problem,
we designed and implemented two solutions: the first weights CPU allocations according to core performance, while the second adapts CPU allocations to reach a given instruction execution rate (Ips) regardless the
core types. We demonstrate that such scheduler extensions can enforce
predictability with a negligible overhead on application performance.

",20.890749979661237,20.042337278106512,170
EuroPar_17_018.txt,15.61540600798805,13.814555584075098,EuroPar,4096,"
. Time critical applications are appealing to deploy in clouds
due to the elasticity of cloud resources and their on-demand nature. However, support for deploying application components with strict deadlines
on their deployment is lacking in current cloud providers. This is particularly important for adaptive applications that must automatically and
seamlessly scale, migrate, or recover swiftly from failures. A common
deployment procedure is to transmit application packages from the application provider to the cloud, and install the application there. Thus, users
need to manually deploy their applications into clouds step by step with
no guarantee regarding deadlines. In this work, we propose a Deadlineaware Deployment System (DDS) for time critical applications in clouds.
DDS enables users to automatically deploy applications into clouds. We
design bandwidth-aware EDF scheduling algorithms in DDS that minimize the number of deployments that miss their deadlines and maximize
the utilization of network bandwidth. In the evaluation, we show that
DDS leverages network bandwidth sufficiently, and significantly reduces
the number of missed deadlines during deployment.

",16.32212239822248,14.635108481262328,170
EuroPar_17_019.txt,15.537813076716066,14.081434728868889,EuroPar,4438,"
. Most message logging protocols rely on a centralized event
logger to store information (i.e., the determinants) to allow the recovery
of an application process. This centralized approach, besides suffering
from the single point of failure problem, represents a bottleneck for the
efficiency of message logging protocols. In this work, we present a faulttolerant distributed event logger based on consensus that outperforms
the centralized approach. We implemented the event logger of MPI determinants using the Paxos algorithm. Our event logger inherits the Paxos
properties: safety is guaranteed even if the system is asynchronous and
liveness is guaranteed despite processes failures. Experimental results are
reported for the performance of the distributed event logger based both
on classic Paxos and parallel Paxos applied to AMG (Algebraic MultiGrid) and NAS Parallel Benchmark applications.

",17.122413403193683,16.212054263565893,131
EuroPar_17_020.txt,14.594672075219282,11.945985130412055,EuroPar,4314,"
. Single-Source Shortest Paths (SSSP) is a well-studied graph
problem. Examples of SSSP algorithms include the original Dijkstra’s
algorithm and the parallel A-stepping and KLA-SSSP algorithms. In
this paper, we use a novel Abstract Graph Machine (AGM) model to
show that all these algorithms share a common logic and differ from one
another by the order in which they perform work. We use the AGM
model to thoroughly analyze the family of algorithms that arises from
the common logic. We start with the basic algorithm without any ordering (Chaotic), and then we derive the existing and new algorithms by
methodically exploring semantic and spatial ordering of work. Our experimental results show that new derived algorithms show better performance than the existing distributed memory parallel algorithms, especially at higher scales.

",16.11434528070225,14.355151515151515,134
EuroPar_17_021.txt,13.701398527465354,11.851805151112494,EuroPar,5021,"
. This paper reports our experiments to compare various
deployment strategies of memcached-like in-memory storage for Hadoop
on supercomputers, where each node often does not have a local disk
but shares a slow central disk. For the experiments, we developed our
own memcached-like file system, named SEMem, for Hadoop. Since
SEMem was designed for supercomputers, it uses MPI for communication. SEMem is configurable to adopt various deployment strategies and
our experiments revealed that a good deployment strategy was allocating some nodes that work only for in-memory storage but do not directly
perform map-reduce computation.

",16.827784334635936,15.758877551020412,99
EuroPar_17_022.txt,13.868531484121984,11.62904565746619,EuroPar,4769,"
. The Tucker decomposition is a higher-order analogue of the
singular value decomposition and is a popular method of performing
analysis on multi-way data (tensors). Computing the Tucker decomposition of a sparse tensor is demanding in terms of both memory and computational resources. The primary kernel of the factorization is a chain of
tensor-matrix multiplications (TTMc). State-of-the-art algorithms accelerate the underlying computations by trading off memory to memoize the
intermediate results of TTMc in order to reuse them across iterations.
We present an algorithm based on a compressed data structure for sparse
tensors and show that many computational redundancies during TTMc
can be identified and pruned without the memory overheads of memoization. In addition, our algorithm can further reduce the number of
operations by exploiting an additional amount of user-specified memory.
We evaluate our algorithm on a collection of real-world and synthetic
datasets and demonstrate up to 20.7x speedup while using 28.5x less
memory than the state-of-the-art parallel algorithm.

",17.93193317476759,16.867134404057484,172
EuroPar_17_023.txt,15.422369604007638,13.727974485420244,EuroPar,3952,"
. Solving large numbers of small linear algebra problems simultaneously is becoming increasingly important in many application areas.
Whilst many researchers have investigated the design of efficient batch
linear algebra kernels for GPU architectures, the common approach for
many /multi-core CPUs is to use one core per subproblem in the batch.
When solving batches of very small matrices, 2 x 2 for example, this
design exhibits two main issues: it fails to fully utilize the vector units
and the cache of modern architectures, since the matrices are too small.
Our approach to resolve this is as follows: given a batch of small matrices
spread throughout the primary memory, we first reorganize the elements
of the matrices into a contiguous array, using a block interleaved memory
format, which allows us to process the small independent problems as a
single large matrix problem and enables cross-matrix vectorization. The
large problem is solved using blocking strategies that attempt to optimize
the use of the cache. The solution is then converted back to the original
storage format. To explain our approach we focus on two BLAS routines:
general matrix-matrix multiplication (GEMM) and the triangular solve
(TRSM). We extend this idea to LAPACK routines using the Cholesky
factorization and solve (POSV). Our focus is primarily on very small
matrices ranging in size from 2 x 2 to 32 x 32. Compared to both MKL
and OpenMP implementations, our approach can be up to 4 times faster
for GEMM, up to 14 times faster for TRSM, and up to 40 times faster for
POSV on the new Intel Xeon Phi processor, code-named Knights Landing (KNL). Furthermore, we discuss strategies to avoid data movement
between sockets when using our interleaved approach on a NUMA node.

",15.429909175157395,14.718338019368954,292
EuroPar_17_024.txt,14.066368025176374,11.720705278370382,EuroPar,5244,"
. The Sparse Matrix-Vector Multiplication (SpMV) is an
important building block in High Performance Computing. Performance
improvements for the SpMV are often gained by the development of new
optimized sparse matrix formats either by utilizing special sparsity patterns of a matrix or by taking bottlenecks of a hardware architecture
into account. In this work a requirements analysis is done for sparse
matrix formats with an emphasis on the parallel SpMV for large general
sparse matrices. Based on these requirements, three new sparse matrix
formats were developed, each combining several optimization techniques
and addressing different optimization goals/hardware architectures. The
CSR5 Bit Compressed (CSR5BC) format is an extension to the existing
CSR5 format and optimized for GPUs. The other two formats, Hybrid
Compressed Slice Storage (HCSS) and Local Group Compressed Sparse
Row (LGCSR), are new formats optimized for multi-core/-processor
architectures including the Xeon Phi Knights Landing. Results show
that all three storage formats deliver good parallel SpMV performance
on their target architectures over a large set of test matrices compared
to other well performing formats in vendor and research libraries.

",16.954822765917157,15.486550907655886,182
EuroPar_17_025.txt,14.707687707892205,12.348106503976439,EuroPar,4979,"
. Kronecker algebra is a matrix calculus which allows the generation of thread interleavings from the source-code of a program. Thread
interleavings have been shown effective for proving the absence of deadlocks. Because the number of interleavings grows exponentially in the
number of threads, deadlock analysis is still a challenging problem.

To make the computation of thread interleavings tractable, we propose
a lazy, parallel evaluation method for Kronecker algebra. Our method
incorporates the constraints induced by synchronization constructs. To
reduce problem size, only interleavings legal under the locking behavior of a program are considered. We leverage the data-parallelism of
Kronecker sum- and product-operations for multicores and GPUs. Proposed algebraic transformations further improve performance. For one
synthetic and two real-world benchmarks, our GPU implementation is
up to 5453x faster than our multi-threaded version. Lazy evaluation significantly reduces memory consumption compared to both the sequential
and the multicore versions of the SPIN model-checker.

",14.554592549557764,13.715794871794873,157
EuroPar_17_026.txt,15.510273805419256,14.030623202301054,EuroPar,3709,"
. Manycore optimizations are essential for achieving performance worthy of anticipated exascale systems. Utilization of manycore
chips is inevitable to attain the desired floating point performance of
these energy-austere systems. In this work, we revisit ExaFMM, the
open source Fast Multiple Method (FMM) library, in light of highly
tuned shared-memory parallelization and detailed performance analysis
on the new highly parallel Intel manycore architecture, Knights Landing
(KNL). We assess scalability and performance gain using task-based parallelism of the FMM tree traversal. We also provide an in-depth analysis of the most computationally intensive part of the traversal kernel
{i.e., the particle-to-particle (P2P) kernel), by comparing its performance
across KNL and Broadwell architectures. We quantify different configurations that exploit the on-chip 512-bit vector units within different taskbased threading paradigms. MPI communication-reducing and NUMAaware approaches for the FMM’s global tree data exchange are examined
with different cluster modes of KNL. By applying several algorithm- and
architecture-aware optimizations for FMM, we show that the N-Body
kernel on 256 threads of KNL achieves on average 2.8x speedup compared to the non-vectorized version, whereas on 56 threads of Broadwell,
it achieves on average 2.9x speedup. In addition, the tree traversal kernel on KNL scales monotonically up to 256 threads with task-based programming models. The MPI-based communication-reducing algorithms
show expected improvements of the data locality across the KNL on-chip
network.

",17.80541091248751,16.55595358649789,242
EuroPar_17_027.txt,15.358908947100979,13.242340215429671,EuroPar,4633,"
. Pattern matching on large graphs is the foundation for a
variety of application domains. The continuously increasing size of the
underlying graphs requires highly parallel in-memory graph processing
engines that need to consider non-uniform memory access (NUMA) and
concurrency issues to scale up on modern multiprocessor systems. To
tackle these aspects, a fine-grained graph partitioning becomes increasingly important. Hence, we present a classification of graph partitioning strategies and evaluate representative algorithms on medium and
large-scale NUMA systems in this paper. As a scalable pattern matching
processing infrastructure, we leverage a data-oriented architecture that
preserves data locality and minimizes concurrency-related bottlenecks on
NUMA systems. Our in-depth evaluation reveals that the optimal partitioning strategy depends on a variety of factors and consequently, we
derive a set of indicators for selecting the optimal partitioning strategy
suitable for a given graph and workload.

",19.287186520377343,18.38904109589041,147
EuroPar_17_028.txt,16.603839676038884,14.457584530386743,EuroPar,3911,"
. This paper describes a dynamic framework for mapping the
threads of parallel applications to the computation cores of parallel systems. We propose a feedback-based mechanism where the performance
of each thread is collected and used to drive the reinforcement-learning
policy of assigning affinities of threads to CPU cores. The proposed
framework is flexible enough to address different optimization criteria,
such as maximum processing speed and minimum speed variance among
threads. We evaluate the framework on the Ant Colony optimization parallel benchmark from the heuristic optimization application domain, and
demonstrate that we can achieve an improvement of 12% in the execution time compared to the default operating system scheduling /mapping
of threads under varying availability of resources (e.g. when multiple
applications are running on the same system).

",20.736966565827903,19.7525,130
EuroPar_17_029.txt,14.589515175725527,12.787485732859697,EuroPar,5038,"
. We propose a technique called speculative lagging, which
improves performance by dynamically adding periods of idle execution
into the message-oriented system. The speculation is guided by a statistical model, which predicts context switches that benefit from delays. We
analytically derive the expected speedup, which, for a fixed confidence,
allows identifying lagging opportunities in O(1) time, without a performance overhead. We describe the corresponding speculation algorithm
and use it to extend an existing scheduler. Comparison with other actor
frameworks on standard benchmarks shows improvements of up to 2.1x.

",15.381575749822971,14.952000000000002,91
EuroPar_17_030.txt,15.346423861041803,13.677447308111358,EuroPar,4888,"
. Finite difference methods are commonplace in scientific computing. Despite their apparent regularity, they often exhibit load imbalance that damages their efficiency. We characterize the spatial and temporal load imbalance of Ondes3D, a seismic wave propagation simulator.
We reveal that this imbalance originates from the nature of the input
data and from low-level CPU optimizations. Such dynamic imbalance
should therefore be quite common and is intractable by any static approach or classical code reorganization. An effective solution, with few
code modifications, combines domain over-decomposition and dynamic
load balancing (e.g., with AMPI), migrating data and computation at
the granularity of an MPI rank. It generally requires a careful tuning
of the over-decomposition level, the load balancing heuristic and frequency. These choices are quite dependent on application and platform
characteristics. In this paper, we propose a methodology that leverages
the capabilities of the SimGrid framework to conduct such study at low
experimental cost. It combines emulation, simulation, and application
modeling that requires minimal code modification and yet manages to
capture both spatial and temporal load imbalance, faithfully predicting its overall performance. We compare simulation and real executions
results and show how our strategy can be used to determine the best load
balancing configuration for a given application/hardware configuration.

",17.332850977054683,16.266889952153115,211
EuroPar_17_031.txt,15.691223873077021,14.052535512317203,EuroPar,4671,"
. Energy efficiency in data centres is addressed through workload management usually to reduce the operational costs and as a byproduct, the environmental footprint. This includes to minimise total
power consumption or to minimise the power issued from non-renewable
energy sources. Hence, the performance requirements of the client’s applications are either totally overlooked or strictly enforced.

To encourage profitable sustainability in data centres, we consider the
total financial gain as a trade-off between energy efficiency and client
satisfaction. We propose Carver to orchestrate energy-adaptive applications, according to performance and environmental preferences and
given forecasts of the renewable energy production. We validated Carver
by simulating a testbed powered by the grid and a photovoltaic array
and running the Web service HP LIFE.

",17.693802365651003,17.060284552845534,125
EuroPar_17_032.txt,14.915757859047172,12.71073051238844,EuroPar,4493,"
. Besides correctness, scalability is one of the top priorities
of parallel programmers. With manual analytical performance modeling often being too laborious, developers increasingly resort to empirical
performance modeling as a viable alternative, which learns performance
models from a limited amount of performance measurements. Although
powerful automatic techniques exist for this purpose, they usually struggle with the situation where performance data representing two or more
different phenomena are conflated into a single performance model. This
not only generates an inaccurate model for the given data, but can also
either fail to point out existing scalability issues or create the appearance of such issues when none are present. In this paper, we present an
algorithm to detect segmentation in a sequence of performance measurements and estimate the point where the behavior changes. Our method
correctly identified segmentation in more than 80% of 5.2 million synthetic tests and confirmed expected segmentation in three application
case studies.

",19.454632303725965,18.263496732026145,155
EuroPar_17_033.txt,16.52983254899675,14.909735840065036,EuroPar,4380,"
. As the complexity and diversity of computer hardware and
the elaborateness of network technologies have made the implementation of portable and efficient algorithms more challenging, the need to
understand application communication patterns has become increasingly
relevant. This paper presents details of the design and evaluation of a
communication-monitoring infrastructure developed in the Open MPI
software stack that can expose a dynamically configurable level of detail
concerning application communication patterns.

",23.326708150471678,24.86285714285714,71
EuroPar_17_034.txt,17.213221062166074,15.361494528732546,EuroPar,4537,"
. In this article, performance expectations for MPI neighborhood collective operations are formulated as self-consistent performance
guidelines. A microbenchmark and an experimental methodology are
presented to assess these guidelines. Measurement results from a large,
InfiniBand-based cluster, the Vienna Scientific Cluster (VSC), as well
as from a small commodity cluster computer are shown and discussed
to illustrate the methodology and to gain first insights into the performance of current MPI implementations. Results show that the examined
libraries seem to be sensitive to the order in which topological neighbors are specified, and that in some cases Cartesian topologies can be
outperformed by simulating them with distributed graph topologies.
",19.78447435784618,18.111658878504674,108
EuroPar_17_035.txt,15.897633848965352,14.082494508227299,EuroPar,5200,"
. De novo genome assembly is one of the most important
and challenging computational problems in modern genomics; further, it
shares algorithms and communication patterns important to other graph
analytic and irregular applications. Unlike simulations, it has no floating point arithmetic and is dominated by small memory transactions
within and between computing nodes. In this work, we focus on the
highly scalable HipMer assembler and identify the dominant algorithms
and communication patterns, also using microbenchmarks to capture the
workload. We evaluate HipMer on a variety of platforms from the latest HPC systems to ethernet clusters. HipMer performs well on all single
node systems, including the Xeon Phi manycore architecture. Given large
enough problems, it also demonstrates excellent scaling across nodes in
an HPC system, but requires a high speed network with low overhead
and high injection rates. Our results shed light on the architectural features that are most important for achieving good parallel efficiency on
this and related problems.

",16.954822765917157,15.841971066907778,159
EuroPar_17_036.txt,15.80012430251793,14.071136115160353,EuroPar,5120,"
. MapReduce environments offer great scalability by restricting the programming model to only map and reduce operators. This
abstraction simplifies many difficult problems occuring in generic distributed computations like fault tolerance and synchronization, hiding
them from the programmer. There are, however, algorithms that cannot
be easily or efficiently expressed in MapReduce, such as recursive functions. In this paper we extend the Apache Spark runtime so that it can
support recursive queries. We also introduce a new parallel and more
lightweight scheduling mechanism, ideal for scheduling a very large set
of tiny tasks. We implemented the aformentioned scheduler and found
that it simplifies the code for recursive computation and can perform
up to 2.1x faster than the default Spark scheduler.

",17.315433740611066,15.180000000000003,120
EuroPar_17_037.txt,15.072139865766758,13.48338977460143,EuroPar,3826,"
. In parallel database systems, data is partitioned and replicated across multiple independent nodes to improve system performance
and increase robustness. In current practice of database partitioning
design, all replicas are uniformly partitioned, however, different statements may prefer contradictory partitioning plans, so a single plan cannot achieve the overall optimal performance for the workload.

In this paper, we propose a novel approach of replica-aware data partitioning design to address the contradictions. According to the access
graph of SQL statements, we use the k-medoids algorithm to classify
workload into statement clusters, then we use the branch-and-bound
algorithm to search for the optimal partitioning plan for each cluster.
Finally, we organize replicas with these plans, and route statements to
their preferred replicas. We use TPC-E, TPC-H and National College
and University Enrollment System (NACUES) to evaluate our approach.
The evaluation results demonstrate that our approach improves system
performance by up to 4x over the current practice of partitioning design.

",17.122413403193683,16.088800705467374,163
EuroPar_17_038.txt,13.669616654691367,11.052210428342878,EuroPar,4112,"
. The scalability of sparse matrix-vector multiplication
(SpMV) on distributed memory systems depends on multiple factors
that involve different communication cost metrics. The irregular sparsity
pattern of the coefficient matrix manifests itself as high bandwidth (total
and/or maximum volume) and/or high latency (total and/or maximum
message count) overhead. In this work, we propose a hypergraph partitioning model which combines two earlier models for one-dimensional
partitioning, one addressing total and maximum volume, and the other
one addressing total volume and total message count. Our model relies
on the recursive bipartitioning paradigm and simultaneously addresses
three cost metrics in a single partitioning phase in order to reduce volume
and latency overheads. We demonstrate the validity of our model on a
large dataset that contains more than 300 matrices. The results indicate
that compared to the earlier models, our model significantly improves
the scalability of SpMV.

",17.693802365651003,17.32418367346939,148
EuroPar_17_039.txt,12.78644413453091,10.72506394855851,EuroPar,3899,"
. We studied a search engine service in order to evaluate the
impact of the traffic pattern on network performance. This paper focuses
on how the routing algorithm can improve the query latency of a search
engine. The architecture of the service includes three main components:
Front Service, Cache Service and Index Service. This service receives
queries from users, and after a process of seeking in a cluster, a set of
results are returned to users. This workload produces unbalanced traffic throughout the network. As a result, this behavior impacts the network performance in terms of latency and throughput and increases the
user timeout. This paper proposes an application-driven routing policy based on the application architecture which merges a set of criteria and prioritizes the Cache Service messages. We evaluated the performance using real traces and simulation techniques. The experiment
results show a reduction of network latency and throughput when we
apply the application-driven routing policy.

",13.383793397009324,12.483397027600851,158
EuroPar_17_040.txt,16.013259784998965,13.642369439071569,EuroPar,4093,"
. Container-based infrastructures have surged in popularity,
offering advantages in agility and scaling, while also presenting new challenges in resource utilization due to unnecessary library duplication. In
this paper, we consider sharing libraries across containers, and study the
impact of such a strategy on overall resource requirements, scheduling,
and utilization. Our analysis and simulations suggest significant benefits
arising from library sharing. Furthermore, a small fraction of libraries
shared between any two containers, on average, is enough to reap most
of the benefits, and even naive schedulers, such as a First Fit scheduler,
succeed at doing so. We also propose a score maximization, mixed-integer
linear-programming scheduler for handling bulk request arrivals (such as
large jobs composed of many smaller tasks), which compares favorably
against state-of-the-art schedulers in these scenarios.

",19.686211704642208,18.295278195488724,134
EuroPar_17_041.txt,14.952119200223333,12.836457481113246,EuroPar,4146,"
. To deploy a distributed application on the cloud, cost,
resource and communication constraints have to be considered to select
the most suitable Virtual Machines (VMs), from private and public cloud
providers. This process becomes very complex in large scale scenarios
and, as this problem is NP-Hard, its automation must take scalability
into consideration. In this work, we propose a heuristic able to calculate
initial placements for distributed component-based applications on possibly multiple clouds with the objective of minimizing VM renting costs
while satisfying applications’ resource and communication constraints.
We evaluate the heuristic performance and determine its limitations by
comparing it to other placement approaches, namely exact algorithms
and meta-heuristics. We show that the proposed heuristic is able to compute a good solution much faster than them.

",18.243605946275583,17.065798449612405,130
EuroPar_17_042.txt,15.433578945913752,13.632988444316052,EuroPar,4734,"
. Energy consumption is an important concern for large-scale
data-centers, which results in huge monetary cost for data-center operators. Due to the hardware heterogeneity and contentions between concurrent workloads, straggler mitigation is important to many Big Data
applications running in large-scale data-centers and the speculative execution technique is widely-used to handle stragglers. Although a large
number of studies have been proposed to improve the performance of Big
Data applications using speculative execution, few of them have studied
the energy efficiency of their solutions. In this paper, we propose two techniques to improve the energy efficiency of speculative executions while
ensuring comparable performance. Specifically, we propose a hierarchical straggler detection mechanism which can greatly reduce the number
of killed speculative copies and hence save the energy consumption. We
also propose an energy-aware speculative copy allocation method which
considers the trade-off between performance and energy when allocating
speculative copies. We implement both techniques into Hadoop and evaluate them using representative MapReduce benchmarks. Results show
that our solution can reduce the energy waste on killed speculative copies
by up to 100% and improve the energy efficiency by 20% compared to
state-of-the-art mechanisms.

",17.693802365651003,17.23687814070352,200
EuroPar_17_043.txt,14.888156241751819,12.669334845826054,EuroPar,3784,"
. Public cloud providers offer a wide range of instance types,
with different processing and interconnection speeds, as well as varying
prices. Furthermore, the tasks of many parallel applications show different computational demands due to load imbalance. These differences
can be exploited for improving the cost efficiency of parallel applications in many cloud environments by matching application requirements
to instance types. In this paper, we introduce the concept of heterogeneous cloud systems consisting of different instance types to leverage
the different computational demands of large parallel applications for
improved cost efficiency. We present a mechanism that automatically
suggests a suitable combination of instances based on a characterization
of the application and the instance types. With such a heterogeneous
cloud, we are able to improve cost efficiency significantly for a variety of
MPI-based applications, while maintaining a similar performance.

",18.7741,17.236521739130435,139
EuroPar_17_044.txt,15.588437446339988,13.91118109029614,EuroPar,4539,"
. Supercomputers are becoming more heterogeneous. They are
composed by several machines with different computation capabilities
and different kinds and families of accelerators, such as GPUs or Intel
Xeon Phi coprocessors. Programming these machines is a hard task, that
requires a deep study of the architectural details, in order to exploit
efficiently each computational unit.

In this paper, we present an extension of a GPU-CPU heterogeneous
programming model, to include support for Intel Xeon Phi coprocessors.
This contribution extends the previous model and its implementation, by
taking advantage of both the GPU communication model and the CPU
execution model of the original approach, to derive a new approach for
the Xeon Phi. Our experimental results show that using our approach,
the programming effort needed for changing the kind of target devices
is highly reduced for several study cases. For example, using our model
to program a Mandelbrot benchmark, the 97% of the application code is
reused between a GPU implementation and a Xeon Phi implementation.

",16.439396014739867,16.201645021645025,166
EuroPar_17_045.txt,15.24453307348687,12.87935256840031,EuroPar,3962,"
. In recent years, several lightweight thread (LWT) libraries
have emerged to tackle exascale challenges. These offer programming
models (PMs) based on user-level threads and incorporate their own
lightweight mechanisms. However, each library proposes its own PM,
exposing different semantics and hindering portability.

To address this drawback, we have designed Generic Lightweight
Thread (GLT), an application programming interface that frames the
functionality of the most popular LWT libraries for high-performance
computing under a single PM. We implement GLT on top of Argobots,
MassiveThreads, and Qthreads. We provide GLT as a dynamic library,
as well as in the form of a static version based on macro preprocessing
resolution to reduce overhead. This paper discusses the GLT PM and
demonstrates its minimal performance impact.

",16.084390811093357,13.423832752613244,124
EuroPar_17_046.txt,13.766267110266554,11.833623127608039,EuroPar,5712,"
. We propose PASCAL, a parallel unified algorithmic framework for generalized N-body problems. PASCAL utilizes tree data
structures and user-controlled pruning or approximations to reduce the
asymptotic runtime complexity from being linear in the number of data
points to be logarithmic. In PASCAL, the domain scientists express
their N-body problem in terms of application-specific operations, and
PASCAL generates the pruning and approximation conditions automatically from this high-level specification. In order to evaluate PASCAL,
we generate solutions for six problems: k-nearest neighbors, range search,
Euclidean minimum spanning tree, kernel density estimation, expectation maximization (EM), and Hausdorff distance chosen from various
domains.

We show that applying domain-specific optimizations and parallelizations to the algorithms generated by PASCAL achieves 10x to 230x
speedup compared to state-of-the-art libraries on a dual-socket Intel
Xeon processor with 16 cores on real world datasets. We also obtain
a novel out-of-the-box asymptotically optimal algorithm for Hausdorff
distance calculation and an improved algorithm for EM. This shows the
impact and potential of PASCAL in rapidly extending to a larger class
of problems that are yet to be explored.
",17.93193317476759,18.095407629020198,192
EuroPar_17_047.txt,14.985893962826673,12.861581808677474,EuroPar,3816,"
. Fault tolerance becomes an important feature at large computer systems where the mean time between failure decreases. Checkpointing is a method often used to provide resilience. We present an
in-memory checkpointing library based on a PGAS API implemented
with GASPI/GPI. It offers a substantial benefit when recovering from
failure and leverages existing fault tolerance features of GASPI/GPI.
The overhead of the library is negligible when testing it with a simple
stencil code and a real life seismic imaging method.

",14.836745963215662,12.103560975609756,83
EuroPar_17_048.txt,13.871618955308136,11.45813536530704,EuroPar,5646,"
. In data centers, up to dozens of tasks are colocated on a single physical machine. Machines are used more efficiently, but tasks’ performance deteriorates, as colocated tasks compete for shared resources.
As tasks are heterogeneous, the resulting performance dependencies are
complex. In our previous work [18] we proposed a new combinatorial
optimization model that uses two parameters of a task—its size and its
type—to characterize how a task influences the performance of other
tasks allocated to the same machine.

In this paper, we study the egalitarian optimization goal: maximizing the worst-off performance. This problem generalizes the classic makespan minimization on multiple processors (P||Cmax}. We prove
that polynomially-solvable variants of P||Cmax are NP-hard and hard to
approximate when the number of types is not constant. For a constant
number of types, we propose a PTAS, a fast approximation algorithm,
and aseries of heuristics. We simulate the algorithms on instances derived
from a trace of one of Google clusters. Algorithms aware of jobs’ types
lead to better performance compared to algorithms solving P||Cmax
The notion of type enables us to model degeneration of performance
caused by colocation using standard combinatorial optimization methods. Types add a layer of additional complexity. However, our results—
approximation algorithms and good average-case performance—show
that types can be handled efficiently.

",15.470042427545799,13.919122807017544,223
EuroPar_17_049.txt,15.526782691899815,13.00081399892613,EuroPar,4651,"
. We study the problem of executing an application represented by a precedence task graph on a multi-core machine composed
of standard computing cores and accelerators. Contrary to most existing approaches, we distinguish the allocation and the scheduling phases
and we mainly focus on the allocation part of the problem: choose the
most appropriate type of computing unit for each task. We address both
off-line and on-line settings. In the first case, we establish strong lower
bounds on the worst-case performance of a known approach based on
Linear Programming for solving the allocation problem. Then, we refine
the scheduling phase and we replace the greedy list scheduling policy used
in this approach by a better ordering of the tasks. Although this modification leads to the same approximability guarantees, it performs much
better in practice. In the on-line case, we assume that the tasks arrive in
any, not known in advance, order which respects the precedence relations
and the scheduler has to take irrevocable decisions about their allocation and execution. In this setting, we propose the first online scheduling
algorithm which takes into account precedences. Our algorithm is based
on adequate rules for selecting the type of processor where to allocate
the tasks and it achieves a constant factor approximation guarantee if
the ratio of the number of CPUs over the number of GPUs is bounded.
Finally, all the previous algorithms have been experimented on a large
number of simulations built on actual libraries. These simulations assess
the good practical behavior of the algorithms with respect to the stateof-the-art solutions whenever these exist or baseline algorithms.

",16.35954948731386,14.57021133847702,272
EuroPar_17_050.txt,14.628806744663976,12.006772219352051,EuroPar,4081,"
. Hybrid platforms embedding accelerators such as GPUs or
Xeon Phis are increasingly used in computing. When scheduling tasks
on such platforms, one has to take into account that a task execution
time depends on the type of core used to execute it. We focus on the
problem of minimizing the total completion time (or makespan) when
scheduling independent tasks on two processor types, also known as the
(Pm, Pk)||Cmax problem. We propose BALANCEDESTIMATE and BALANCEDMAKESPAN, two novel 2-approximation algorithms with low complexity. Their approximation ratio is both on par with the best approximation algorithms using dual approximation techniques (which are, thus,
of high complexity) and significantly smaller than the approximation
ratio of existing low-cost approximation algorithms. We compared both
algorithms by simulations to existing strategies in different scenarios.
These simulations showed that their performance is among the best ones
in all cases.

",16.439396014739867,15.076190476190476,146
EuroSys_17_001.txt,14.40785624514955,12.58557827884994,EuroSys,10032,"
Pandia is a system for modeling the performance of inmemory parallel workloads. It generates a description of
a workload from a series of profiling runs, and combines
this with a description of the machine’s hardware to model
the workload’s performance over different thread counts and
different placements of those threads.
The approach is “comprehensive” in that it accounts for
contention at multiple resources such as processor functional
units and memory channels. The points of contention for a
workload can shift between resources as the degree of parallelism and thread placement changes. Pandia accounts for
these changes and provides a close correspondence between
predicted performance and actual performance. Testing a set
of 22 benchmarks on 2 socket Intel machines fitted with
chips ranging from Sandy Bridge to Haswell we see median
differences of 1.05% to 0% between the fastest predicted
placement and the fastest measured placement, and median
errors of 8% to 4% across all placements.
Pandia can be used to optimize the performance of a given
workload—for instance, identifying whether or not multiple
processor sockets should be used, and whether or not the
workload benefits from using multiple threads per core. In
addition, Pandia can be used to identify opportunities for
reducing resource consumption where additional resources
are not matched by additional performance—for instance,
limiting a workload to a small number of cores when its
scaling is poor.
",16.975882523387877,15.939021739130435,234
EuroSys_17_002.txt,15.75749738099697,14.175758956081477,EuroSys,8655,"

Exploit mitigations, by themselves, do not stop determined and well-resourced adversaries from compromising
vulnerable software through memory corruption. Multivariant execution environments (MVEEs) add additional assurance by executing multiple, diversified copies (variants)
of the same program in lockstep while monitoring their behavior for signs of attacks (divergence). While executing
multiple copies of the same program requires additional
computational resources, modern MVEEs run many workloads at near-native speed and can detect adversaries before
they leak secrets or achieve persistence on the host system.

Multi-threaded programs are challenging to execute in
lockstep by an MVEE. If the threads in a set of variants are
not scheduled in the exact same order, the variants will diverge from each other in terms of the system calls they make.
While benign, such divergence undermines the MVEEs ability detect divergence caused by malicious program inputs.
To address this problem, we developed an MVEE-specific
synchronization scheme that lets us execute a set of multithreaded variants in lockstep without causing benign divergence. Our fully-fledged MVEE runs the PARSEC 2.1 and
SPLASH-2x parallel benchmarks (with four worker threads
per variant) with a slowdown of less than 15% relative to unprotected execution. Addressing this longstanding compatibility issue makes MVEESs a viable defense for a far greater
range of realistic workloads.

",17.122413403193683,15.515503875968992,217
EuroSys_17_003.txt,17.455718655227326,15.47457614773019,EuroSys,7750,"

Most software profiling tools quantify average performance
and rely on a program’s control flow graph to organize and
report results. However, in interactive server applications,
performance predictability is often an equally important
measure. Moreover, the end user is often concerned with
the performance of a semantically defined interval of execution, such as a request or transaction, which may not directly
map to any single function in the call graph, especially in
high-performance applications that use asynchrony or eventbased programming. It is difficult to distinguish functionality
that lies on the critical path of a semantic interval from other
activity (e.g., periodic logging or side operations) that may
nevertheless appear prominent in a conventional profile. Existing profilers lack the ability to (i) aggregate results for a
semantic interval and (ii) attribute its performance variance
to individual functions.

We propose a profiler called VProfiler that, given the
source code of a software system and programmer annotations indicating the start and end of semantic intervals of
interest, is able to identify the dominant sources of latency
variance in a semantic context. Using a novel abstraction,
called a variance tree, VProfiler analyzes the thread interleaving and deconstructs overall latency variance into variances and covariances of the execution time of individual
functions. It then aggregates latency variance along a backwards path of dependence relationships among threads from
the end of an interval to its start. We evaluate VProfiler’s effectiveness on three popular open-source projects (MySQL,
Postgres, and Apache Web Server). By identifying a few culprit functions in these complex code bases, VProfiler allows
us to eliminate 27%-82% of the overall latency variance of
these systems with a modest programming effort.

",20.362288242458213,18.429606498194946,281
EuroSys_17_004.txt,13.645230753031338,11.520023915754141,EuroSys,7803,"

In native Linux systems, spinlock’s implementation relies
on the assumption that both the lock holder thread and lock
waiter threads cannot be preempted. However, in a virtualized environment, these threads are scheduled on top of virtual CPUs (vCPU) that can be preempted by the hypervisor
at any time, thus forcing lock waiter threads on other vCPUs to busy wait and to waste CPU cycles. This leads to
the well-known Lock Holder Preemption (LHP) and Lock
Waiter Preemption (LWP) issues.

In this paper, we propose I-Spinlock (for Informed. Spinlock), a new spinlock implementation for virtualized environments. Its main principle is to only allow a thread to
acquire a lock if and only if the remaining time-slice of its
vCPU is sufficient to enter and leave the critical section.
This is possible if the spinlock primitive is aware (informed)
of its time-to-preemption (by the hypervisor).

We implemented I-Spinlock in the Xen virtualization system. We show that our solution is compliant with both paravirtual and hardware virtualization modes. We performed
extensive performance evaluations with various reference
benchmarks and compared our solution to previous solutions. The evaluations demonstrate that I-Spinlock outperforms other solutions, and more significantly when the number of core increases.

",14.554592549557764,12.285011086474505,207
EuroSys_17_005.txt,16.35495141232373,14.768540263935101,EuroSys,10158,"

Datacenters are under-utilized, primarily due to unused resources on over-provisioned nodes of latency-critical jobs.
Such idle resources can be used to run batch data analytic
jobs to increase datacenter utilization, but these transient resources must be evicted whenever latency-critical jobs require them again. Resource evictions often lead to cascading
recomputations, which is usually handled by checkpointing
intermediate results on stable storages of eviction-free reserved resources. However, checkpointing has major shortcomings in its substantial overhead of transferring data back
and forth. In this work, we step away from such approaches
and focus on observing the job structure and the relationships between computations of the job. We carefully mark
the computations that are most likely to cause a large number
of recomputations upon evictions, to run them reliably using reserved resources. This lets us retain corresponding intermediate results effortlessly without any additional checkpointing. We design Pado, a general data processing engine,
which carries out our idea with several optimizations that
minimize the number of additional reserved nodes. Evaluation results show that Pado outperforms Spark 2.0.0 by up to
5.1x, and checkpoint-enabled Spark by up to 3.8.

",17.5058628484301,15.746491228070177,195
EuroSys_17_006.txt,15.881307523551222,14.50579417678846,EuroSys,8715,"

Many shared computing clusters allow users to utilize excess idle resources at lower cost or priority, with the proviso
that some or all may be taken away at any time. But, exploiting such dynamic resource availability and the often fluctuating markets for them requires agile elasticity and effective acquisition strategies. Proteus aggressively exploits such
transient revocable resources to do machine learning (ML)
cheaper and/or faster. Its parameter server framework, AgileML, efficiently adapts to bulk additions and revocations
of transient machines, through a novel 3-stage active-backup
approach, with minimal use of more costly non-transient
resources. Its BidBrain component adaptively allocates resources from multiple EC2 spot markets to minimize average cost per work as transient resource availability and cost
change over time. Our evaluations show that Proteus reduces
cost by 85% relative to non-transient pricing, and by 43%
relative to previous approaches, while simultaneously reducing runtimes by up to 37%.

",18.062587368997235,17.41513071895425,154
EuroSys_17_007.txt,15.569168055513458,14.16575615373872,EuroSys,10035,"
Recent advances in formal verification techniques enabled
the implementation of distributed systems with machinechecked proofs. While results are encouraging, the importance of distributed systems warrants a large scale evaluation
of the results and verification practices.
This paper thoroughly analyzes three state-of-the-art, formally verified implementations of distributed systems: IronFleet, Verdi, and Chapar. Through code review and testing,
we found a total of 16 bugs, many of which produce serious
consequences, including crashing servers, returning incorrect results to clients, and invalidating verification guarantees. These bugs were caused by violations of a wide-range
of assumptions on which the verified components relied. Our
results revealed that these assumptions referred to a small
fraction of the trusted computing base, mostly at the interface of verified and unverified components. Based on our
observations, we have built a testing toolkit called PK, which
focuses on testing these parts and is able to automate the detection of 13 (out of 16) bugs.
",16.954822765917157,15.54323688969259,159
EuroSys_17_008.txt,14.079251031125409,12.102911709684353,EuroSys,7276,"

We propose MiniCrypt, the first key-value store that reconciles encryption and compression without compromising
performance. At the core of MiniCrypt is an observation on
data compressibility trends in key-value stores, which enables grouping key-value pairs into small key packs, together
with a set of distributed systems techniques for retrieving,
updating, merging and splitting encrypted packs. Our evaluation shows that MiniCrypt compresses data by as much
as 4 times with respect to the vanilla key-value store, and
can increase the server’s throughput by up to two orders of
magnitude by fitting more data in main memory.

",18.243605946275583,17.66181818181818,101
EuroSys_17_009.txt,14.52379008725623,12.8503550036523,EuroSys,6777,"

Clouds offer an opaque I/O API to their customers: details of
the underlying resources (network topology, disk drives) or
their current load are kept hidden. Tenants can profile the I/O
performance in their VMs and optimise accordingly, but the
side effect is increased load. Certain cloud providers try to
discourage profiling by enforcing strict I/O isolation, at the
cost of reduced utilisation in the average case. In this paper
we challenge this status quo and propose CloudTalk, an API
that allows tenants to communicate with the cloud provider
and receive hints used to optimise their workloads.

We have built a distributed implementation of CloudTalk
that scales to hundreds of machines and provides significant
performance benefits in many cases. Further, we have implemented changes to Hadoop and HDFS that use CloudTalk to
decide which machines to use for task placement and replica
selection. Our experiments in a local cluster and on Amazon
EC2 show that CloudTalk helps improve performance by as
much as two times for a wide range of scenarios.

",15.151101081350806,13.828918249380678,174
EuroSys_17_010.txt,15.314804267706485,13.17245728700301,EuroSys,8945,"

Despite the obvious importance, performance issues related
to synchronization primitives are still lacking adequate attention. No literature extensively investigates categories,
root causes, and fixing strategies of such performance issues. Existing work primarily focuses on one type of problems, while ignoring other important categories. Moreover,
they leave the burden of identifying root causes to programmers. This paper first conducts an extensive study of categories, root causes, and fixing strategies of performance
issues related to explicit synchronization primitives. Based
on this study, we develop two tools to identify root causes of
a range of performance issues. Compare with existing work,
our proposal, SyncPerf, has three unique advantages. First,
SyncPerf’s detection is very lightweight, with 2.3% performance overhead on average. Second, SyncPerf integrates
information based on callsites, lock variables, and types of
threads. Such integration helps identify more latent problems. Last but not least, when multiple root causes generate
the same behavior, SyncPerf provides a second analysis
tool that collects detailed accesses inside critical sections
and helps identify possible root causes. SyncPerf discovers
many unknown but significant synchronization performance
issues. Fixing them provides a performance gain anywhere
from 2.5% to 42%. Low overhead, better coverage, and informative reports make SyncPerf an effective tool to find
synchronization performance bugs in the production environment.

",15.81161729744533,14.227358168147642,213
EuroSys_17_011.txt,14.177178026167681,12.35226144998111,EuroSys,12076,"

We perform a detailed vertical analysis of application performance atop a range of modern file systems and SSD FTLs.
We formalize the “unwritten contract” that clients of SSDs
should follow to obtain high performance, and conduct our
analysis to uncover application and file system designs that
violate the contract. Our analysis, which utilizes a highly
detailed SSD simulation underneath traces taken from real
workloads and file systems, provides insight into how to better construct applications, file systems, and FTLs to realize
robust and sustainable performance.

",17.5058628484301,17.67176470588235,86
EuroSys_17_012.txt,13.858280804444636,12.869941707512531,EuroSys,11474,"

The importance of the Event-Driven Architecture (EDA) has
never been greater. Web servers and the IoT alike have begun
to adopt the EDA, and the popular server-side EDA framework, Node.js, boasts the world’s largest package ecosystem. While multi-threaded programming has been well studied in the literature, concurrency bug characteristics and useful development tools remain largely unexplored for serverside EDA-based applications.

We present the first (to the best of our knowledge) concurrency bug characteristic study of real world open-source
event-driven applications, based in Node.js. Like multithreaded programs, event-driven programs are prone to concurrency bugs like atomicity violations and order violations.
Our study shows the forms that atomicity violations and ordering violations take in the EDA context, and points out
the limitations of existing concurrency error detection tools
developed for client-side EDA applications.

Based on our bug study, we propose Node.fz, a novel testing aid for server-side event-driven programs. Node.fz is a
schedule fuzzing test tool for event-driven programs, embodied for server-side Node.js programs. Node.fz randomly perturbs the execution of a Node.js program, allowing Node.js
developers to explore a variety of possible schedules. Thanks
to its low overhead, Node.fz enables a developer to explore
a broader “schedule space” with the same test time budget,
ensuring that applications will be stable in a wide variety
of deployment conditions. We show that Node.fz can expose
known bugs much more frequently than vanilla Node.js, and
that it can uncover new bugs.

",15.062637766997835,14.804340175953083,261
EuroSys_17_013.txt,14.172725425339781,12.71400740650802,EuroSys,9237,"

Remote Direct Memory Access (RDMA) has been widely
deployed in modern data centers. However, existing usages of RDMA lead to a dilemma between performance
and redesign cost. They either directly replace socket-based
send/receive primitives with the corresponding RDMA
counterpart (server-reply), which only achieves moderate
performance improvement; or push performance further by
using one-sided RDMA operations to totally bypass the
server (server-bypass), at the cost of redesigning the software.

In this paper, we introduce two interesting observations
about RDMA. First, RDMA has asymmetric performance
characteristics, which can be used to improve server-reply’s
performance. Second, the performance of server-bypass is
not as good as expected in many cases, because more rounds
of RDMA may be needed if the server is totally bypassed.
We therefore introduce a new RDMA paradigm called Remote Fetching Paradigm (RFP). Although RFP requires
users to set several parameters to achieve the best performance, it supports the legacy RPC interfaces and hence
avoids the need of redesigning application-specific data
structures. Moreover, with proper parameters, it can achieve
even higher IOPS than that of the previous paradigms.

We have designed and implemented an in-memory keyvalue store based on RFP to evaluate its effectiveness. Experimental results show that RFP improves performance
by 1.6x~4x compared with both server-reply and serverbypass paradigms.

",15.668782140382113,13.92328259975319,223
EuroSys_17_014.txt,15.367199447679216,14.5313353035353,EuroSys,6456,"

Large-scale information processing often relies on subset
matching for data classification and routing. Examples are
publish/subscribe and stream processing systems, database
systems, social media, and information-centric networking.
For instance, an advanced Twitter-like messaging service
where users might follow specific publishers as well as specific topics encoded as tag sets must join a stream of published messages with the users and their preferred tag sets so
that the user tag set is a subset of the message tags.

Subset matching is an old but also notoriously difficult
problem. We present TagMatch, a system that solves this
problem by taking advantage of a hybrid CPU/GPU stream
processing architecture. TagMatch targets large-scale applications with thousands of matching operations per seconds
against hundreds of millions of tag sets. We evaluate TagMatch on an advanced message streaming application, with
very positive results both in absolute terms and in comparison with existing systems. As a notable example, our experiments demonstrate that TagMatch running on a single,
commodity machine with two GPUs can easily sustain the
traffic throughput of Twitter even augmented with expressive tag-based selection.

",15.579741850924794,14.793629032258064,187
EuroSys_17_015.txt,15.526329800241665,13.630303988913273,EuroSys,8644,"

Processing a one trillion-edge graph has recently been demonstrated by distributed graph engines running on clusters of
tens to hundreds of nodes. In this paper, we employ a single
heterogeneous machine with fast storage media (e.g., NVMe
SSD) and massively parallel coprocessors (e.g., Xeon Phi)
to reach similar dimensions. By fully exploiting the heterogeneous devices, we design a new graph processing engine,
named MOSAIC, for a single machine. We propose a new
locality-optimizing, space-efficient graph representation—
Hilbert-ordered tiles, and a hybrid execution model that enables vertex-centric operations in fast host processors and
edge-centric operations in massively parallel coprocessors.

Our evaluation shows that for smaller graphs, MOSAIC
consistently outperforms other state-of-the-art out-of-core
engines by 3.2-58.6x and shows comparable performance
to distributed graph engines. Furthermore, MOSAIC can
complete one iteration of the Pagerank algorithm on a trillionedge graph in 21 minutes, outperforming a distributed diskbased engine by 9.2x.

",18.599290044081553,17.15974683544304,164
EuroSys_17_016.txt,14.56187689365099,12.645211291188566,EuroSys,7324,"

Shielded execution based on Intel SGX provides strong security guarantees for legacy applications running on untrusted
platforms. However, memory safety attacks such as Heartbleed can render the confidentiality and integrity properties
of shielded execution completely ineffective. To prevent these
attacks, the state-of-the-art memory-safety approaches can be
used in the context of shielded execution.

In this work, we first showcase that two prominent softwareand hardware-based defenses, AddressSanitizer and Intel
MPX respectively, are impractical for shielded execution due
to high performance and memory overheads. This motivated
our design of SGXBOUNDS—an efficient memory-safety
approach for shielded execution exploiting the architectural
features of Intel SGX. Our design is based on a simple combination of tagged pointers and compact memory layout.

We implemented SGXBOUNDS based on the LLVM compiler framework targeting unmodified multithreaded applications. Our evaluation using Phoenix, PARSEC, and RIPE
benchmark suites shows that SGXBOUNDS has performance
and memory overheads of 17% and 0.1% respectively, while
providing security guarantees similar to AddressSanitizer
and Intel MPX. We have obtained similar results with SPEC
CPU2006 and four real-world case studies: SQLite, Memcached, Apache, and Nginx.

",18.99602597827317,18.239266615737204,189
EuroSys_17_017.txt,14.446652717494818,12.24345331169139,EuroSys,9518,"
Log-structured merge (LSM) data stores enable to store and
process large volumes of data while maintaining good performance. They mitigate the I/O bottleneck by absorbing updates in a memory layer and transferring them to the disk
layer in sequential batches. Yet, the LSM architecture fundamentally requires elements to be in sorted order. As the
amount of data in memory grows, maintaining this sorted order becomes increasingly costly. Contrary to intuition, existing LSM systems could actually lose throughput with larger
memory components.
In this paper, we introduce FloDB, an LSM memory
component architecture which allows throughput to scale on
modern multicore machines with ample memory sizes. The
main idea underlying FloDB is essentially to bootstrap the
traditional LSM architecture by adding a small in-memory
buffer layer on top of the memory component. This buffer
offers low-latency operations, masking the write latency of
the sorted memory component. Integrating this buffer in the
classic LSM memory component to obtain FloDB is not trivial and requires revisiting the algorithms of the user-facing
LSM operations (search, update, scan). FloDB’s two layers
can be implemented with state-of-the-art, highly-concurrent
data structures. This way, as we show in the paper, FloDB
eliminates significant synchronization bottlenecks in classic
LSM designs, while offering a rich LSM API.
We implement FloDB as an extension of LevelDB,
Google’s popular LSM key-value store. We compare FloDB’s
performance to that of state-of-the-art LSMs. In short,
FloDB’s performance is up to one order of magnitude higher
than that of the next best-performing competitor in a wide
range of multi-threaded workloads.

",15.903189008614273,13.487777777777776,275
EuroSys_17_018.txt,15.014022538560056,12.859969697566118,EuroSys,10319,"
Portability and efficiency are usually antagonists in multicore computing. In order to develop efficient code, one needs
to take into account the topology of the target multi-cores
(e.g., for locality). This clearly hampers code portability. In
this paper, we show that you can have the cake and eat it too.
We introduce MCTOP, an abstraction of multi-core
topologies augmented with important low-level hardware information, such as memory bandwidths and communication
latencies. We show how to automatically generate MCTOP
using libmctop, our library that leverages the determinism of cache-coherence protocols to infer the topology of
multi-cores using only latency measurements.
MCTOP enables developers to accurately and portably
define high-level performance optimization policies. We
illustrate several such policies through four examples:
(i-ii) thread placement in OpenMP and in a MapReduce library, (iii) a topology-aware mergesort algorithm, as well as
(iv) automatic backoff schemes for locks. We illustrate the
portability of these optimizations on five processors from Intel, AMD, and Oracle, with low effort.
",16.99224021665606,15.054043392504934,171
EuroSys_17_019.txt,16.368678483331315,15.679891097526419,EuroSys,5790,"

Well-run datacenter application architectures are heavily instrumented to provide detailed traces of messages and remote
invocations. Reconstructing user sessions, call graphs, transaction trees, and other structural information from these messages, a process known as sessionization, is the foundation
for a variety of diagnostic, profiling, and monitoring tasks
essential to the operation of the datacenter.

We present the design and implementation of a system
which processes log streams at gigabits per second and
reconstructs user sessions comprising millions of transactions
per second in real time with modest compute resources, while
dealing with clock skew, message loss, and other real-world
phenomena that make such a task challenging. Our system
is based on the Timely Dataflow framework for low latency,
data-parallel computation, and we demonstrate its utility with
a number of use-cases and traces from a large, operational,
mission-critical enterprise data center.


This paper describes TS, a system for recovering structural information (sessions, spans, call graphs, transaction trees, etc.)
from a large datacenter logging infrastructure in real time,
with low latency, using only modest computing resources.
",21.012824612059983,21.032202247191012,179
EuroSys_17_020.txt,15.503899672038209,13.153590608950847,EuroSys,5434,"

While virtualization only introduces a small overhead on
machines with few cores, this is not the case on larger ones.
Most of the overhead on the latter machines is caused by the
Non-Uniform Memory Access (NUMA) architecture they
are using. In order to reduce this overhead, this paper shows
how NUMA placement heuristics can be implemented inside
Xen. With an evaluation of 29 applications on a 48-core
machine, we show that the NUMA placement heuristics can
multiply the performance of 9 applications by more than 2.

",14.554592549557764,12.567272727272726,89
EuroSys_17_021.txt,16.85796243268728,16.07184907546227,EuroSys,10932,"

In current architectures, page tables are the fundamental
mechanism that allows contemporary OSs to isolate user
processes, binding each thread to a specific page table. A
thread cannot therefore directly call another process’s function or access its data; instead, the OS kernel provides data
communication primitives and mediates process synchronization through inter-process communication (IPC) channels, which impede system performance.

Alternatively, the recently proposed CODOMs architecture provides memory protection across software modules.
Threads can cross module protection boundaries inside the
same process using simple procedure calls, while preserving
memory isolation.

We present dIPC (for “direct IPC”), an OS extension
that repurposes and extends the CODOMs architecture to allow threads to cross process boundaries. It maps processes
into a shared address space, and eliminates the OS kernel
from the critical path of inter-process communication. dIPC
is 64.12 faster than local remote procedure calls (RPCs),
and 8.87 x faster than IPC in the L4 microkernel. We show
that applying dIPC to a multi-tier OLTP web server improves performance by up to 5.12 x (2.13x on average), and
reaches over 94% of the ideal system efficiency.

",16.785175570968402,16.594980694980695,191
EuroSys_17_022.txt,16.030838014006147,14.438505551778839,EuroSys,11695,"

A key use of software-defined networking is to enable scaleout of network data plane elements. Naively scaling networking elements, however, can cause incorrect behavior.
For example, we show that an IDS system which operates
correctly as a single network element can erroneously and
permanently block hosts when it is replicated.

In this paper, we provide a system, COCONUT, for seamless scale-out of network forwarding elements; that is, an
SDN application programmer can program to what functionally appears to be a single forwarding element, but which
may be replicated behind the scenes. To do this, we identify the key property for seamless scale out, weak causality,
and guarantee it through a practical and scalable implementation of vector clocks in the data plane. We prove that COCONUT enables seamless scale out of networking elements,
1.e., the user-perceived behavior of any COCONUT element
implemented with a distributed set of concurrent replicas is
provably indistinguishable from its singleton implementation. Finally, we build a prototype of COCONUT and experimentally demonstrate its correct behavior. We also show
that its abstraction enables a more efficient implementation
of seamless scale-out compared to a naive baseline.

",18.511140095513987,16.017916666666668,193
EuroSys_17_023.txt,14.21448881989578,12.881759518293503,EuroSys,10399,"

The commoditization of high-performance networking has
sparked research interest in the RDMA capability of this
hardware. One-sided RDMA primitives, in particular, have
generated substantial excitement due to the ability to directly
access remote memory from within an application without
involving the TCP/IP stack or the remote CPU. This paper
considers how to leverage RDMA to improve the analytical
performance of parallel database systems. To shuffle data
efficiently using RDMA, one needs to consider a complex
design space that includes (1) the number of open connections, (2) the contention for the shared network interface,
(3) the RDMA transport function, and (4) how much memory should be reserved to exchange data between nodes during query processing. We contribute six designs that capture
salient trade-offs in this design space. We comprehensively
evaluate how transport-layer decisions impact the query performance of a database system for different generations of
InfiniBand. We find that a shuffling operator that uses the
RDMA Send/Receive transport function over the Unreliable
Datagram transport service can transmit data up to 4x faster
than an RDMA-capable MPI implementation in a 16-node
cluster. The response time of TPC-H queries improves by as
much as 2x.

",16.52667757954773,15.577904228855719,202
EuroSys_17_024.txt,14.655055357460565,12.653984432326386,EuroSys,11215,"

Applications running in modern multithreaded environments
are sometimes overthreaded. The excess threads do not improve performance, and in fact may act to degrade performance via scalability collapse, which can manifest even
when there are fewer ready threads than available cores.
Often, such software also has highly contended locks. We
leverage the existence of such locks by modifying the lock
admission policy so as to intentionally limit the number of
distinct threads circulating over the lock in a given period.
Specifically, if there are more threads circulating than are
necessary to keep the lock saturated (continuously held), our
approach will selectively cull and passivate some of those
excess threads. We borrow the concept of swapping from
the field of memory management and impose concurrency
restriction (CR) if a lock suffers from contention. The resultant admission order is unfair over the short term but we
explicitly provide long-term fairness by periodically shifting
threads between the set of passivated threads and those actively circulating. Our approach is palliative, but is often effective at avoiding or reducing scalability collapse, and in
the worst case does no harm. Specifically, throughput is either unaffected or improved, and unfairness is bounded, relative to common test-and-set locks which allow unbounded
bypass and starvation '. By reducing competition for shared
resources, such as pipelines, processors and caches, concurrency restriction may also reduce overall resource consumption and improve the overall load carrying capacity of a system.

Robert Malthus [70] argued for population control, cautioning that societies

would collapse as increasing populations competed for resources. His dire
predictions did not come to pass as food production — which had previously
been stagnant — improved to keep pace with population growth.

| Bypass occurs when a thread T acquires a lock but there exist other waiting
threads that arrived earlier than 7.

",16.573440645216312,15.004000000000001,300
EuroSys_17_025.txt,15.440124947348504,13.46293136982495,EuroSys,1207,"
",NA,NA,1
EuroSys_17_026.txt,18.180990922860744,17.402285953358398,EuroSys,8814,"

Understanding and managing the propagation of states in
operating systems has become an intractable problem due
to their sheer size and complexity. Despite modularization
efforts, it remains a significant barrier to many contemporary
computing goals: process migration, fault isolation and
tolerance, live update, software virtualization, and more.
Though many previous OS research endeavors have achieved
these goals through ad-hoc, tedious methods, we argue that
they have missed the underlying reason why these goals are
so challenging: state spill.

State spill occurs when a software entity’s state undergoes
lasting changes as a result of a transaction from another entity.
In order to increase awareness of state spill and its harmful
effects, we conduct a thorough study of modern OSes and
contribute a classification of design patterns that cause state
spill. We present STATESPY, an automated tool that leverages
cooperative static and runtime analysis to detect state spill in
real software entities. Guided by STATESPY, we demonstrate
the presence of state spill in 94% of Android system services.
Finally, we analyze the harmful impacts of state spill and
suggest alternative designs and strategies to mitigate them.

",16.373557378465907,14.98766891891892,187
EuroSys_17_027.txt,15.491214071910317,14.481196473551638,EuroSys,7926,"

Graphics is one of the major energy drain sources in smartphone apps. To optimize the app graphics energy, however,
developers face the challenge of highly complex graphics
rendering process, which involves multiple system layers
including the app, the framework, the GPU, and the asynchronous interactions among them. Current diagnostic tools
can profile the resource usage from certain layers, but fall
short in stitching together profiling information across all the
layers which is needed to provide developers with the visual
effect-energy tradeoff at the app source-code level.

In this paper, we design and implement a holistic graphics
energy diagnosis tool, GfxDoctor!, that helps developers to
systematically diagnose energy inefficiencies in app graphics at the app source-code level, by precisely quantifying (1)
the visual effect of each UI update, and (2) the aggregate
energy drain spent in traversing the entire frame rendering
stack due to each UI update. GfxDoctor overcomes three
challenges faced in deriving per-UI-update visual effect and
energy accounting, asynchrony across system layers, UI update batching, and “black-box” GPU, with two key techniques — lightweight view-frame-ID-based information flow
tracking, and OpenGL record-and-replay plus frame diffing.
We show the effectiveness of GfxDoctor by profiling a randomly sampled set of 30 popular Android apps which reveals
three types of graphics energy bugs happening in 8 out of the
30 apps. Removing these bugs reduces the app energy drain
by 46% to 90%.

",18.699421769314853,18.28392857142857,241
EuroSys_17_028.txt,14.905519774251701,12.856997845212117,EuroSys,10261,"

The combination of the explosive growth in digital data and
the need to preserve much of this data in the long term
has made it an imperative to find a more cost-effective way
than HDD arrays and more easily accessible way than tape
libraries to store massive amounts of data. While modern
optical discs are capable of guaranteeing more than 50year data preservation without migration, individual optical
disks’ lack of the performance and capacity relative to HDDs
or tapes has significantly limited their use in datacenters.
This paper presents a Rack-scale Optical disc library System, or ROS in short, that provides a PB-level total capacity
and inline accessibility on thousands of optical discs built
within a 42U Rack. A rotatable roller and robotic arm separating and fetching the discs are designed to improve disc
placement density and simplify the mechanical structure. A
hierarchical storage system based on SSD, hard disks and
optical discs are presented to hide the delay of mechanical
operation. On the other hand, an optical library file system
is proposed to schedule mechanical operation and organize
data on the tiered storage with a POSIX user interface to provide an illusion of inline data accessibility. We evaluate ROS
on a few key performance metrics including operation delays of the mechanical structure and software overhead in a

prototype PB-level ROS system. The results show that ROS
stacked on Samba and FUSE can provide almost 323MB/s
read and 236MB/s write throughput, about 53ms file write
and 15ms read latency via 10GbE network for external users,
exhibiting its inline accessibility. Besides, ROS is able to effectively hide and virtualize internal complex operational behaviors and be easily deployable in datacenters.

",18.71604785175511,17.989906103286383,285
EuroSys_17_029.txt,17.475127345648488,16.502326756581287,EuroSys,9813,"
Storage systems need to support high-performance for specialpurpose data processing applications that run on an evolving
storage device technology landscape. This puts tremendous
pressure on storage systems to support rapid change both in
terms of their interfaces and their performance. But adapting storage systems can be difficult because unprincipled
changes might jeopardize years of code-hardening and performance optimization efforts that were necessary for users
to entrust their data to the storage system. We introduce
the programmable storage approach, which exposes internal services and abstractions of the storage stack as building
blocks for higher-level services. We also build a prototype
to explore how existing abstractions of common storage system services can be leveraged to adapt to the needs of new
data processing systems and the increasing variety of storage devices. We illustrate the advantages and challenges of
this approach by composing existing internal abstractions
into two new higher-level services: a file system metadata
load balancer and a high-performance distributed sharedlog. The evaluation demonstrates that our services inherit
desirable qualities of the back-end storage system, including the ability to balance load, efficiently propagate service
metadata, recover from failure, and navigate trade-offs between latency and throughput using leases.

",19.854308518879517,18.680127298444138,203
EuroSys_17_030.txt,15.385755546248014,13.551891256421218,EuroSys,9559,"

We tackle the problem of reducing tail latencies in distributed key-value stores, such as the popular Cassandra
database. We focus on workloads of multiget requests, which
batch together access to several data elements and parallelize
read operations across the data store machines. We first analyze a production trace of a real system and quantify the
skew due to multiget sizes, key popularity, and other factors. We then proceed to identify opportunities for reduction
of tail latencies by recognizing the composition of aggregate requests and by carefully scheduling bottleneck operations that can otherwise create excessive queues. We design
and implement a system called Rein, which reduces latency
via inter-multiget scheduling using low overhead techniques.
We extensively evaluate Rein via experiments in Amazon
Web Services (AWS) and simulations. Our scheduling algorithms reduce the median, 95*”, and 99*” percentile latencies
by factors of 1.5, 1.5, and 1.9, respectively.

",18.08858127442927,15.798639455782315,151
EuroSys_17_031.txt,16.055569216301873,15.04339380676242,EuroSys,11138,"

This paper presents the design, implementation, and evaluation of SATURN, a metadata service for geo-replicated systems. SATURN can be used in combination with several distributed and replicated data services to ensure that remote
operations are made visible in an order that respects causality, a requirement central to many consistency criteria.

SATURN addresses two key unsolved problems inherent
to previous approaches. First, it eliminates the tradeoff between throughput and data freshness, when deciding what
metadata to use for tracking causality. Second, it enables
genuine partial replication, a key property to ensure scalability when the number of geo-locations increases. SATURN
addresses these challenges while keeping metadata size constant, independently of the number of clients, servers, data
partitions, and locations. By decoupling metadata management from data dissemination, and by using clever metadata
propagation techniques, it ensures that the throughput and
visibility latency of updates on a given item are (mostly)
shielded from operations on other items or locations.

We evaluate SATURN in Amazon EC2 using realistic
benchmarks under both full and partial geo-replication. Results show that weakly consistent datastores can lean on
SATURN to upgrade their consistency guarantees to causal
consistency with a negligible penalty on performance.
",17.631426480028413,17.505042301184435,198
EuroSys_17_032.txt,15.4846040165965,14.293366902730394,EuroSys,8621,"

Use-after-free vulnerabilities due to dangling pointers are
an important and growing threat to systems security. While
various solutions exist to address this problem, none of them
is sufficiently practical for real-world adoption. Some can
be bypassed by attackers, others cannot support complex
multithreaded applications prone to dangling pointers, and
the remainder have prohibitively high overhead. One major
source of overhead is the need to synchronize threads on
every pointer write due to pointer tracking.

In this paper, we present DangSan, a use-after-free detection system that scales efficiently to large numbers of
pointer writes as well as to many concurrent threads. To
significantly reduce the overhead of existing solutions, we
observe that pointer tracking is write-intensive but requires
very few reads. Moreover, there is no need for strong consistency guarantees as inconsistencies can be reconciled at read
(i.e., object deallocation) time. Building on these intuitions,
DangSan’s design mimics that of log-structured file systems,
which are ideally suited for similar workloads. Our results
show that DangSan can run heavily multithreaded applications, while introducing only half the overhead of previous
multithreaded use-after-free detectors.

",15.903189008614273,14.07724867724868,192
EuroSys_17_033.txt,15.16986763807591,14.03986956561808,EuroSys,10150,"
Today, network operators are increasingly playing the role of
part-time detectives: they must routinely diagnose intricate
problems and malfunctions, e.g., routing or performance issues, and they must often perform forensic investigations of
past misbehavior, e.g., intrusions or cybercrimes. However,
the current Internet architecture offers little direct support for
them. A variety of solutions have been proposed, but each
solution tends to address only one specific problem. Moreover, each solution proposes a different fix that is incompatible with the others, which complicates deployment.

In this paper, we make the observation that most of the
existing solutions share a common “functional core”, which
suggests that it may be possible to add a single primitive to
the Internet architecture that can support a wide variety of
diagnostic and forensic tasks. We then present one specific
candidate that we call secure packet provenance (SPP). We
show that SPP is easy to add to the current architecture, that
it can be implemented efficiently in both software and hardware, and that it can be used to approximate (and sometimes
surpass) the capabilities offered by a variety of existing diagnostic and forensic systems.

",18.3970566412798,16.47242169595111,190
EuroSys_17_034.txt,16.216677614307635,14.800159934720522,EuroSys,12144,"

With the advent of trusted execution environments provided
by recent general purpose processors, a class of replication
protocols has become more attractive than ever: Protocols
based on a hybrid fault model are able to tolerate arbitrary
faults yet reduce the costs significantly compared to their
traditional Byzantine relatives by employing a small subsystem trusted to only fail by crashing. Unfortunately, existing
proposals have their own price: We are not aware of any
hybrid protocol that is backed by a comprehensive formal
specification, complicating the reasoning about correctness
and implications. Moreover, current protocols of that class
have to be performed largely sequentially. Hence, they are
not well-prepared for just the modern multi-core processors
that bring their very own fault model to a broad audience. In
this paper, we present Hybster, a new hybrid state-machine
replication protocol that is highly parallelizable and specified formally. With over 1 million operations per second using only four cores, the evaluation of our Intel SGX-based
prototype implementation shows that Hybster makes hybrid
state-machine replication a viable option even for today’s
very demanding critical services.

",18.946978176291534,18.67986338797815,185
EuroSys_17_035.txt,14.597019329590701,12.485097876359848,EuroSys,10128,"

The problem of efficient concurrent memory reclamation in
unmanaged languages such as C or C++ is one of the major challenges facing the parallelization of billions of lines
of legacy code. Garbage collectors for C/C++ can be inefficient; thus, programmers are often forced to use finelycrafted concurrent memory reclamation techniques. These
techniques can provide good performance, but require considerable programming effort to deploy, and have strict requirements, allowing the programmer very little room for
error.

In this work, we present Forkscan, a new conservative
concurrent memory reclamation scheme which is fully automatic and surprisingly scalable. Forkscan’s semantics place
it between automatic garbage collectors (it requires the programmer to explicitly retire nodes before they can be reclaimed), and concurrent memory reclamation techniques
(as it does not assume that nodes are completely unlinked
from the data structure for correctness). Forkscan’s implementation exploits these new semantics for efficiency: we
leverage parallelism and optimized implementations of signaling and copy-on-write in modern operating systems to efficiently obtain and process consistent snapshots of memory
that can be scanned concurrently with the normal program
operation.

Empirical evaluation on a range of classical concurrent
data structure microbenchmarks shows that Forkscan can
preserve the scalability of the original code, while main
taining an order of magnitude lower latency than automatic
garbage collection, and demonstrating competitive performance with finely crafted memory reclamation techniques.

",21.45064796953576,20.92689440993789,229
EuroSys_17_036.txt,16.27888064834073,14.346734755573149,EuroSys,9044,"

Data structures for non-volatile memories have to be designed such that they can be atomically modified using
transactions. Existing atomicity methods require data to be
copied in the critical path which significantly increases the
latency of transactions. These overheads are further amplified for transactions on byte-addressable persistent memories where often the byte ranges modified for data structure
updates are significantly smaller compared to the granularity at which data can be efficiently copied and logged.
We propose Kamino-Tx that provides a new way to perform transactional updates on non-volatile byte-addressable
memories (NVM) without requiring any copying of data in
the critical path. Kamino-Tx maintains an additional copy of
data off the critical path to achieve atomicity. But in doing
so Kamino-Tx has to overcome two important challenges of
safety and minimizing NVM storage overhead. We propose
amore dynamic approach to maintaining the additional copy
of data to reduce storage overheads. To further mitigate the
storage overhead of using Kamino-Tx in a replicated setting,
we develop Kamino-Tx-Chain, a variant of Chain Replication where replicas perform in-place updates and do not
maintain data copies locally; replicas in Kamino-Tx-Chain
leverage other replicas as copies to roll back or forward for
atomicity. Our results show that using Kamino-Tx increases
throughput by up to 9.5x for unreplicated systems and up to
2.2x for replicated settings.

",18.36309006607702,16.992503576537917,236
EuroSys_17_037.txt,16.235204687356553,15.39725607452047,EuroSys,10957,"

The abundance of memory corruption and disclosure vulnerabilities in kernel code necessitates the deployment of
hardening techniques to prevent privilege escalation attacks.
As more strict memory isolation mechanisms between the
kernel and user space, like Intel’s SMEP, become commonplace, attackers increasingly rely on code reuse techniques
to exploit kernel vulnerabilities. Contrary to similar attacks
in more restrictive settings, such as web browsers, in kernel exploitation, non-privileged local adversaries have great
flexibility in abusing memory disclosure vulnerabilities to
dynamically discover, or infer, the location of certain code
snippets and construct code-reuse payloads. Recent studies
have shown that the coupling of code diversification with the
enforcement of a “read XOR execute” (R*X) memory safety
policy is an effective defense against the exploitation of userland software, but so far this approach has not been applied
for the protection of the kernel itself.

In this paper, we fill this gap by presenting kR*X: a kernel
hardening scheme based on execute-only memory and code
diversification. We study a previously unexplored point in
the design space, where a hypervisor or a super-privileged
component is not required. Implemented mostly as a set
of GCC plugins, kR’X is readily applicable to the x86-64
Linux kernel and can benefit from hardware support (e.g.,
MPX on modern Intel CPUs) to optimize performance. In
full protection mode, kR*X incurs a low runtime overhead
of 4.04%, which drops to 2.32% when MPX is available.

",18.903936251131103,17.94,246
EuroSys_17_038.txt,15.906072224768497,14.209660135672411,EuroSys,10113,"
As modern 64-bit x86 processors no longer support the
segmentation capabilities of their 32-bit predecessors, most
research projects assume that strong in-process memory
isolation is no longer an affordable option. Instead of strong,
deterministic isolation, new defense systems therefore rely on
the probabilistic pseudo-isolation provided by randomization
to “hide” sensitive (or safe) regions. However, recent attacks
have shown that such protection is insufficient; attackers can
leak these safe regions in a variety of ways.
In this paper, we revisit isolation for x86-64 and argue that
hardware features enabling efficient deterministic isolation do
exist. We first present a comprehensive study on commodity
hardware features that can be repurposed to isolate safe
regions in the same address space (e.g., Intel MPX and
MPK). We then introduce MemSentry, a framework to harden
modern defense systems with commodity hardware features
instead of information hiding. Our results show that some
hardware features are more effective than others in hardening
such defenses in each scenario and that features originally
conceived for other purposes (e.g., Intel MPX for bounds
checking) are surprisingly efficient at isolating safe regions
compared to their software equivalent (i.e., SFI).

",17.28802050969988,17.477976190476188,196
EuroSys_17_039.txt,15.420723295917057,13.987495659943455,EuroSys,10020,"

In this paper, we present DStress, a system that can efficiently perform computations on graphs that contain confidential data. DStress assumes that the graph is physically
distributed across many participants, and that each participant only knows a small subgraph; it protects privacy by enforcing tight, provable limits on how much each participant
can learn about the rest of the graph.

We also study one concrete instance of this problem: measuring systemic risk in financial networks. Systemic risk is
the likelihood of cascading bankruptcies — as, e.g., during
the financial crisis of 2008 — and it can be quantified based
on the dependencies between financial institutions; however,
the necessary data is highly sensitive and cannot be safely
disclosed. We show that DStress can implement two different systemic risk models from the theoretical economics literature. Our experimental evaluation suggests that DStress
can run the corresponding computations in about five hours,
whereas a naive approach could take several decades.

",17.315433740611066,16.334285714285716,156
EuroSys_17_040.txt,14.010797357368403,12.04136251004871,EuroSys,9828,"

Intel Software Guard eXtensions (SGX) enable secure and
trusted execution of user code in an isolated enclave to protect against a powerful adversary. Unfortunately, running
I/O-intensive, memory-demanding server applications in enclaves leads to significant performance degradation. Such
applications put a substantial load on the in-enclave system
call and secure paging mechanisms, which turn out to be the
main reason for the application slowdown. In addition to the
high direct cost of thousands-of-cycles long SGX management instructions, these mechanisms incur the high indirect
cost of enclave exits due to associated TLB flushes and processor state pollution.

We tackle these performance issues in Eleos by enabling
exit-less system calls and exit-less paging in enclaves. Eleos
introduces a novel Secure User-managed Virtual Memory
(SUVM) abstraction that implements application-level paging inside the enclave. SUVM eliminates the overheads of
enclave exits due to paging, and enables new optimizations
such as sub-page granularity of accesses.

We thoroughly evaluate Eleos on a range of microbenchmarks and two real server applications, achieving notable
system performance gains. memcached and a face verification server running in-enclave with Eleos, achieves up to
2.2x and 2.3x higher throughput respectively while working on datasets up to 5x larger than the enclave’s secure
physical memory.

",17.971250198000288,17.285768779342728,217
EuroSys_17_041.txt,15.985870059530363,14.650692704984788,EuroSys,9474,"

In order to keep the costs of operating in-memory storage
on the public cloud low, we devise novel ideas and enabling modeling and optimization techniques for combining conventional Amazon EC2 instances with the cheaper
spot and burstable instances. Whereas a naturally appealing
way of using failure-prone spot instances is to selectively
store unpopular (“cold”) content, we show that a form of
“hot-cold mixing” across regular and spot instances might
be more cost-effective. To overcome performance degradation resulting from spot instance revocations, we employ
a highly available passive backup using the recently emergent burstable instances. We show how the idiosyncratic resource allocations of burstable instances make them ideal
candidates for such a backup. We implement all our ideas
in an EC2-based memcached prototype. Using simulations
and live experiments on our prototype, we show that (i) our
hot-cold mixing, informed by our modeling of spot prices,
helps improve cost savings by 50-80% compared to only using regular instances, and (ii) our burstable-based backup
helps reduce performance degradation during spot revocation, e.g., the 95% latency during failure recovery improves
by 25% compared to a backup based on regular instances.

",20.10790988173199,19.09938144329897,196
FAST_17_001.txt,13.646822318556918,11.681731911127219,FAST,9672,"
NetApp® WAFL is a transactional file system that uses
the copy-on-write mechanism to support fast write performance and efficient snapshot creation. However,
copy-on-write increases the demand on the file system
to find free blocks quickly; failure to do so may impede
allocations for incoming writes. Efficiency is also important, because the task may consume CPU and other
resources. In this paper, we describe the evolution (over
more than a decade) of WAFL’s algorithms and data
structures for reclaiming space with minimal impact on
the overall storage appliance performance.
",15.903189008614273,14.159220430107528,94
FAST_17_002.txt,13.634923106336359,11.169843776824038,FAST,9446,"
TTFLASH is a “tiny-tail” flash drive (SSD) that eliminates GC-induced tail latencies by circumventing GCblocked I/Os with four novel strategies: plane-blocking
GC, rotating GC, GC-tolerant read, and GC-tolerant
flush. It is built on three SSD internal advancements:
powerful controllers, parity-based RAIN, and capacitorbacked RAM, but is dependent on the use of intra-plane
copyback operations. We show that TTFLASH comes significantly close to a “no-GC” scenario. Specifically, between 99-99.99th percentiles, TTFLASH is only 1.0 to
2.6 slower than the no-GC case, while a base approach
suffers from 5—138x GC-induced slowdowns.
",15.903189008614273,13.443235294117649,106
FAST_17_003.txt,14.54523378275901,12.623308584578613,FAST,9997,"
Most storage systems that write in a log-structured manner need a mechanism for garbage collection (GC), reclaiming and consolidating space by identifying unused
areas on disk. In a deduplicating storage system, GC is
complicated by the possibility of numerous references to
the same underlying data. We describe two variants of
garbage collection in a commercial deduplicating storage
system, a logical GC that operates on the files containing
deduplicated data and a physical GC that performs sequential I/O on the underlying data. The need for the second approach arises from a shift in the underlying workloads, in which exceptionally high duplication ratios or
the existence of millions of individual small files result in
unacceptably slow GC using the file-level approach. Under such workloads, determining the liveness of chunks
becomes a slow phase of logical GC. We find that physical GC decreases the execution time of this phase by up
to two orders of magnitude in the case of extreme workloads and improves it by approximately 10-60% in the
common case, but only after additional optimizations to
compensate for its higher initialization overheads.
",18.599290044081553,18.312837837837836,186
FAST_17_004.txt,13.529257306502629,11.662920114144992,FAST,7309,"
File systems must allocate space for files without
knowing what will be added or removed in the future.
Over the life of a file system, this may cause suboptimal file placement decisions which eventually lead to
slower performance, or aging. Traditional file systems
employ heuristics, such as collocating related files and
data blocks, to avoid aging, and many file system implementors treat aging as a solved problem.

However, this paper describes realistic as well as synthetic workloads that can cause these heuristics to fail,
inducing large performance declines due to aging. For
example, on ext4 and ZFS, a few hundred git pull operations can reduce read performance by a factor of 2;
performing a thousand pulls can reduce performance by
up to a factor of 30. We further present microbenchmarks
demonstrating that common placement strategies are extremely sensitive to file-creation order; varying the creation order of a few thousand small files in a real-world
directory structure can slow down reads by 15 — 175 x,
depending on the file system.

We argue that these slowdowns are caused by poor layout. We demonstrate a correlation between read performance of a directory scan and the locality within a file
system’s access patterns, using a dynamic layout score.

In short, many file systems are exquisitely prone to
read aging for a variety of write workloads. We show,
however, that aging is not inevitable. BetrFS, a file system based on write-optimized dictionaries, exhibits almost no aging in our experiments. BetrFS typically outperforms the other file systems in our benchmarks; aged
BetrFS even outperforms the unaged versions of these
file systems, excepting Btrfs. We present a framework
for understanding and predicting aging, and identify the
key features of BetrFS that avoid aging.
",14.663929897715473,13.086920415224913,291
FAST_17_005.txt,13.604724146365893,11.791184348022504,FAST,8141,"
Traditionally, file systems were implemented as part
of OS kernels. However, as complexity of file systems
grew, many new file systems began being developed in
user space. Nowadays, user-space file systems are often
used to prototype and evaluate new approaches to file
system design. Low performance is considered the main
disadvantage of user-space file systems but the extent
of this problem has never been explored systematically.
As a result, the topic of user-space file systems remains
rather controversial: while some consider user-space file
systems a toy not to be used in production, others develop full-fledged production file systems in user space.
In this paper we analyze the design and implementation of the most widely known user-space file system
framework—FUSE—and characterize its performance
for a wide range of workloads. We instrumented FUSE
to extract useful statistics and traces, which helped us analyze its performance bottlenecks and present our analysis results. Our experiments indicate that depending on
the workload and hardware used, performance degradation caused by FUSE can be completely imperceptible
or as high as -83% even when optimized; and relative
CPU utilization can increase by 31%.
",15.742502247213078,14.606314766839379,194
FAST_17_006.txt,14.067090575897748,12.710012136292608,FAST,9287,"
Cloud-based storage provides reliability and ease-ofmanagement. Unfortunately, it can also incur significant
costs for both storing and communicating data, even after
using techniques such as chunk-based deduplication and
delta compression. The current trend of providing access
to past versions of data exacerbates both costs.

In this paper, we show that deterministic recomputation of data can substantially reduce the cost of cloud
storage. Borrowing a well-known dualism from the faulttolerance community, we note that any data can be equivalently represented by a log of the nondeterministic inputs needed to produce that data. We design a file system, called Knockoff, that selectively substitutes nondeterministic inputs for file data to reduce communication
and storage costs. Knockoff compresses both data and
computation logs: it uses chunk-based deduplication for
file data and delta compression for logs of nondeterminism. In two studies, Knockoff reduces the average cost
of sending files to the cloud without versioning by 21%
and 24%; the relative benefit increases as versions are
retained more frequently.
",15.078166124597352,15.076190476190472,169
FAST_17_007.txt,16.76262789794932,14.952707400205622,FAST,9314,"
Recent improvements in both the performance and
scalability of shared-nothing, transactional, in-memory
NewSQL databases have reopened the research question
of whether distributed metadata for hierarchical file systems can be managed using commodity databases. In this
paper, we introduce HopsFS, a next generation distribution of the Hadoop Distributed File System (HDFS) that
replaces HDFS’ single node in-memory metadata service,
with a distributed metadata service built on a NewSQL
database. By removing the metadata bottleneck, HopsFS
enables an order of magnitude larger and higher throughput clusters compared to HDFS. Metadata capacity has
been increased to at least 37 times HDFS’ capacity, and
in experiments based on a workload trace from Spotify,
we show that HopsFS supports 16 to 37 times the throughput of Apache HDFS. HopsFS also has lower latency for
many concurrent clients, and no downtime during failover.
Finally, as metadata is now stored in a commodity database, it can be safely extended and easily exported to
external systems for online analysis and free-text search.
",18.7741,17.384761904761906,169
FAST_17_008.txt,14.65901046902485,13.246354028873519,FAST,8886,"
Drive-Managed SMR (Shingled Magnetic Recording) disks
offer a plug-compatible higher-capacity replacement for
conventional disks. For non-sequential workloads, these
disks show bimodal behavior: After a short period of high
throughput they enter a continuous period of low throughput.

We introduce ext4-lazy', a small change to the Linux
ext4 file system that significantly improves the throughput
in both modes. We present benchmarks on four different
drive-managed SMR disks from two vendors, showing that
ext4-lazy achieves 1.7-5.4x improvement over ext4 on a
metadata-light file server benchmark. On metadata-heavy
benchmarks it achieves 2-13 improvement over ext4 on
drive-managed SMR disks as well as on conventional disks.
",14.554592549557764,13.17568421052632,117
FAST_17_009.txt,13.730945837611188,11.654048770357306,FAST,6414,"
Shingled Magnetic Recording (SMR) is a new technique
for increasing areal data density in hard drives. Drivemanaged SMR (DM-SMR) drives employ a shingled
translation layer to mask internal data management and
support block interface to the host software. Two major challenges of designing an efficient shingled translation layer for DM-SMR drives are metadata overhead
and garbage collection overhead.

In this paper we introduce SMaRT, an approach to
Shingled Magnetic Recording Translation which adapts
its data management scheme as the drive utilization
changes. SMaRT uses a hybrid update strategy which
performs in-place update for the qualified tracks and outof-place updates for the unqualified tracks. Background
Garbage Collection (GC) operations and on-demand GC
operations are used when the free space becomes too
fragmented. SMaRT also has a specially crafted space
allocation and track migration scheme that supports automatic cold data progression to minimize GC overhead
in the long term.

We implement SMaRT and compare it with a regular Hard Disk Drive (HDD) and a simulated Seagate
DM-SMR drive. The experiments with several block
I/O traces demonstrate that SMaRT performs better than
the Seagate drive and even provides comparable performance as regular HDDs when drive space usage is below
a certain threshold.
",16.04434344847333,13.786181229773465,207
FAST_17_010.txt,15.811317937888635,14.788764283129108,FAST,7355,"
This paper presents a simple yet effective design solution to facilitate technology scaling for hard disk drives
(HDDs) being deployed in data centers. Emerging magnetic recording technologies improve storage areal density mainly through reducing the track pitch, which however makes HDDs subject to higher read retry rates. More
frequent HDD read retries could cause intolerable tail latency for large-scale systems such as data centers. To
reduce the occurrence of costly read retry, one intuitive
solution is to apply erasure coding locally on each HDD
or JBOD Gust a bunch of disks). To be practically viable,
local erasure coding must have very low coding redundancy, which demands very long codeword length (e.g.,
one codeword spans hundreds of 4kB sectors) and hence
large file size. This makes local erasure coding mainly
suitable for data center applications. This paper contends
that local erasure coding should be implemented transparently within filesystems, and accordingly presents a
basic design framework and elaborates on important design issues. Meanwhile, this paper derives the mathematical formulations for estimating its effect on reducing
HDD read tail latency. Using Reed-Solomon (RS) based
erasure codes as test vehicles, we carried out detailed
analysis and experiments to evaluate its implementation
feasibility and effectiveness. We integrated the developed design solution into ext4 to further demonstrate its feasibility and quantitatively measure its impact on average
speed performance of various big data benchmarks.
",16.76809479433877,16.168074235807865,231
FAST_17_011.txt,14.403787532302122,13.144994271481945,FAST,9120,"
We analyze how modern distributed storage systems behave in the presence of file-system faults such as data
corruption and read and write errors. We characterize
eight popular distributed storage systems and uncover
numerous bugs related to file-system fault tolerance. We
find that modern distributed systems do not consistently
use redundancy to recover from file-system faults: a
single file-system fault can cause catastrophic outcomes
such as data loss, corruption, and unavailability. Our results have implications for the design of next generation
fault-tolerant distributed and cloud storage systems.
",16.218646115125612,15.326456043956046,92
FAST_17_012.txt,15.046147545017252,13.594086397856636,FAST,7829,"
We present Omid — a transaction processing service that
powers web-scale production systems at Yahoo. Omid
provides ACID transaction semantics on top of traditional key-value storage; its implementation over Apache
HBase is open sourced as part of Apache Incubator.
Omid can serve hundreds of thousands of transactions
per second on standard mid-range hardware, while incurring minimal impact on the speed of data access in
the underlying key-value store. Additionally, as expected
from always-on production services, Omid is highly
available.
",15.579741850924794,14.822981927710845,84
FAST_17_013.txt,15.826368189050278,14.139238692911814,FAST,10594,"
Recent research has shown that applications often incorrectly implement crash consistency. We
present ccfs, a file system that improves the correctness
of application-level crash consistency protocols while
maintaining high performance. A key idea in ccfs is the
abstraction of a stream. Within a stream, updates are
committed in program order, thus helping correctness;
across streams, there are no ordering restrictions, thus
enabling scheduling flexibility and high performance.
We empirically demonstrate that applications running
atop ccfs achieve high levels of crash consistency. Further, we show that ccfs performance under standard filesystem benchmarks is excellent, in the worst case on par
with the highest performing modes of Linux ext4, and in
some cases notably better. Overall, we demonstrate that
both application correctness and high performance can
be realized in a modern file system.
",15.903189008614273,13.487012987012989,133
FAST_17_014.txt,15.659087240785095,13.938671880513251,FAST,8163,"
We introduce a low-cost incremental checksum technique that protects metadata blocks against in-memory
scribbles, and a lightweight digest-based transaction auditing mechanism that enforces file system consistency
invariants. Compared with previous work, our techniques reduce performance overhead by an order of magnitude. They also help distinguish scribbles from logic
bugs. We also present a mechanism to pinpoint the cause
of scribbles on production systems. Our techniques have
been productized in the NetApp® WAFL® (Write Anywhere File Layout) file system with negligible performance overhead, greatly reducing corruption-related incidents over the past five years, based on millions of runtime hours.
",16.404322709996244,14.669242718446608,102
FAST_17_015.txt,17.678446850189644,16.710249220300295,FAST,7910,"
This paper describes Mirador, a dynamic placement
service implemented as part of an enterprise scale-out
storage product. Mirador is able to encode multidimensional placement goals relating to the performance, failure response, and workload adaptation of the
storage system. Using approaches from dynamic constraint satisfaction, Mirador migrates both data and client
network connections in order to continuously adapt and
improve the configuration of the storage system.
",16.32212239822248,16.76878787878788,67
FAST_17_016.txt,13.802905870295596,12.127583004107692,FAST,8263,"
Anomalies in the runtime behavior of software systems,
especially in distributed systems, are inevitable, expensive, and hard to locate. To detect and correct such
anomalies (like instability due to a growing memory
consumption, failure due to load spikes, etc.) one has
to automatically collect, store, and analyze the operational data of the runtime behavior, often represented as
time series. There are efficient means both to collect and
analyze the runtime behavior. But traditional time series databases do not yet focus on the specific needs of
anomaly detection (generic data model, specific built-in
functions, storage efficiency, and fast query execution).
The paper presents Chronix, a domain specific time series database targeted at anomaly detection in operational
data. Chronix uses an ideal compression and chunking of
the time series data, a methodology for commissioning
Chronix’ parameters to a sweet spot, a way of enhancing the data with attributes, an expandable set of analysis functions, and other techniques to achieve both faster
query times and a significantly smaller memory footprint. On benchmarks Chronix saves 20%-68% of the
space that other time series databases need to store the
data and saves 80%-92% of the data retrieval time and
73%-97% of the runtime of analyzing functions.
",18.548980349730343,18.107526132404185,206
FAST_17_017.txt,15.252394086517768,14.04840564143537,FAST,8856,"
Object stores are becoming pervasive due to their
scalability and simplicity. Their broad adoption, however, contrasts with their rigidity for handling heterogeneous workloads and applications with evolving requirements, which prevents the adaptation of the system to
such varied needs. In this work, we present Crystal, the
first Software-Defined Storage (SDS) architecture whose
core objective is to efficiently support multi-tenancy in
object stores. Crystal adds a filtering abstraction at the
data plane and exposes it to the control plane to enable high-level policies at the tenant, container and object granularities. Crystal translates these policies into a
set of distributed controllers that can orchestrate filters at
the data plane based on real-time workload information.
We demonstrate Crystal through two use cases on top of
OpenStack Swift: One that proves its storage automation
capabilities, and another that differentiates IO bandwidth
in a multi-tenant scenario. We show that Crystal is an extensible platform to deploy new SDS services for object
stores with small overhead.
",16.439396014739867,15.057402597402596,166
FAST_17_018.txt,14.201334739398924,12.221538487122654,FAST,8676,"
Recent interest in persistent memory (PM) has stirred development of index structures that are efficient in PM.
Recent such developments have all focused on variations
of the B-tree. In this paper, we show that the radix tree,
which is another less popular indexing structure, can be
more appropriate as an efficient PM indexing structure.
This is because the radix tree structure is determined by
the prefix of the inserted keys and also does not require
tree rebalancing operations and node granularity updates.
However, the radix tree as-is cannot be used in PM. As
another contribution, we present three radix tree variants,
namely, WORT (Write Optimal Radix Tree), WOART
(Write Optimal Adaptive Radix Tree), and ART+CoW.
Of these, the first two are optimal for PM in the sense that
they only use one 8-byte failure-atomic write per update
to guarantee the consistency of the structure and do not
require any duplicate copies for logging or CoW. Extensive performance studies show that our proposed radix
tree variants perform considerable better than recently
proposed B-tree variants for PM such NVTree, wB+Tree,
and FPTree for synthetic workloads as well as in implementations within Memcached.
",16.061879428645646,14.212757537688443,198
FAST_17_019.txt,13.759310204518442,12.38171095539533,FAST,8246,"
Recent advances in flash memory technology have reduced the cost-per-bit of flash storage devices such as
solid-state drives (SSDs), thereby enabling the development of large-capacity SSDs for enterprise-scale storage.
However, two major concerns arise in designing SSDs.
The first concern is the poor performance of random
writes in an SSD. Server workloads such as databases
generate many random writes; therefore, this problem
must be resolved to enable the usage of SSDs in enterprise systems. The second concern is that the size of
the internal DRAM of an SSD is proportional to the capacity of the SSD. The peculiarities of flash memory require an address translation layer called flash translation
layer (FTL) to be implemented within an SSD. The FTL
must maintain the address mapping table in the internal DRAM. Although the previously proposed demand
map loading technique can reduce the required DRAM
size, the technique aggravates the poor random performance. We propose a novel address reshaping technique
called sequentializing in host and randomizing in device
(SHRD), which transforms random write requests into
sequential write requests in the block device driver by
assigning the address space of the reserved log area in
the SSD. Unlike previous approaches, SHRD can restore
the sequentially written data to the original location without requiring explicit copy operations by utilizing the address mapping scheme of the FTL. We implement SHRD
in a real SSD device and demonstrate the improved performance resulting from SHRD for various workloads.
",15.186304673781336,13.6139146567718,246
FAST_17_020.txt,14.684042447272748,12.998232379926865,FAST,8038,"
As graphs continue to grow, external memory graph processing systems serve as a promising alternative to inmemory solutions for low cost and high scalability. Unfortunately, not only does this approach require considerable efforts in programming and IO management, but its
performance also lags behind, in some cases by an order
of magnitude. In this work, we strive to achieve an ambitious goal of achieving ease of programming and high IO
performance (as in-memory processing) while maintaining graph data on disks (as external memory processing).
To this end, we have designed and developed Graphene
that consists of four new techniques: an IO request centric programming model, bitmap based asynchronous IO,
direct hugepage support, and data and workload balancing. The evaluation shows that Graphene can not only
run several times faster than several external-memory
processing systems, but also performs comparably with
in-memory processing on large graphs.
",19.686211704642208,17.79028571428572,148
FAST_17_021.txt,14.644177522650242,13.165255470270246,FAST,9133,"
Modern systems use networks extensively, accessing
both services and storage across local and remote networks. Latency is a key performance challenge, and
packing multiple small operations into fewer large ones
is an effective way to amortize that cost, especially after years of significant improvement in bandwidth but
not latency. To this end, the NFSv4 protocol supports
a compounding feature to combine multiple operations.
Yet compounding has been underused since its conception because the synchronous POSIX file-system API issues only one (small) request at a time.

We propose vNF'S, an NFSv4.1-compliant client that
exposes a vectorized high-level API and leverages NFS
compound procedures to maximize performance. We
designed and implemented vNFS as a user-space RPC
library that supports an assortment of bulk operations on
multiple files and directories. We found it easy to modify
several UNIX utilities, an HTTP/2 server, and Filebench
to use vNFS. We evaluated vNFS under a wide range of
workloads and network latency conditions, showing that
vNFS improves performance even for low-latency networks. On high-latency networks, vNFS can improve
performance by as much as two orders of magnitude.
",16.45884130781739,14.022624113475182,191
FAST_17_022.txt,14.540751850897909,13.365239236385591,FAST,9767,"
We introduce a replay tool that can be used to replay captured I/O workloads for performance evaluation of highperformance storage systems. We study several sources
in the stock operating system that introduce the uncertainty of replaying a workload. Based on the remedies of
these findings, we design and develop a new replay tool
called hfplayer that can more accurately replay intensive
block I/O workloads in a similar unscaled environment.
However, to replay a given workload trace in a scaled
environment, the dependency between I/O requests becomes crucial. Therefore, we propose a heuristic way of
speculating I/O dependencies in a block I/O trace. Using
the generated dependency graph, hfplayer is capable of
replaying the I/O workload in a scaled environment. We
evaluate hfplayer with a wide range of workloads using
several accuracy metrics and find that it produces better
accuracy when compared with two exiting available replay tools.

",15.719379583869454,13.912063492063496,154
FAST_17_023.txt,14.743661413464284,12.923605083475163,FAST,8647,"
Ensuring stable performance for storage stacks is important, especially with the growth in popularity of
hosted services where customers expect QoS guarantees. The same requirement arises from benchmarking
settings as well. One would expect that repeated, carefully controlled experiments might yield nearly identical performance results—but we found otherwise. We
therefore undertook a study to characterize the amount
of variability in benchmarking modern storage stacks. In
this paper we report on the techniques used and the results of this study. We conducted many experiments using several popular workloads, file systems, and storage
devices—and varied many parameters across the entire
storage stack. In over 25% of the sampled configurations, we uncovered variations higher than 10% in storage performance between runs. We analyzed these variations and found that there was no single root cause: it
often changed with the workload, hardware, or software
configuration in the storage stack. In several of those
cases we were able to fix the cause of variation and reduce it to acceptable levels. We believe our observations
in benchmarking will also shed some light on addressing
stability issues in production systems.
",14.975302809339372,13.247702702702703,186
FAST_17_024.txt,15.539963340784812,13.7618197936815,FAST,8316,"
In data-intensive applications, such as databases and keyvalue stores, reducing the request handling latency is important for providing better data services. In such applications, I/O-intensive background tasks, such as checkpointing, are the major culprit in worsening the latency
due to the contention in shared I/O stack and storage.
To minimize the contention, properly prioritizing I/Os
is crucial but the effectiveness of existing approaches is
limited for two reasons. First, statically deciding the priority of an I/O is insufficient since high-priority tasks
can wait for low-priority I/Os due to I/O priority inversion. Second, multiple independent layers in modern
storage stacks are not holistically considered by existing approaches which thereby fail to effectively prioritize
I/Os throughout the I/O path.

In this paper, we propose a request-centric I/O prioritization that dynamically detects and prioritizes I/Os delaying request handling at all layers in the I/O path. The
proposed scheme is implemented on Linux and is evaluated with three applications, PostgreSQL, MongoDB,
and Redis. The evaluation results show that our scheme
achieves up to 53% better request throughput and 42 x
better 99 percentile request latency (84 ms vs. 3581
ms), compared to the default configuration in Linux.
",17.410965686947208,16.109754784689,210
FAST_17_025.txt,13.223712830367528,10.78249501898041,FAST,8938,"
As Solid-State Drives (SSDs) become commonplace in
data-centers and storage arrays, there is a growing demand for predictable latency. Traditional SSDs, serving block I/Os, fail to meet this demand. They offer a
high-level of abstraction at the cost of unpredictable performance and suboptimal resource utilization. We propose that SSD management trade-offs should be handled
through Open-Channel SSDs, a new class of SSDs, that
give hosts control over their internals. We present our
experience building LightNVM, the Linux Open-Channel
SSD subsystem. We introduce a new Physical Page Address I/O interface that exposes SSD parallelism and storage media characteristics. LightNVM integrates into traditional storage stacks, while also enabling storage engines to take advantage of the new I/O interface. Our experimental results demonstrate that LightNVM has modest host overhead, that it can be tuned to limit read latency variability and that it can be customized to achieve
predictable I/O latencies.
",14.731742533061166,13.108335987261146,158
FAST_17_026.txt,14.614779932904572,13.05312066996574,FAST,9211,"
A longstanding goal of SSD virtualization has been to
provide performance isolation between multiple tenants
sharing the device. Virtualizing SSDs, however, has traditionally been a challenge because of the fundamental
tussle between resource isolation and the lifetime of the
device — existing SSDs aim to uniformly age all the regions of flash and this hurts isolation. We propose utilizing flash parallelism to improve isolation between virtual SSDs by running them on dedicated channels and
dies. Furthermore, we offer a complete solution by also
managing the wear. We propose allowing the wear of different channels and dies to diverge at fine time granularities in favor of isolation and adjusting that imbalance at
a coarse time granularity in a principled manner. Our experiments show that the new SSD wears uniformly while
the 99th percentile latencies of storage operations in a
variety of multi-tenant settings are reduced by up to 3.1x
compared to software isolated virtual SSDs.
",17.315433740611066,16.41090909090909,156
FAST_17_027.txt,14.149654220589241,12.172657050379964,FAST,9342,"
In recent years, flash-based key-value cache systems
have raised high interest in industry, such as Facebook’s
McDipper and Twitter’s Fatcache. These cache systems
typically use commercial SSDs to store and manage
key-value cache data in flash. Such a practice, though
simple, is inefficient due to the huge semantic gap
between the key-value cache manager and the underlying
flash devices. In this paper, we advocate to reconsider
the cache system design and directly open device-level
details of the underlying flash storage for key-value
caching. This co-design approach bridges the semantic
gap and well connects the two layers together, which
allows us to leverage both the domain knowledge of
key-value caches and the unique device properties. In
this way, we can maximize the efficiency of key-value
caching on flash devices while minimizing its weakness.
We implemented a prototype, called DIDACache, based
on the Open-Channel SSD platform. Our experiments on
real hardware show that we can significantly increase the
throughput by 35.5%, reduce the latency by 23.6%, and
remove unnecessary erase operations by 28%.
",14.37465228746014,13.117727900552488,186
HCW_17_001.txt,17.481433327214912,16.610192032042708,HCW,9082,"
The increased use of application-specific computational devices turns even low-power chips into high-performance
computers. Not only additional accelerators (e.g., GPU, DSP,
or even FPGA), but also heterogeneous CPU clusters form
modern computer systems. Programming these chips is however
challenging, due to management overhead, data transfer delays,
and a missing unification of the programming flow. Moreover,
most accelerators require device specific optimizations. Thus, for
application developers, fulfilling software’s initial intention to
serve high portability is one of the most ambitious objectives.
In this work, we present a software abstraction layer unifying
the programming flow for parallel and heterogeneous platforms.
Therefore, we offer a generic C++ API for parallelizing on heterogeneous CPU clusters and offloading to accelerators, specifically
addressing applications with strict real-time constraints. At a free
configurable choice of parallelization- and offloading-frameworks
(e.g., TBB, OpenCL) without affecting the portability, we also
include automatic profiling methods. While offering high configurability of the architecture mapping, these methods ease the
development of optimum scheduling strategies — e.g., in terms
of power, throughput, or latency. To demonstrate the use of the
proposed methods, we present heterogeneous implementations of
the Semi-Global Matching and Histograms of Oriented Gradients
algorithms as exemplary advanced driver-assistance algorithms.
We provide an in-depth discussion of scheduling strategies for
execution on a Samsung Exynos 5422 MPSoC, an Intel Xeon
Phi manycore, and a general-purpose processor equipped with a
Nallatech PClIe-385N FPGA accelerator card.

",18.63122029486172,17.680686533383632,244
HCW_17_002.txt,16.164059494597133,14.176780185863809,HCW,8340,"
Heterogeneous chip-multiprocessors with integrated CPU and GPU cores on the same die allow sharing
of critical memory system resources among the applications
executing on the two types of cores. In this paper, we explore memory system management driven by the quality of
service (QoS) requirement of the GPU applications executing
simultaneously with CPU applications in such heterogeneous
platforms. Our proposal dynamically estimates the level of
QoS (e.g., frame rate in 3D scene rendering) of the GPU
application. Unlike the prior proposals, our algorithm does not
require any profile information and does not assume tile-based
deferred rendering. If the estimated quality of service meets the
minimum acceptable QoS level, our proposal employs a lightweight mechanism to dynamically adjust the GPU memory
access rate so that the GPU is able to just meet the required
QoS level. This frees up memory system resources which
can be shifted to the co-running CPU applications. Detailed
simulations done on a heterogeneous chip-multiprocessor with
one GPU and four CPU cores running heterogeneous mixes
of DirectX, OpenGL, and CPU applications show that our
proposal improves the CPU performance by 18% on average.

",18.848423458724294,17.79079365079365,191
HCW_17_003.txt,15.49052533363648,14.600797232924574,HCW,6508,"
We present Nswap2L-FS, a fast, adaptable, and
heterogeneous storage system for backing file data in clusters.
Nswap2L-FS particularly targets backing temporary files, such
as those created by data-intensive applications for storing
intermediate results. Our work addresses the problem of how
to efficiently and effectively make use of heterogeneous storage
devices that are increasingly common in clusters. Nswap2L-FS
implements a two-layer device design. The top layer transparently manages a set of bottom layer physical storage devices,
which may include SSD, HDD, and its own implementation of
network RAM. Nswap2L-FS appears to node operating systems
as a single, fast backing storage device for file systems, hiding
the complexity of heterogeneous storage management from OS
subsystems. Internally, it implements adaptable and tunable
policies that specify where data should be placed and whether
data should be migrated from one underlying physical device
to another based on resource usage and the characteristics
of different devices. We present solutions to challenges that
are specific to supporting backing filesystems, including how
to efficiently support a wide range of I/O request sizes and
balancing fast storage goals with expectations of persistence
of stored file data. Nswap2L-FS defines relaxed persistence
guarantees on individual file writes to achieve faster V/O
accesses; less stringent persistence semantics allow it to make
use of network RAM to store file data, resulting in faster file /O
to applications. Relaxed persistence guarantees are acceptable
in many situations, particularly those involving short-lived data
such as temporary files. Nswap2L-FS provides a persistence
snapshot mechanism that can be used by applications or
checkpointing systems to ensure that file data are persistent at
certain points in their execution. Nswap2L-FS is implemented
as a Linux block device driver that can be added as a file
partition on individual cluster nodes. Experimental results
show that file-intensive applications run faster when using
Nswap2L-FS as backing store. Additionally, its adaptive data
placement and migration policies, which make effective use
of different underlying physical storage devices, result in
performance exceeding that of any single device.

",17.28802050969988,16.14998964803313,346
HCW_17_004.txt,15.58542232047024,13.744002968099,HCW,8198,"
Computing systems have become increasingly
heterogeneous contributing to higher performance and power
efficiency. However, this is at the cost of increasing the overall
complexity of designing such systems. One key challenge in the
design of heterogeneous systems is the efficient scheduling of
computational load. To address this challenge, this paper thoroughly
analyzes state of the art scheduling policies and proposes a new
dynamic scheduling heuristic: Alternative Processor within
Threshold (APT). This heuristic uses a flexibility factor to attain
efficient usage of the available hardware resources, taking
advantage of the degree of heterogeneity of the system. In a GPUCPU-FPGA system, tested on workloads with and without data
dependencies, this approach improved overall execution time by
16% and 18% when compared to the second best heuristic.

",17.122413403193683,16.0406,126
HCW_17_005.txt,15.825748240321598,13.743844997800696,HCW,8760,"
We design resource management heuristics that
assign serial tasks to the nodes of a heterogeneous high
performance computing (HPC) system. The value of completing
these tasks is modeled using monotonically decreasing utility
functions that represent the time-varying importance of the task.
The value of completing a task is equal to its utility function at
the time of its completion. The overall performance of this system
is measured using the total utility earned by all tasks during
some interval of time. To maximize the performance of such a
system where the preemption of tasks is possible, we have
designed, analyzed, and compared a set of resource allocation
heuristic techniques. We combine two utility-aware heuristics
with three different preemption techniques to create six
preemption-capable heuristics. We also consider the two utilityaware heuristics without preemption. We use simulation studies
to evaluate this set of eight heuristics and compare them with an
FCFS heuristic, which is often used in real systems, and random
assignments. In general, our set of eight heuristics is able to
significantly outperform the comparison heuristics, and the
preemption-capable heuristics are able to significantly increase
the utility earned compared to the heuristics that do not use
preemption. We analyze the performance tradeoffs among the
different preemption-capable heuristics under a variety of
oversubscribed workload environments.

",17.693802365651003,15.267045871559635,219
HCW_17_006.txt,16.697647920343822,14.814803596806623,HCW,7108,"
Many of todays important applications of our
everyday lives, e.g. weather forecast, design of plane and car
shapes, medical analysis or even search engine queries depend
on massively-parallel computer programs that are executed in
data centers hosting thousands of computers. A large amount
of electrical energy is used to power them, and it is of
primary importance to compute more efficiently to sustain the
increasing demand of computing power while keeping energy
consumption reasonable. One promising research path in this
domain is heterogeneous systems. The rationale for that is that
at least parts of applications execute more efficiently depending
on the computing resource (processors, accelerators, etc.).

Nevertheless, the exploitation of these heterogeneous platforms raises new challenges in terms of application management optimization on available computing resources. The aim
of our work is to determine effective algorithms to exploit
these heterogeneous platforms by finding the best mapping
and scheduling of an application to optimize the execution time
and energy consumption with respect to various constraints.
To achieve this goal, there is a need of a detailed modeling
of the applications and the underlying hardware to be able
to find realistic solutions. In this paper, we propose such as
model, provide two implementations with state-of-the-art tools
and preliminary mapping and scheduling numerical results.

",18.903936251131103,17.722877906976745,217
HCW_17_007.txt,16.36216687323824,14.63601313902678,HCW,5667,"
This paper presents a method of cloud resource
allocation designed to take into account both consumers and
providers’ interests. This comes in contrast to today’s providercentered models that subject users to more restrictive terms and
conditions. Both parties’ interests are computed in the form of
integer constraints. Costs and availability are embedded as key
objectives and performance criteria in the model. We propose
a hybrid resolution method based on an evolutionary algorithm
augmented with a tabu search and compare performance with
other resource allocation algorithms. The comparison results reveal the efficiency of the proposed hybrid evolutionary algorithms
for consumer and provider centric cloud resources allocation.

",15.470042427545799,15.059761904761906,107
HiPC_17_001.txt,15.265184871803797,13.17700286101935,HiPC,8461,"
With rapidly increasing parallelism, DRAM performance and power have surfaced as primary constraints from
consumer electronics to high performance computing (HPC)
for a variety of applications, including bulk-synchronous dataparallel applications which are key drivers for multi-core, with
examples including image processing, climate modeling, physics
simulation, gaming, face recognition, and many others. We
present the last-level collective prefetcher (LLCP), a purely
hardware last-level cache (LLC) prefetcher that exploits the
highly correlated prefetch patterns of data-parallel algorithms
that would otherwise not be recognized by a prefetcher that
is oblivious to data parallelism. LLCP generates prefetches on
behalf of multiple cores in memory address order to maximize
DRAM efficiency and bandwidth, and can prefetch from multiple memory pages without expensive translations. Compared
to well-established other prefetchers, LLCP improves execution
time by 5.5% on average (10% maximum), increases DRAM
bandwidth by 9% to 18%, decreases DRAM rank energy
by 6%, produces 27% more timely prefetches, and increases
coverage by 25% at minimum.

",24.50423929779172,23.92804878048781,166
HiPC_17_002.txt,16.230592068645418,14.806050884955752,HiPC,7224,"
Accessing external resources (e.g., loading input
data, checkpointing snapshots, and out-of-core processing) can
have a significant impact on the performance of applications. However, no existing programming systems for highperformance computing directly manage and optimize external
accesses. As a result, users must explicitly manage external
accesses alongside their computation at the application level,
which can result in both correctness and performance issues.

We address this limitation by introducing Iris, a task-based
programming model with semantics for external resources.
Iris allows applications to describe their access requirements
to external resources and the relationship of those accesses
to the computation. Iris incorporates external I/O into a
deferred execution model, reschedules external I/O to overlap
T/O with computation, and reduces external I/O when possible.
We evaluate Iris on three microbenchmarks representative of
important workloads in HPC and a full combustion simulation,
S3D. We demonstrate that the Iris implementation of S3D
reduces the external I/O overhead by up to 20x, compared
to the Legion and the Fortran implementations.

",18.7741,16.622273391812865,173
HiPC_17_003.txt,13.974929879449327,11.396072306072305,HiPC,7449,"
 Triangle counting is an important building block
for finding key players in a graph. It is an integral part of
the popular clustering coefficient analytic and can be used
for pattern matching in social networks. A triangle, which is
also a 3-clique, represents a strong connection between three
players that are all connected. While counting triangles is not
overly expensive from a computational standpoint, especially in
comparison to centrality metrics (such as betweenness centrality
and closeness centrality), it can still prove to be prohibitive
for large scale networks, especially for those with a power-law
distribution. This problem only deepens for dynamic graphs
where the network is constantly changing, requiring constant
updating of the graph and the analytic. In this paper, we
present a new dynamic graph algorithm for counting triangles
that is based on an inclusion-exclusion formulation. While
our algorithm is independent of the computing platform, we
show performance results on an NVIDIA GPU. Our approach
handles 32 million updates per second, or up to 11 million
updates per second if the graph data structure is also updated.
In past approaches, when a vertex was affected due to an edge
insertion or deletion, it was necessary to find the triangles from
scratch for that given vertex. Our new formulation does not
need this and only requires considering the affected edges. As
such our algorithm is typically several hundred times faster
than the past approach - in some cases up to 819X faster.

",15.668782140382113,13.565827123695978,245
HiPC_17_004.txt,16.42153583089538,15.183710443261425,HiPC,7604,"
A good design abstraction framework for high performance computing should provide a higher level programming
abstraction that strikes a balance between the abstraction and
visibility over the hardware so that the software developer can
write a portable software without having to understand the hardware nuances, yet exploit the compute power optimally. In this
paper we have analyzed a popular design abstraction framework
called “Thrust” from NVIDIA, and proposed an extension called
Thrust++ that provides abstraction over the memory hierarchy
of an NVIDIA GPU. Thrust++ allows developers to make efficient
use of shared memory and overall, provides better control
over the GPU memory hierarchy while writing applications in
Thrust style for the CUDA backend. We have shown that when
applications are written for the CUDA backend using Thrust++,
they have minimal performance degradation when compared
to their equivalent CUDA versions. Further, Thrust++ provides
almost 4x speedup when compared to Thrust, for certain compute
intensive kernels that repeatedly use the reduce operation.

",18.026119701940384,18.444952380952376,161
HiPC_17_005.txt,15.356353746822677,13.530551338443512,HiPC,6716,"
In Machine Learning, the parent set identification
problem is to find a set of random variables that best explain
selected variable given the data and some predefined scoring
function. This problem is a critical component to structure
learning of Bayesian networks and Markov blankets discovery,
and thus has many practical applications, ranging from fraud
detection to clinical decision support.

In this paper, we introduce a new distributed memory
approach to the exact parent sets assignment problem. To
achieve scalability, we derive theoretical bounds to constraint
the search space when MDL scoring function is used, and we
reorganize the underlying dynamic programming such that the
computational density is increased and fine-grain synchronization is eliminated. We then design efficient realization of our
approach in the Apache Spark platform. Through experimental
results, we demonstrate that the method maintains strong
scalability on a 500-core standalone Spark cluster, and it can
be used to efficiently process data sets with 70 variables, far
beyond the reach of the currently available solutions.

",17.693802365651003,17.663802395209583,168
HiPC_17_006.txt,15.96690057097009,14.86694025139122,HiPC,8146,"
The Oak Ridge Leadership Computing Facility
(OLCF) runs Titan, the No. 4 supercomputer in the world, to
deliver over four billion compute core hours every year to several
scientific domains, in their pursuit of leadership science. In this
paper, we analyze four years worth of heterogeneous log data
sources from the OLCF resource fabric, capturing metadata
on entities such as users (2,546), scientific project allocations
(674), jobs (1,352,402) and publications (1,146), to derive insights
into the trends in core hour usage and publications, across
35 science domains. We have constructed a scalable graph to
represent the OLCF entities and apply rich graph analytics for
our analysis. Based on this, we have analyzed the metadata
across five dimensions, namely (1) quantitative analysis of Titan
system usage, (2) quantitative analysis of OLCF publications, (3)
correlation analysis between system usage and publications, (4)
text analysis to derive OLCF research trends, and (5) utilization
of graph mining for association analysis. To the best of our
knowledge, our work is the first of its kind to apply graphbased big data techniques to provide comprehensive insights
on an HPC center’s core hour usage and users’ publication
trends. Our results provide valuable details into an HPC center’s
core allocation program, measuring the productivity of scientific
domains, the interplay between core usage and research output,
accelerating collaboration, and in predicting new connections
between resource entities.
",20.10790988173199,21.67337004405287,234
HiPC_17_007.txt,15.866522754996023,15.275080732484078,HiPC,7855,"
Recent work have used both failure logs and resource use data separately (and together) to detect system failureinducing errors and to diagnose system failures. System failure
occurs as a result of error propagation and the (unsuccessful)
execution of error recovery mechanisms. Knowledge of error
propagation patterns and unsuccessful error recovery is important for more accurate and detailed failure diagnosis, and
knowledge of recovery protocols deployment is important for improving system reliability. This paper presents the CORRMEXT
framework which carries failure diagnosis another significant
step forward by analyzing and reporting error propagation
patterns and degrees of success and failure of error recovery
protocols. CORRMEXT uses both error messages and resource
use data in its analyses. Application of CORRMEXT to data
from the Ranger supercomputer have produced new insights.
CORRMEXT has: (i) identified correlations between resource
use counters that capture recovery attempts after an error, (ii)
identified correlations between error events to capture error
propagation patterns within the system, (iii) identified error
propagation and recovery paths during system execution to
explain system behaviour, (iv) showed that the earliest times of
change in system behaviour can only be identified by analyzing
both the correlated resource use counters and correlated errors.
CORRMEXT will be installed on the HPC clusters at the Texas
Advanced Computing Center in Autumn 2017.

",17.553077303434723,18.4425,215
HiPC_17_008.txt,17.58244979672652,16.927744484974813,HiPC,5732,"
The ever-increasing energy demands of modern High
Performance Computing (HPC) platforms is undeniably one of
the most critical aspects for the future design and evolution
of such systems. The capability of managing their energy
consumption not only allows for significant reduction in electricity costs but is also a step forward on the road towards
the exascale. Powercapping is a widely studied technique that
contributes to address this challenge by instantaneously setting
and maintaining a predefined power threshold (power cap)
that cannot be exceeded. However, the lack of a centralized
mechanism responsible for efficiently allocating the available
power among resources and jobs may ultimately yield to fragmentation, low system utilization and increased user waiting
times. Additionally, power cap violations can lead to high
risk scenarios and/or increase operational costs. This paper
proposes to prevent such issues with the introduction of the
Enhanced Power Adaptive Scheduling (E-PAS) algorithm. The
E-PAS algorithm combines scheduling and resource management mechanisms, correlating estimated and real power
consumption data in order to optimize the resource utilization
of the platform under a predefined power cap. The algorithm
has been implemented in the widely used open-source resource
and job management system SLURM and is planned to be
pushed in a future mainstream version. Its effectiveness has
been evaluated through real-scale experiments respectively on
an ARM- and an Intel-based cluster of comparable size. All
experiments have been performed using synthetic workloads
from a set of mini-applications.

",17.353723509956247,16.800590163934427,245
HiPC_17_009.txt,14.198741068988028,12.399420737083407,HiPC,5593,"
 Neuromorphic computing is a promising
candidate to accelerate big data processing. Recently,
several attempts have been made to design neuromorphic
accelerators for popular machine learning algorithms,
such as reservoir computing, deep learning, spiking
neurons etc. Deep learning accelerator which involves
convolutional neural networks (CNNs) have received
widespread attention for their accuracy and efficiency.
This paper proposes ConvLight, a novel deep learning
accelerator based on memristor integrated photonic
computing framework. While the use of on-chip photonic
circuits for analog computing is well known, no prior work
has demonstrated a full-fledged accelerator based on
photonic components. In particular, this paper makes the
following novel contributions: (i) A multilayer deep
learning architecture design is proposed using compute
efficient memristors and photonic components for the first
time. (ii) A pipelined design for each CNN layer is
presented for maximizing throughput and _ enabling
parallelism across the layers. (iii) Simulation of ConvLight
architecture with standard photonic tools for
demonstrating the execution of DNN and CNN workloads
yielding 25X, 60X, and 40X improvements in
computational efficiency, throughput, and energy
efficiency (respectively) compared to state-of-the-art
design.

",21.19438992294339,20.807732240437158,185
HiPC_17_010.txt,14.868608125304753,13.512933698941069,HiPC,6256,"
Training Convolutional Neural Network (CNN)
is a computationally intensive task whose parallelization has
become critical in order to complete the training in an
acceptable time. However, there are two obstacles to developing
a scalable parallel CNN in a distributed-memory computing
environment. One is the high degree of data dependency
exhibited in the model parameters across every two adjacent
mini-batches and the other is the large amount of data to be
transferred across the communication channel. In this paper,
we present a parallelization strategy that maximizes the overlap
of inter-process communication with the computation. The
overlapping is achieved by using a thread per compute node
to initiate communication after the gradients are available.
The output data of backpropagation stage is generated at
each model layer, and the communication for the data can
run concurrently with the computation of other layers. To
study the effectiveness of the overlapping and its impact on
the scalability, we evaluated various model architectures and
hyperparameter settings. When training VGG-A model using
ImageNet data sets, we achieve speedups of 62.97 x and 77.97
on 128 compute nodes using mini-batch sizes of 256 and 512,
respectively.

",17.122413403193683,17.357610103626943,196
HiPC_17_011.txt,13.376845130027384,11.164176595617295,HiPC,11846,"
Parallel programming models and paradigms are
increasingly becoming more expressive with a steady increase
in the number of cores that can be placed on a single chip.
Concurrent data structures for shared memory parallel programs are now being used in operating systems, middle-ware,
and device drivers. In such a shared memory model, processes
communicate and synchronize by applying primitive operations
on memory words. To implement concurrent data structures
that are linearizable and possibly lock-free or wait-free, it is
often necessary to add additional information to memory words
in a data structure. This additional information can range from
a single bit to multiple bits that typically represent thread ids,
request ids, timestamps, and other application dependent fields.
Since most processors can perform compare-And-Set (CAS)
or load-link/store-conditional (LL/SC) operations on only 64
bits at a time, current approaches either use some bits in a
memory word to pack additional information (packing), or use
the bits to store a pointer to an object that contains additional
information (redirection), and the original data item.

The former approach restricts the number of bits for each
additional field and this reduces the range of the field, and the
latter approach is wasteful in terms of space. We propose a
novel and universal method called a memory word expander in
this paper. It caches information for a set of memory locations
that need to be augmented with additional information. It
supports traditional atomic get, set, and CAS operations, and
tries to maintain state for a minimum number of entries.
We experimentally demonstrate that it is possible to reduce
the runtime memory footprint by 20-35% for algorithms that
use redirection. For algorithms that use packing, the use
of the EXPANDER can make them feasible. The performance
overhead is within 2-13% for 32 threads. When we compare the
performance of the EXPANDER based non-blocking algorithms
with the version that uses locks, we have a performance gain
of at least 10-100X.

",16.439396014739867,14.099315654405476,335
HiPC_17_012.txt,14.776028659028455,12.433528740791594,HiPC,6007,"
Streaming graph data mining has become a significant issue in high performance graph mining due to the
increasing appearance of graph data sets as streams. In this
paper we propose Acacia-Stream which is a scalable distributed
streaming graph database engine developed with X10 programming language. Graph streams are partitioned using a streaming
graph partitioner algorithm in Acacia-Stream and streaming
graph processing queries are run on the graph streams. The
partitioned data sets are persisted on secondary storage across
X10 places. We investigate on the use of three different streaming
graph partitioner algorithms called hash, Linear Deterministic
Greedy, and Fennel algorithms and report their performance.
Furthermore, to demonstrate Acacia-Stream’s streaming graph
processing capabilities we implement streaming triangle counting
with Acacia-Stream. We present performance results gathered
from Acacia-Stream with different large scale streaming data
sets in both horizontal and vertical scalability experiments.
Furthermore, we compare streaming graph loading performance
of Acacia-Stream with Neo4j and Oracle’s PGX graph database
servers. From these experiments we observed that AcaciaStream’s Fennel partitioner based graph uploader can upload
a 948MB rmat22 graph in 1283.42 seconds which is 38% faster
than PGX graph database server and 12.8 times faster than Neo4j
database server. Acacia-Stream’s Streaming Partitioner’s batch
size adjustments based optimizations reduced the time used by
the network communications almost by half.

",16.76809479433877,14.813615384615385,229
HiPC_17_013.txt,14.870829429687944,13.148071078898848,HiPC,6232,"
Learning the structure of Bayesian networks, even
in the static case, is NP-hard, compelling much of the research
to focus on heuristic-based approaches. However, there are
instances where exact solutions are desirable especially for
small network sizes. In this work, we present a dynamic
programming based exact solution to learn dynamic Bayesian
network structure. Our method simultaneously learns intra- as
well as higher order inter-time-slice interactions in the network.
For n variables, our exact solution requires O(n?.2""(“+1))
computations to learn M-th order network. To handle such
high computational requirements, we present a parallel exact
solution to push the limit on the size of the networks that can be
learned. Given p = 2° processors, the parallel algorithm runs
in O(n?.2”™ (2”—* + k)) time and achieves optimal parallel
efficiency when 2""—* > k, Using MPI+X parallel programming
model, the parallel algorithm linearly scales to 1,024 cores of
a 64-node Intel Xeon InfiniBand cluster, sustaining > 99% of
parallel efficiency. We also show that the learned networks
on gene network datasets are of high fidelity compared to
heuristic-based techniques.

",15.903189008614273,14.188044871794876,189
HiPC_17_014.txt,14.62280505139153,12.68497066723053,HiPC,6594,"
MPI process placement is an important step to Memory node 1 Memory node 2
achieve scalable performance on modern non-uniform memory 1 |__BRAM _| DRAM z
access (NUMA) systems. A recent study on NUMA architec- wv 3
tures has shown that, on modern NUMA systems, the memory s Ng
congestion problem could cause more severe performance [cr c2 ca ca L3 cache leap Mg coche
degradation than the data locality problem because heavy | cs C6 C7 | C8 c5| C6 C7} C8
congestion on memory controllers could cause long latencies.
However, conventional work on MPI process placement has fo- i {
cused on locality to minimize the remote-access communication.
Moreover, maximizing the locality may actually degrade per- |e | a ca Glielala sede
formance because the load imbalance among nodes in a modern L3 cache saan L3 cache
NUMA system may increase. Thus, a process placement algo- on }cs) 05 c7 | ca | 106) C7 C8 z
rithm must be designed to consider memory congestion. In this a a
paper, a method to reconcile both the locality and the memory Ss t =
congestion on modern NUMA systems is proposed. This method ae eines
statically analyzes the application communication pattern to
optimize the process placement. A data clustering method is
applied to the time-series data of the MPI communications in Figure 1. A modern NUMA system, with four nodes and eight cores per

node.

order to identify data traffics that potentially cause memory
congestion. The proposed method has been evaluated with
the NPB kernels on a real NUMA system and a simulation
environment. Experimental results show that the proposed
method can achieve 1.6x performance improvement compared
with the current state-of-the-art strategy.

",15.786528217884907,14.961636988441118,284
HiPC_17_015.txt,14.368799800835571,11.749103537491159,HiPC,7418,"
The implementation of 2D-3v (2D in space and 3D
in velocity space) PIC-MCC (Particle-In-Cell Monte Carlo Collision) method described in this paper involves the computational
solution of Vlasov-Poisson equations, which provides the spatial
and temporal evolution of the charged-particle velocity distribution functions in plasmas under the effect of self-consistent
electromagnetic (EM) fields and collisions. Stringent numerical
constraints associated with a PIC code makes it computationally
prohibitive on CPUs in case of large problem sizes (total number
of particles, number of grid points and simulation time-scale). We
present the design and implementation of a Graphics Processing
Unit (GPU) based 2D-3v PIC code using the CUDA C APIs
for Kepler architecture. Several parallelization and optimization
techniques have been presented in this paper with special emphasis on shuffle intrinsic specific to Nvidia Kepler architecture (or
later), which significantly improves the performance compared
to existing GPU implementations in the literature. On a test bed
comprising of a serial implementation on Xeon E5 CPU and
parallel implementations on Nvidia Tesla K40 graphics card, we
have achieved a speedup of up to 60x in double precision mode.
Effect of important numerical parameters on speedup has been
investigated. Finally, we compare the performance of our best
parallel implementation on different GPUs (Kepler as well as
Maxwell) and analyze the effect of hardware architecture on the
performance of the PIC code.

",19.287186520377343,19.765886699507394,233
HiPC_17_016.txt,14.203887384502707,12.165564678793263,HiPC,6729,"
Frequent subgraph pattern mining (FSM) finds subgraph patterns that occur in a graph database with a frequency
that is more than a given threshold. In FSM, the notion of
occurrence captures the presence or absence of a node and an
edge in a binary fashion and considers relevance of each edge or
node as same. However, an edge or a node may have different
relevancy score. Therefore, the utility of a pattern should be
defined using the relevance score of participating edges or nodes.
This paper defines the utility notion of a pattern using this idea
and presents algorithms to mine high-utility patterns from a given
graph database. A significant issue in high-utility pattern mining
is that the anti-monotonic property no longer holds contrary to
the FSM. Hence pruning of the search space becomes a daunting
task. To address this issue, we incorporate a function to estimate
an upper-bound utility of a pattern object that also satisfies the
anti-monotonic property. This paper presents three optimization
heuristics for the solution on a distributed platform, namely, a
novel use of bloom filter to avoid exploration of non-candidates,
avoidance of sending database information with each pattern,
and avoidance of sending pattern embeddings with each pattern.
The experimental study on Apache Spark shows the effectiveness
of our proposed optimization strategies.

",15.112257680678326,13.850882882882882,223
HiPC_17_017.txt,14.180827009883572,12.663377413579656,HiPC,6284,"
Scan based error detection architectures for hybrid, carry-free radix-2 and radix-4 addition operations using
redundant arithmetic are presented in this paper. Such addition
operations have been chosen as representative examples as they
are free from carry propagation delay and are ideal from the
viewpoint of technology mapping of the logic elements onto the
FPGA slices. The architectures have been conceived following the
design paradigm of target FPGA specific primitive instantiation
coupled with location constraints, without any degradation in
the speed of circuit operation as compared to the original circuit
implementation without the scan operation. Our architectures
also comfortably outperform the existing state-of-the-art error
detection architectures in terms of speed and consumes less area.

",19.53771442962202,19.41418067226891,120
HiPC_17_018.txt,13.461582606512344,11.217547704752807,HiPC,5822,"
Hines matrices arise in the simulations of mathematical models describing initiation and propagation of action
potentials in a neuron. In this work, we exploit the structural
properties of Hines matrices and design a scalable, linear work,
recursive parallel algorithm for solving a system of linear
equations where the underlying matrix is a Hines matrix, using
the Exact Domain Decomposition Method (EDD). We give a
general form for representing a Hines matrix and use the general
form to prove that the intermediate matrix obtained via the EDD
has the same structural properties as that of a Hines matrix.

Using the above observation, we propose a novel decomposition
strategy called fine decomposition which is suitable for a GPU
architecture. Our algorithmic approach R-FINE-TPT based on
fine decomposition outperforms the previously known approach
in all the cases and gives a speedup of 2.5x on average for a variety
of input neuron morphologies. We further perform experiments
to understand the behaviour of R-FINE-TPT approach and show
its robustness. We also employ a machine learning technique
called linear regression to effectively guide recursion in our
algorithm.

",17.93193317476759,16.850115830115836,187
HiPC_17_019.txt,14.836745963215662,12.550153185970945,HiPC,6230,"
Modern high-performance communication runtime
systems have taken advantage of advanced features on highperformance networks (e.g. InfiniBand) to deliver optimal performance. High-performance communication over InfiniBand
typically requires the communication buffers to be registered first.
However, buffer registration and deregistration are costly operations, which leads to performance degradation if they happen
frequently. To hide this overhead, many existing communication
runtime choose to design a high-performance registration cache
to reduce the number of buffer registrations, but such type
of designs still need some amount of buffers to be registered
and cached, which leads to multiple issues such as performance
overhead, high memory consumption for bookkeeping, and code
complexity for maintaining the registration cache. To solve these
issues, a recently introduced feature for InfiniBand called Implicit
On-Demand Paging (ODP) is getting momentum. This feature
enables one process to register its complete memory address
space for I/O accesses. To fully take advantage of Implicit-ODP,
it is critical to fully understand the behavior and benefits of
Implicit-ODP on InfiniBand and performance/memory tradeoffs it presents. This paper first presents an analysis of the
Implicit-ODP feature and studies its basic performance with
InfiniBand verbs-level micro-benchmarks. Then, we describe the
design tradeoffs with Implicit-ODP and the various optimizations
at MPI runtime. We propose and design communication protocols
that can leverage the Implicit-ODP feature at the MPI level.
The experimental results at the micro-benchmark level and
application level show that our proposed design can deliver
comparable performance to the existing pin-down scheme, while
it does not need registration cache in the MPI runtime. To the
best of our knowledge, this is the first work to study and analyze
the Implicit-ODP feature and design a registration caching free
MPI library with it.

",18.160329797683527,16.085656565656567,299
HiPC_17_020.txt,15.177967317933302,13.12389175029038,HiPC,6311,"
The maximal independent set (MIS) graph problem
arises in many applications such as computer vision, information
theory, molecular biology, and process scheduling. The growing
scale of graph data suggests the use of distributed memory
hardware as a cost-effective approach to providing necessary
compute and memory resources. Existing distributed memory
parallel MIS algorithms rely on synchronous communication and
use techniques such as subgraph computations. In this paper,
we present an asynchronous distributed-memory parallel graph
algorithm that relies on a virtual directed acyclic graph (DAG)
that is created during the algorithm execution. We introduce
two additional algorithms that save computations by ordering
generated work. The first algorithm applies ordering globally to
reduce computations, and the second algorithm applies ordering
locally at the level of threads to minimize the synchronization
overhead. We use two different implementations of Luby’s
algorithm variants as baseline to compare the performance
of the presented algorithms: (1) vertex-centric Luby A and
Luby B implementations, and (2) the CombBLAS linear-algebra
Luby A implementation. Results show that proposed algorithms
outperform both implementations of Luby algorithms, especially
in distributed execution. Furthermore, we show that for lowdiameter graphs the algorithm that applies global ordering scales
better than other algorithms and for high diameter graphs
the original asynchronous algorithm and thread-level ordering
algorithm show better performance.

",20.267338824336647,19.26218125960062,219
HiPC_17_021.txt,15.012496630244154,13.289757412398924,HiPC,6619,"
Graphics Processing Units (GPUs) are increasingly
used to accelerate portions of general-purpose applications.
Higher level language extensions have been proposed to help nonexperts bridge the gap between a host and the GPU’s threading
model. Recent updates to the OpenMP standard allow a user
to parallelize code on a GPU using the well known fork-join
programming model for CPUs.

Mapping this model to the architecturally visible threading
model of typical GPUs has been challenging. In this work
we propose a novel approach using the technique of Warp
Specialization. We show how to specialize one warp (a unit of 32
GPU threads) to handle sequential code on a GPU. When this
master warp reaches a user-specified parallel region, it awakens
unused GPU warps to collectively execute the parallel code. Based
on this method, we have implemented a Clang-based, OpenMP
4.5 compliant, open source compiler for GPUs.

Our work achieves a 3.6x (and up to 32x) performance
improvement over a baseline that does not exploit fork-join parallelism on an NVIDIA k40m GPU across a set of 25 kernels. Compared to state-of-the-art compilers (Clang-ykt, GCC-OpenMP,
GCC-OpenACC) our work is 2.1 - 7.6x faster. Our proposed
technique is simpler to implement, robust, and performant.

",13.023866798666859,11.185550239234452,215
HiPC_17_022.txt,15.627047673027594,13.879586347428681,HiPC,7387,"
Tight data movement lower bounds are known for
dense matrix-vector multiplication and dense matrix-matrix
multiplication and practical implementations exist on GPUs
that achieve performance quite close to the roofline bounds
based on operational intensity. For large dense matrices,
matrix-vector multiplication is bandwidth-limited and its performance is significantly lower than matrix-matrix multiplication. However, in contrast, the performance of sparse matrixmatrix multiplication (SpGEMM) is generally much lower than
that of sparse matrix-vector multiplication (SpMV).

In this paper, we use a combination of lower-bounds and
upper-bounds analysis of data movement requirements, as well
as hardware counter based measurements to gain insights into
the performance limitations of existing implementations for
SpGEMM on GPUs. The analysis motivates the development
of an adaptive work distribution strategy among threads and
results in performance enhancement for SpGEMM code on
GPUs.

",18.458006810337128,18.171428571428574,141
HiPC_17_023.txt,16.785842355254875,15.781732068149832,HiPC,6313,"
Cosmological N-body simulations rank among the
most computationally intensive efforts today. A key challenge
is the analysis of structure, substructure, and the merger
history for many billions of compact particle clusters, called
halos. Effectively representing the merging history of halos is
essential for many galaxy formation models used to generate
synthetic sky catalogs, an important application of modern
cosmological simulations. Generating realistic mock catalogs
requires computing the halo formation history from simulations
with large volumes and billions of halos over many time steps,
taking hundreds of terabytes of analysis data. We present
fast parallel algorithms for producing halo merger trees and
tracking halo substructure from a single-level, density-based
clustering algorithm. Merger trees are created from analyzing
the halo-particle membership function in adjacent snapshots,
and substructure is identified by tracking the “cores” of
merging halos — sets of particles near the halo center. Core
tracking is performed after creating merger trees and uses
the relationships found during tree construction to associate
substructures with hosts. The algorithms are implemented with
MPI and evaluated on a Cray XK7 supercomputer using up to
16,384 processes on data from HACC, a modern cosmological
simulation framework. We present results for creating merger
trees from 101 analysis snapshots taken from the Q Continuum,
a large volume, high mass resolution, cosmological simulation
evolving half a trillion particles.

",18.599290044081553,18.21468468468468,224
HiPC_17_024.txt,14.38464516356866,11.758279504561646,HiPC,5292,"
The problem of task scheduling with communication
delays is NP-hard. State-space search algorithms such as A*
have been shown to be a promising approach to solving this
problem optimally. A recently proposed state-space model for
task scheduling, known as Allocation-Ordering (AQ), allows
state-space search methods to be applied to the problem of
optimal task scheduling without the need for duplicate avoidance mechanisms. This paper examines the performance of two
parallel search algorithms when applied to both the AO model
and the older ELS state-space model. This suggests that its use
may provide an advantage with many different variations on
state-space search. This paper explores the application of AO
to some of these variants, namely depth-first branch-and-bound
(DFBnB) and parallel search. We also present an update to
the formulation of AO that prevents invalid states from being
considered during a search. An evaluation shows that AO gives
a clear advantage to DFBnB and allows greater scalability for
parallel search algorithms. The update to AO’s formulation has
no significant impact on performance either way.

",14.39478758073233,11.941721611721611,184
HiPC_17_025.txt,14.318448969606603,12.220117685553799,HiPC,6507,"
Sparse graph processing generates highly irregular
Memory Access Patterns (MAP) which lack locality and result in
poor cache performance. In this paper, we propose a novel graph
ordering algorithm that addresses this problem. We observe that
existing reordering algorithms primarily try to improve cache
line utilization by enhancing spatial locality. They are oblivious
to cache data reuse which reflects the temporal locality that
MAP can possess. Our premise is that peak efficiency can be
achieved by a graph order for which the resulting MAP exhibit
both spatial and temporal locality. Therefore, we first introduce
a new metric Profit, that quantifies cache data reuse leading to a
heuristic pH that enhances temporal locality in the MAP of graph
algorithms. Then we define a notion of dynamically matching
MAP with cache contents in a way that jointly maximizes both
cache data reuse and cache line utilization. To perform this joint
optimization, we develop a Block Reordering algorithm which
utilizes pH to rearrange blocks of consecutive nodes with high
spatial locality. We evaluate our algorithm using 8 real world
datasets and 4 representative graph algorithms. Experimental
results show that graphs obtained by Block Reordering can
achieve upto 2.3x speedup over the original graph order and
consistently outperform the existing state of the art reordering
technique by 20% to 25% reduction in cache misses.

",16.52667757954773,15.249090909090906,222
HiPC_17_026.txt,15.2148929723636,13.181359518222738,HiPC,5939,"
Increasing scale of data centers and the density of
server nodes pose significant challenges in producing power and
energy efficient cooling infrastructures. Current fan based air
cooling systems have significant inefficiencies in their operation
causing oscillations in fan power consumption and temperature
variations among cores. In this paper, we identify the cause
these problems and propose proactive cooling mechanisms to
mitigate the power peaks and temperature variations.

An accurate temperature prediction model lies behind the
basis of our solutions. We use a neural network-based modeling approach for predicting core temperatures of different
workloads, under different core frequencies, fan speed levels,
and ambient temperature. The model provides guidance for
our proactive cooling mechanisms. We propose a preemptive
and decoupled fan control mechanism that can remove the
power peaks in fan power consumption and reduce the maximum cooling power by 53.3% on average as well as energy
consumption by 22.4%. Moreover, through our decoupled fan
control method and thermal-aware load balancing algorithm,
we show that temperature variations in large scale platforms
can be reduced from 25° C to 2° C, making cooling systems
more efficient with negligible performance overhead.

",17.833180683606166,16.085647905759167,192
HiPC_17_027.txt,16.426587551242097,14.624432909490857,HiPC,7821,"
Compiler high-level automatic optimization and
parallelization techniques are well suited for some classes of
simulation or signal processing applications, however they usually
don’t take into account domain-specific knowledge nor the
possibility to change or to remove some computations to achieve
“good enough’’ results. Differently, production simulation and
signal processing codes have adaptive capabilities: they are
designed to compute precise results only where it matters if
the complete problem is not tractable or if computation time
must be short. In this paper, we present a new way to provide
adaptive capabilities to compute-intensive codes automatically.
It relies on domain-specific knowledge provided through special
pragmas by the programmer in the input code and on polyhedral
compilation techniques to continuously regenerate at runtime a
code that performs heavy computations only where it matters.
We present experimental results on several applications where
our strategy enables significant computation savings and speedup
while maintaining a good precision, with a minimal effort from
the programmer.

",20.075844112070616,20.646000000000004,164
HiPC_17_028.txt,14.41755642540237,12.389918183040244,HiPC,7110,"
GPUs have become an essential component for
building compute clusters with high compute density and high
performance per watt. As such clusters scale to have 1000s
of GPUs, efficiently moving data between the GPUs becomes
imperative to get maximum performance. NVSHMEM is an
implementation of the OpenSHMEM standard for NVIDIA GPU
clusters which allows communication to be issued from inside
GPU kernels. In earlier work, we have shown how NVSHMEM
can be used to achieve better application performance on GPUs
connected through PCIe or NVLink. As part of this effort,
we implement IB verbs for Mellanox InfiniBand adapters in
CUDA. We evaluate different design alternatives, taking into
consideration the relaxed memory model, automatic memory
access coalescing and thread hierarchy on the GPU. We also
consider correctness issues that arise in these designs. We
take advantage of these designs transparently or through API
extensions in NVSHMEM. With micro-benchmarks, we show
that a Nvidia Pascal P100 GPU is able saturate the network
bandwidth using only one or two of its 56 available streaming
multiprocessors (SM). On a single GPU using a single IB EDR
adapter, we achieve a throughput of around 90 million messages
per second. In addition, we implement a 2dstencil application
kernel using NVSHMEM and compare its performance with a
CUDA-aware MPI-based implementation that uses GPUDirect
RDMA. Speedups in the range of 23% to 42% are seen for
input sizes large enough to fill the occupancy of Nvidia Pascal
P100 GPUs on 2 to 4 nodes indicating that there are gains to be
had by eliminating the CPU from the communication path when
all computation runs on the GPU.

",15.429909175157395,14.269812834224599,273
HiPC_17_029.txt,17.95044829541893,16.095451970124426,HiPC,6063,"
To expose hidden parallelism from programs with
complex dependences, modern compilers employ memory profilers to augment imprecise static analyses. Since dynamic dependence patterns among instructions can vary widely depending
on the context, such as function call site stack and loop nest
level, context-aware memory profiling is of great value for precise
memory profiling. However, recording memory dependences with
full context information causes huge overheads in terms of CPU
cycles and memory space. Existing profilers mitigate this problem
by compromising precision, coverage, or both. This paper proposes a new precise Context-Aware Memory Profiling (CAMP)
framework that efficiently traces all the memory dependences
with full context information. CAMP statically analyzes a context
tree of a program that illustrates all the possible dynamic
contexts, and simplifies context management during profiling. For
14 programs from SPEC CINT2000 and CINT2006 benchmark
suites, CAMP increases speculative parallelism opportunities by
12.6% on average and by up to 63.0% compared to the baseline
context-oblivious, loop-aware memory profiler.

",18.699421769314853,17.09142857142857,166
HiPC_17_030.txt,14.554592549557764,13.286052078294201,HiPC,7306,"
ARM processors have dominated the mobile device
market in the last decade due to their favorable computing
to energy ratio. In this age of Cloud data centers and Big
Data analytics, the focus is increasingly on power efficient
processing, rather than just high throughput computing. ARM’s
first commodity server-grade processor is the recent AMD A1100series processor, based on a 64-bit ARM Cortex A57 architecture.
In this paper, we study the performance and energy efficiency of a
server based on this ARM64 CPU, relative to a comparable server
running an AMD Opteron 3300-series x64 CPU, for Big Data
workloads. Specifically, we study these for Intel’s HiBench suite of
web, query and machine learning benchmarks on Apache Hadoop
v2.7 in a pseudo-distributed setup, for data sizes up to 20GB
files, 5M web pages and 500M tuples. Our results show that the
ARM64 server’s runtime performance is comparable to the x64
server for integer-based workloads like Sort and Hive queries,
and only lags behind for floating-point intensive benchmarks like
PageRank, when they do not exploit data parallelism adequately.
We also see that the ARM64 server takes are the energy, and has
an Energy Delay Product (EDP) that is 50 — 71% lower than the
x64 server. These results hold promise for ARM64 data centers
hosting Big Data workloads to reduce their operational costs,
while opening up opportunities for further analysis.

",16.373557378465907,15.963103448275863,237
HiPC_17_031.txt,15.865081690353566,14.339040803388027,HiPC,5664,"
Nowadays best performing automatic parallelizers
and data locality optimizers for static control programs rely on
the polyhedral model. Polyhedral compilation consists of three
phases: (1) abstracting the input code into a mathematical
view; (2) analyzing and transforming this representation into
an optimized alternative; (3) generating the corresponding code
while ensuring it is semantically equivalent to the input code.
During this last phase, state-of-the-art polyhedral compilers
generate only one type of parallelism when targeting multicore
shared memory architectures: parallel loops via the OpenMP
omp parallel for directive.

In this work, we propose to explore how a polyhedral
compiler could exploit parallel region constructs. Instead of
initializing a new set of threads each time the code enters
a parallel loop and synchronizing them when exiting it, the
threads are initialized once for all at the entrance of the region
of interest, and synchronized only when it is necessary.

Technically, we propose to embed the whole region containing parallel loops in an omp parallel construct. Inside the
parallel region, the single construct is used when some code
needs to be executed sequentially; the for construct is used to
distribute loop iterations between threads. Thanks to the power
of the polyhedral dependence analysis, we compute when it is
valid to add the optional nowait clause, to omit the implicit
barrier at the end of a worksharing construct and thus to
reduce even more control overhead.

Through a set of experiments on the PolyBench benchmarks,
we show that resulting codes can overwhelm the performance
obtained by the Pluto polyhedral compiler.

",18.71604785175511,17.4977519379845,259
HiPC_17_032.txt,14.897188522706688,12.580487976985292,HiPC,7804,"
Thread schedulers are designed to dynamically
map parallel programs to processors to optimize performance

 

function Quicksort (A)
if (#A < 2) then A

aes : . : lse let pivot = A[#A/2];
metrics including memory footprint, number of cache misses at ee “8 is = {6 tA B| 6 < pLvEE);
each cache level, and load balance, so as to minimize the total eq = {e in Al e == pivot};
running time of the program. Programs with dynamic memory gr = ie an] e f panokd i , n
- . ret = {Quicksort(v):v in s,gr]};
allocation pose particular challenges for thread schedulers, and in ret[0] ++ eq ++ ret[1];

indeed prior schedulers that are provably cache- and timeefficient on multi-level cache hierarchies require static memory
allocation. Not only do many thread schedulers fail to reuse
memory effectively, but there is often an inherent tradeoff
between parallelism and memory use in algorithms.

In this paper, we present the first runtime thread scheduler
for multi-level cache hierarchies, called the space-bounded
recursive-PDF scheduler, that is provably space-, cache-, and
time-efficient for parallel programs that dynamically allocate
memory. Our bounds hold for nested parallel programs with
good regularity as measured by the effective cache complexity—
a program-centric metric. The cache and time bounds are
asymptotically optimal, while the space bound is asymptotically
optimal for highly parallel and regular programs.

",19.78447435784618,19.910252100840342,225
HiPC_17_033.txt,15.223359238357268,13.79564004719705,HiPC,6401,"
In this paper, we combine high-performance computing science with computational neuroscience methods to
show how to speed-up cutting-edge methods for mapping and
evaluation of the large-scale network of brain connections. More
specifically, we use a recent factorization method of the Linear
Fascicle Evaluation model (i.e., LiFE [1], [2]) that allows for
statistical evaluation of brain connectomes. The method called
ENCODE [3], [4] uses a Sparse Tucker Decomposition approach
to represent the LiFE model. We show that we can implement
the optimization step of the ENCODE method using MPI and
OpenMP programming paradigms. Our approach involves the
parallelization of the multiplication step of the ENCODE method.
We model our design theoretically and demonstrate empirically
that the design can be used to identify optimal configurations for
the LiFE model optimization via ENCODE method on different
hardware platforms. In addition, we co-design the MPI runtime
with the LiFE model to achieve profound speed-ups. Extensive
evaluation of our designs on multiple clusters corroborates our
theoretical model. We show that on a single node on TACC
Stampede2, we can achieve speed-ups of up to 8.7x as compared
to the original approach.

",15.021129683784007,14.844835924006908,196
HiPC_17_034.txt,15.497303212757878,13.208897066084258,HiPC,6356,"
Hybrid multicore processors (HMPs) are poised to
dominate the landscape of the next generation of computing on
the desktop as well as on exascale systems. HMPs consist of
general purpose CPU cores along with specialized co-processors
and can provide high performance for a wide spectrum of
applications at significantly lower energy requirements per FLOP.

In this paper, we develop parallel algorithms and software
for constructing multi-resolution SAR images on HMPs. We
develop several load balancing algorithms for optimizing time
performance and energy on HMPs. We also present a systematic
approach for deriving the energy-time performance trade-offs on
HMPs in the presence of Dynamic Voltage Frequency Scaling.
Pareto-optimal curves are presented on a system consisting of 24
traditional cores and a GPU.

",17.693802365651003,14.701587301587306,127
HiPC_17_035.txt,16.44036625901092,14.307586186707656,HiPC,8482,"
 Several applications from computational linguistic
and genomics perform similarity analysis between sequences of
characters based on Hamming and Levenshtein distance
measures. Hamming and Levenshtein distance-based matching
maps well onto non-deterministic finite automata (NFAs), which
have been accelerated on GPUs. However, designed with the
flexibility to support generic topologies, existing NFA engines
have inefficiencies when processing fixed-topology NFAs.

In this work we target this problem and propose two methods
to improve the preprocessing and traversal performance of
Levenshtein and Hamming distance NFAs. Our methods are
based on the following observation: for these fixed-topologies, the
transitions do not need to be stored in device memory, but they
can be inferred from the reference string (i.e., the string to be
matched against) and the NFA topology alone. Our first, basic
implementation (implicit-active-sq) minimizes preprocessing by
bypassing NFA construction and packing, but exhibits several
traversal inefficiencies. Our optimized method  (implicitrearranged-sv) includes space and time optimizations for global
memory and radically different shared memory access patterns
within the kernel, while at the same time incurring only modest
preprocessing overhead over the basic implementation. Our
experimental evaluation shows that, on large NFAs consisting of
several millions states, implicit-rearranged-sv outperforms
traditional GPU engines both in terms of traversal throughput
(3-22x speedup) and preprocessing time (856-12,237x speedup).

",20.40282108145781,20.213506493506497,223
HiPC_17_036.txt,14.682126097088155,12.90008607234778,HiPC,5265,"
Factorizing large matrices by QR with column
pivoting (QRCP) is substantially more expensive than QR
without pivoting, owing to communication costs required for
pivoting decisions. In contrast, randomized QRCP (RQRCP)
algorithms have proven themselves empirically to be highly

xGEQPF and PxGEQPF respectively. They are considerably
faster, while maintaining same numerical behavior.

Given a target rank 1 < k < min (m, n) in partial QRCP,
equation (1) can be written in a 2 x 2 block form as

competitive with high-performance implementations of QR in R R R R
processing time, on uniprocessor and shared memory machines, AT=Q ( 6. z,. ) _ ( Qi Qe ) ( o z, ) ;
and as reliable as QRCP in pivot quality. 22 22

We show that RQRCP algorithms can be as reliable as
QRCP with failure probabilities exponentially decaying in
oversampling size. We also analyze efficiency differences among
different RQRCP algorithms. More importantly, we develop
distributed memory implementations of RQRCP that are significantly better than QRCP implementations in ScaLAPACK.

As a further development, we introduce the concept of
and develop algorithms for computing spectrum-revealing QR
factorizations for low-rank matrix approximations, and demonstrate their effectiveness against leading low-rank approximation methods in both theoretical and numerical reliability and
efficiency.

",18.001758247042904,16.50294117647059,203
HiPC_17_037.txt,15.093674328559974,13.099909137835969,HiPC,6587,"
Neighborhood collectives were added to the Message
Passing Interface (MPI) to better support sparse communication
patterns found in many applications. These new collectives
encourage more scalable programming styles, and greatly extend
the scope of MPI collectives by allowing users to define their own
collective communication patterns.

In this paper, we describe a new, distributed algorithm for
computing improved communication schedules for neighborhood
collectives. We show how to discover common process neighborhoods in fully general MPI distributed graph topologies,
and how to exploit this information to build message-combining
communication schedules for the MPI neighborhood collectives.
Our experimental results show considerable performance improvements for application communication topologies of various
shapes and sizes. On average, the performance gain is around
50%, but it can also be as much as 71% for topologies with
larger numbers of neighbors.

",17.879347455551382,16.610185185185184,136
HiPC_17_038.txt,13.624668674531257,11.276640609963142,HiPC,6586,"
We present PKT, a new shared-memory parallel
algorithm and OpenMP implementation for the truss decomposition of large sparse graphs. A k-truss is a dense subgraph
definition that can be considered a relaxation of a clique.
Truss decomposition refers to a partitioning of all the edges
in the graph based on their k-truss membership. The truss
decomposition of a graph has many applications. We show
that our new approach PKT consistently outperforms other
truss decomposition approaches for a collection of large sparse
graphs and on a 24-core shared-memory server. PKT is based
on a recently proposed algorithm for k-core decomposition.

",14.06817628641468,11.351190476190478,106
HiPC_17_039.txt,16.717305183235013,14.815454545454546,HiPC,6340,"
Approximate computing enables processing of
large-scale graphs by trading off quality for performance.
Approximate computing techniques have become critical not
only due to the emergence of parallel architectures but also due
to the availability of large scale datasets enabling data-driven
discovery. Using two prototypical graph algorithms, PageRank
and community detection, we present several approximate
computing heuristics to scale the performance with minimal
loss of accuracy. We present several heuristics including loop
perforation, data caching, incomplete graph coloring and
synchronization, and evaluate their efficiency. We demonstrate
performance improvements of up to 83% for PageRank and
up to 450x for community detection, with low impact on
accuracy for both the algorithms. We expect the proposed
approximate techniques will enable scalable graph analytics
on data of importance to several applications in science and
their subsequent adoption to scale similar graph algorithms.

",20.581828153500815,18.458571428571428,141
HiPC_17_040.txt,14.65684702500895,13.291762883235489,HiPC,6127,"
Manycore processors such as Intel Knights Landing (KNL), the second generation Xeon Phi manycore processor
from Intel comes equipped with up to 288 threads and 16
gigabytes of high-bandwidth on-chip multi-channel DRAM
(MCDRAM) that bear the potential to significantly improve
the performance of both compute-bound and memory-bound
applications. For this potential to be realized, it is imperative
to exploit KNL’s highly threaded environment and careful use
of the limited MCDRAM resource. In this work, we focus on
achieving effective utilization of KNL’s resources through the
design of a kernel-based communication engine that makes use
of multiple kernel threads and generic work request abstraction
scheme to accelerate MPI data movement operations. Being
a kernel-based approach, our designs are application-pattern
agnostic and aim to have minimal contention with the application’s compute and memory requirements. We have compared
our proposed designs with other prevalent schemes employed
by modern MPI libraries. The experimental evaluation shows
that the proposed designs provide up to 2.5X improvement
at the microbenchmark-level and improve the total execution
time of the MPI+OpenMP version of HPCG by up to 15%
when compared with other approaches. Furthermore, using the
CNTK Deep Learning framework, we demonstrate a significant
improvement over existing approaches in the total training time
(execution time) with the Multi-level perceptron (MLP) model
and MNIST image recognition dataset.

",18.548980349730343,18.44979413599501,233
HiPC_17_041.txt,15.326429302461094,13.758811010481146,HiPC,4653,"
Hierarchical data representations have been
shown to be effective tools for coping with large-scale scientific
data. Writing hierarchical data on supercomputers, however,
is challenging as it often involves all-to-one communication
during aggregation of low-resolution data which tends to span
the entire network domain, resulting in several bottlenecks. We
introduce the concept of indexing templates, which succinctly
describe data organization and can be used to alter movement of data in beneficial ways. We present two techniques,
domain partitioning and localized aggregation, that leverage
indexing templates to alleviate congestion and synchronization
overheads during data aggregation. We report experimental
results that show significant I/O speedup using our proposed
schemes on two of today’s fastest supercomputers, Mira and
Shaheen II, using the Uintah and S3D simulation frameworks.

",18.026119701940384,18.2705625,130
HotCloud_17_001.txt,16.56579467762251,14.689320598069052,HotCloud,3655,"

Over the past decade, we have witnessed exponential growth in the density (petabyte-level) and breadth
(across geo-distributed datacenters) of data distribution.
It becomes increasingly challenging but imperative to
minimize the response times of data analytic queries over
multiple geo-distributed datacenters. However, existing
scheduling-based solutions have largely been motivated
by pre-established mantras (e.g., bandwidth scarcity).
Without data-driven insights into performance bottlenecks at runtime, schedulers might blindly assign tasks
to workers that are suffering from unidentified bottlenecks.

In this paper, we present Lube, a system framework
that minimizes query response times by detecting and
mitigating bottlenecks at runtime. Lube monitors geodistributed data analytic queries in real-time, detects potential bottlenecks, and mitigates them with a bottleneckaware scheduling policy. Our preliminary experiments
on a real-world prototype across Amazon EC2 regions
have shown that Lube can detect bottlenecks with over
90% accuracy, and reduce the median query response
time by up to 33% compared to Spark’s built-in localitybased scheduler.

",18.548980349730343,17.742962313759865,166
HotCloud_17_002.txt,14.524165085284608,13.09709937351295,HotCloud,3721,"

We propose Bohr, a similarity aware geo-distributed data
analytics system that minimizes query completion time.
The key idea is to exploit similarity between data in different data centers (DCs), and transfer similar data from
the bottleneck DC to other sites with more WAN bandwidth. Though these sites have more input data to process, these data are more similar and can be more efficiently aggregated by the combiner to reduce the intermediate data that needs to be shuffled across the WAN.
Thus our similarity aware approach reduces the shuffle
time and in turn the query completion time (QCT).

We design and implement Bohr based on OLAP data
cubes to perform efficient similarity checking among
datasets in different sites. Evaluation across ten sites of
AWS EC2 shows that Bohr decreases the QCT by 30%
compared to state-of-the-art solutions.

",15.021129683784007,14.244285714285713,141
HotCloud_17_003.txt,17.3858877647253,15.677142857142861,HotCloud,2648,"

Applications have commonly been oblivious to their
cloud runtimes. This is primarily because they started
their journey in IaaS clouds, running on a guestOS inside
VMs. Then to increase performance, many guestOSes
have been paravirtualized making them virtualization
aware, so that they can bypass some of the virtualization layers, as in virtio. This approach still kept applications unmodified. Recently, we are witnessing a rapid
adoption of containers due to their packaging benefits,
high density, fast start-up and low overhead. Applications are increasingly being on-boarded to PaaS clouds
in the form of application containers or appc, where
they are run directly on a cloud substrate like Kubernetes or Docker Swarm. This shift in deployment practices present an opportunity to make applications aware
of their cloud. In this paper, we present Paracloud framework for application containers and discuss the Paracloud interface (PaCI) for three cloud operations namely
migration, auto-scaling and load-balancing.

",16.439396014739867,14.451932773109245,154
HotCloud_17_004.txt,14.965219502524835,12.881556680102388,HotCloud,3283,"

Large scale in-memory key-value stores like RAMCloud
can perform millions of operations per second per server
with a few microseconds of access latency. However,
these systems often only provide simple feature sets,
and the lack of extensibility is an obstacle for building
higher-level services. We evaluate the possibility of using
JavaScript for shipping computation to data and for extending database functionality by comparing against other
possible approaches. Microbenchmarks are promising;
the V8 JavaScript runtime provides near native performance with reduced isolation costs when compared with
native code and hardware-based protections. We conclude
with initial thoughts on how this technology can be deployed for fast procedures that operate on in-memory data,
that maximize gains from JIT, and that exploit the kernelbypass DMA capabilities of modern network cards.

",17.80541091248751,16.876854961832063,132
HotCloud_17_005.txt,15.18147889625212,13.447010874436113,HotCloud,3934,"

Web services and applications are complex systems.
Layers of abstraction and virtualization allow flexible
and scalable deployment. But they also introduce complications if one wants predictable performance and easy
trouble-shooting. We propose to support the designers,
testers, and maintainers of such systems by annotating
system components with performance models. Our goal
is to formulate annotations that can be used as oracles
in performance testing, that can provide valuable guidance for debugging, and that can also inform designers
by predicting the performance profile of an assembly of
annotated components. We present an initial formulation
of such annotations together with their concrete derivation from the execution of a complex web service.

",16.728156217252725,14.623636363636361,111
HotCloud_17_006.txt,15.222071271660873,13.588109908285592,HotCloud,3305,"
The performance of mobile phone processors has been
steadily increasing, causing the performance gap between server and mobile processors to narrow with mobile processors sporting superior performance per unit
energy. Fueled by the slowing of Moore’s Law, the overall performance of single-chip mobile and server processors have likewise plateaued. These trends and the glut
of used and partially broken smartphones which become
environmental e-waste motivate creating cloud servers
out of decommissioned mobile phones. This work proposes creating a compute dense server built out of used
and partially broken smartphones (e.g. screen can be broken). This work evaluates the total cost of ownership
(TCO) benefit of using servers based on decommissioned
mobile devices and analyzes some of the architectural
design trade-offs in creating such servers.
",17.353723509956247,15.500251968503935,130
HotCloud_17_007.txt,16.39362981844241,14.689480812102865,HotCloud,3639,"
Heterogeneity is a growing concern for scheduling on
the cloud. Hardware is increasingly heterogeneous (e.g.,
GPUs, FPGAs, diverse I/O devices), emphasizing the
need to build schedulers that identify the internal structure of applications and utilize available hardware resources to their full potential. In this paper we present
our initial efforts to build a scheduler that tackles heterogeneity (in hardware and in software) as a primary
concern. Our scheduler, HCl (Heterogeneous Cluster),
models applications as annotated directed acyclic graphs
(DAGs), where each node represents a task. It maps tasks
onto hardware nodes, also organized in DAGs. Initial results using application models based on TPC-DS queries
running on Apache Spark show that HCl can improve
significantly upon approaches that do not consider heterogeneity and generate schedules that approach the critical path in length.
",16.92669308720184,15.945225563909776,135
HotCloud_17_008.txt,16.882532363461703,15.993414397426658,HotCloud,3796,"

Emerging cloud markets like spot markets and batch
computing services scale up services at the granularity
of whole VMs. In this paper, we observe that GPU workloads underutilize GPU device memory, leading us to
explore the benefits of reallocating heterogeneous GPUs
within existing VMs. We outline approaches for upgrading and downgrading GPUs for OpenCL GPGPU workloads, and show how to minimize the chance of cloud operator VM termination by maximizing the heterogeneous
environments in which applications can run.

",29.308192230251223,37.068461538461534,79
HotCloud_17_009.txt,16.775464007167393,15.248246909380484,HotCloud,4280,"

Emerging Cyber-Physical Systems (CPS) such as connected vehicles and smart cities span large geographical
areas. These systems are increasingly distributed and interconnected. Hence, a hierarchy of cloudlet and cloud
deployments will be key to enable scaling, while simultaneously hosting the intelligence behind these systems.
Given that CPS applications are often safety-critical, existing techniques focus on reducing latency to provide
real-time performance. While low latency is useful, a
shared and precise notion of time is key to enabling coordinated action in distributed CPS. In this position paper,
we argue for a global Quality of Time (QoT)-based architecture, centered around a shared virtualized notion
of time, based on the timeline abstraction [1]. Our architecture allows applications to specify their QoT requirements, while exposing timing uncertainty to the application. The timeline abstraction with the associated
knowledge of QoT enables scalable geo-distributed coordination in CPS, while providing avenues for fault tolerance and graceful degradation in the face of adversity.

",17.553077303434723,16.105000000000004,161
HotCloud_17_010.txt,16.169344447983246,15.598470596594584,HotCloud,4058,"

Infrastructure-as-a-Service clouds are rapidly evolving
into market-like environments that offer a wide range of
server contracts. Amazon EC2’s spot market is the clearest example of this trend: it operates over 5000 markets
globally where users can rent servers for a variable price.
To exploit spot instances, while mitigating the risk of
price spikes and revocations, many researchers and startups have developed techniques for modeling and predicting prices to optimize spot server selection. However,
prior approaches focus largely on predicting individual
server prices, which is akin to predicting the price of a
single stock. We argue that researchers should instead
focus on “index-based” modeling and prediction that aggregates prices from many markets in each region and
availability zone. We show that, for applications flexible enough to select and “trade” servers globally, making
decisions based on broader indices lowers costs and improves availability compared to index-agnostic policies.

",17.122413403193683,16.02684210526316,153
HotCloud_17_011.txt,15.818870667953899,14.913627463994164,HotCloud,3164,"

Dynamic and elastic resource allocation to Virtual Network Functions (VNFs) in accordance with varying
workloads is a must for realizing promised reductions
in capital and operational expenses in Network Functions Virtualization (NFV). However, workload heterogeneity and complex relationships between resources allocated to a VNF and the resulting capacity makes elastic resource flexing a challenging task. We propose an
NFV resource flexing system, ENVI, that uses a combination of VNF-level features and infrastructure-level
features to construct a machine-learning-based decision
engine for detecting resource flexing events. ENVI also
extracts the dependence relationship among VNFs in deployed Service Function Chains (SFCs) to carefully plan
the sequence of resource flexing steps upon scaling detection. We present preliminary results for the accuracy
of ENVI’s resource flexing decision engine with two different VNFs, namely, the caching proxy Squid and the
intrusion detection system Suricata. Our preliminary results show that using a combination of features to train a
neural network model is a promising approach for scaling detection.

",17.879347455551382,18.591969696969702,167
HotCloud_17_012.txt,15.004783243502246,14.020827788246379,HotCloud,2424,"

Latency-sensitive applications like virtualized telecom
and industrial IoT systems require a service for ultrafast state externalization to become cloud-native. In this
paper we propose a distributed shared memory system,
called DAL, which achieves the lowest possible latency
by transparently co-locating individual data items with
applications working on them. Upon changes in data access patterns, the system automatically adapts data locations to keep the number of remote operations at a minimum. By avoiding the costs of network transport and
using shared memory communication, the system can
achieve 1 ps data access latency. We envision DAL as a
platform component which enables latency-sensitive applications to take advantage of the cloud.

",17.58133193835471,16.640642857142854,113
HotCloud_17_013.txt,16.440385443919062,14.968240566366742,HotCloud,3904,"

Verification is often regarded as a one-time procedure undertaken after a protocol is specified but before it is implemented. However, in practice, protocols continually
evolve with the addition of new capabilities and performance optimizations. Existing verification tools are illsuited to “tracking” protocol evolution and programmers
are too busy (or too lazy?) to simultaneously co-evolve
specifications manually. This means that the correctness
guarantees determined at verification time can erode as
protocols evolve. Existing software quality techniques
such as regression testing and root cause analysis, which
naturally support system evolution, are poorly suited to
reasoning about fault tolerance properties of a distributed
system because these properties require a search of the
execution schedule rather than merely replaying inputs.
This paper advocates that our community should explore
the intersection of testing and verification to better ensure
quality for distributed software and presents our experience evolving a data replication protocol at Elastic using
a novel bug-finding technology called Lineage Driven
Fault Injection (LDFID) as evidence.

",19.430816780756558,18.802020905923346,165
HotCloud_17_014.txt,13.443421746398805,11.597609814945656,HotCloud,3907,"

Consensus protocols are used to provide consistency
guarantees over replicated data in a distributed system,
and allow a set of replicas to work together as a coherent group. Raft is a consensus protocol that is designed
to be easy to understand and implement. It is equivalent to Multi-Paxos in fault-tolerance and performance.
It uses a leader based approach for coordinating replication to a majority. The leader regularly informs the
followers of its existence using heartbeats. All reads
and writes go through the leader to ensure strong consistency. However, read-heavy workloads increase load
on the leader since the followers in Raft are maintained
as cold standbys. Since the algorithm itself guarantees
replication to at least a majority, why not exploit this fact
to serve strongly consistent reads without a leader? We
propose mechanisms to use quorum reads in Raft to offload the leader and better utilize the cluster. We integrate our approach in CockroachDB, an open-source distributed SQL database which uses Raft and leader leases,
to compare our proposed changes. The evaluation results with the YCSB benchmark illustrate that quorum
reads result in an increased throughput of the system under read-heavy workloads, as well as lower read/write
latencies.

",13.88584557914918,12.242740707568291,204
HotCloud_17_015.txt,15.731167134044135,14.132019967079373,HotCloud,3533,"

Using multiple datacenters allows for higher availability,
load balancing and reduced latency to customers of cloud
services. To distribute multiple copies of data, cloud
providers depend on inter-datacenter WANs that ought to
be used efficiently considering their limited capacity and
the ever-increasing data demands. In this paper, we focus
on applications that transfer objects from one datacenter to several datacenters over dedicated inter-datacenter
networks. We present DCCast, a centralized Point to
Multi-Point (P2MP) algorithm that uses forwarding trees
to efficiently deliver an object from a source datacenter
to required destination datacenters. With low computational overhead, DCCast selects forwarding trees that
minimize bandwidth usage and balance load across all
links. With simulation experiments on Google’s GScale
network, we show that DCCast can reduce total bandwidth usage and tail Transfer Completion Times (TCT)
by up to 50% compared to delivering the same objects
via independent point-to-point (P2P) transfers.

",17.879347455551382,17.191315789473688,154
HotCloud_17_016.txt,17.618919080996143,16.927959976002743,HotCloud,2913,"

The web serving protocol stack is constantly changing
and evolving to tackle technological shifts in networking infrastructure and website complexity. For example,
Cubic to tackle high throughput, SPDY to tackle loss
and QUIC to tackle security issues and lower connection
setup time. Accordingly, there are a plethora of protocols
and configuration parameters that enable the web serving protocol stack to address a variety of realistic conditions. Yet, despite the diversity in end-user networks
and devices, today, most content providers have adopted
a “one-size-fits-all” approach to configuring user facing
web stacks (CDN servers).

In this paper, we illustrate the drawbacks through empirical evidence that this “one-size-fits-all” approach results in sub-optimal performance and argue for a novel
framework that extends existing CDN architectures to
provide programmatic control over the configuration options of the CDN serving stack.

",18.026119701940384,17.424028169014083,143
HotCloud_17_017.txt,14.7947665845868,13.776551922233814,HotCloud,3898,"

Many popular Web services use CDNs to host their
content closer to users and thus improve page load times.
While this model’s success is beyond question, it has its
limits: for users with poor last-mile latency even to a
nearby CDN node, the many RTTs needed to fetch a Web
page add up to large delays. Thus, in this work, we explore a complementary model of speeding up Web page
delivery — a content gathering network (CGN), whereby
users establish their own geo-distributed presence, and
use these points of presence to proxy content for them.
We show that deploying only 14 public cloud-based CGN
nodes puts the closest node within a median RTT of
merely 4.8 ms (7.2 ms) from servers hosting the top 10k
(100k) most popular Web sites. The CGN node nearest
to a server can thus obtain content from it rapidly, and
then transmit it to the client over fewer (limited by available bandwidth) high-latency interactions using aggressive transport protocols. This simple approach reduces
the median page load time across 100 popular Web sites
by as much as 53%, and can be deployed immediately
without depending on any changes to Web servers at an
estimated cost of under $1 per month per user.

",14.554592549557764,16.153333333333332,213
HotCloud_17_018.txt,15.572458643603156,14.246068702290078,HotCloud,3929,"

We present FRAPpuccino (or FRAP), a provenancebased fault detection mechanism for Platform as a Service (PaaS) users, who run many instances of an application on a large cluster of machines. FRAP models,
records, and analyzes the behavior of an application and
its impact on the system as a directed acyclic provenance
graph. It assumes that most instances behave normally
and uses their behavior to construct a model of legitimate
behavior. Given a model of legitimate behavior, FRAP
uses a dynamic sliding window algorithm to compare a
new instance’s execution to that of the model. Any instance that does not conform to the model is identified
as an anomaly. We present the FRAP prototype and experimental results showing that it can accurately detect
application anomalies.

",15.903189008614273,14.737741935483871,126
HotCloud_17_019.txt,14.647559847345129,13.015330237358103,HotCloud,3183,"

One way to establish trust in a service is to know what
code it is running. However, verified code identity is
currently not possible for programs launched on a cloud
by another party. We propose an approach to integrate
support for code attestation—authenticated statements of
code identity—into layered cloud platforms and services.

To illustrate, this paper describes TapCon, an attesting container manager that provides source-based attestation and network-based authentication for containers
on a trusted cloud platform incorporating new features
for code attestation. TapCon allows a third party to verify
that an attested container is running specific code bound
securely to an identified source repository. We also show
how to use attested code identity as a basis for access
control. This structure enables new use cases such as
joint data mining, in which two data owners agree on a
safe analytics program that protects the privacy of their
inputs, and then ensure that only the designated program
can access their data.

",15.719379583869454,14.484947735191636,165
HotI_17_001.txt,14.173969222310014,12.178271805273834,HotI,5817,"
Interconnection networks must meet the communication demands of current High-Performance Computing systems. In order to interconnect efficiently the end nodes of these
systems with a good performance-to-cost ratio, new network
topologies have been proposed in the last years which leverage
high-radix switches, such as Slim Fly. Adversarial-like traffic
patterns, however, may reduce severely the performance of Slim
Fly networks when using only minimal-path routing. In order to
mitigate the performance degradation in these scenarios, Slim
Fly networks should configure an oblivious or adaptive nonminimal routing. The non-minimal routing algorithms proposed
for Slim Fly usually rely on Valiant’s algorithm to select the paths,
at the cost of doubling the average path-length, as well as the
number of Virtual Channels (VCs) required to prevent deadlocks.
Moreover, Valiant may introduce additional inefficiencies when
applied to Slim Fly networks, such as the “turn-around problem”
that we analyze in this work. With the aim of overcoming these
drawbacks, we propose in this paper two variants of the Valiant’s
algorithm that improve the non-minimal path selection in Slim
Fly networks. They are designed to be combined with adaptive
routing algorithms that rely on Valiant to select non-minimal
paths, such as UGAL or PAR, which we have adapted to the Slim
Fly topology. Through the results from simulation experiments,
we show that our proposals improve the network performance
and/or reduce the number of required VCs to prevent deadlocks,
even in scenarios with adversarial-like traffic.
",16.99224021665606,16.058533333333333,253
HotI_17_002.txt,14.466702840147452,12.699391440595157,HotI,4677,"
The network plays a key role in High-Performance
Computing (HPC) system efficiency. Unfortunately, current
HPC routing solutions are not application-aware, and therefore
cannot deal with the sudden HPC traffic bursts and their
resulting congestion peaks.
To address this problem, we introduce Routing Keys, a
scalable routing paradigm for HPC networks that decouples
intra- and inter-application flow contention. Our Application
Routing Key (ARK) algorithm proactively allows each self-aware
application to route its flows according to a predetermined
routing key, i.e., its own intra-application contention-free routing. In addition, in our Network Routing Key (NRK) algorithm, a
centralized scheduler chooses between several routing keys for
the communication phases of each application, and therefore
reduces inter-application contention while maintaining intraapplication contention-free routing and avoiding scalability
issues. Using extensive evaluations, we show that both ARK
and NRK significantly improve the communication runtime by
up to 2.7x.

",18.243605946275583,17.53661073825504,152
HotI_17_003.txt,14.868190838771554,13.380259832905004,HotI,7121,"
The advent of non-volatile memory (NVM) technologies has added an interesting nuance to the node level memory
hierarchy. With modern 100 Gb/s networks, the NVM tier of storage can often be slower than the high performance network in the
system; thus, a new challenge arises in the datacenter. Whereas prior efforts have studied the impacts of multiple sources targeting
one node (i-e., incast) and have studied multiple flows causing congestion in inter-switch links, it is now possible for a single flow from a
single source to overwhelm the bandwidth of a key portion of the memory hierarchy. This can subsequently spread to the switches and
lead to congestion trees in a flow-controlled network or excessive packet drops without flow control. In this work we describe protocols
which avoid overwhelming the receiver in the case of a source/sink rate mismatch. We design our protocols on top of Portals 4, which
enables us to make use of network offload. Our protocol yields up to 4x higher throughput in a 5k node Dragonfly topology for a
permutation traffic pattern in which only 1% of all nodes have a memory write-bandwidth limitation of 1/8th of the network bandwidth.
",15.151101081350806,14.532602545968885,203
HotI_17_004.txt,13.928270881157037,12.283312428724368,HotI,6062,"
Throughput performance, an important metric for
interconnection networks, is often quantified by the aggregate
throughput for a set of representative traffic patterns. A number of models have been developed to estimate the aggregate
throughput for a given traffic pattern on an interconnection
network. Since all of the models predict the same property
of interconnection networks, ideally, they should give similar
performance results or at least follow the same performance
trend. In this work, we examine four commonly used interconnect
throughput models and identify the cases when all models show
similar trends, when different models yield different trends, and
when different models produce contradictory results. Our study
reveals important properties of the models and demonstrates
the subtle differences among them, which are important for
an interconnect designer/researcher to understand, in order to
properly select a throughput model in the process of interconnect
evaluation.

",18.669449996058646,18.008755244755246,144
HotI_17_005.txt,15.644052767433216,13.683758514671855,HotI,5454,"
 Meeting the exponential increase in the global
demand for bandwidth has become a major concern for
today’s data centers. The scalability of any data center is
defined by the maximum capacity and port count of the
switching devices it employs, limited by total pin bandwidth on
current electronic switch ASICs. Optical switches can provide
higher capacity and port counts, and hence, can be used to
transform data center scalability. We have recently
demonstrated a 1000-port star-coupler based wavelength
division multiplexed (WDM) and time division multiplexed
(TDM) optical switch architecture offering a bandwidth of 32
Tbit/s with the use of fast wavelength-tunable transmitters and
high-sensitivity coherent receivers. However, the major
challenge in deploying such an optical switch to replace
current electronic switches lies in designing and implementing
a scalable scheduler capable of operating on packet timescales.

In this paper, we present a pipelined and highly parallel
electronic scheduler that configures the high-radix (1000-port)
optical packet switch. The scheduler can process requests from
1000 nodes and allocate timeslots across 320 wavelength
channels and 4000 wavelength-tunable transceivers within a
time constraint of lps. Using the Opencell NanGate 45nm
standard cell library, we show that the complete 1000-port
parallel scheduler algorithm occupies a circuit area of
52.7mm/’, 4-8x smaller than that of a high-performance switch
ASIC, with a clock period of less than 8ns, enabling 138
scheduling iterations to be performed in 1s. The performance
of the scheduling algorithm is evaluated in comparison to
maximal matching from graph theory and conventional
software-based wavelength allocation heuristics. The parallel
hardware scheduler is shown to achieve similar matching
performance and network throughput while being orders of
magnitude faster.

",18.564090476187538,17.462547703180217,286
HotI_17_006.txt,16.44572470802785,14.54354686634544,HotI,5776,"
Maximizing bandwidth utilization of optical on
chip interconnects is essential to compensate for static power
overheads in optical networks-on-chip. Shared optical buses were
shown to be a power-efficient, modular design solution with
tremendous power saving potential by allowing optical bandwidth
to be shared by all connected nodes. Previous proposals resolve
bus contention by scheduling senders sequentially on the entire
optical bandwidth; however, logically splitting a bus into subchannels to allow both sequential and parallel data transmission
has been shown to be highly efficient in electrical interconnects
and could also be applied to shared optical buses.
In this paper, we propose an efficient subchannel scheduling
algorithm that aims to minimize the number of bus utilization
cycles by assigning sender-receiver pairs both to subchannels and
time slots. We present both a distributed and a centralized bus
arbitration scheme and show that both can be implemented with
low overheads. Our results show that subchannel scheduling can
more than double throughput on shared optical buses compared
to sequential scheduling without any power overheads in most
cases. Arbitration latency overheads compared to state-of-theart sequential schemes are moderate-to-low for significant bus
bandwidths and only noticeable for low injection rates.

",20.130776976110326,17.975735607675905,202
HotI_17_007.txt,13.660824863698782,12.139446331427084,HotI,5354,"
Because of their performance characteristics, highperformance fabrics like Infiniband or OmniPath are interesting
technologies for many local area network applications, including
data acquisition systems for high-energy physics experiments like
the ATLAS experiment at CERN. This paper analyzes existing
APIs for high-performance fabrics and evaluates their suitability
for data acquisition systems in terms of performance and domain
applicability.

The study finds that existing software APIs for highperformance interconnects are focused on applications in highperformance computing with specific workloads and are not
compatible with the requirements of data acquisition systems.
To evaluate the use of high-performance interconnects in data
acquisition systems, a custom library called NetIO has been
developed and is compared against existing technologies.

NetIO has a message queue-like interface which matches the
ATLAS use case better than traditional HPC APIs like MPI. The
architecture of NetIO is based on an interchangeable back-end
system which supports different interconnects. A libfabric-based
back-end supports a wide range of fabric technologies including
Infiniband. On the front-end side, NetIO supports several highlevel communication patterns that are found in typical data
acquisition applications like client/server and publish/subscribe.
Unlike other frameworks, NetIO distinguishes between highthroughput and low-latency communication, which is essential
for applications with heterogeneous traffic patterns. This feature
of NetIO allows experiments like ATLAS to use a single network
for different traffic types like physics data or detector control.

Benchmarks of NetIO in comparison with the message queue
implementation @MQ are presented. NetIO reaches up to 2x
higher throughput on Ethernet and up to 3x higher throughput
on FDR Infiniband compared to @MQ on Ethernet. The latencies
measured with NetIO are comparable to @MQ latencies.

",16.850672712058472,16.367857142857144,281
HotI_17_008.txt,15.007365755926706,13.33281001209264,HotI,5858,"
In High-Performance Computing (HPC) systems,
the design of the interconnection network is crucial. Indeed,
the network topology, the switch architecture and the routing
scheme determine the network performance and ultimately the
system one. As the number of endnodes in HPC systems grows,
and the supported applications become increasingly demanding
for communication, the use of techniques to deal with network
congestion and its negative effects gains importance. For that
purpose, routing schemes such as adaptive or oblivious try to
balance the network traffic in order to prevent and/or eliminate
congestion. On the other hand, there are deterministic routing
schemes that balance the number of paths per link with the aim
of reducing the head-of-line blocking derived from congestion
situations. Furthermore, other techniques to deal with congestion
are based on queuing schemes. This approach is based on storing
separately different packet flows at the ports buffers, so that the
head-of-line blocking and/or buffer-hogging are reduced. Existing
queuing schemes use different policies to separate flows, and they
can be implemented in different ways. However, most queuing
schemes are often used and designed assuming that the network
is configured with deterministic routing, while actually they could
be combined also with adaptive or oblivious routing.

This paper analyzes the behavior of different queuing schemes
under different routing algorithms: deterministic, adaptive or
oblivious. We focus on fat-tree networks, configured with the
most common routing algorithms of each type suitable for that
topology. In order to evaluate these configurations, we have
run simulation experiments modeling large fat-trees built from
switches with radices available in the market, and supporting
several queuing schemes. The experiments results show how
different the performance of the queuing schemes may be when
combined with either deterministic or oblivious/adaptive routing.
Indeed, from these results we can conclude that some combinations of queuing schemes and routings are counterproductive.

",15.994108938978627,14.427206551410375,315
HotI_17_009.txt,14.175741079098877,12.531035198226355,HotI,5788,"
Hybrid data center networks (HDCNs), where each
ToR switch is installed with a directional antenna, emerge as a
candidate helping alleviate the over-subscription problem in traditional data centers. Meanwhile, as virtualization techniques develop
rapidly, there is a trend that traditional network functions that
are implemented in hardware will also be virtualized into virtual
machines. However, how to place virtual network functions (VNFs)
into data centers to meet the customer requirements in a hybrid
data center network environment is a challenging problem. In this
paper, we study the VNF placement in hybrid data center networks,
and provide a joint VNF placement and antenna scheduling model.
We further simplify it to a mixed integer programming (MIP)
problem. Due to the hardness of a MIP problem, we develop
a heuristic algorithm to solve it. To the best of our knowledge,
this is the first work concerning VNF placement in the context of
HDCNs. Our extensive simulations demonstrate the effectiveness
of the proposed algorithm, which make it a very suitable and
promising solution for VNF placement in HDCN environment.

",15.414825405933506,14.109318181818185,177
HotI_17_010.txt,15.205068748325175,14.11242547868758,HotI,4306,"
High Performance Computing(HPC) applications
are highly optimized to maximize allocated resources for the
job such as compute resources, memory and storage. Optimal
performance for MPI applications requires the best possible
affinity across all the allocated resources. Typically, setting
process affinity to compute resources is well defined, ice MPI
processes on a compute node have processor affinity set for one to
one mapping between MPI processes and the physical processing
cores. Several well defined methods exist to efficiently map MPI
processes to a compute node. With the growing complexity of
HPC systems, platforms are designed with complex compute
and I/O subsystems. Capacity of I/O devices attached to a node
are expanded with PCle switches resulting in large numbers of
PCle endpoint devices. With a lot of heterogeneity in systems,
applications programmers are forced to think harder about
affinitizing processes as it affects performance based on not only
compute but also NUMA placement of IO devices. Mapping of
process to processor cores and the closest IO device(s) is not
straightforward. While operating systems do a reasonable job
of trying to keep a process physically located near the processor
core(s) and memory, they lack the application developer’s knowledge of process workflow and optimal IO resource allocation
when more than one IO device is connected to the compute node.

In this paper we look at ways to assuage the problems of
affinity choices by abstracting the device selection algorithm
from MPI application layer. MPI continues to be the dominant
programming model for HPC and hence our focus in this paper
is limited to providing a solution for MPI based applications. Our
solution can be extended to other HPC programming models such
as Partitioned Global Address Space(PGAS) or a hybrid MPI and
PGAS based applications. We propose a solution to solve NUMA
effects at the MPI runtime level independent of MPI applications.
Our experiments are conducted on a two node system where
each node consists of two socket Intel®) Xeon®) servers, attached
with up to four Intel®) Omni-Path fabric devices connected over
PCle. The performance benefits seen by MPI applications by
affinitizing MPI processes with best possible network device is
evident from the results where we notice up to 40% improvement
in uni-directional bandwidth, 48% bi-directional bandwidth, 32%
improvement in latency measurements and finally up to 40%
improvement in message rate.

",17.58133193835471,16.01578894472362,397
HotI_17_011.txt,14.877034888371409,13.297164574555882,HotI,6167,"
Deep Learning over Big Data (DLoBD) is becoming
one of the most important research paradigms to mine value
from the massive amount of gathered data. Many emerging deep
learning frameworks start running over Big Data stacks, such
as Hadoop and Spark. With the convergence of HPC, Big Data,
and Deep Learning, these DLoBD stacks are taking advantage
of RDMA and multi-/many-core based CPUs/GPUs. Even though
a lot of activities are happening in the field, there is a lack of
systematic studies on analyzing the impact of RDMA-capable
networks and CPU/GPU on DLoBD stacks. To fill this gap, we
propose a systematical characterization methodology and conduct extensive performance evaluations on three representative
DLoBD stacks (i.e., CaffeOnSpark, TensorFlowOnSpark, and
BigDL) to expose the interesting trends regarding performance,
scalability, accuracy, and resource utilization. Our observations
show that RDMA-based design for DLoBD stacks can achieve
up to 2.7x speedup compared to the IPoIB based scheme. The
RDMA scheme can also scale better and utilize resources more
efficiently than the IPoIB scheme over InfiniBand clusters. For
most cases, GPU-based deep learning can outperform CPU-based
designs, but not always. We see that for LeNet on MNIST, CPU
+ MKL can achieve better performance than GPU and GPU +
cuDNN on 16 nodes. Through our evaluation, we see that there
are large rooms to improve the designs of current generation
DLoBD stacks further.

",14.265292616868656,13.253094017094018,235
HotI_17_012.txt,14.516273293886414,13.03807969960156,HotI,6368,"
Scientific HPC applications along with the emerging
class of Big Data and Machine Learning workloads are rapidly
driving the fabric scale both on premises and in the cloud.
Achieving high aggregate fabric throughput is paramount to
the overall performance of the application. However, achieving
high fabric throughput at scale can be challenging - that is,
the application communication pattern will need to map well
on to the target fabric architecture, and the multi-layered host
software stack in the middle will need to orchestrate that mapping
optimally to unleash the full performance.

In this paper, we investigate low-level optimizations to the host
software stack with the goal of improving the aggregate fabric
throughput, and hence, application performance. We develop
and present a number of optimization and tuning techniques
that are key driving factors to the fabric performance at scale
- such as, Fine-grained interleaving, improved pipelining, and
careful resource utilization and management. We believe that
these low-level optimizations can be commonly leveraged by
several programming models and their runtime implementations
making these optimizations broadly applicable. Using a set of
well-known MPI-based scientific applications, we demonstrate
that these optimizations can significantly improve the overall
fabric throughput and the application performance. Interestingly,
we also observe that some of these optimizations are inter-related
and can additively contribute to the overall performance.

",18.377959752453624,17.609977477477475,223
HotI_17_013.txt,15.142008428280388,14.009790914331713,HotI,5740,"
RDMA (Remote Direct Memory Access) is a technology that enables user applications to perform direct data
transfer between the virtual memory of processes on remote
endpoints, without operating system involvement or intermediate
data copies. Achieving zero intermediate data copies using
RDMA requires specialized network interface hardware. Software RDMA drivers emulate RDMA semantics in software to
allow the use of RDMA without investing in such hardware,
although they cannot perform zero-copy transfers. Nonetheless,
software RDMA drivers are useful for research, application
development, testing, debugging, or as a less expensive desktop
client for a centralized RDMA server application running on
RDMA-capable hardware.

Existing software RDMA drivers perform data transfer in
the kernel. Data Plane Development Kit (DPDK) provides a
framework for mapping Ethernet interface cards into userspace
and performing bulk packet transfers. This in turn allows a
software RDMA driver to perform data transfer in userspace.
We present our software RDMA driver, urdma, that performs
data transfer in userspace, discuss its design and implementation,
and demonstrate that it can achieve lower small message latency
than existing kernel-based implementations while maintaining
high bandwidth utilization for large messages.

",16.218646115125612,16.747640374331557,188
HotOS_17_001.txt,16.17258305338472,15.439674807940133,HotOS,4296,"
",NA,NA,1
HotOS_17_002.txt,14.961734498326415,13.769809609932548,HotOS,4238,"

Cloud scale provides the vast resources necessary to replace failed
components, but this is useful only if those failures can be detected.
For this reason, the major availability breakdowns and performance
anomalies we see in cloud environments tend to be caused by subtle
underlying faults, i.e., gray failure rather than fail-stop failure. In this
paper, we discuss our experiences with gray failure in production
cloud-scale systems to show its broad scope and consequences. We
also argue that a key feature of gray failure is differential observability: that the system’s failure detectors may not notice problems even
when applications are afflicted by them. This realization leads us to
believe that, to best deal with them, we should focus on bridging the
gap between different components’ perceptions of what constitutes
failure.

",15.112257680678326,14.715022900763358,134
HotOS_17_003.txt,17.46733639369068,16.496179138823738,HotOS,4600,"
Many large applications are now built using collections of
microservices, each of which is deployed in isolated containers and which interact with each other through the use
of remote procedure calls (RPCs). The use of microservices
improves scalability — each component of an application can
be scaled independently — and deployability. However, such
applications are inherently distributed and current tools do not
provide mechanisms to reason about and ensure their global
behavior. In this paper we argue that recent advances in formal methods and software packet processing pave the path
towards building mechanisms that can ensure correctness for
such systems, both when they are being built and at runtime.
These techniques impose minimal runtime overheads and are
amenable to production deployments.

",17.122413403193683,15.903764705882356,120
HotOS_17_004.txt,14.562437035582864,12.775050320050322,HotOS,3868,"

From the inception of the cloud, running multi-tenant workloads
has put strain on the Linux kernel’s abstractions. After years of
having its abstractions bypassed via virtualization, the kernel has
responded with a native container abstraction that is eagerly being
applied in the cloud. In this paper, we point out that history is repeating itself: with the introduction of serverless computing, even
the native container abstraction is ill-suited. We show that bypassing the kernel with unikernels can yield at least a factor of 6 better
latency and throughput. Facing a more complex kernel than ever
and a relatively undemanding computing model, we must revisit
the question of whether the kernel should try to adapt, we should
continue bypassing the kernel, or if it is finally time to try a new
native OS for this important future cloud workload.

",16.15616582465906,15.031856115107917,141
HotOS_17_005.txt,15.815486277796907,14.910651423641074,HotOS,3974,"
 c Model TensorFlow MXNet Caffe2 CNTK
In recent years, deep learning has pervaded many areas of computing due to the confluence of an explosive growth of large-scale
computing capabilities, availability of datasets, and advances in 
learning techniques. While this rapid growth has resulted in diverse deep learning frameworks, it has also led to inefficiencies for
both the users and developers of these frameworks. Specifically,
adopting useful techniques across frameworks — both to perform
learning tasks and to optimize performance — involves significant
repetitions and reinventions.

In this paper, we observe that despite their diverse origins, many
of these frameworks share architectural similarities. We argue that
by introducing a common representation of learning tasks and
a hardware abstraction model to capture compute heterogeneity,
we might be able to relieve machine learning researchers from
dealing with low-level systems issues and systems researchers from
being tied to any specific framework. We expect this decoupling to
accelerate progress in both domains.

",16.52667757954773,16.786974522292997,158
HotOS_17_006.txt,16.748652168924323,15.41753623188406,HotOS,2700,"
Machine learning applications are increasingly deployed
not only to serve predictions using static models, but also
as tightly-integrated components of feedback loops involving dynamic, real-time decision making. These applications pose a new set of requirements, none of which
are difficult to achieve in isolation, but the combination
of which creates a challenge for existing distributed execution frameworks: computation with millisecond latency at high throughput, adaptive construction of arbitrary task graphs, and execution of heterogeneous kernels
over diverse sets of resources. We assert that a new distributed execution framework is needed for such ML applications and propose a candidate approach with a proofof-concept architecture that achieves a 63x performance
improvement over a state-of-the-art execution framework
for a representative application.
",22.641843272026104,23.84451612903226,125
HotOS_17_007.txt,16.406994587392724,15.181700563829043,HotOS,4342,"

Many modern distributed storage systems emphasize availability
and partition tolerance over consistency, leading to many systems
that provide weak data consistency. However, weak data consistency is difficult for both system designers and users to reason
about. Formal specifications offer precise descriptions of consistency behavior, but they require expertise and specialized tools
to apply to real software systems. In this paper, we propose and
describe consistency oracles, an alternative way of specifying the
consistency model of a system that provides interactive answers,
making them easier and more flexible to use in a variety of ways.
A consistency oracle mimics the interface of a distributed storage
system, but returns all possible values that may be returned under
a given consistency model. This allows consistency oracles to be
directly applied in the testing and verification of both distributed
storage systems and the client software that uses those systems.

",17.5058628484301,17.516379310344835,146
HotOS_17_008.txt,14.39478758073233,12.597001915708812,HotOS,4116,"

Modern applications use multiple storage abstractions
such as the file system, key-value stores, and embedded
databases such as SQLite. Maintaining consistency of
data spread across multiple abstractions is complex and
error-prone. Applications are forced to copy data unnecessarily and use long sequences of system calls to update
state in a consistent manner. Not only does this create
implementation complexity, it also introduces potential
performance problems from redundant IO and fsync ()
calls, which fragment disk writes into small, random
IOs. In this paper, we propose that the operating system
should provide transactions across multiple storage abstractions; we can build such transactions with low development cost by taking advantage of a well-tested piece of
software: the file-system journal. We present the design
of our cross-abstraction transactions and some preliminary results, showing such transactions can increase performance by 31% in certain cases.

",16.11434528070225,15.321388888888894,145
HotOS_17_009.txt,14.746519837966982,12.951821016396714,HotOS,4505,"

When flash was introduced, wear-out was a known problem. Over
time, a number of techniques have been developed to estimate the
expected number of program/erase cycles under typical usage patterns, and sufficiently over-provision the cells such that the device
meets its expected lifespan, even if individual cells fail. This paper
started as a simple experiment: measuring whether the lifespan of
flash devices in smartphones and other mobile devices, match the
estimates. To our surprise, we find that, in a matter of days, simple,
unprivileged applications can render the drive of several smartphones (and thus, the phone) inoperable. This result is concerning,
as it means that installing malicious or poorly-written software
could destroy the device itself. We experimentally demonstrate
the problem, discuss reasons why it occurs, and consider potential
solutions.
",15.470042427545799,14.259511278195493,134
HotOS_17_010.txt,16.17027201809844,14.374911978950127,HotOS,4940,"
Future systems will be omni-programmable: alongside CPUs, GPUs
and FPGAs, they will execute user code near-storage, near-network,
near-memory, or on other Near-X accelerator Units, NXUs. This paper explores the design space of OS support for omni-programmable
systems, aiming to simplify the development of efficient applications that span multiple heterogeneous processors and near-data
accelerators. OmmniX is an accelerator-centric OS architecture that
extends standard OS abstractions, such as task execution and I/O,
into NXUs while maintaining a coherent view of the system among
all the processors. OmniX enables NXUs to directly invoke tasks
and access I/O services among themselves, excluding the CPU from
the performance-critical control plane operations. The host CPU
serves as a controller — for protection, device configuration and
monitoring. We discuss the hardware trends that motivate our work,
outline OmniX design principles, and sketch the core implementation ideas while highlighting missing hardware features, in the
hope of motivating hardware vendors to implement them soon.

",18.062587368997235,17.233181818181823,166
HotOS_17_011.txt,14.451129876698179,12.809695708131958,HotOS,4742,"
. If large-scale quantum computers become commonplace, the operating system will have to provide novel
abstractions to capture the power of this bizarre new hardware. In this paper, we consider this and other systemslevel issues that quantum computers would raise, and we
demonstrate that these machines would offer surprising
speed-ups for a number of everyday systems tasks, such
as unit testing and CPU scheduling.

",17.122413403193683,17.054230769230774,66
HotOS_17_012.txt,18.39518315742205,16.92532000576887,HotOS,4429,"
",NA,NA,1
HotOS_17_013.txt,16.68598849955895,15.633490236248637,HotOS,4323,"

The public cloud is moving to a Platform-as-a-Service model where
services such as data management, machine learning or image
classification are provided by the cloud operator while applications
are written in high-level languages and leverage these services.

Managed languages such as Java, Python or Scala are widely
used in this setting. However, while these languages can increase
productivity, they are often associated with problems such as unpredictable garbage collection pauses or warm-up overheads.

We argue that the reason for these problems is that current
language runtime systems were not initially designed for the cloud
setting. To address this, we propose seven tenets for designing future
language runtime systems for cloud data centers. We then outline
the design of a general substrate for building such runtime systems,
based on these seven tenets.

",14.314028922438442,13.986764705882354,137
HotOS_17_014.txt,15.603988498722362,14.062578700779117,HotOS,4722,"
Rust is a new system programming language that offers a practical
and safe alternative to C. Rust is unique in that it enforces safety
without runtime overhead, most importantly, without the overhead
of garbage collection. While zero-cost safety is remarkable on its
own, we argue that the superpowers of Rust go beyond safety. In
particular, Rust’s linear type system enables capabilities that cannot
be implemented efficiently in traditional languages, both safe and
unsafe, and that dramatically improve security and reliability of
system software. We show three examples of such capabilities:
zero-copy software fault isolation, efficient static information flow
analysis, and automatic checkpointing. While these capabilities
have been in the spotlight of systems research for a long time,
their practical use is hindered by high cost and complexity. We
argue that with the adoption of Rust these mechanisms will become
commoditized.

",16.084390811093357,14.758048289738433,144
HotOS_17_015.txt,15.954183578675032,15.161303503380456,HotOS,3235,"

The paper reports on first steps towards a systematic design
process that ensures quantitative stochastic requirements like
requirements on the expected energy consumption or resilience
requirements by construction. The idea is to automatically extract a formal model from a configurable system and to use
formal analysis techniques to automatically determine a configuration such that the system meets the quantitative requirements. As a proof of concept we present a tool that supports
the automated synthesis of protocol parameters for IPC (interprocess communication). The tool takes as input a Lua script
describing the communication structure of several processes.
This script is annotated with quantitative information such
as error probabilities and timing information. The output is a
Markov chain specified in the input language of the prominent
probabilistic model checker PRISM. This Markov chain yields
the basis for quantitative formal analysis of failure scenarios
caused by hardware faults in IPC channels. The results yield
the basis for finding optimal values for protocol parameters
that tune, e.g., the level of resiliency. As an initial demonstration of the tool, we analyze and adjust system parameters of
a simple scenario with a few communicating processes and

report on results. Though achieved under simplified assumptions, the results presented here are a proof-of-concept towards
the vision of automated system configuration.

",16.76809479433877,15.914878504672899,216
HotOS_17_016.txt,14.716445700393802,12.675167876756714,HotOS,4085,"

Operating systems have been shown to waste machine resources by leaving cores idle while work is ready to be
scheduled. This results in suboptimal performance for user
applications, and wasted power.

Recent progress in formal verification methods have led
to operating systems being proven safe, but operating systems have yet to be proven free of performance bottlenecks.
In this paper we instigate the first effort in proving performance properties of operating systems by designing a multicore scheduler that is proven to be work-conserving.

",15.247664890283005,14.092857142857145,85
HotOS_17_017.txt,16.579202588070828,14.452340269187257,HotOS,3508,"
",NA,NA,1
HotOS_17_018.txt,16.042720420700856,14.600838540753433,HotOS,3922,"

The datacenter is becoming fully heterogeneous, integrating multiple OS-capable CPUs of different Instruction Set Architectures in
separate machines. These machines present diverse performance
and power consumption profiles and we show that significant potential benefits for both metrics can be expected, should these
machines be able to cooperate in the processing of datacenter, multiprogrammed workloads. We advocate that this cooperation should
be enabled at the level of the OS, relieving the programmer from
any effort related to the heterogeneity of the managed machines.
We propose a distributed OS architecture running on a fully heterogeneous computer cluster, enabling this cooperation through
three main components: the abstraction of the entire cluster in a
single system image, a distributed shared memory system, and a
heterogeneous scheduler.

",21.19438992294339,20.769979674796748,124
HotOS_17_019.txt,16.07818468340257,14.922444783695592,HotOS,3573,"

Big data systems such as Spark are built around the idea
of splitting an iterative parallel program into tiny tasks
with other aspects of system design built around this
basic design principle. Unfortunately, in spite of immense engineering effort, tiny tasks have unavoidably
large overheads. We use the example of logistic regression — a common machine learning primitive — to compare the performance of Spark to different designs that
converge to a hand-coded parallel MPI-based implementation. We conclude that Spark leaves orders of magnitude performance on the table, due to its insistence on
setting the granularity of a task to a single iteration. We
counter a common argument for the tiny task approach —
namely better resilience to faults — by demonstrating that
optimum job checkpoint intervals are far longer than the
duration of the tiny tasks favored in Spark’s design. We
propose an alternative approach that relies on an autoparallelizing compiler tightly integrated with the MPI
runtime, illustrating the opposite end of the spectrum
where task granularities are as large as possible.

",17.122413403193683,16.916812865497075,173
HotOS_17_020.txt,16.215063373546215,14.38998648381354,HotOS,4189,"
 Why did my workload run so slowly? Is hardware degradation

Users often struggle to reason about the performance of today’s
systems. Without an understanding of what factors are most important to performance, users do not know how to tune their system’s
hardware and software configuration to improve performance. We
argue that performance clarity — making it easy to understand where
bottlenecks lie and the performance implications of various system
changes — should be a first class design goal. To illustrate that this is
possible, we propose an architecture for data analytics frameworks
in which jobs are decomposed into schedulable units called monotasks that each consume a single resource. By untangling the use of
different resources, using monotasks allows the system to trivially
report time used on each resource and the resource bottleneck. Our
prototype implementation of monotasks for Apache Spark is APIcompatible and achieves performance parity with Spark, and yields
a simple performance model that can predict the effects of future
hardware and software changes.
",15.903189008614273,14.702471516213848,166
HotOS_17_021.txt,14.516443886965039,12.595573439692739,HotOS,4501,"

We highlight the problem of scalability bugs, a new class of bugs
that appear in “cloud-scale” distributed systems. Scalability bugs
are latent bugs that are cluster-scale dependent, whose symptoms
typically surface in large-scale deployments, but not in small or
medium-scale deployments. The standard practice to test large distributed systems is to deploy them on a large number of machines
(“real-scale testing”), which is difficult and expensive. New methods are needed to reduce developers’ burdens in finding, reproducing, and debugging scalability bugs. We propose “scale check,” an
approach that helps developers find and replay scalability bugs at
real scales, but do so only on one machine and still achieve a high
accuracy (i.e., similar observed behaviors as if the nodes are deployed in real-scale testing).

",14.554592549557764,13.974615384615387,132
HotOS_17_022.txt,16.426087933410553,15.367131482237646,HotOS,3820,"

A production system’s printed logs are often the only source
of runtime information available for postmortem debugging,
performance analysis and profiling, security auditing, and
user behavior analytics. Therefore, the quality of this data is
critically important. Recent work has attempted to enhance
log quality by recording additional variable values, but logging statement placement, i.e., where to place a logging statement, which is the most challenging and fundamental problem for improving log quality, has not been adequately addressed so far. This position paper proposes we automate the
placement of logging statements by measuring how much
uncertainty, 1.e., the expected number of possible execution
code paths taken by the software, can be removed by adding
a logging statement to a basic block. Guided by ideas from
information theory, we describe a simple approach that automates logging statement placement. Preliminary results suggest that our algorithm can effectively cover, and further improve, the existing logging statement placements selected
by developers. It can compute an optimal logging statement
placement that disambiguates the entire function call path
with only 0.218% of slowdown.

",17.77360955136429,17.138095238095236,181
HotOS_17_023.txt,15.77408204403054,13.963208002039924,HotOS,3844,"
It is time to reconsider memory protection. The emergence of large non-volatile main memories, scalable
interconnects, and rack-scale computers running large
numbers of small “micro services” creates significant
challenges for memory protection based solely on MMU
mechanisms. Central to this is a tension between protection and translation: optimizing for translation performance often comes with a cost in protection flexibility.
We argue that a key-based memory protection scheme,
complementary to but separate from regular page-level
translation, is a better match for this new world. We
present MaKC, a new architecture which combines two
levels of capability-based protection to scale fine-grained
memory protection at both user and kernel level to large
numbers of protection domains without compromising
efficiency at scale or ease of revocation.
",18.878054631974784,16.334625,129
HotOS_17_024.txt,16.290885704034196,14.393567057623667,HotOS,4248,"

Since the dawn of computing, memory capacity has been a primary
limitation in system design. Forthcoming memory technology such
as Intel and Micron’s 3D XPoint memory and other technologies
may provide far larger memory capacity than ever before. Furthermore, these new memory technologies are inherently persistent
and save data across system crashes or power failures.

We conjecture that current operating systems are ill-equipped
for an environment where there is ample memory. For example,
operating systems do substantial work for every page allocated,
which adds unnecessary overhead when dealing with terabytes of
memory.

We suggest that now is the time for a complete rethinking of
memory management for both operating systems and language
runtimes considering excess memory capacity. We propose a new
guiding principle: Order(1) operation, so that memory operations
have low constant time independent of size. We describe a concrete proposal of this principle with the idea of file-only memory,
in which most dynamic memory allocation is managed with filesystem mechanisms rather than common virtual memory mechanisms.

",18.10804710084791,16.297500000000003,172
HotOS_17_025.txt,14.159590769470984,13.091733463035023,HotOS,4112,"

This paper presents a storage system that can hide the presence of hidden data alongside a larger volume of public data.
Encryption allows a user to hide the contents of data, but not
the fact that sensitive data is present. Under duress, the owner
of high-value data can be coerced by a powerful adversary to
disclose decryption keys. Thus, private users and corporations
have an interest in hiding the very presence of some sensitive
data, alongside a larger body of less sensitive data (e.g., the
operating system and other benign files); this property is called
plausible deniability. Existing plausible deniability systems do
not fulfill all of the following requirements: (1) resistance to
multiple snapshot attacks where an attacker compares the state
of the device over time; (2) ensuring that hidden data won’t
be destroyed when the public volume is modified by a user
unaware of the hidden data; and (3) disguising writes to secret
data as normal system operations on public data.

We explain why existing solutions do not meet all these
requirements and present the Ever-Changing Disk (ECD), a
generic scheme for plausible deniability storage systems that
meets all of these requirements. An ECD stores hidden data
inside a large volume of pseudorandom data. Portions of this
volume are periodically migrated in a log-structured manner.
Hidden writes can then be interchanged with normal firmware
operations. The expected access patterns and time until hidden
data is overwritten are completely predictable, and insensitive
to whether data is hidden. Users control the rate of internal
data migration (R), trading write bandwidth to hidden data for
longevity of the hidden data. For a typical 2TB disk and setting
of R, a user preserves hidden data by entering her secret key
every few days or weeks.

",15.021129683784007,14.757500000000004,298
HotOS_17_026.txt,15.402521221168211,14.424343156115192,HotOS,4728,"

Nearly all modern software has security flaws—either known or
unknown by the users. However, metrics for evaluating software
security (or lack thereof) are noisy at best. Common evaluation
methods include counting the past vulnerabilities of the program,
or comparing the size of the Trusted Computing Base (TCB), measured in lines of code (LoC) or binary size. Other than deleting
large swaths of code from project, it is difficult to assess whether a
code change decreased the likelihood of a future security vulnerability. Developers need a practical, constructive way of evaluating
security.

This position paper argues that we actually have all the tools
needed to design a better, empirical method of security evaluation.
We discuss related work that estimates the severity and vulnerability of certain attack vectors based on code properties that can be
determined via static analysis. This paper proposes a grand, unified
model that can predict the risk and severity of vulnerabilities in a
program. Our prediction model uses machine learning to correlate
these code features of open-source applications with the history of
vulnerabilities reported in the CVE (Common Vulnerabilities and
Exposures) database. Based on this model, one can incorporate an
analysis into the standard development cycle that predicts whether
the code is becoming more or less prone to vulnerabilities.

",15.774802946060372,15.319816901408455,214
HotOS_17_027.txt,14.944361084152646,13.332146949578792,HotOS,5700,"
Encrypted databases, a popular approach to protecting data from
compromised database management systems (DBMS’s), use abstract
threat models that capture neither realistic databases, nor realistic
attack scenarios. In particular, the “snapshot attacker” model used to
support the security claims for many encrypted databases does not
reflect the information about past queries available in any snapshot
attack on an actual DBMS.

We demonstrate how this gap between theory and reality causes
encrypted databases to fail to achieve their “provable security”
guarantees.

",19.946969662950774,19.4425,82
HotOS_17_028.txt,16.78748214569492,16.55699959127978,HotOS,4175,"

Users today enjoy access to a wealth of services that rely
on user-contributed data, such as recommendation services,
prediction services, and services that help classify and interpret data. The quality of such services inescapably relies on
trustworthy contributions from users. However, validating the
trustworthiness of contributions may rely on privacy-sensitive
contextual data about the user, such as a user’s location or
usage habits, creating a conflict between privacy and trust:
users benefit from a higher-quality service that identifies and
removes illegitimate user contributions, but, at the same time,
they may be reluctant to let the service access their private
information to achieve this high quality.

We argue that this conflict can be resolved with a pragmatic Glimmer of Trust, which allows services to validate
user contributions in a trustworthy way without forfeiting
user privacy. We describe how trustworthy hardware such
as Intel’s SGX can be used on the client-side—in contrast
to much recent work exploring SGX in cloud services—to
realize the Glimmer architecture, and demonstrate how this
realization is able to resolve the tension between privacy and
trust in a variety of cases.

",20.829396946661657,21.12871957671958,192
HotOS_17_029.txt,16.226245271710653,14.871531651415172,HotOS,4009,"
",NA,NA,1
HotStorage_17_001.txt,14.821371503578156,13.968858881853176,HotStorage,2979,"

Delta synchronization (sync) is known to be crucial for
network-level efficiency of cloud storage services (e.g.,
Dropbox). Practical delta sync techniques are, however, only available for PC clients and mobile apps,
but not web browsers—the most pervasive and OSindependent access method. To understand obstacles of
web-based delta sync, we implemented a traditional delta
sync solution (named WebRsync) for web browsers using JavaScript, and find that WebRsync severely suffers
from the inefficiency of JavaScript execution inside web
browsers, thus leading to frequent stagnation and even
crashing. Given that the computation burden on the web
browser mainly stems from data chunk search and comparison, we reverse the traditional delta sync approach by
lifting all chunk search and comparison operations from
the client side into the server side. Inevitably, this brings
enormous computation overhead to the servers. Hence,
we further leverage locality matching and a more efficient checksum to reduce the overhead. The resulting
solution (called WebR2sync+) outpaces WebRsync by
an order of magnitude, and it is able to simultaneously
support ~7300 web clients’ delta sync using an ordinary
VM server based on a Dropbox-like system architecture.

",16.613394197324528,16.67045112781955,190
HotStorage_17_002.txt,13.343405976438678,11.726113355242926,HotStorage,3752,"

Disaggregation of resources in the data center, especially
at the rack-scale, offers the opportunity to use valuable
resources more efficiently. It is common that mass storage racks in large-scale clouds are filled with servers with
Hard Disk Drives (HDDs) attached directly to each of
them, either using SATA or SAS depending on the number of HDDs.

What does disaggregated storage mean for these
racks? We define four categories of in-rack disaggregation: complete, dynamic elastic, failure, and configuration disaggregation. We explore the benefits and impact of these design points by building a highly flexible
research storage fabric, that allows us to build example
systems that embody the four designs.

",14.554592549557764,13.372504504504509,112
HotStorage_17_003.txt,14.728657954372316,13.201686765092603,HotStorage,3722,"

While NoSQL databases are gaining popularity for business applications, they pose unique challenges towards
backup and recovery. Our solution, BARNS addresses
these challenges, namely taking: a) cluster consistent
backup and ensuring repair free restore, b) storage efficient backups, and c) topology oblivious backup and restore. Due to eventual consistency semantics of these databases, traditional database backup techniques of performing quiesce do not guarantee cluster consistent
backup. Moreover, taking crash consistent backup increases recovery time due to the need for repairs. In this
paper, we provide detailed solutions for taking backup of
two popular, but architecturally different NoSQL DBs,
Cassandra and MongoDB, when hosted on shared storage. Our solution leverages database distribution and
partitioning knowledge along with shared storage features such as snapshots, clones to efficiently perform
backup and recovery of NoSQL databases. Our solution
gets rid of replica copies, thereby saving ~66% backup
space (under 3x replication). Our preliminary evaluation
shows that we require a constant restore time of ~2-3
mins, independent of backup dataset and cluster size.

",16.975882523387877,16.20823830409357,170
HotStorage_17_004.txt,15.945464101590215,13.99062623569656,HotStorage,3699,"

We report our experience building and evaluating
pmemcached, a version of memcached ported to byteaddressable persistent memory. Persistent memory is
expected to not only improve overall performance of applications’ persistence tier, but also vastly reduce the
“warm up” time needed for applications after a restart.
We decided to test this hypothesis on memcached, a popular key-value store. We took the extreme view of persisting memcached’s entire state, resulting in a virtually
instantaneous warm up phase. Since memcached is already optimized for DRAM, we expected our port to be
a straightforward engineering effort. However, the effort
turned out to be surprisingly complex during which we
encountered several non-trivial problems that challenged
the boundaries of memcached’s architecture. We detail
these experiences and corresponding lessons learned.

",16.954822765917157,14.879885714285717,128
HotStorage_17_005.txt,13.721294105573655,11.353403908608328,HotStorage,3917,"
Recently, with the emergence of low-latency NVM storage, software overhead has become a greater bottleneck
than storage latency, and memory mapped file I/O has
gained attention as a means to avoid software overhead.
However, according to our analysis, memory mapped file
I/O incurs a significant amount of additional overhead. To
utilize memory mapped file I/O to its true potential, such
overhead should be alleviated. We propose map-ahead,
mapping cache, and extended madvise techniques to maximize the performance of memory mapped file I/O on lowlatency NVM storage systems. This solution can avoid
both page fault overhead and page table entry construction overhead. Our experimental results show throughput
improvements of 38-70% in microbenchmarks and performance improvements of 6-18% in real applications
compared to existing memory mapped I/O mechanisms.

",17.879347455551382,14.949444444444449,136
HotStorage_17_006.txt,13.444391091990568,11.882109633626179,HotStorage,3920,"

Modern file systems employ complex techniques to ensure that they can recover efficiently in the event of a
crash. However, there is little infrastructure for systematically testing crash consistency in file systems. We
introduce CrashMonkey, a simple, flexible, file-systemagnostic test framework to systematically check file systems for inconsistencies if a failure occurs during a filesystem operation. CrashMonkey is modular and flexible,
allowing the users to easily specify different test workloads and custom consistency checks.

",16.52667757954773,15.637166666666669,76
HotStorage_17_007.txt,14.466211821449711,12.809375404922932,HotStorage,2929,"

File system checkers serve as the last line of defense to
recover a corrupted file system back to a consistent state.
Therefore, their reliability is critically important. Motivated by real accidents, in this paper we study the behavior of file system checkers under faults. We systematically inject emulated faults to interrupt the checkers
and examine the impact on the file system images. In
doing so, we answer two important questions: Does running the checker after an interrupted-check successfully
return the file system to a correct state? If not, what goes
wrong? Our results show that there are vulnerabilities in
popular file system checkers which could lead to unrecoverable data loss under faults.

",12.785403640627713,11.381820480404553,114
HotStorage_17_008.txt,17.5878758517253,16.589705646879562,HotStorage,3371,"
",NA,NA,1
HotStorage_17_009.txt,15.247664890283005,13.300898043864851,HotStorage,3354,"

Energy consumption is a key concern for mobile devices.
Prior research has focused on the screen and the network
as the major sources of energy consumption. Through
carefully designed measurement-based experiments, we
show that for certain storage-intensive workloads, the
storage subsystem on an Android smartphone consumes
a significant amount of energy (36%), on par with screen
energy consumption. We analyze the energy consumption of different storage primitives, such as sequential
and random writes, on two popular mobile file systems,
ext4 and F2FS. In addition, since most Android applications use SQLite for storage, we analyze the energy
consumption of different SQLite operations. We present
several interesting results from our analysis: for example, random writes consume 19x higher energy than sequential writes, and that F2FS consumes half the energy as ext4 for most workloads. We believe our results
contribute useful design guidelines for the developers of
energy-efficient mobile file systems.

",16.785175570968402,14.625506149479666,152
HotStorage_17_010.txt,16.183140106946677,14.105593965184735,HotStorage,3785,"

Encryption is often employed to protect sensitive information stored in memory and storage. It is the most
powerful countermeasure against data breach, but it has
performance overhead. As a low-cost alternative to encryption, an access-control memory (ACM) has been introduced, which integrates an access-control mechanism
with memory. While ACM minimizes the performance
overhead of encryption, it provides similar levels of security as to encryption method. ACM reveals information only when the access codes are correct. However,
if an adversary attempts to access data directly from
memory cells through a physical attack without going
through a standard interface, the vulnerability could occur. This paper discusses feasibility and countermeasures for physical attacks, including fault injection attack, power analysis attack, chip modification, microprobing, and imaging for ACM. Moreover, as a concrete example of ACM, we compare the security aspects
of SSDs when the write buffers in the SSDs employ
ACM with emerging non-volatile memories such as STTRAM, PRAM, and RRAM.

",17.693802365651003,15.638874223602485,162
HotStorage_17_011.txt,14.90622815163357,14.317881639015699,HotStorage,2552,"

Virtual Guard (Vguard) is a track-based static mapping
translation layer for shingled magnetic recording (SMR)
drives. Data is written in-place by caching data from the next
track in the shingling direction, allowing direct overwrite
of sectors in the target track. This enables Vguard to take
advantage of track-level locality, nearly eliminating cleaning
for many workloads. We compare performance of Vguard
to an available drive-managed SMR drive analyzed and
modeled in previous research. Vguard reduces the 99.9%
latency by 15x for real-world traces, and maximum latency
by 32% for synthetic random write workloads.
",14.554592549557764,12.04816494845361,99
HotStorage_17_012.txt,12.990829087741155,11.453516430688179,HotStorage,3007,"

NAND flash memory based storage devices use Flash
Translation Layer (FTL) to translate logical addresses
of I/O requests to corresponding flash memory addresses. Mobile storage devices typically have RAM with
constrained size, thus lack in memory to keep the whole
mapping table. Therefore, mapping tables are partially
retrieved from NAND flash on demand, causing random-read performance degradation.

In order to improve random read performance, we
propose HPB (Host Performance Booster) which uses
host system memory as a cache for FTL mapping table.
By using HPB, FTL data can be read from host memory
faster than from NAND flash memory. We define transactional protocols between host device driver and storage device to manage the host side mapping cache. We
implement HPB on Galaxy S7 smartphone with UFS
device. HPB is shown to have a performance improvement of 58 - 67% for random read workload.

",13.427899808715576,10.93277777777778,145
HotStorage_17_013.txt,13.533629876511455,11.449988620556557,HotStorage,3343,"
",NA,NA,1
HotStorage_17_014.txt,15.137341203794609,13.200004177342098,HotStorage,3045,"

To reduce the performance and lifespan loss caused by
the partial-stripe writes in SSD RAIDs, we propose two
schemes: parity-stream separation and SLC/MLC convertible programming. Parity-stream separation splits
the parity block stream from the data block stream to
decrease valid page copy during garbage collection. In
the convertible programming scheme, the flash memory
blocks that are allocated for parity data are programmed
in SLC mode to reduce the wear caused by programming
stress, while the other flash memory blocks are written in
MLC mode as usual. Evaluation shows that our scheme
decreased garbage collection overhead by up to 58% and
improved lifespan by up to 54%, assuming that the MLC
write stress was 3.5 times that of the SLC.

",16.218646115125612,14.822012195121953,125
HotStorage_17_015.txt,13.555575755939262,11.492556876664654,HotStorage,2544,"

There is need of differentiated I/O service when applications with diverse performance-needs share a storagedevice. NVMe specification provides a method called
Weighted-Round-Robin-with-urgent-priority | (WRR)
which can help in providing such differentiated I/O service. In Round-Robin arbitration all I/O queues are
treated to be of equal priority, leading to symmetric I/O
processing. While in WRR arbitration, queues can be
marked urgent, high, medium or low, with provision for
different weightage for each category. Onus is on host
to associate priority with I/O queues and define weights.
We find that very little has been done in current Linux
ecosystem when it comes to supporting WRR and making benefits reach to application. In this paper we propose a method that introduces WRR support in Linux
NVMe driver. This method delivers WRR capability to
applications without the need of rebuilding them. Unlike affinity-based approach, it does not limit computeability of application. Our results demonstrate that modified driver indeed provides differentiated I/O performance among applications. Proposed work modifies
only NVMe driver and is generic enough to be included
in mainstream Linux kernel for supporting WRR.

",14.158211354625415,12.820140581068415,194
HotStorage_17_016.txt,15.20443048600136,13.967153436870866,HotStorage,3476,"

In large scale data centers, controlling tail latencies of
IO requests keeps storage performance bounded and
predictable, which is critical for infrastructure resource
planning. This work provides a transparent mechanism
for applications to pass prioritized IO commands to storage devices. As a consequence, we observe much shorter
tail latencies for prioritized IO while impacting nonprioritized IO in a reasonable manner. We also provide a
detailed description of the changes we made to the Linux
Kernel that enable applications to pass IO priorities to a
storage device. Our results show that passing priorities to
the storage device is capable of decreasing tail latencies
by a factor of 10x while decreasing IOPS minimally.

",16.404322709996244,15.392324324324324,112
HotStorage_17_017.txt,14.895726920698603,13.146448673034758,HotStorage,3607,"

Interactive data center applications suffer from the tail
latency problem. Since most modern data center applications take the sharded architecture to serve scale-out services, a request comprises multiple sub-requests handled
in individual back-end nodes. Depending on the state
of each back-end node, a node may issue multiple I/Os
for a single sub-request. Since traditional I/O scheduling operates in an application-agnostic manner, it sometimes causes a long latency gap between the responses
of sub-requests, thereby delaying the response to endusers. In this paper, we propose a request-aware cooperative I/O scheduling scheme to reduce the tail latency of
a database application. Our proposed scheme captures
request arrival order at the front-end of an application
and exploits it to make a decision for I/O scheduling in
individual back-end nodes. We implemented a prototype
based on MongoDB and the Linux kernel and evaluated
it with a read-intensive scan workload. Experimental results show that our proposed scheme effectively reduces
the latency gap between sub-requests, thereby reducing
the tail latency.

",15.903189008614273,14.29388888888889,181
HotStorage_17_018.txt,14.572543059525561,13.22518291573136,HotStorage,3110,"

Modern robots collect a wealth of rich sensor data during
their operation. While such data allows interesting analysis and sophisticated algorithms, it is simply infeasible
to store all the data that is generated. However, collecting
only samples of the data greatly minimizes the usefulness
of the data. We present CC-LOG, a new logging system
built on top of the widely-used Robot Operating System that uses a combination of classification and compression techniques to reduce storage requirements. Experiments using the Building-Wide Intelligence Robot, a
mobile autonomous mobile platform capable of operating for long periods of time in human-inhabited environments, showed that our proposed system can reduce storage requirements by more than an order of magnitude.
Our results indicate that there is significant unrealized
potential in optimizing infrastructure commonly used in
robotics applications and research.

",16.92669308720184,16.828868613138685,138
HotStorage_17_019.txt,17.26200103195191,15.634091531603435,HotStorage,3282,"

Modern image storage services, especially those associated with social media services, host massive collections of images. These images are often replicated at
many different resolutions to support different devices
and contexts, incurring substantial capacity overheads.
One approach to alleviate these overheads is to resize
them at request time. However, this approach can be inefficient, as reading full-size source images for resizing
uses more bandwidth than reading pre-resized images.
We propose repurposing the progressive JPEG standard
and customizing the organization of image data to reduce
the bandwidth overheads of dynamic resizing. We show
that at a PSNR of 32 dB, dynamic resizing with progressive JPEG provides 2.5x read data savings over baseline
JPEG, and that progressive JPEG with customized encode parameters can further improve these savings (up
to 5.8x over the baseline). Finally, we characterize the
decode overheads of progressive JPEG to assess the feasibility of directly decoding progressive JPEG images on
energy-limited devices. Our approach does not require
modifications to current JPEG software stacks.

",17.833180683606166,15.797956586826349,170
HotStorage_17_020.txt,16.623559516670493,16.023256833825467,HotStorage,3321,"
Data access is swiftly becoming a bottleneck in visual
data processing, providing an opportunity to influence
the way visual data is treated in the storage system. To
foster this discussion, we identify two key areas where
storage research can strongly influence visual processing run-times: efficient metadata storage and new storage formats for visual data. We propose a storage architecture designed for efficient visual data access that
exploits next generation hardware and give preliminary
results showing how it enables efficient vision analytics.
",19.620377997778096,19.101707317073174,83
HotStorage_17_021.txt,16.102916281614707,14.627105902343946,HotStorage,3589,"

High accuracy scientific simulations on high performance computing (HPC) platforms generate large
amounts of data. To allow data to be efficiently analyzed, simulation outputs need to be refactored, compressed, and properly mapped onto storage tiers. This
paper presents Canopus, a progressive data management
framework for storing and analyzing big scientific data.
Canopus allows simulation results to be refactored into
a much smaller dataset along with a series of deltas with
fairly low overhead. Then, the refactored data are compressed, mapped, and written onto storage tiers. For
data analytics, refactored data are selectively retrieved
to restore data at a specific level of accuracy that satisfies analysis requirements. Canopus enables end users to
make trade-offs between analysis speed and accuracy onthe-fly. Canopus is demonstrated and thoroughly evaluated using blob detection on fusion simulation data.

",15.903189008614273,15.246977611940299,135
HPCA_17_001.txt,15.396202083966756,13.404720900494134,HPCA,8956,"
Accelerating Convolutional Neural Networks (CNNs)
on GPUs usually involves two stages: training and inference.
Traditionally, this two-stage process is deployed on high-end
GPU-equipped servers. Driven by the increase in compute
power of desktop and mobile GPUs, there is growing interest in
performing inference on various kinds of platforms. In contrast
to the requirements of high throughput and accuracy during the
training stage, end-users will face diverse requirements related
to inference tasks. To address this emerging trend and new
requirements, we propose Pervasive CNN (P-CNN), a user
satisfaction-aware CNN inference framework. P-CNN is
composed of two phases: cross-platform offline compilation and
run-time management. Based on users’ requirements, offline
compilation generates the optimal kernel using architectureindependent techniques, such as adaptive batch size selection
and coordinated fine-tuning. The runtime management phase
consists of accuracy tuning, execution, and calibration. First,
accuracy tuning dynamically identifies the fastest kernels with
acceptable accuracy. Next, the run-time kernel scheduler
partitions the optimal computing resource for each layer and
schedules the GPU thread blocks. If its accuracy is not
acceptable to the end-user, the calibration stage selects a slower
but more precise kernel to improve the accuracy. Finally, we
design a user satisfaction metric for CNNs to evaluate our
Pervasive deign. Our evaluation results show P-CNN can
provide the best user satisfaction for different inference tasks.

",15.402047736947555,14.191739130434787,231
HPCA_17_002.txt,13.339154967643793,10.870958074534162,HPCA,10096,"
The memory wall continues to be a major performance bottleneck. While small on-die caches have been effective
so far in hiding this bottleneck, the ever-increasing footprint of
modern applications renders such caches ineffective. Recent advances in memory technologies like embedded DRAM (eDRAM)
and High Bandwidth Memory (HBM) have enabled the integration of large memories on the CPU package as an additional
source of bandwidth other than the DDR main memory. Because
of limited capacity, these memories are typically implemented
as a memory-side cache. Driven by traditional wisdom, many of
the optimizations that target improving system performance have
been tried to maximize the hit rate of the memory-side cache.
A higher hit rate enables better utilization of the cache, and is
therefore believed to result in higher performance.

In this paper, we challenge this traditional wisdom and present
DAP, a Dynamic Access Partitioning algorithm that sacrifices
cache hit rates to exploit under-utilized bandwidth available at
main memory. DAP achieves a near-optimal bandwidth partitioning between the memory-side cache and main memory
by using a light-weight learning mechanism that needs just
sixteen bytes of additional hardware. Simulation results show
a 13% average performance gain when DAP is implemented on
top of a die-stacked memory-side DRAM cache. We also show
that DAP delivers large performance benefits across different
implementations, bandwidth points, and capacity points of the
memory-side cache, making it a valuable addition to any current
or future systems based on multiple heterogeneous bandwidth
sources beyond the on-chip SRAM cache hierarchy.

",17.80541091248751,16.015247104247106,260
HPCA_17_003.txt,16.186248309152848,14.225972826076788,HPCA,8754,"

The rate of network packets encapsulating requests from clients can significantly affect the utilization, and thus performance and sleep states of processors in servers deploying a
power management policy. To improve energy efficiency,
servers may adopt an aggressive power management policy
that frequently transitions a processor to a low-performance
or sleep state at a low utilization. However, such servers may
not respond to a sudden increase in the rate of requests from
clients early enough due to a considerable performance penalty of transitioning a processor from a sleep or low-performance state to a high-performance state. This in turn entails
violations of a service level agreement (SLA), discourages
server operators from deploying an aggressive power management policy, and thus wastes energy during low-utilization periods. For both fast response time and high energy-efficiency, we propose NCAP, Network-driven, packet Context-Aware Power management for client-server architecture.
NCAP enhances a network interface card (NIC) and its driver
such that it can examine received and transmitted network
packets, determine the rate of network packets containing latency-critical requests, and proactively transition a processor
to an appropriate performance or sleep state. To demonstrate
the efficacy, we evaluate on-line data-intensive (OLDI) applications and show that a server deploying NCAP consumes
37~61% lower processor energy than a baseline server while
satisfying a given SLA at various load levels.

",20.40282108145781,19.551861471861475,231
HPCA_17_004.txt,16.76809479433877,15.476118907590333,HPCA,7650,"
While emerging accelerator-centric architectures offer orders-of-magnitude performance and energy improvements,
use cases and adoption can be limited by their rigid programming model. A unified virtual address space between the host
CPU cores and customized accelerators can largely improve
the programmability, which necessitates hardware support for
address translation. However, supporting address translation for
customized accelerators with low overhead is nontrivial. Prior
studies either assume an infinite-sized TLB and zero page walk
latency, or rely on a slow IOMMU for correctness and safety—
which penalizes the overall system performance.
To provide efficient address translation support for
accelerator-centric architectures, we examine the memory access
behavior of customized accelerators to drive the TLB augmentation and MMU designs. First, to support bulk transfers of
consecutive data between the scratchpad memory of customized
accelerators and the memory system, we present a relatively small
private TLB design to provide low-latency caching of translations
to each accelerator. Second, to compensate for the effects of the
widely used data tiling techniques, we design a shared level-two
TLB to serve private TLB misses on common virtual pages,
eliminating duplicate page walks from accelerators working on
neighboring data tiles that are mapped to the same physical
page. This two-level TLB design effectively reduces page walks by
75.8% on average. Finally, instead of implementing a dedicated
MMU which introduces additional hardware complexity, we
propose simply leveraging the host per-core MMU for efficient
page walk handling. This mechanism is based on our insight that
the existing MMU cache in the CPU MMU satisfies the demand of
customized accelerators with minimal overhead. Our evaluation
demonstrates that the combined approach incurs only a 6.4%
performance overhead compared to the ideal address translation.
",18.63122029486172,17.613636363636363,289
HPCA_17_005.txt,13.933619181494208,11.577552969541866,HPCA,6971,"
Much prior work has studied cache replacement, but
a large gap remains between theory and practice. The design of
many practical policies is guided by the optimal policy, Belady’s
MIN. However, MIN assumes perfect knowledge of the future
that is unavailable in practice, and the obvious generalizations
of MIN are suboptimal with imperfect information. What, then,
is the right metric for practical cache replacement?

We propose that practical policies should replace lines based
on their economic value added (EVA), the difference of their
expected hits from the average. Drawing on the theory of Markov
decision processes, we discuss why this metric maximizes the
cache’s hit rate. We present an inexpensive implementation of
EVA and evaluate it exhaustively. EVA outperforms several prior
policies and saves area at iso-performance. These results show
that formalizing cache replacement yields practical benefits.

",14.712192995108573,12.536376811594206,141
HPCA_17_006.txt,14.964868318834561,13.07944901961088,HPCA,8433,"
Performance isolation is an important goal in serverclass environments. Partitioning the last-level cache of a chip
multiprocessor (CMP) across co-running applications has proven
useful in this regard. Two popular approaches are (a) hardware
support for way partitioning, or (6) operating system support
for set partitioning through page coloring. Unfortunately, neither
approach by itself is scalable beyond a handful of cores without
incurring in significant performance overheads.

We propose SWAP, a scalable and fine-grained cache management technique that seamlessly combines set and way partitioning. By cooperatively managing cache ways and sets, SWAP
(‘Set and WAy Partitioning”) can successfully provide hundreds
of fine-grained cache partitions for the manycore era.

SWAP requires no additional hardware beyond way partitioning. In fact, SWAP can be readily implemented in existing
commercial servers whose processors do support hardware way
partitioning. In this paper, we prototype SWAP on a 48-core
Cavium ThunderX platform running Linux, and we show average
speedups over no cache partitioning that are twice as large as
those attained with ThunderX’s hardware way partitioning alone.

",15.616094167265928,13.840190476190475,177
HPCA_17_007.txt,15.449095278376443,13.588914544257282,HPCA,8354,"
Today’s caches tightly couple data with metadata
(Address Tags) at the cache line granularity. The co-location of
data and its identifying metadata means that they require multiple approaches to locate data (associative way searches and
level-by-level searches), evict data (coherent writebacks buffers
and associative level-by-level searches) and keep data coherent
(directory indirections and associative level-by-level searches).
This results in complex implementations with many corner
cases, increased latency and energy, and limited flexibility for
data optimizations.

We propose splitting the metadata and data into two separate
structures: a metadata hierarchy and a data hierarchy. The
metadata hierarchy tracks the location of the data in the
data hierarchy. This allows us to easily apply many different
optimizations to the data hierarchy, including smart data
placement, dynamic coherence, and direct accesses.

The new split cache hierarchy, Direct-to-Master (D2M),
provides a unified mechanism for cache searching, eviction,
and coherence, that eliminates level-by-level data movement
and searches, associative cache address tags comparisons and
about 90% of the indirections through a central directory.
Optimizations such as moving LLC slices to the near-side of
the network and private/shared data classification can easily
be built on top off D2M to further improve its efficiency. This
approach delivers a 54% improvement in cache hierarchy EDP
ys. a mobile processor and 40% vs. a server processor, reduces
network traffic by an average of 70%, reduces the L1 miss
latency by 30% and is especially effective for workloads with
high cache pressure.

",18.481644305966572,18.45918635170604,256
HPCA_17_008.txt,14.02221865932863,12.177040419163514,HPCA,7849,"
Exploring the design space of the memory hierarchy requires the use of effective methodologies, tools, and
models to evaluate different parameter values. Reuse distance
is of one of the locality models used in the design exploration
and permits analytical cache miss estimation, program characterization, and synthetic trace generation. Unfortunately, the
reuse distance is limited to a single locality granularity. Hence,
it is not a suitable model for caches with hybrid line sizes,
such as sectored caches, an increasingly popular choice for
large caches. In this work, we introduce a generalization to
the reuse distance, which is able to capture locality seen at
multiple granularities. We refer to it as Hierarchical Reuse
Distance (HRD). The proposed model has same profiling and
synthesis complexity as the traditional reuse distance, and our
results show that HRD reduces the average miss rate error
on sectored caches by more than three times. In addition, it
has superior characteristics in exploring multi-level caches with
conventional single line size. For instance, our method increases
the accuracy on L2 and L3 by a factor of 4 and converges three
orders of magnitude faster.

",16.04434344847333,14.99150537634409,187
HPCA_17_009.txt,16.801107222006546,16.52410450151216,HPCA,6701,"
The increasingly-stringent power and energy requirements of emerging
embedded applications have led to a strong recent interest in aggressive power
gating techniques. Conventional techniques for aggressive power gating perform
module-based power gating in processors, where power domains correspond to
RTL modules. We observe that there can be significant power benefits from
module-oblivious power gating, where power domains can include an arbitrary set
of gates, possibly from multiple RTL modules. However, since it is not possible
to infer the activity of module-oblivious power domains from software alone,
conventional software-based power management techniques cannot be applied for
module-oblivious power gating in processors. Also, since module-oblivious domains
are not encapsulated with a well-defined port list and functionality like RTL
modules, hardware-based management of module-oblivious domains is prohibitively
expensive. In this paper, we present a technique for low-cost management of moduleoblivious power domains in embedded processors. The technique involves symbolic
simulation-based co-analysis of a processor’s hardware design and a software
binary to derive profitable and safe power gating decisions for a given set of
module-oblivious domains when the software binary is run on the processor. Our
technique is automated, does not require programmer intervention, and incurs low
management overhead. We demonstrate that module-oblivious power gating based
on our technique reduces leakage energy by 2x with respect to state-of-the-art
aggressive module-based power gating for a common embedded processor.

",17.879347455551382,17.66774104683196,244
HPCA_17_010.txt,15.664080640945329,13.288149011227883,HPCA,8517,"
Energy management is a key issue for mobile
devices. On current Android devices, power management relies
heavily on OS modules known as governors. These modules are
created for various hardware components, including the CPU,
to support DVFS. They implement algorithms that attempt to
balance performance and power consumption.

In this paper we make the observation that the existing
governors are (1) general-purpose by nature (2) focused on
power reduction and (3) are not energy-optimal for many applications. We thus establish the need for an application-specific
approach that could overcome these drawbacks and provide
higher energy efficiency for suitable applications. We also show
that existing methods manage power and performance in an
independent and isolated fashion and that co-ordinated control
of multiple components can save more energy. In addition, we
note that on mobile devices, energy savings cannot be achieved
at the expense of performance. Consequently, we propose a
solution that minimizes energy consumption of specific applications while maintaining a user-specified performance target.
Our solution consists of two stages: (1) offline profiling and
(2) online controlling. Utilizing the offline profiling data of the
target application, our control theory based online controller
dynamically selects the optimal system configuration (in this
paper, combination of CPU frequency and memory bandwidth)
for the application, while it is running. Our energy management
solution is tested on a Nexus 6 smartphone with 6 real-world
applications. We achieve 4 — 31% better energy than default
governors with a worst case performance loss of < 1%.

",17.03242331605538,14.727142857142855,252
HPCA_17_011.txt,14.796072564617155,13.555408536723977,HPCA,7652,"
Power capping is a mechanism to ensure that the
power consumption of clusters does not exceed the provisioned
resources. A fast power capping method allows for a safe
over-subscription of the rated power distribution devices,
provides equipment protection, and enables large clusters to
participate in demand-response programs. However, current
methods have a slow response time with a large actuation
latency when applied across a large number of servers as
they rely on hierarchical management systems. We propose
a fast decentralized power capping (DPC) technique that
reduces the actuation latency by localizing power management
at each server. The DPC method is based on a maximum
throughput optimization formulation that takes into account
the workloads priorities as well as the capacity of circuit
breakers. Therefore, DPC significantly improves the cluster
performance compared to alternative heuristics. We implement
the proposed decentralized power management scheme on a
real computing cluster. Compared to state-of-the-art hierarchical methods, DPC reduces the actuation latency by 72% up to
86% depending on the cluster size. In addition, DPC improves
the system throughput performance by 16%, while using only
0.02% of the available network bandwidth. We describe how to
minimize the overhead of each local DPC agent to a negligible
amount. We also quantify the traffic and fault resilience of our
decentralized power capping approach.

",15.186304673781336,13.986363636363638,222
HPCA_17_012.txt,13.814696862603302,11.768539534883722,HPCA,7381,"

In datacenter networks, big scale, high performance and faulttolerance, low-cost, and graceful expandability are pursued
features. Recently, random regular networks, as the Jellyfish,
have been proposed for satisfying these stringent requirements. However, their completely unstructured design entails
several drawbacks. As a related alternative, in this paper we
propose Random Folded Clos (RFC) networks. They constitute a compromise between total randomness and maintaining
some topological structure. As it will be shown, RFCs preserve important properties of Clos networks that provide a
straightforward deadlock-free equal-cost multi-path routing
and enough randomness to gracefully expanding. These networks are minutely compared, in topological and cost terms,
against fat-trees, orthogonal fat-trees and random regular
graphs. Also, experiments are carried out to simulate their
performance under synthetic traffics that emulate common
loads in datacenters. It is shown that RFCs constitute an interesting alternative to currently deployed networks since they
appropriately balance all the important design requirements.
Moreover, they do that at much lower cost than the fat-tree,
their natural competitor. Being able up to connect the same
number of compute nodes, saving up to 95% of the cost, and
giving similar performance.

",15.062637766997835,13.096439393939395,193
HPCA_17_013.txt,16.34394709866367,14.253208814765689,HPCA,11073,"
",NA,NA,1
HPCA_17_014.txt,15.017285484658586,13.461488003376108,HPCA,8828,"

Owing to increasing demand of faster and larger DRAM system, the DRAM system accounts for a large portion of the
total power consumption of computing systems. As memory traffic and DRAM bandwidth grow, the row activation
and I/O power consumptions are becoming major contributors to total DRAM power consumption. Thus, reducing
row activation and I/O power consumptions has big potential for improving the power and energy efficiency of the
computing systems. To this end, we propose a partial row
activation scheme for memory writes, in which DRAM is rearchitected to mitigate row overfetching problem of modern
DRAMs and to reduce row activation power consumption. In
addition, accompanying I/O power consumption in memory
writes is also reduced by transferring only a part of cache
line data that must be written to partially opened rows. In
our proposed scheme, partial rows ranging from a one-eighth
row to a full row can be activated to minimize row activation
granularity for memory writes and the full bandwidth of the
conventional DRAM can be maintained for memory reads.
Our partial row activation scheme is shown to reduce total
DRAM power consumption by up to 32% and 23% on average, which outperforms previously proposed schemes in
DRAM power saving with almost no performance loss.

",17.28802050969988,16.42614555256065,213
HPCA_17_015.txt,16.12509753747121,15.410748178425454,HPCA,8999,"

As the amount of digital data the world generates explodes,
data centers and HPC systems that process this big data will
require high bandwidth and high capacity main memory. Unfortunately, conventional memory technologies either provide
high memory capacity (e.g., DDRx memory) or high bandwidth
(GDDRx memory), but not both. Memory networks, which
provide both high bandwidth and high capacity memory by
connecting memory modules together via a network of pointto-point links, are promising future memory candidates for
data centers and HPCs. In this paper, we perform the first
exploration to understand the power characteristics of memory
networks. We find idle I/O links to be the biggest power
contributor in memory networks. Subsequently, we study idle
VO power in more detail. We evaluate well-known circuitlevel I/O power control mechanisms such as rapid on off,
variable link width, and DVFS. We also adapt prior works
on memory power management to memory networks. The
adapted schemes together reduce I/O power by 32% and 21%,
on average, for big and small networks, respectively. We also
explore novel power management schemes specifically targeting
memory networks, which yield another 29% and 17% average
T/O power reduction for big and small networks, respectively.

",15.6451,13.843445544554456,204
HPCA_17_016.txt,15.17526167202011,13.306387691765483,HPCA,10207,"
DRAM is the primary technology used for main memory in
modern systems. Unfortunately, as DRAM scales down to smaller
technology nodes, it faces key challenges in both data integrity
and latency, which strongly affects overall system reliability and
performance. To develop reliable and high-performance DRAMbased main memory in future systems, it is critical to characterize,
understand, and analyze various aspects (e.g., reliability, latency)
of existing DRAM chips. To enable this, there is a strong need
for a publicly-available DRAM testing infrastructure that can
flexibly and efficiently test DRAM chips in a manner accessible
to both software and hardware developers.

This paper develops the first such infrastructure, SoftMC (Soft
Memory Controller), an FPGA-based testing platform that can
control and test memory modules designed for the commonlyused DDR (Double Data Rate) interface. SoftMC has two key
properties: (i) it provides flexibility to thoroughly control memory
behavior or to implement a wide range of mechanisms using DDR
commands; and (ii) it is easy to use as it provides a simple and
intuitive high-level programming interface for users, completely
hiding the low-level details of the FPGA.

We demonstrate the capability, flexibility, and programming
ease of SoftMC with two example use cases. First, we implement
a test that characterizes the retention time of DRAM cells. Experimental results we obtain using SoftMC are consistent with
the findings of prior studies on retention time in modern DRAM,
which serves as a validation of our infrastructure. Second, we validate two recently-proposed mechanisms, which rely on accessing
recently-refreshed or recently-accessed DRAM cells faster than
other DRAM cells. Using our infrastructure, we show that the
expected latency reduction effect of these mechanisms is not observable in existing DRAM chips, which demonstrates the usefulness of SoftMC in testing new ideas on existing memory modules.
We discuss several other use cases of SoftMC, including the ability to characterize emerging non-volatile memory modules that
obey the DDR standard. We hope that our open-source release of
SoftMC fills a gap in the space of publicly-available experimental
memory testing infrastructures and inspires new studies, ideas,
and methodologies in memory system design.

",18.65328738118533,17.10039215686275,359
HPCA_17_017.txt,13.738329385521528,12.130156141974563,HPCA,8947,"
Future SoCs are expected to have irregular onchip topologies, either at design time due to heterogeneity
in the size of core/accelerator tiles, or at runtime due to
link/node failures or power-gating of network elements such
as routers/router datapaths. A key challenge with irregular
topologies is that of routing deadlocks (cyclic dependence
between buffers), since conventional XY or turn-model based
approaches are no longer applicable.
Most prior works in heterogeneous SoC design, resiliency,
and power-gating, have addressed the deadlock problem by
constructing spanning trees over the physical topology; messages are routed via the root removing cyclic dependencies.
However, this comes at a cost of tree construction at runtime,
and increased latency and energy for certain flows as they are
forced to use non-minimal routes. In this work, we sweep the
design space of possible topologies as the number of disconnected components (links/routers) increase, and demonstrate
that while most of the resulting topologies are deadlock prone
(i.e., have cycles), the injection rates at which they deadlock are
often much higher than the injection rates of real applications,
making the current solutions highly conservative.
We propose a novel framework for deadlock-freedom called
Static Bubble, that can be applied at design time to the
underlying mesh topology, and guarantees deadlock-freedom
for any runtime topology derived from this mesh due to powergating or failure of router/link. We present an algorithm to
augment a subset of routers in any n×m mesh (21 routers in
a 64-core mesh) with an additional buffer called static bubble,
such that any dependence chain has at least one static bubble.
We also present the microarchitecture of a low-cost (less than
1% overhead) FSM at every router to activate one static bubble
for deadlock recovery. Static Bubble enhances existing solutions
for NoC resiliency and power-gating by providing up to 30%
less network latency, 4x more throughput and 50% less EDP.

",17.879347455551382,18.845061728395063,325
HPCA_17_018.txt,15.380049246889268,13.788146972688434,HPCA,7579,"

Optical on-chip communication is considered a promising candidate to overcome latency and energy bottlenecks of electrical interconnects. Although recently proposed hybrid Networks-on-chip (NoCs), which implement both electrical and optical links, improve power
efficiency, they often fail to combine these two interconnect technologies efficiently and suffer from considerable
laser power overheads caused by high-bandwidth optical links. We argue that these overheads can be avoided
by inserting a higher quantity of low-bandwidth optical
links in a topology, as this yields lower optical loss and
in turn laser power. Moreover, when optimally combined with electrical links for short distances, this can
be done without trading off latency.

We present the effectiveness of this concept with Lego,
our hybrid, mesh-based NoC that provides high power
efficiency by utilizing electrical links for local traffic, and
low-bandwidth optical links for long distances. Electrical links are placed systematically to outweigh the serialization delay introduced by the optical links, simplify
router microarchitecture, and allow to save optical resources. Our routing algorithm always chooses the link
that offers the lowest latency and energy. Compared to
state-of-the-art proposals, Lego increases throughputper-watt by at least 40%, and lowers latency by 35% on
average for synthetic traffic. On SPLASH-2/PARSEC
workloads, Lego improves power efficiency by at least
37% (up to 3.5x).

",17.755912252390015,16.24801801801802,224
HPCA_17_019.txt,14.081028152550157,12.692906505907253,HPCA,7565,"
Server workloads benefit from execution on manycore processors due to their massive request-level parallelism.
A key characteristic of server workloads is the large instruction
footprints. While a shared last-level cache (LLC) captures the
footprints, it necessitates a low-latency network-on-chip (NOC)
to minimize the core stall time on accesses serviced by the LLC.
As strict quality-of-service requirements preclude the use of
lean cores in server processors, we observe that even state-ofthe-art single-cycle multi-hop NOCs are far from ideal because
they impose significant NOC-induced delays on the LLC access
latency, and diminish performance.

Most of the NOC delay is due to per-hop resource allocation.
In this paper, we take advantage of proactive resource allocation (PRA) to eliminate per-hop resource allocation time in
single-cycle multi-hop networks to reach a near-ideal network
for servers. PRA is undertaken during (1) the time interval
in which it is known that LLC has the requested data, but
the data is not yet ready, and (2) the time interval in which
a packet is stalled in a router because the required resources
are dedicated to another packet. Through detailed evaluation
targeting a 64-core processor and a set of server workloads, we
show that our proposal improves system performance by 12%
over the state-of-the-art single-cycle multi-hop mesh NOC.

",16.061879428645646,15.454482758620696,233
HPCA_17_020.txt,16.1505439594256,14.453668402336806,HPCA,7819,"
In future performance improvement of the basic building block of supercomputers has to come through
increased integration enabled by 3D (vertical) and 2.5D (horizontal) die-stacking. But to take advantage of this integration
we need an interconnection network between the memory and
compute die that not only can provide an order of magnitude
higher bandwidth but also consume an order of magnitude less
power than today’s state of the art electronic interconnects. We
show how Arrayed Waveguide Grating Router-based photonic
interconnects implemented on the silicon interposer can be used
to realize a 16 <x 16 photonic Network-on-Chip (NoC) with a
bisection bandwidth of 16 Tb/s. We propose a baseline network,
which consumes 2.57 pJ/bit assuming 100% utilization. We
show that the power is dominated by the electro-optical
interface of the transmitter, which can be reduced by a more
aggressive design that improves the energy per bit to 0.454
pJ/bit at 100% utilization. Compared to recently proposed
interposer-based electrical NoC’s we show an average performance improvement of 25% on the PARSEC benchmark suite
on a 64-core system using the Gem5 simulation framework.

",18.062587368997235,17.45955497382199,196
HPCA_17_021.txt,14.486035367078124,12.220710295961574,HPCA,7599,"

This paper presents SecMC, a secure memory controller
that provides efficient memory scheduling with a strong
quantitative security guarantee against timing channel
attacks. The first variant, named SecMC-NI, eliminates timing channels while allowing a tight memory
schedule by interleaving memory requests that access
different banks or ranks. Experimental results show
that SecMC-NI significantly (45% on average) improves
the performance of the best known scheme that does
not rely on restricting memory placements. To further
improve the performance, the paper proposes SecMCBound, which enables trading-off security for performance with a quantitative information theoretic bound
on information leakage. The experimental results show
that allowing small information leakage can yield significant performance improvements.

",18.458006810337128,16.824,114
HPCA_17_022.txt,14.980163454887272,13.093078826976583,HPCA,9525,"
Previous work has demonstrated that systems with
unencrypted DRAM interfaces are susceptible to cold boot
attacks - where the DRAM in a system is frozen to give it
sufficient retention time and is then re-read after reboot, or
is transferred to an attacker’s machine for extracting sensitive
data. This method has been shown to be an effective attack
vector for extracting disk encryption keys out of locked devices.
However, most modern systems incorporate some form of
data scrambling into their DRAM interfaces making cold boot
attacks challenging. While first added as a measure to improve
signal integrity and reduce power supply noise, these scramblers today serve the added purpose of obscuring the DRAM
contents. It has previously been shown that scrambled DDR3
systems do not provide meaningful protection against cold
boot attacks. In this paper, we investigate the enhancements
that have been introduced in DDR4 memory scramblers in
the 6” generation Intel Core (Skylake) processors. We then
present an attack that demonstrates these enhanced DDR4
scramblers still do not provide sufficient protection against cold
boot attacks. We detail a proof-of-concept attack that extracts
memory resident AES keys, including disk encryption keys.

The limitations of memory scramblers we point out in this
paper motivate the need for strong yet low-overhead fullmemory encryption schemes. Existing schemes such as Intel’s
SGX can effectively prevent such attacks, but have overheads
that may not be acceptable for performance-sensitive applications. However, it is possible to deploy a memory encryption
scheme that has zero performance overhead by forgoing integrity checking and replay attack protections afforded by Intel
SGX. To that end, we present analyses that confirm modern
stream ciphers such as ChaCha8 are sufficiently fast that it is
now possible to completely overlap keystream generation with
DRAM row buffer access latency, thereby enabling the creation
of strongly encrypted DRAMs with zero exposed latency.
Adopting such low-overhead measures in future generation of
products can effectively shut down cold boot attacks in systems
where the overhead of existing memory encryption schemes
is unacceptable. Furthermore, the emergence of non-volatile
DIMMs that fit into DDR4 buses is going to exacerbate the risk
of cold boot attacks. Hence, strong full memory encryption is
going to be even more crucial on such systems.

",17.370101895934148,15.597051671732526,379
HPCA_17_023.txt,13.506818969022046,11.307973684210527,HPCA,7214,"
Path ORAM (Oblivious RAM) is a recently proposed ORAM protocol for preventing information leakage from
memory access sequences. It receives wide adoption due to its
simplicity, practical efficiency and asymptotic efficiency. However,
Path ORAM has extremely large memory bandwidth demand,
leading to severe memory competition in server settings, e.g., a
server may service one application that uses Path ORAM and
one or multiple applications that do not. While Path ORAM
synchronously and intensively uses all memory channels, the
non-secure applications often exhibit low access intensity and
large channel level imbalance. Traditional memory scheduling
schemes lead to wasted memory bandwidth to the system and
large performance degradation to both types of applications.

In this paper, we propose CP-ORAM, a Cooperative Path
ORAM design, to effectively schedule the memory requests from
both types of applications. CP-ORAM consists of three schemes:
P-Path,R-Path, and W-Path. P-Path assigns and enforces scheduling priority for effective memory bandwidth sharing. R-Path
maximizes bandwidth utilization by proactively scheduling read
operations from the next Path ORAM access. W-Path mitigates
contention on busy memory channels with write redirection. We
evaluate CP-ORAM and compare it to the state-of-the-art. Our
results show that CP-ORAM helps to achieve 20% performance
improvement on average over the baseline Path ORAM for the
secure application in a four-channel server setting.

 

",15.903189008614273,14.31887554585153,231
HPCA_17_024.txt,15.903189008614273,14.206601771469632,HPCA,8089,"
Information leaks based on timing side channels in
computing devices have serious consequences for user security
and privacy. In particular, malicious applications in multi-user
systems such as data centers and cloud-computing environments can exploit memory timing as a side channel to infer
a victim’s program access patterns/phases. Memory timing
channels can also be exploited for covert communications by
an adversary.
We propose Camouflage, a hardware solution to mitigate
timing channel attacks not only in the memory system, but
also along the path to and from the memory system (e.g.
NoC, memory scheduler queues). Camouflage introduces the
novel idea of shaping memory requests’ and responses’ interarrival time into a pre-determined distribution for security
purposes, even creating additional fake traffic if needed.
This limits untrusted parties (either cloud providers or coscheduled clients) from inferring information from another
security domain by probing the bus to and from memory, or
analyzing memory response rate. We design three different
memory traffic shaping mechanisms for different security
scenarios by having Camouflage work on requests, responses,
and bi-directional (both) traffic. Camouflage is complementary
to ORAMs and can be optionally used in conjunction with
ORAMs to protect information leaks via both memory access
timing and memory access patterns.
Camouflage offers a tunable trade-off between system security and system performance. We evaluate Camouflage’s security and performance both theoretically and via simulations,
and find that Camouflage outperforms state-of-the-art solutions
in performance by up to 50%.

",18.14513906131404,16.951073025335322,248
HPCA_17_025.txt,13.715363384822343,12.141387440892995,HPCA,8949,"
With current DRAM technology reaching its limit,
emerging heterogeneous memory systems have become attractive to continue scaling memory performance. This paper
argues for using a small, fast memory closer to the processor
as part of a flat address space where the memory system
is composed of two or more memory types. OS-transparent
management of such memory has been proposed in prior works
such as CAMEO and Part of Memory (PoM). Data migration
is typically handled either at coarse granularity with high
bandwidth overheads (as in PoM) or at fine granularity with
low hit rate (as in CAMEO). Prior work uses restricted address
mapping from only congruence groups in order to simplify the
mapping. At any time, only one page (block) from a congruence
group is resident in the fast memory.

In this paper, we present a flat address space organization
called SILC-FM that uses large granularity but allows subblocks from two pages to coexist in an interleaved fashion in
fast memory. Data movement is done at subblocked granularity,
avoiding fetching of useless subblocks and consuming less
bandwidth compared to migrating the entire large block.
SILC-FM can achieve more spatial locality hits than CAMEO
and PoM due to page-level operation and interleaving blocks
respectively. The interleaved subblock placement improves
performance by 55% on average over a static placement scheme
without data migration. We also selectively lock hot blocks
to prevent them from being involved in hardware swapping
operations. Additional features such as locking, associativity
and bandwidth balancing improve performance by 11%, 8%,
and 8% respectively, resulting in a total of 82% performance
improvement over a no migration static placement scheme.
Compared to the best state-of-the-art scheme, SILC-FM gets
performance improvement of 36% with 13% energy savings.

",15.705129121369687,13.980887372013651,294
HPCA_17_026.txt,14.214605359287706,12.109275187702554,HPCA,9159,"
Non-volatile memory (NVM) is emerging as a
fast byte-addressable alternative for storing persistent data.
Ensuring atomic durability in NVM requires logging. Existing
techniques have proposed software logging either by using
streaming stores for an undo log; or, by relying on the combination of cl£1lush and mfence for a redo log. These techniques
are suboptimal because they waste precious execution cycles to
implement logging, which is fundamentally a data movement
operation. We propose ATOM, a hardware log manager based
on undo logging that performs the logging operation out of the
critical path. We present the design principles behind ATOM
and two techniques to optimize its performance. Our results
show that ATOM achieves an improvement of 27% to 33% for
micro-benchmarks and 60% for TPC-C over a baseline undo
log design.

",13.925175675911131,12.734391534391538,135
HPCA_17_027.txt,14.036809694862693,12.039594232756201,HPCA,7505,"

Modern solid state drives (SSDs) unnecessarily confine host programs to the conventional block I/O interface, leading to suboptimal performance and resource
under-utilization. Recent attempts to replace or extend
this interface with a key-value-oriented interface and/or
built-in support for transactions offer some improvements, but the details of their implementations make
them a poor match for many applications.

This paper presents the key-addressable, multi-log
SSD (KAML), an SSD with a key-value interface that
uses a novel multi-log architecture and stores data as
variable-sized records rather than fixed-sized sectors.
Exposing a key-value interface allows applications to
remove a layer of indirection between application-level
keys (e.g., database record IDs or file inode numbers)
and data stored in the SSD. KAML also provides
native transaction support tuned to support fine-grained
locking, achieving improved performance compared
to previous designs that require page-level locking.
Finally, KAML includes a caching layer analogous to
a conventional page cache that leverages host DRAM
to improve performance and provides additional transactional features.

We have implemented a prototype of KAML on a
commercial SSD prototyping platform, and our results
show that compared with existing key-value stores,
KAML improves the performance of online transaction
processing (OLTP) workloads by 1.1x - 4.0x, and
NoSQL key-value store applications by 1.1x —- 3.0x.

",17.613555460941566,18.000204996796928,229
HPCA_17_028.txt,15.388794176250101,13.51766647462036,HPCA,7877,"
Multi Level Cell (MLC) Phase Change Memory
(PCM) is an enhancement of PCM technology, which provides
higher capacity by allowing multiple digital bits to be stored in
a single PCM cell. However, the retention time of MLC PCM is
limited by the resistance drift problem and refresh operations
are required. Previous work shows that there exists a trade-off
between write latency and retention—a write scheme with more
SET iterations and smaller current provides a longer retention
time but at the cost of a longer write latency. Otherwise, a write
scheme with fewer SET iterations achieves high performance
for writes but requires a greater number of refresh operations
due to its significantly reduced retention time, and this hurts
the lifetime of MLC PCM.

In this paper, we show that only a small part of memory
(i.e., hot memory regions) will be frequently accessed in a given
period of time. Based on such an observation, we propose
Region Retention Monitor (RRM), a novel structure that records
and predicts the write frequency of memory regions. For
every incoming memory write operation, RRM select a proper
write latency for it. Our evaluations show that RRM helps
the system improves the balance between system performance
and memory lifetime. On the performance side, the system
with RRM bridges 77.2% of the performance gap between
systems with long writes and systems with short writes. On
the lifetime side, a system with RRM achieves a lifetime of 6.4
years, while systems using only long writes and short writes
achieve lifetimes of 10.6 and 0.3 years, respectively. Also, we can
easily control the aggressiveness of RRM through an attribute
called hot_threshold. A more aggressively configured RRM can
achieve the performance which is only 3.5% inferior than the
system using static short writes, while still achieve a lifetime
of 5.78 years.

",15.247664890283005,13.4568399339934,311
HPCA_17_029.txt,16.192481085580287,15.134003714362201,HPCA,7683,"

Reliability to soft errors is an increasingly important
issue as technology continues to shrink. In this paper,
we show that applications exhibit different reliability
characteristics on big, high-performance cores versus
small, power-efficient cores, and that there is significant opportunity to improve system reliability through
reliability-aware scheduling on heterogeneous multicore
processors. We monitor the reliability characteristics of
all running applications, and dynamically schedule applications to the different core types in a heterogeneous
multicore to maximize system reliability. Reliabilityaware scheduling improves reliability by 25.4% on average (and up to 60.2%) compared to performance-optimized scheduling on a heterogeneous multicore processor with two big cores and two small cores, while degrading performance by 6.3% only. We also introduce
a novel system-level reliability metric for multiprogram
workloads on (heterogeneous) multicores. We further
show that our reliability-aware scheduler is robust across
core count, number of big and small cores, and their frequency settings. The hardware cost in support of our
reliability-aware scheduler is limited to 296 bytes per
core.

",18.99602597827317,19.563781512605043,174
HPCA_17_030.txt,15.797470592208708,13.975348959827226,HPCA,8319,"
In 2013, U.S. data centers accounted for 2.2%
of the country’s total electricity consumption, a figure that
is projected to increase rapidly over the next decade. Many
important workloads are interactive, and they demand strict
levels of quality-of-service (QoS) to meet user expectations,
making it challenging to reduce power consumption due to
increasing performance demands.

This paper introduces Hipster, a technique that combines
heuristics and reinforcement learning to manage latency-critical
workloads. Hipster’s goal is to improve resource efficiency in
data centers while respecting the QoS of the latency-critical
workloads. Hipster achieves its goal by exploring heterogeneous
multi-cores and dynamic voltage and frequency scaling (DVFS).
To improve data center utilization and make best usage of the
available resources, Hipster can dynamically assign remaining
cores to batch workloads without violating the QoS constraints
for the latency-critical workloads. We perform experiments
using a 64-bit ARM big.LITTLE platform, and show that,
compared to prior work, Hipster improves the QoS guarantee for
Web-Search from 80% to 96%, and for Memcached from 92%
to 99%, while reducing the energy consumption by up to 18%.

",16.954822765917157,16.088986175115206,192
HPCA_17_031.txt,13.848795221049148,12.409417589126917,HPCA,6797,"
Task colocation improves datacenter utilization
but introduces resource contention for shared hardware. In
this setting, a particular challenge is balancing performance
and fairness. We present Cooper, a game-theoretic framework
for task colocation that provides fairness while preserving
performance. Cooper predicts users’ colocation preferences
and finds stable matches between them. Its colocations satisfy
preferences and encourage strategic users to participate in
shared systems. Given Cooper’s colocations, users’ performance
penalties are strongly correlated to their contributions to
contention, which is fair according to cooperative game theory.
Moreover, its colocations perform within 5% of prior heuristics.

",15.151101081350806,14.87905775075988,96
HPCA_17_032.txt,15.18875894171654,13.290892517165187,HPCA,8219,"

In the near future, die-stacked DRAM will be increasingly present in conjunction with off-chip memories in
hybrid memory systems. Research on this subject revolves around using the stacked memory as a cache
or as part of a flat address space. This paper proposes MemPod, a scalable and efficient memory management mechanism for flat address space hybrid memories. MemPod monitors memory activity and periodically migrates the most frequently accessed memory
pages to the faster on-chip memory. MemPod’s partitioned architectural organization allows for efficient
scaling with memory system capabilities. Further, a
big data analytics algorithm is adapted to develop an
efficient, low-cost activity tracking technique.

MemPod improves the average main memory access
time of multi-programmed workloads, by up to 29% (9%
on average) compared to the state of the art, and that
will increase as the differential between memory speeds
widens. MemPod’s novel activity tracking approach
leads to significant cost reduction (~12800x lower storage space requirements) and improved future prediction
accuracy over prior work which maintains a separate
counter per page.

",16.373557378465907,15.125250000000001,177
HPCA_17_033.txt,15.237561874862553,13.503597727833274,HPCA,8020,"
Brain-inspired hyperdimensional (HD) computing
emulates cognition tasks by computing with hypervectors as
an alternative to computing with numbers. At its very core,
HD computing is about manipulating and comparing large
patterns, stored in memory as hypervectors: the input symbols
are mapped to a hypervector and an associative search is performed for reasoning and classification. For every classification
event, an associative memory is in charge of finding the closest
match between a set of learned hypervectors and a query
hypervector by using a distance metric. Hypervectors with
the i.i.d. components qualify a memory-centric architecture to
tolerate massive number of errors, hence it eases cooperation of
various methodological design approaches for boosting energy
efficiency and scalability. This paper proposes architectural
designs for hyperdimensional associative memory (HAM) to
facilitate energy-efficient, fast, and scalable search operation
using three widely-used design approaches. These HAM designs
search for the nearest Hamming distance, and linearly scale
with the number of dimensions in the hypervectors while
exploring a large design space with orders of magnitude
higher efficiency. First, we propose a digital CMOS-based HAM
(D-HAM) that modularly scales to any dimension. Second,
we propose a resistive HAM (R-HAM) that exploits timing
discharge characteristic of nonvolatile resistive elements to
approximately compute Hamming distances at a lower cost.
Finally, we combine such resistive characteristic with a currentbased search method to design an analog HAM (A-HAM)
that results in faster and denser alternative. Our experimental
results show that R-HAM and A-HAM improve the energydelay product by 9.6x and 1347x compared to D-HAM
while maintaining a moderate accuracy of 94% in language
recognition.

",18.878054631974784,18.444470588235294,276
HPCA_17_034.txt,16.256793504265534,14.115459469408545,HPCA,6319,"
With the emergence of data science, graph computing has become increasingly important these days. Unfortunately,
graph computing typically suffers from poor performance when
mapped to modern computing systems because of the overhead
of executing atomic operations and inefficient utilization of the
memory subsystem. Meanwhile, emerging technologies, such as
Hybrid Memory Cube (HMC), enable the processing-in-memory
(PIM) functionality with offloading operations at an instruction
level. Instruction offloading to the PIM side has considerable
potentials to overcome the performance bottleneck of graph
computing. Nevertheless, this functionality for graph workloads
has not been fully explored, and its applications and shortcomings
have not been well identified thus far.

In this paper, we present GraphPIM, a full-stack solution
for graph computing that achieves higher performance using
PIM functionality. We perform an analysis on modern graph
workloads to assess the applicability of PIM offloading and
present hardware and software mechanisms to efficiently make
use of the PIM functionality. Following the real-world HMC 2.0
specification, GraphPIM provides performance benefits for graph
applications without any user code modification or ISA changes.
In addition, we propose an extension to PIM operations that can
further bring performance benefits for more graph applications.
The evaluation results show that GraphPIM achieves up to a
2.4x speedup with a 37% reduction in energy consumption.

",17.80541091248751,16.065697674418608,218
HPCA_17_035.txt,15.931365700501846,14.847334247223923,HPCA,7395,"

Computational applications are subject to various kinds
of numerical errors, ranging from deterministic roundoff errors to soft errors caused by non-deterministic bit
flips, which do not lead to application failure but corrupt application results. Non-deterministic bit flips are
typically mitigated in hardware using various error correcting codes (ECC). But in practice, due to performance and cost concerns, these techniques do not guarantee error-free execution. On large-scale computing
platforms, soft errors occur with non-negligible probability in RAM and on the CPU, and it has become clear
that applications must tolerate them. For some applications, this tolerance is intrinsic as result quality can
remain acceptable even in the presence of soft errors
{e.g., data analysis applications, multimedia applications). Tolerance can also be built into the application,
resolving data corruptions in software during application execution. By contrast, today’s optical networks
hold on to a rigid error-free standard, which imposes
limits on network performance scalability. In this work
we propose high-bandwidth, low-latency approximate
networks with the following three features: (1) Optical links that exploit multi-level quadrature amplitude modulation (QAM) for achieving high bandwidth;
(2) Avoidance of forward error correction (FEC), which
makes optical link error-prone but affords lower latency;
and (3) The use of symbol mapping coding between
bit sequence and QAM to ensure data integrity that is
sufficient for practical soft-error-tolerant applications.
Discrete-event simulation results for application benchmarks show that approx networks achieve speedups up
to 2.94 when compared to conventional networks.

",18.001758247042904,17.89984126984127,256
HPCA_17_036.txt,14.734532635910934,12.778509974055343,HPCA,4677,"
This paper presents the Compute Cache architecture that enables in-place computation in caches. Compute
Caches uses emerging bit-line SRAM circuit technology to repurpose existing cache elements and transforms them into active very large vector computational units. Also, it significantly
reduces the overheads in moving data between different levels
in the cache hierarchy.

Solutions to satisfy new constraints imposed by Compute
Caches such as operand locality are discussed. Also discussed
are simple solutions to problems in integrating them into a
conventional cache hierarchy while preserving properties such
as coherence, consistency, and reliability.

Compute Caches increase performance by 1.9x and reduce
energy by 2.4x for a suite of data-centric applications, including text and database query processing, cryptographic kernels,
and in-memory checkpointing. Applications with larger fraction of Compute Cache operations could benefit even more, as
our micro-benchmarks indicate (54x throughput, 9x dynamic
energy savings).

",17.122413403193683,16.548395303326817,149
HPCA_17_037.txt,15.586200849614439,13.698817913884092,HPCA,8095,"
Contemporary server workloads feature massive
instruction footprints stemming from deep, layered software
stacks. The active instruction working set of the entire stack
can easily reach into megabytes, resulting in frequent frontend stalls due to instruction cache misses and pipeline flushes
due to branch target buffer (BTB) misses. While a number of
techniques have been proposed to address these problems, every
one of them requires dedicated metadata structures, translating
into significant storage and complexity costs.

In this paper, we ask the question whether it is possible
to achieve high-performance control flow delivery without the
metadata costs of prior techniques. We revisit a previously proposed approach of branch-predictor-directed prefetching, which
leverages just the branch predictor and BTB to discover and
prefetch the missing instruction cache blocks by exploring the
program control flow ahead of the core front-end. Contrary
to conventional wisdom, we find that this approach can be
effective in covering instruction cache misses in modern CMPs
with long LLC access latencies and multi-MB server binaries.
Our first contribution lies in explaining the reasons for the
efficacy of branch-predictor-directed prefetching. Our second
contribution is in Boomerang, a metadata-free architecture for
control flow delivery. Boomerang leverages a branch-predictordirected prefetcher to discover and prefill not only the instruction
cache blocks, but also the missing BTB entries. Crucially, we
demonstrate that the additional hardware cost required to
identify and fill BTB misses is negligible. Our experimental
evaluation shows that Boomerang matches the performance of the
state-of-the-art control flow delivery scheme without the latter’s
high metadata and complexity overheads.

",17.332850977054683,15.980548885077187,267
HPCA_17_038.txt,15.034775161231977,13.240230430432494,HPCA,8672,"
Higher integration lowers total cost of ownership
(TCO) in the data center by reducing equipment cost and
lowering energy consumption. However, higher integration also
makes it difficult to achieve guaranteed quality of service (QoS)
for shared resources. Unlike many other resources, memory
bandwidth cannot be finely controlled by software in existing
systems. As a result, many systems running critical, bandwidthsensitive applications remain underutilized to protect against
bandwidth interference.

In this paper, we propose a novel hardware architecture
allowing practical, software-controlled partitioning of memory
bandwidth. Proportionally Allocated Bandwidth at the Source
and Target (PABST) precisely controls the bandwidth of applications by throttling request rates at the source and prioritizes
requests at the target. We show that PABST is work conserving,
such that excess bandwidth beyond the requested allocation will
not go unused. For applications sensitive to memory latency,
we pair PABST with a simple priority scheme at the memory
controller. We show that when combined, the system is able to
lower TCO by providing performance isolation across a wide
range of workloads, even when co-located with memory-intensive
background jobs.

",16.32212239822248,15.136206261510129,182
HPCA_17_039.txt,14.999450988514589,13.258359261133311,HPCA,8094,"
Memory access latency has a significant impact
on application performance. Unfortunately, the random access
latency of DRAM has been scaling relatively slowly, and often
directly affects the critical path of execution, especially for
applications with insufficient locality or memory-level parallelism. The existing low-latency DRAM organizations either
incur significant area overhead or burden the software stack
with non-uniform access latency. This paper proposes two
microarchitectural techniques to provide uniformly low access
time over the entire DRAM chip. The first technique is SALAD,
a new DRAM device architecture that provides symmetric
access latency with asymmetric DRAM bank organizations.
Because local regions have lower data transfer time due to
their proximity to the I/O pads, SALAD applies high aspectratio (i.e., low-latency) mats only to remote regions to offset
the difference in data transfer time, resulting in symmetrically
low latency across regions. The second technique is SOUP
(skewed organization of banks with pipelined accesses), which
leverages asymmetry in column access latency within a region
due to non-uniform distance to the column decoders. By
starting I/O transfers as soon as data from near cells arrive,
instead of waiting for the entire column data, SOUP further
saves two memory clock cycles for column accesses for ail
regions. The resulting design, called SOUP-N-SALAD, improves
IPC and EDP by 9.6% (11.2%) and 18.2% (21.8%) over the
baseline DDR4 device, respectively, for memory-intensive SPEC
CPU2006 workloads without any software modifications, while
incurring only 3% (6%) area overhead.

",17.879347455551382,17.662311827956987,254
HPCA_17_040.txt,15.925140325422078,14.542157411360133,HPCA,10092,"

Current control flow integrity (CFI) enforcement approaches

either require instrumenting application executables and even
shared libraries, or are unable to defend against sophisticated attacks due to relaxed security policies, or both; many
of them also incur high runtime overhead. This paper observes that the main obstacle of providing transparent and
strong defense against sophisticated adversaries is the lack
of sufficient runtime control flow information. To this end,
this paper describes FlowGuard, a lightweight, transparent
CFI enforcement approach by a novel reuse of Intel Processor Trace (IPT), a recent hardware feature that efficiently
captures the entire runtime control flow. The main challenge
is that IPT is designed for offline performance analysis and
software debugging such that decoding collected control flow
traces is prohibitively slow on the fly. FlowGuard addresses
this challenge by reconstructing applications’ conservative
control flow graphs (CFG) to be compatible with the compressed encoding format of IPT, and labeling the CFG edges
with credits in the help of fuzzing-like dynamic training. At
runtime, FlowGuard separates fast and slow paths such that
the fast path compares the labeled CFGs with the IPT traces
for fast filtering, while the slow path decodes necessary IPT
traces for strong security. We have implemented and evaluated FlowGuard on a commodity Intel Skylake machine
with IPT support. Evaluation results show that FlowGuard
is effective in enforcing CFI for several applications, while
introducing only small performance overhead. We also show
that, with minor hardware extensions, the performance overhead can be further reduced.

",18.12316971661352,17.232361673414307,248
HPCA_17_041.txt,13.6552832125668,12.027265511632208,HPCA,7097,"

Convolutional neural networks (CNNs) are the heart of
deep learning applications. Recent works PRIME [1]
and ISAAC [2] demonstrated the promise of using resistive random access memory (ReRAM) to perform neural
computations in memory. We found that training cannot
be efficiently supported with the current schemes. First,
they do not consider weight update and complex data
dependency in training procedure. Second, ISAAC attempts to increase system throughput with a very deep
pipeline. It is only beneficial when a large number of
consecutive images can be fed into the architecture. In
training, the notion of batch (e.g. 64) limits the number
of images can be processed consecutively, because the
images in the next batch need to be processed based on
the updated weights. Third, the deep pipeline in ISAAC
is vulnerable to pipeline bubbles and execution stall.

In this paper, we present PipeLayer, a ReRAM-based
PIM accelerator for CNNs that support both training
and testing. We analyze data dependency and weight update in training algorithms and propose efficient pipeline
to exploit inter-layer parallelism. To exploit intra-layer
parallelism, we propose highly parallel design based on
the notion of parallelism granularity and weight replication. With these design choices, PipeLayer enables the
highly pipelined execution of both training and testing,
without introducing the potential stalls in previous work.
The experiment results show that, PipeLayer achieves
the speedup of 42.45x compared with GPU platform on
average. The average energy saving of PipeLayer compared with GPU implementation is 7.17x.

",13.707050652182112,12.896190476190476,250
HPCA_17_042.txt,13.705194708549584,12.088969273074078,HPCA,8390,"
Convolutional Neural Networks (CNN) are very
computation-intensive. Recently, a lot of CNN accelerators based
on the CNN intrinsic parallelism are proposed. However, we
observed that there is a big mismatch between the parallel types
supported by computing engine and the dominant parallel types
of CNN workloads. This mismatch seriously degrades resource
utilization of existing accelerators. In this paper, we propose a
flexible dataflow architecture (FlexFlow) that can leverage the
complementary effects among feature map, neuron, and synapse
parallelism to mitigate the mismatch. We evaluated our design
with six typical practical workloads, it acquires 2-10x performance
speedup and 2.5-10x power efficiency improvement compared
with three state-of-the-art accelerator architectures. Meanwhile,
FlexFlow is highly scalable with growing computing engine scale.

",16.613394197324528,15.726271777003486,125
HPCA_17_043.txt,15.074751617848229,13.073691859866052,HPCA,7697,"
Technology constraints have increasingly led to the
adoption of specialized coprocessors, i.e. hardware accelerators.
The first challenge that computer architects encounter is identifying “‘what to specialize in the program’’. We demonstrate that
this requires precise enumeration of program paths based on
dynamic program behavior. We hypothesize that path-based [4]
accelerator offloading leads to good coverage of dynamic
instructions and improve energy efficiency. Unfortunately, hot
paths across programs demonstrate diverse control flow behavior. Accelerators (typically based on dataflow execution),
often lack an energy-efficient, complexity effective, and high
performance (eg. branch prediction) support for control flow.

We have developed NEEDLE, an LLVM based compiler
framework that leverages dynamic profile information to
identify, merge, and offload acceleratable paths from whole
applications. NEEDLE derives insight into what code coverage
(and consequently energy reduction) an accelerator can achieve.
We also develop a novel program abstraction for offload
calledBraid, that merges common code regions across different
paths to improve coverage of the accelerator while trading
off the increase in dataflow size. This enables coarse grained
offloading, reducing interaction with the host CPU core. To
prepare the Braids and paths for acceleration, NEEDLE generates software frames. Software frames enable energy efficient
speculative execution on accelerators. They are accelerator
microarchitecture independent support speculative execution
including memory operations. NEEDLE is automated and has
been used to analyze 225K paths across 29 workloads. It filtered
and ranked 154K paths for acceleration across unmodified
SPEC, PARSEC and PERFECT workload suites. We target
NEEDLE’s offload regions toward a CGRA and demonstrate
34% performance and 20% energy improvement.

",16.373557378465907,15.192662937743194,260
HPCA_17_044.txt,15.583302738127852,13.732333549395026,HPCA,7463,"
In this paper, we evaluate the error criticality of
radiation-induced errors on modern High-Performance Computing (HPC) accelerators (Intel Xeon Phi and NVIDIA K40)
through a dedicated set of metrics. We show that, as long as
imprecise computing is concerned, the simple mismatch detection
is not sufficient to evaluate and compare the radiation sensitivity
of HPC devices and algorithms. Our analysis quantifies and
qualifies radiation effects on applications’ output correlating the
number of corrupted elements with their spatial locality. Also,
we provide the mean relative error (dataset-wise) to evaluate
radiation-induced error magnitude.

We apply the selected metrics to experimental results obtained
in various radiation test campaigns for a total of more than 400
hours of beam time per device. The amount of data we gathered
allows us to evaluate the error criticality of a representative
set of algorithms from HPC suites. Additionally, based on
the characteristics of the tested algorithms, we draw generic
reliability conclusions for broader classes of codes. We show
that arithmetic operations are less critical for the K40, while
Xeon Phi is more reliable when executing particles interactions
solved through Finite Difference Methods. Finally, iterative stencil
operations seem the most reliable on both architectures.

",17.5058628484301,16.676666666666666,201
HPCA_17_045.txt,14.436118245405893,12.405738809041988,HPCA,9259,"
GPU adoption for general purpose computing has
been accelerating. To support a large number of concurrently
active threads, GPUs are provisioned with a very large register
file (RF). The RF power consumption is a critical concern. One
option to reduce the power consumption dramatically is to
use near-threshold voltage(NTV) to operate the RF. However,
operating MOSFET devices at NTV is fraught with stability
and reliability concerns. The adoption of FinFET devices in
chip industry is providing a promising path to operate the
RF at NTV while satisfactorily tackling the stability and
reliability concerns. However, the fundamental problem of NTV
operation, namely slow access latency, remains. To tackle this
challenge in this paper we propose to build a partitioned RF
using FinFET technology. The partitioned RF design exploits
our observation that applications exhibit strong preference to
utilize a small subset of their registers. One way to exploit
this behavior is to cache the RF content as has been proposed
in recent works. However, caching leads to unnecessary area
overheads since a fraction of the RF must be replicated.
Furthermore, we show that caching is not efficient as we
increase the number of issued instructions per cycle, which is
the expected trend in GPU designs. The proposed partitioned
RF splits the registers into two partitions: the highly accessed
registers are stored in a small RF that switches between
high and low power modes. We use the FinFET’s back gate
control to provide low overhead switching between the two
power modes. The remaining registers are stored in a large
RF partition that always operates at NTV. The assignment of
the registers to the two partitions will be based on statistics
collected by the a hybrid profiling technique that combines the
compiler based profiling and the pilot warp profiling technique
proposed in this paper. The partitioned FinFET RF is able to
save 39% and 54% of the RF leakage and the dynamic energy,
respectively, and suffers less than 2% performance overhead.

",15.331531391939887,12.801411474164137,331
HPCA_17_046.txt,16.43035062054889,14.770792527157926,HPCA,8453,"

The GPU has provide higher throughput by integrating more
execution resources into a single chip without unduly compromising power efficiency. With the power wall challenge,
however, increasing the throughput will require significant
improvement in power efficiency. To accomplish this goal,
we propose G-Scalar, a cost-effective generalized scalar execution architecture for GPUs in this paper. G-Scalar offers
two key advantages over prior architectures supporting scalar
execution for only non-divergent arithmetic/logic instructions. First, G-Scalar is more power-efficient as it can also
support scalar execution of divergent and special-function instructions, the fraction of which in contemporary GPU applications has notably increased. Second, G-Scalar is less expensive as it can share most of its hardware resources with register value compression, of which adoption has been strongly
promoted to reduce high power consumption of accessing the
large register file. Compared with the baseline and previous
scalar architectures, G-Scalar improves power efficiency by
24% and 15%, respectively, at a negligible cost.

",17.613555460941566,16.487705627705633,166
HPCA_17_047.txt,15.622971960229282,13.657484709865738,HPCA,7895,"
Modern processors can greatly increase energy efficiency through techniques such as dynamic voltage and frequency
scaling. Traditional predictive schemes are limited in their effectiveness by their inability to plan for the performance and energy
characteristics of upcoming phases. To date, there has been little
research exploring more proactive techniques that account for
expected future behavior when making decisions.

This paper proposes using Model Predictive Control (MPC) to
attempt to maximize the energy efficiency of GPU kernels without
compromising performance. We develop performance and power
prediction models for a recent CPU-GPU heterogeneous processor. Our system then dynamically adjusts hardware states based
on recent execution history, the pattern of upcoming kernels, and
the predicted behavior of those kernels. We also dynamically
trade off the performance overhead and the effectiveness of
MPC in finding the best configuration by adapting the horizon
length at runtime. Our MPC technique limits performance loss by
proactively spending energy on the kernel iterations that will gain
the most performance from that energy. This energy can then be
recovered in future iterations that are less performance sensitive.
Our scheme also avoids wasting energy on low-throughput phases
when it foresees future high-throughput kernels that could better
use that energy.

Compared to state-of-the-practice schemes, our approach
achieves 24.8% energy savings with a performance loss (including
MPC overheads) of 1.8%. Compared to state-of-the-art historybased schemes, our approach achieves 6.6% chip-wide energy
savings while simultaneously improving performance by 9.6%.

",16.728156217252725,14.853780487804883,251
HPCA_17_048.txt,14.923947899081078,12.830447318350554,HPCA,7066,"
Recent work has argued that sequential consistency (SC) in GPUs can perform on par with weak memory
models, provided ordering stalls are made less frequent by
relaxing ordering for private and read-only data. In this
paper, we address the complementary problem of reducing
stall latencies for both read-only and read-write data.
We find that SC stalls are particularly problematic for workloads with inter-workgroup sharing, and occur primarily due
to earlier stores in the same thread; a substantial part of the
overhead comes from the need to stall until write permissions
are obtained (to ensure write atomicity). To address this, we
propose RCC, a GPU coherence protocol which grants write
permissions without stalling but can still be used to implement
SC. RCC uses logical timestamps to determine a global memory
order and L1 read permissions; even though each core may see
a different logical “time,” SC ordering can still be maintained.
Unlike previous GPU SC proposals, our design does not
require invasive core changes and additional per-core storage
to classify read-only/private data. For workloads with interworkgroup sharing overall performance is 29% better and
energy is 25% less than in best previous GPU SC proposals,
and within 7% of the best non-SC design.
consistency [9] similar to WO [10] or RC [11] models.
Correctly inserting fences is difficult, however, especially
in GPUs where all practical programs are concurrent and
performance-sensitive. The authors of [9] found missing
fences in a variety of peer-reviewed publications, and even
vendor guides [12]. Such bugs are very difficult to detect:
some occurred in as few as 4 out of 100,000 executions
in real hardware, and most occurred in fewer than 1% of
executions [9]. Code fenced properly for a specific GPU
may not even work correctly on other GPUs from the same
vendor: some of these bugs were observable in Fermi and
Kepler but not in older or newer microarchitectures [9].
SC hardware is desirable, then, if it can be implemented
without significant performance loss. Recent work [13, 14]
has argued that this is possible in GPUs: unlike CPUs,
which lack enough instruction-level parallelism (ILP) to
cover the additional latency of SC stalls, GPUs can leverage
abundant thread-level parallelism (TLP) to cover most SC
stalls. The authors of [14] propose reducing the frequency
of the remaining SC stalls by relaxing SC for read-only and
private data; classifying these at runtime, however, requires
complex changes to GPU core microarchitecture and carries
an area overhead in devices where silicon is already at
a premium. Moreover, both studies focused on SC built
using CPU coherence protocols (MOESI and MESI) with
write-back L1 caches. In GPUs, however, write-through L1s
perform better [15]: GPU L1 caches have very little space per
thread, so a write-back policy brings infrequently written data
into the L1 only to write it back soon afterwards. Commercial
",16.915093595376604,15.31667677380231,487
HPCA_17_049.txt,15.508309007532503,14.012411509907832,HPCA,9075,"
The performance of 3D rendering of Graphics
Processing Unit that converts 3D vector stream into 2D frame
with 3D image effects significantly impacts users gaming
experience on modern computer systems. Due to its high
texture throughput requirement, main memory bandwidth
becomes a critical obstacle for improving the overall rendering
performance. 3D-stacked memory systems such as Hybrid
Memory Cube provide opportunities to significantly overcome
the memory wall by directly connecting logic controllers to
DRAM dies. Although recent works have shown promising
improvement in performance by utilizing HMC to accelerate
special-purpose applications, a critical challenge of how to
effectively leverage its high internal bandwidth and computing
capability in GPU for 3D rendering remains unresolved. Based
on the observation that texel fetches greatly impact off-chip
memory traffic, we propose two architectural designs to enable
Processing-In-Memory based GPU for efficient 3D rendering.
Additionally, we employ camera angles of pixels to control
the performance-quality tradeoff of 3D rendering. Extensive
evaluation across several real-world games demonstrates that
our design can significantly improve the performance of texture
filtering and 3D rendering by an average of 3.97X (up to 6.4X)
and 43% (up to 65%) respectively, over the baseline GPU.
Meanwhile, our design provides considerable memory traffic
and energy reduction without sacrificing rendering quality.

",19.661554785965695,18.78154342723005,216
HPCA_17_050.txt,14.911588945826086,13.302575752562,HPCA,6951,"
Dynamic parallelism (DP) is a promising feature
for GPUs, which allows on-demand spawning of kernels on the
GPU without any CPU intervention. However, this feature has
two major drawbacks. First, the launching of GPU kernels can
incur significant performance penalties. Second, dynamicallygenerated kernels are not always able to efficiently utilize
the GPU cores due to hardware-limits. To address these two
concerns cohesively, we propose SPAWN, a runtime framework that controls the dynamically-generated kernels, thereby
directly reducing the associated launch overheads and queuing
latency. Moreover, it allows a better mix of dynamicallygenerated and original (parent) kernels for the scheduler to
effectively hide the remaining overheads and improve the
utilization of the GPU resources. Our results show that, across
13 benchmarks, SPAWN achieves 69% and 57% speedup over
the flat (non-DP) implementation and baseline DP, respectively.

",15.343465313023842,15.271035196687372,139
HPCC_17_001.txt,16.246488166745433,15.079279032389476,HPCC,6508,"
With the increase of scale and complexity seen in a
variety of leadership-class scientific computation and simulation
applications, it has become more important to understand their
I/O performance characteristics. The user-observed performance
is a combination of properties of how the application is using
the HPC facility, as well as how others’ use of the facility causes
variability in the static machine capabilities. Our work leverages
statistical analysis of I/O performance data gathered with fine
time resolution over a full week from Titan supercomputer. Based
on observed properties of the distribution of I/O latencies, we
build a three-state hidden Markov model (HMM) to characterize
the end-to-end I/O performance on Titan. We parameterize our
model using part of the field-gathered I/O performance data and
validate it against the rest. The validation results demonstrate
that our model can capture the dynamics of end-to-end I/O
performance on Titan accurately.

",17.5058628484301,16.638081761006294,160
HPCC_17_002.txt,13.991012781750296,11.581145444087355,HPCC,4906,"
Projecting a high-dimensional dataset onto a lower
dimensional space can improve the efficiency of knowledge
discovery and facilitate real-time data analysis. One technique
for dimension reduction, weighted multi-dimensional scaling
(WMDS), approximately preserves pairwise weighted distances
during the transformation; but its O(f(n)d) algorithm impedes
real-time performance on large datasets.

Thus, we present CLARET, our fast and portable parallel
WMDS tool that combines algorithmic concepts adapted and
extended from the stochastic force-based MDS (SF-MDS)
and Glimmer. To further improve CLARET’s performance
for real-time data analysis, we propose a preprocessing step
that computes approximate weighted Euclidean distances by
combining a novel data mapping called stretching and Johnson
Lindestrauss’ lemma in O(log d) time in place of the original
O(d) time. This preprocessing step reduces the complexity of
WMDS from O(f(n)d) to O( f (n) log d), which for large d is a
significant computational gain. Finally, we present a case study
of CLARET by integrating it into an interactive visualization
tool called V2PI to facilitate real-time analytics. To ensure
the quality of the projections, we propose a geometric shape
matching-based alignment process and a quality metric.

",18.243605946275583,17.264857142857142,202
HPCC_17_003.txt,15.718661923220548,14.158242995788317,HPCC,5411,"
Recent interest in machine-learning based methods
has produced many sophisticated models for performance modeling and optimization. These models tend to be sensitive to
parameters of the underlying architecture and hence yield the
highest prediction accuracy when trained on the target platform.
Training a classifier, however, is a fairly involved process and requires knowledge of statistics and machine learning that the end
users of such models may not possess. This paper presents a new
framework for automatically generating machine-learning based
performance models. A tool-chain is developed that provides
automated mechanisms for sample generation, dynamic feature
extraction, feature selection, data labeling, validation and hyper
parameter tuning. We describe the design and implementation of
this system and demonstrate its efficacy by developing a learning
heuristic for register allocation in GPU kernels. Results show that
auto-generated models can predict register thresholds that lead to
integer factor performance improvements over kernels produced
by state-of-the-art optimizing compilers.

",17.122413403193683,16.58880650994575,159
HPCC_17_004.txt,15.693838144067119,14.530567283456222,HPCC,6547,"
Many scientific and engineering computations rely
on the scalable solution of large sparse linear systems. Preconditioned Krylov methods are widely used and offer many
algorithmic choices whose performance varies depending on the
characteristics of the linear system. In previous work, we have
shown that the performance of different Krylov methods at
small scales can be modeled using a small number of features
based on structural and numerical properties of the input linear
system. In this paper, we focus on comparing the scalability of
parallel Krylov methods given different input properties without
requiring extensive empirical measurements. We consider the
PETSc implementations of Newton-Krylov methods to produce
scalability rankings based on our new comparative modeling
approach. The model-based ranking is validated by comparison
with empirical results on a numerical simulation of driven fluid
flow in a cavity.

",17.693802365651003,17.173394160583943,138
HPCC_17_005.txt,15.365636987312236,13.238198478355375,HPCC,6517,"
A typical application normally includes the main
program and some shared libraries. The main program can
arbitrarily execute the code of those shared libraries due to
the coexistence in the same memory space. This feature can be
used by attackers to carry out code-reuse attacks. Meanwhile,
the shared libraries are shared across multiple applications,
which can help attackers to simplify the process of code-reuse
attacks. Isolating shared library into a separate execution environment and restricting the access to this library is a promising
countermeasure, while the existing isolation approaches need to
either modify the main program, or break the shared feature of
the library. In this paper, we present Libsec, an efficient and
transparent approach to isolate shared libraries, without the
need of source code of the main program or shared libraries.
Libsec adopts commodity hardware virtualization extension to
isolate shared libraries from the main program. Meanwhile,
Libsec relies on static instrumentation and dynamic processing
to provide interfaces for legitimate cross-domain invocations. By
this way, Libsec can guarantee that the main program and
shared libraries are executed in the corresponding execution
environment respectively, while cross-domain invocation is only
allowed via specific interfaces, thus preventing the main program
from jumping directly to the shared library. We implement a
prototype of Libsec in KVM. To demonstrate its effectiveness,
we deploy it to some real-world applications and libraries, such
as Nginx and OpenSSL libraries. Security evaluation shows that
Libsec can prevent the attacker from utilizing the functions or
instruction sequences of the shared library for code-reuse attack.
Performance evaluation shows that the performance and space
overhead caused by Libsec are appropriate.

",17.03242331605538,14.679635036496354,275
HPCC_17_006.txt,15.08011312503793,13.181360849203198,HPCC,5575,"
 In many developing countries, rapid urbanisation
of cities coupled with the lack of proper urban planning has made
various streets susceptible to waterlogging during heavy rains.
This severely affects the traffic movement across an entire city and
leads to disruption in work. Since an important component of
smart cities is developing an efficient urban mobility system, the
authors have developed a method which helps in detection of areas
prone to waterlogging and prediction of severity of waterlogging
in these areas in the future. The areas susceptible to waterlogging
are detected with the help of elevation of the area and the past
travel time data. Elevation of an area is an indicator of the level or
height of an area, so the low-lying areas are more prone to
accumulation of water when it rains. Similarly, the past travel time
data of an area also serves as a measure to find out the intensity of
water logging as the larger the accumulation of water in an area,
the more is the travel time. The past data pertaining to
waterlogging severity in an area with respect to parameters such
as the amount of rainfall and day of the week is used to train a
neural network, which is then used to predict the possibility of
waterlogging and its intensity in that area in the future.

",17.93193317476759,17.36522741832159,224
HPCC_17_007.txt,14.43557375658256,13.569617879681314,HPCC,4785,"
 Knowing how many people occupy a building,
and where they are located, is a key component of smart building
services. Commercial, industrial and residential buildings often
incorporate systems used to determine occupancy. However,
relatively simple sensor technology and control algorithms limit
the effectiveness of smart building services. In this paper we
propose to replace sensor technology with time series models that
can predict the number of occupants at a given location and time.

We use Wi-Fi datasets readily available in abundance for
smart building services and train Auto Regression Integrating
Moving Average (ARIMA) models and Long Short-Term
Memory (LSTM) time series models. As a use case scenario of
smart building services, these models allow forecasting of the
number of people at a given time and location in 15, 30 and 60
minutes time intervals at building as well as Access Point (AP)
level. For LSTM, we build our models in two ways: a separate
model for every time scale, and a combined model for the three
time scales. Our experiments show that LSTM combined model
reduced the computational resources with respect to the number
of neurons by 74.48 % for the AP level, and by 67.13 % for the
building level. Further, the root mean square error (RMSE) was
reduced by 88.2% - 93.4% for LSTM in comparison to ARIMA
for the building levels models and by 80.9 % - 87% for the AP
level models.

",15.322241378113624,14.51095851216023,239
HPCC_17_008.txt,14.980885813094137,13.592789815401755,HPCC,2790,"
 — The methodology of system design of modern
intelligent control systems in the electric power industry and
their features are considered when providing the required
comfort in multi-zone buildings with the use of multi-agent
power consumption and comfort management systems. Such a
control system covers all the monitored zones of a building and, if
necessary, allows providing the greatest possible overall comfort
in the building while reducing the required electric power.

The purpose of this study is to develop a comfort
management system in a multi-zoned building that can provide
the greatest possible comfort while reducing the required electric
power.

",18.946978176291534,19.187647058823533,103
HPCC_17_009.txt,16.817409349069404,17.011819157216063,HPCC,5982,"
We describe the Auto-Tuning Framework (ATF) — function and a specification of the tuning parameters (i.e.,

a novel generic approach for automatic program optimization heir names and the ranges of possible values), OpenTuner
by choosing the most suitable values of program parameters, automatically generates a search space and explores it in

such as number of parallel threads, tile sizes, etc. Our frame- f dsliaed shisative b ; ‘itil d
work combines four advantages over the state-of-the-art auto- terms of a user-defined objective by using pre-implemente:

tuning: i) it is generic regarding the programming language, Search techniques. However, OpenTuner does not provide
application domain, tuning objective (e.g., high performance mechanisms for expressing dependencies between parameters.

and/or low energy consumption), and search technique; ii) it For example, the important routine GEMM (GEneral Matrix
can auto-tune a broader class of applications by allowing tuning Multiplication) [13] when implemented in OpenCL has various

parameters to be interdependent, e.g., when one parameter is ; ‘le si k ; hich h
divisible by another parameter; iii) it allows tuning parameters tuning parameters (tile size, work-group size, etc.) which have

with substantially larger ranges by implementing an optimized dependencies between them, e.g., some parameter must be disearch space generation process; and iv) its interface is arguably visible by another parameter [15]. Due to these dependencies,
simpler than the interfaces of current auto-tuning frameworks. QpenTuner is not capable of tuning GEMM. The OpenTuner
We demonstrate ATF’s efficacy by comparing it to the state-of- community has offered workarounds, e.g., re-designing the
the-art auto-tuning approaches OpenTuner and CLTune, showing : . .
better tuning results with less programmer’s effort. user program so that its tuning parameters become independent [9], or setting a penalty value for configurations where the
",23.72669961743115,26.345000000000002,303
HPCC_17_010.txt,15.139726780803425,13.293940644122824,HPCC,6933,"
Sparse-Matrix Vector products (SpMV) are highly
irregular computational kernels that can be found in a diverse
collection of high-performance science applications. Performance
for this important kernel is often highly correlated with the associated matrix sparsity, which, in turn, governs the computational
granularity, and therefore, the efficiency of the memory system.

In this paper, we propose to extend the current set of Kokkos
profiling tools with an autotuner that can iterate over possible
choices for thread-team size and vector width, taking advantage of
runtime information, while, choosing the optimal parameters for
a particular input. This approach allows an iterative application
that calls the same kernel multiple times to continue to progress
towards a solution while, at the same time, alleviating the
burden from the application programmer of knowing details
of the underlying hardware and accounting for variable inputs.
We compare the autotuner approach against a fixed approach
that attempts to use all the hardware resources all the time,
and show that the optimal choice made by the autotuner is
significantly different among the two latest classes of accelerator
architectures. After 100 iterations we identify which subset of the
matrices benefit from improved performance, while others are
near the break-even point, where the overhead of the tool has
been completely hidden. We highlight the properties of sparse
matrices that can help determine when autotuning will be of
benefit. Finally, we connect the overhead of the autotuner to
specific sparsity patterns and hardware resources.

",18.903936251131103,18.316198979591835,246
HPCC_17_011.txt,15.379346608396386,12.949321059535666,HPCC,5059,"
Continuous advances in molecular sequencing technologies now allow for inferring evolutionary trees (phylogenies)
on supercomputers that comprise hundreds to thousands of
species at the whole-transcriptome or whole-genome level. The
phylogenetic likelihood function (PLF) consumes 90-95% of
total execution time in such analyses and is therefore typically
parallelized. Recently, the site repeats (SR) technique for substantially accelerating the PLF has been introduced. It identifies
repeating patterns in parts of the likelihood computation and
omits the respective redundant calculations to save time and
space. However, the SR technique induces a parallel load imbalance. In this paper, we introduce a novel randomized data
distribution algorithm to improve load balance (RDDA) for SRbased likelihood calculations. The algorithm is available as opensource code, induces minimal run-time overhead, and yields up
to 25% run time improvements on empirical datasets and up
to 50% for a synthetic, worst-case scenario. This improvement
is substantial as current evolutionary data analyses may require
tens of millions of core hours on supercomputer systems.

",17.971250198000288,16.575202095808383,168
HPCC_17_012.txt,14.387552070942426,12.550790914716497,HPCC,5759,"
Emerging non-volatile main memories (NVMMs)
technologies can provide both data persistence and high performance at memory level. The design of existing file systems
for NVMM has to handle the data durability problem between
CPU cache and NVMM. However, most NVMM-aware file
systems could not meet the strong data consistency requirement
of applications with data structures, e.g. B-Tree. Traditional
techniques, such as copy-on-write and journaling, delivering data
consistency, have defects of write amplification and data copy,
respectively.

In this paper, we explore SNFS, one log-structured file system
with optimization of data consistency based-on non-volatile main
memory, providing high performance for applications with small
writes. Specifically, SNFS adopts a small data-log mechanism
to journal fine-grained data writes. It also uses in-place writes
to minimize memory footprint for small data updating and
accelerates data block locating with hashing strategy. Finally,
we evaluate SNFS’s performance with several write-intensive
workloads, and experimental results show that SNFS improves
the system throughput by up to 23 times compared to state-of-theart file systems and reduces the execution time by up to 65.5%.

",15.616094167265928,13.771560283687947,192
HPCC_17_013.txt,15.04932993605388,12.887741675531057,HPCC,5305,"
Over the last two decades, the performance gap
between RAM and disk drives has been widened by about 50%
per year. This is because the disk access time was improved
only about 8% per year due to the mechanical delays. In order
to improve the disk performance, each disk is equipped with
an on-board disk cache to bridge the performance gap between
the high-speed I/O bus and the slow magnetic media. The onboard disk cache is normally made of SRAM. This is because
the SRAM is faster and relatively insensitive to disturbances
such as electrical noise in contrast to DRAM. However, it is
challenging to increase the capacity of a single-chip SRAM.
Furthermore, SRAM is much more expensive and power
consuming than DRAM. Therefore, the manufacture normally
integrates a relatively small-size on-board disk cache to the
disk drives. This paper proposes to compress the on-board
disk cache to improve the effective cache size, thus improving
the disk performance. Because the prefetching of on-board disk
cache has a significant impact on the disk performance, this
paper only compresses the prefetched data to minimize the
complexity and side effect of compression. The compression is
processed in background without affecting ongoing requests.
Synthetic traces and real traces are employed to evaluate the
proposed method. Experimental results demonstrate that the
average response time can be reduced up to 21% with the
increase of the compression ratio, and the hit ratio achieves
up to 3x improvement.

",13.757784519515319,10.74104417670683,250
HPCC_17_014.txt,14.496149749165347,12.825073691989623,HPCC,6744,"
With the emergence of 3D TLC/QLC NAND flash,
the capacity of flash-based SSD is growing rapidly, from
hundreds of gigabytes to tens/hundreds of terabytes. Accordingly,
the Flash Translation Layer (FTL) within such a large SSD is
confronted with serious problems that have not ever appeared
before. Traditional FTLs either adopt a coarse-grained mapping
mechanism, thus facilitating the mapping table being kept in
DRAM completely, or adopt a fine-grained mapping mechanism
but only keep frequently-accessed mapping information in
DRAM depending on the localities of workloads. We argue that
both of the above policies are unsuitable for SSDs supplying
ultra-large capacity. Firstly, large SSDs introduce so many more
mapping entries than ever before that even the coarse-grained
mapping mechanism cannot produce a compact enough mapping
table to be kept in DRAM completely. Secondly, large SSDs tend
to be deployed in data centers to serve IO requests from massive
numbers of users under various application backgrounds, where
these IO requests exhibit weaker spatial and temporal localities.
As a result, the method that keeps frequently-accessed mapping
information in DRAM is also impractical for large scale SSDs. In
this paper, we propose a novel KV-FTL approach for large scale
SSDs, which mostly maps logical addresses to physical addresses
via a simple hash function while handling hash collision and outof-place data update in the traditional manner, i.e., the mapping
table. Our KV-FTL is able to accelerate address translation by
avoiding loading the mapping table from flash memory to
DRAM, thus improving performance, as well as to reduce the
write-traffic introduced by the mapping table, thus extending the
lifespan of SSDs. To the best of our knowledge, this is the first
time the key-value principle has been applied to FTL design.
Experimental results show that our KV-FTL extends SSD
lifespan by a factor of up to 18.7% with an average of 13.6%;
improves read performance ranging from 18.4% to 50.7% with
an average of 39% with optimization, and in the case of
extremely intensive requests, improves the overall performance
for requests with an average of 47%.

",16.4712008295341,16.82757575757576,360
HPCC_17_015.txt,16.094571787761726,15.050532577418824,HPCC,5581,"

Centralized electrical storage system management in islanded
micro-grids is treated both theoretically and experimentally.
An overview of the types of distributed islands in Wi-Fi
Long Distance networks is the starting point, followed by

examples of WiLDNets and associated reliability problems.

Presentation of an archetypical tree structure communication
system serves to introduce a centralized management system
that may be used to improve reliability of multi-community
private networks. An existing Wi-Fi long distance testbed
with distributed storage serves as an islanded micro-grid for
experimental study. Centralized management algorithms are
tested. The results are generalized to the case of large meshed
communication system.

",15.470042427545799,15.509285714285717,106
HPCC_17_016.txt,13.735146188126883,11.890715560811792,HPCC,3811,"
 Recently, mobile phone call detail records (CDR)
have been used to study mobility patterns in cities. Since home and
workplace are the most important places in people’s lives and
define the structure and activity patterns of the city, identifying
home and work locations and home-work commuting patterns are
of much interest. However, due to decreasing usage of voice calls
and SMS, and low usage while people staying at home, identifying
home and work locations are getting more difficult. In this paper,
we develop a method to exploit daily-aggregated internet usage
data (G-CDR) in addition to CDR for identifying work locations.
We also develop a method for identifying home locations based on
detecting sleeping time. This method can significantly increase
home detection success rate and accuracy. From the identified
home and work locations, we have explored the population
distribution and commuting patterns of people in Bangkok.

",15.343465313023842,13.704362934362937,150
HPCC_17_017.txt,15.803096661111034,14.382250834176478,HPCC,4506,"
Assessing the effectiveness of a demand response
(DR) program requires appropriate metrics of performance. In
this paper, we propose the assessment of an aggregator-based
residential DR program using two newly developed metrics
addressing the economic and environmental aspects of sustainability. The economic sustainability metric of the DR method
is quantified by the economic savings of the customers on
electricity charges and the aggregator’s profit. The environmental
sustainability is quantified by measuring the reduction in capacity
factors of fossil-fueled peaking power plants and the subsequent
reduction in CQ. emissions. A simulation study is performed for
a large-scale power system consisting of 5,555 users and 56,659
schedulable assets using real pricing data from a utility and a
bulk electricity market for a 31-day period. Finally, we apply
high-performance computing methods to the month-long study
to yield a faster computation time.

",17.693802365651003,17.288055555555562,148
HPCC_17_018.txt,13.958420797041413,11.915687119713738,HPCC,3058,"
The block incomplete Cholesky(BIC) is one of the most useful preconditioners for the conjugate gradient(CG) method.
Coloring algorithms are often used for parallelizing the BIC preconditioner, but they are the sequential algorithms. In this paper, we
proposed a hierarchical parallelization for coloring methods. Proposed method is versatile. We demonstrate that this method parallelize
traditional multi-coloring algorithms. Our numerical results confirm that the proposed method does not change the properties of the

coloring methods.

",14.314028922438442,13.41578947368421,77
HPCC_17_019.txt,15.01417649972247,12.815364233021246,HPCC,5885,"
Community detection in complex networks has a
wide range of applications such as detection of cyber-communities
in social networks, recommendations based on the interest group,
and estimating hidden features in a social network. In distributed
frameworks, the primary focus has been scalability. However,
the accuracy of the algorithm’s output is also critical. We
propose the first distributed community detection algorithm based
on the state-of-the-art CEIL scoring function. Our algorithm,
named DCEIL, is fast, scalable and maintains the quality
of communities. DCEIL outperforms the existing state-of-theart distributed Louvain algorithm by 180% on an average in
Normalized Mutual Information (NMI) Index and 6.61% on an
average in Jaccard Index metrics. DCEIL completes execution
for 1 billion edges within 112 minutes and outperforms state-ofthe-art distributed Louvain algorithm by 4.3x. DCEIL critically
exploits three novel heuristics which address the existing issues
with distributed community detection algorithms that have the
hierarchical structure of CEIL or Louvain methods. Further,
our proposed heuristics are generic as well as efficient, and we
illustrate their efficacy by enhancing the accuracy of distributed
Louvain algorithm by 22.91% on an average in Jaccard Index,
and the average execution time by 1.68 over popular datasets.

",17.631426480028413,16.132935323383087,207
HPCC_17_020.txt,12.831717113240831,10.278838013535978,HPCC,3942,"
Multicast communication constrained by endto-end delay and inter-destination delay variation is known
as Delay and Delay Variation Bounded Multicast (DVBM).
These constraints make the multicast communication realtime. In this paper, we propose a multi-core multicast
approach to solve the DVBM problem. The proposed threephase algorithm, Multi-core DVBM Trees (MWCDVBMT),
semi-matches group members to core nodes. The message
is disseminated to group members using trees rooted at the
designated core nodes. Simulation results show that when
existing single-core based algorithms fail to construct a
tree satisfying both constraints, MCDVBMT succeeds using
multiple cores with better inter-destination delay variation
and traffic concentration. However, the cost and the endto-end delay of the communication is higher than that of
single-core algorithms.

",13.925175675911131,13.250634920634923,127
HPCC_17_021.txt,16.325722622712533,15.001259182138483,HPCC,5493,"
In cloud datacenters, energy-efficient Virtual Machine Placement (VMP) mechanism is needed to maximize energy
efficiency. Existing virtual machine (VM) allocation strategies
based on whether the VMs’ resource demands are assumed
to be static or dynamic. Apparently, the former fails to fully
utilize resources while the latter, which is implemented on shorter
timescales, is either complicated or inefficient. Moreover, most
prior VMP algorithms place VMs one by one, which lacks
an optimization space. To handle these problems, we predict
Gaussian distribution patterns of VM demands and propose
an ant-colony-system VM placement algorithm (GACO-VMP)
which synchronously coordinates the VMs with complementary
resource requirements on the same server. The Gaussian distribution pattern is derived from the VMs running the same job. This
mechanism minimizes energy consumption, while guaranteeing
high resource utilization and also balancing resource utilization
across multiple resources. In addition, we design two new metrics,
called cumulative utilization ratio(CUR) and resource balance
distance (RBD), in order to measure the overall resource utilization level and the equilibrium of multi-dimensional resource
utilization, respectively. Simulations based on Google Cluster
real trace indicate that GACO-VMP can achieve remarkable
performance gains over two existing strategies in energy efficiency
3VM migrations, resource utilization and resource balance.

",18.243605946275583,17.699186991869926,206
HPCC_17_022.txt,15.292465652647298,13.105513706489866,HPCC,8392,"
High end scientific applications in the form of workflows are
being executed in cloud for various benefits. But with an increase in the
processing capabilities of the cloud system, the energy consumption has also
increased significantly. Thus energy efficient execution of these scientific
workflows in cloud becomes essential. Existing research on energy efficient
scheduling of scientific workflows in cloud mainly focus on reducing only
the dynamic energy consumption of the compute nodes and uses DVFS
technique.

In this paper, we have proposed six different energy efficient scheduling
approaches for a set of online scientific workflows in a cloud system
considering both static and dynamic energy consumption of the compute
nodes. These approaches are divided into two categories: non-splittable
allocation of VMs on single host, and splittable allocation on multiple hosts.
‘We have compared the performance of our proposed policies with state-ofart energy efficient scheduling policy, EnReal and found that the policies
perform better than EnReal. All three scheduling policies with non-splittable
‘VM allocation perform at par with EnReal in energy consumption but they
do not require any migration of VMs. And all policies under splittable VM
category perform significantly better than EnReal with an average energy
reduction of 70%.

",19.287186520377343,16.981262376237627,203
HPCC_17_023.txt,13.781754267608868,11.60120834143195,HPCC,9389,"
Demand for cloud computing has increased tremendously in
recent time due to various benefits. Along with providing promised services,
the cloud providers also need to focus on the amount of energy consumption
as it has many-fold benefits. Again scheduling algorithms play an important
role in minimizing the energy consumption of a system. Thus, in this paper,
we have proposed approaches for scheduling real time tasks on a virtualized
cloud system without missing their deadlines and minimizing the overall
energy consumption of the cloud system. We have divided the problem
of scheduling real-time tasks on virtualized cloud system into four sub
problems, analyzed and solved them separately. We have provided exact
solutions for scheduling of one type of real time tasks (sub problem 1),
created an approximated model for scheduling of two types of real time
tasks with same deadline (sub problem 2). We have also extended the same
approximated model for scheduling of many real time tasks with same
deadline (sub problem 3). Finally, we came up with four different approaches
for scheduling of general real time tasks using deadline clustering approach
(sub problem 4) and compared their performance in minimizing the overall
energy consumption of the cloud system.

",17.122413403193683,14.579894278606968,202
HPCC_17_024.txt,14.442269305686644,12.235059834765924,HPCC,4152,"
in big cities around the world air pollution is reaching harmful levels. One of the reasons for air contamination is the
automotive emissions. Soft appeals by the authorities to the citizens did not change much of the situation. Many people use the car
in everyday activities, e.g. going to work or every day food shopping in supermarkets. Recent studies showed that a replacement
of the current vehicles through autonomous vehicles would help in
reducing these emissions. A radical replacement is not possible.
The actual infrastructure needs to be extended by technologies
that facilitate autonomous driving. Therefor integration of knowhow from other domains like internet of things, big data analysis
or transportation planning has to be considered. This paper presents a smart service where, based on the information provided by
principals of industry 4.0 and internet of things, the planning of an
autonomous grocery delivery fleet occurs. This service is implemented then at a smaller scale, as an autonomous ordering and
delivery system for coffees, and realised at the Heilbronn University. Furthermore, the paper presents the benefits and the limitations of this kind of system.

",15.186304673781336,12.673853754940712,187
HPCC_17_025.txt,16.768856813212135,16.27351632309667,HPCC,4962,"
Smart cities are a natural evolution of the
recent wave of technology, allowing both to offer better
services to citizens and also to save costs. Nevertheless
the adoption of smart city solutions have to face major
problems, not due to technology but to society: economy
and inertia. Even if the adoption of smart city technologies
can bring to savings, new solutions need to face the general
inertia of public administrations, that would have to face
new expenses, without being familiar with these new kinds
of options, therefore opting out of the perceived risk.
For this reason we feel it is helpful to also consider a
variant of smart city technologies, dubbed “Smart Cheap
City” (SCC for short). A Smart Cheap City solution does
not necessarily aim to maximize benefits to citizen, or to
produce the biggest savings, but instead has a primary
focus: minimizing the cost of implementation. This leads
to lower the entry barrier, facilitating early adoption by
cities, and so acting as a gentle introduction to the benefits
of smart cities, fostering their wider and more effective
adoption. We report on a project based on the SCC
paradigm, tackling the waste management of a city, and
show how smart cheap city solutions can be effectively
used to provide both immediate benefit, and to overcome
the barriers that have so far slowed down their adoption.

",18.243605946275583,17.207623261694057,227
HPCC_17_026.txt,15.573439323641214,14.391923256537982,HPCC,4004,"
The world has become a global village and transforming into advance modern society by applying the
technology has a strong influence in every aspect of our latest smart city technology [1].
daily lives and the fourth industrial revolution that will Smart Education is referred to intelligence of educational
alter the way we live and learn. This inter-connectivity environment, resources, management and services which is
brings a need for greater engagement, experience and used to achieve a positive interaction among its bodies.
efficiency. Catch a sign on a day in a life of a student and Smart Education regards information processing as the
how connectivity and technology helps enrich his/her carrier, and intelligent operation as a measure of it [2].
daily activities. This research aims to study the Smart Education integrates smart tools that are designed
beneficiary of adopting Smart Education technology and developed to be as supplemental intelligent tools. In
among United Arab Emirates (UAE) Universities. This addition, it can also be adjusted to fulfill the students’
type of education is becoming a dominant in academia creative minds. Smart Education has vast horizons of
especially within universities around the globe. Since it knowledge that students can obtain instead of reading the
was introduced, it demonstrates a significant change in lines of their collage textbooks. For instance, in the field of
educational instructional methods. The objective of this | medicine, the learner can enter virtually the human body and
research is to investigate Smart Education instruments recognize its parts using the smart table rather than study
much as its significance and demands. This research used from non-interactive components (e.g. textbooks, web page,
different tools for assessment. Surveys and interviews etc.). In the field of Information Security major, students use
were conducted to group of universities learners and virtual environment to simulate attack scenarios against
educators within the higher education system in the UAE. smart city module instead of the physical system. Smart
The outcome of the research has shown a significant Education creates new capabilities such as 3D educational
support towards the usage of the Smart tools and virtual keyboard [3]. Smart Education creates new industries,
technologies. The survey has also indicated that 72% of for instance; several companies developed the best
the participants preferred the technology approach interactive virtual lab software which is used for connecting
within their curriculum. The research highlighted that teachers and students [4]. Also, the student can enter
there is a great impact of using the Smart technology virtually into a tiny flash-memory and identify its
tools. In the Future, the authors would like to investigate components through smart glasses tool. On the other hand,
the risk of adopting such educational approach. sixth sense technology and virtual laser projected keyboard
are one of these interactive tools that aid the process smart

",16.827784334635936,15.64590511727079,470
HPCC_17_027.txt,15.400982330657346,13.946740654205609,HPCC,4209,"
The MPI two-sided communication model has been
widely used in scientific applications for decades. The nonblocking version of the two-sided routines allows the application to
potentially improve performance on many systems by overlapping communication and computation. In practice, unfortunately,
the overlap is hard to achieve because of the limitations of the
MPI internal progress engine and the underlying network. The
traditional approach to resolving this issue is to implement an
asynchronous progress engine based on either additional threads
or hardware interrupts; however, such approaches may result in
reduced computing power or expensive overheads.

In this paper, we present a portable process-based asynchronous progress approach for two-sided communication in the
PMPI-based Casper framework. It allows the user to specify
an arbitrary number of cores on a multicore or many-core
architecture and offload the point-to-point communication to
these cores, thus ensuring asynchronous progress with low
overhead. Unlike our previous work that supports asynchronous
progress for the MPI one-sided model, a completely new design
is needed for the message-matching-based two-sided model in
order to ensure comprehensive semantics correctness as defined
in the MPI standard. We present a detailed design of this twosided model and compare it with the traditional thread-based
approach on both a multicore Intel Xeon cluster and a manycore Knights Landing cluster.

",16.975882523387877,17.19160714285714,225
HPCC_17_028.txt,16.132432388656245,14.32095011051393,HPCC,5147,"
Stream processing is emerging to react to the changing business situations of real-time processing. The main aim of
this paradigm is to deal with the huge volume of data in the
format of information flows originating from distributed devices.
This consequently poses challenges to the scheduling problem
in cloud data centers regarding the time-varying velocity of
data ingesting and processing. In response to the uncertainties
and complexities of streaming data, we propose a model-based
scheduling scheme for stream processing systems, capturing the
system behavior and providing an optimal allocation strategy to
adapt to the changing work conditions. The proposed scheduling
policy is implemented in Apache Storm, and micro-benchmarks
with various shapes (e.g line, star, and diamond) were used in
the evaluation. A topology that tracks trending topics on Twitter
is also used, where the input is feeding with tweets in realtime. Experimental results show that the proposed solution can
perform estimations that are well aligned with the system performance. The proposed scheduling policy achieves an improved
performance with regards throughput and latency under varying
ingesting rates.

Index Terms—Streaming Data Processing, Resource Allocation/Scheduling, Apache Storm

",16.594172100314452,14.56649122807018,192
HPCC_17_029.txt,15.256520254921227,13.962029999698043,HPCC,3476,"
Deep learning systems have been growing in prominence as a way to automatically characterize objects, trends,
and anomalies. Given the importance of deep learning systems, researchers have been investigating techniques to optimize
such systems. An area of particular interest has been using
large supercomputing systems to quickly generate effective deep
learning networks: a phase often referred to as “training” of
the deep learning neural network. As we scale existing deep
learning frameworks—such as Caffe—on these large supercomputing systems, we notice that the parallelism can help improve
the computation tremendously, leaving data I/O as the major
bottleneck limiting the overall system scalability. In this paper, we
first present a detailed analysis of the performance bottlenecks of
Caffe on large supercomputing systems. Our analysis shows that
the I/O subsystem of Caffe—LMDB—relies on memory-mapped
I/O to access its database, which can be highly inefficient on
large-scale systems because of its interaction with the process
scheduling system and the network-based parallel filesystem.
Based on this analysis, we then present LMDBIO, our optimized
YO plugin for Caffe that takes into account the data access
pattern of Caffe in order to vastly improve I/O performance.
Our experimental results show that LMDBIO can improve the
overall execution time of Caffe by nearly 20-fold in some cases.

",16.218646115125612,16.434428733031677,222
HPCC_17_030.txt,15.324678787708812,14.208531500327116,HPCC,5210,"
Distributed deep learning systems place stringent
requirement on communication bandwidth in its model training
with large volumes of input data under user-time constraint. The
communications take place mainly between cluster of worker
nodes for training data and parameter servers for maintaining
a global trained model. For fast convergence the worker nodes
and parameter servers have to frequently exchange billions of
parameters to quickly broadcast updates and minimize staleness.
Demand on the bandwidth becomes even higher with the introduction of dedicated GPUs in the computation. While RDMAcapable network has a great potential to provide sufficiently high
bandwidth, its current use over TCP/IP or tied to particular
programming models, such as MPI, limits its capability to break
the bandwidth bottleneck.

In this work we propose iRDMA, an RDMA-based parameter
server architecture optimized for high-performance network
environment supporting both GPU- and CPU-based training. It
utilizes native asynchronous RDMA verbs to achieve network line
speed while minimizing the communication processing cost on
both worker and parameter-server sides. Furthermore, iRDMA
exposes the parameter server system as a POSIX-compatible file
API for convenient support of load balance and fault tolerance as
well as its easy use. We have implemented iRDMA at IBM’s deep
learning platform. Experiment results show that our design can
help deep learning applications, including image recognition and
language classification, to achieve near-linear improvement on
convergence speed and training accuracy acceleration by using
distributed computing resources. From the system perspective,
iRDMA can efficiently utilize about 95% network bandwidth of
fast networks to synchronize models among distributed training
processes.

Index Terms—RDMA; deep learning; network;

",16.11434528070225,15.795373134328358,270
HPCC_17_031.txt,15.957596692054324,15.19357412047609,HPCC,3600,"
 Identification of network traffic accurately at its
early stage is very important for network traffic management
and application traffic classification. In recent years, this
becomes very hot topic to identify traffic at its early stage.
Unidirectional and _ bidirectional statistical features are
effective features and widely used in Internet traffic
classification. However, it is important to evaluate and select
effective features for Instant Messaging (IM) application traffic
classification at early stage. In this paper we are interested to
find out robust and effective features at early stage. We firstly
extract 22 statistical features of the first flow on two different
network environment traffic datasets include on HIT and
NIMS datasets. Then mutual information is conducted between
the extract statistical features to select the effective features.
Additionally to select robust features, we execute attribute
selection cfsSubsetEval with Best search evaluator that select
the robust and stable features from the result achieved by
mutual information. And then, we execute 10 well-known
machine learning classifiers. Our experimental results show
that max_fpktl, std_bpktl, max_biat, mean_fpktl, mean_bpktl
and min_biat feature are robust features at early stage traffic
classification.
",15.112257680678326,14.889076502732244,185
HPCC_17_032.txt,13.836226059850908,12.393340473959032,HPCC,5302,"
Automatic number plate recognition (ANPR) is a
significant part in intelligent traffic system. At present, there are
many traditional approaches that have achieved a rather high
accuracy to solve this problem, almost all of which are separated
into three steps of localization, segmentation and recognition.
However, these approaches, especially in segmentation progress,
are limited to some specific conditions including light intensity,
orientations, rotation and distortion angle of plates, etc. In this
paper, distinct from traditional approaches, a network including a
convolutional neural network(CNN) that operates directly on the
image pixels is employed as a substitute of the integration of
segmentation and recognition. The network works on the
TensorFlow framework. Evaluation of this training network is
characters’ recognition accuracy on a test set of 796 number plate
pictures. In result, we achieve a 88.61% accuracy with a training
set of only 7396 photographs that are expanded from 3041 different
number plate pictures, which is a relatively high accuracy,
especially for a deep CNN that usually needs a great number of
samples. We also demonstrate one possible way to enrich data set
and make a test using a even simpler network, which results in a
90.07% accuracy on a test set.

",17.833180683606166,16.462000000000007,203
HPCC_17_033.txt,14.036848699376133,12.489569844558115,HPCC,5578,"
Straggler task is commonly considered as the major
bottleneck in parallel data processing. Previous work mainly
focuses on the coarse-grained straggler detection and optimization such as speculative scheduling. However, fine-grained rootcause analysis of straggler tasks is rarely considered. In addition,
existing work simply depends on empirical analysis, which lacks
of useful guidance to performance optimization. In this paper, we
propose a new methodology of fine-grained straggler root-cause
analysis using machine learning. We collect raw metrics from
Spark event log and hardware sampling tool, and refine them into
high-level metrics for model learning. Then we present the rootcause analysis of stragglers through CART tree. A customized
prune method is also applied to improve analysis accuracy. From
the analysis, we derive several new findings beyond the well
known causes of stragglers. Our work provides a new perspective
on identifying and understanding the inefficiency in parallel data
processing programs by applying machine learning techniques to
fine-grained root-cause analysis of straggler tasks.

",14.265292616868656,12.943181818181824,166
HPCC_17_034.txt,16.07786980055735,14.491548463726442,HPCC,5535,"
In this paper, we performed a comprehensive study
of quantifying and mitigating computational inefficiency of current genomic analysis approaches. First, we found current parallelization approaches that have limited scalability due to either
unexploited parallelism or low utilization of system resource.
Thus, we proposed Spark-Gene, which is on the basis of Spark
in-memory programming model. To test the performance of our
Spark-Gene, we used WGS in the GATK as the test case. We show
that Spark-Gene reduces the execution time of WGS analysis
from 19 hours to 30 minutes with a speedup in excess of 37-fold
at 256 CPU cores. Furthermore, we identified that garbage collection is the major scalable bottleneck of better parallel efficiency
for native in-memory computing model. Second, we quantified
microarchitectural inefficiency for typical genomic applications
and uncovered opportunities for microarchitectural optimizations
for the design of genomic domain-specific accelerator, especially
on specializing concurrency, computation and memory hierarchy.
This paper is to leverage state-of-art big-data technologies to
improve parallelization for genomics analysis and motivate the
integration of accelerators into the genomic analysis computing
system.

",18.903936251131103,18.049290540540543,186
HPCC_17_035.txt,14.90745206726708,12.873804499282219,HPCC,5924,"
In this paper, we present an efficient type-agnostic
approach for finding sub-sequences in data, such as text documents or GPS trajectories. Our approach relies on data
deduplication for creating an inverted index. In contrast with
existing data deduplication techniques that chunk raw sequences
of characters arbitrarily, our approach preserves the semantics
of the original sequence via the notion of token and can be used
to index normalized data. When compared to indexing methods
that preserve the semantics and operate on normalized data, our
method increases the relevance of the inverted index, reduces its
size and improves its performances. Since data normalization
is generally not used beyond the scope of textual data, we
introduce a framework that helps identify the extent to which
data should be normalized regardless of its type. On this basis,
we demonstrate with a dataset made of GPS trajectories that
our method can be used agnostically: it can be used to index
and query data of a completely different type. Finally, we show
that the resulting spatial index is characterized by a better
discrimination than classical spatial indexing approaches.

",17.93193317476759,16.594037267080747,185
HPCC_17_036.txt,15.451138751765065,14.121462053571431,HPCC,5092,"
The densification power law is a concept in the
realm of temporal graph evolution. The number of edges
grows in a power law over the number of nodes over time,
replacing the pre-2005 general assumption of a linear trend.
The densification power law has been verified by several real
networks over a long period of time. In this work, one such
graph, the arXiv citation network is investigated to examine
how the densification power law is working ten years after its
publication. The network is evaluated and compared with the
discussion in a previous work. It is observed that the graph
densification continues over time, but instead of maintaining
a constant densification power exponent, as suggested by
previous work, the exponent is actually dropping over time,
which suggests the densification power law is now fading away.
Here, this fading effect is literature analysed, and it is suggested
that node capability is the major obstacle to the continuation
of the original trend. To fully compare with the previous work
on graph evolution, the change of the average path length over
time is also investigated on our and other’s results. The results
imply the decreasing of the average path length in the temporal
evolution is very slow, which suggests that there exists a new
universal degree of separation in social networks of around
three.

",16.04434344847333,14.974678624813155,225
HPCC_17_037.txt,14.74127802016887,13.121681206222483,HPCC,2635,"
Biological functions in all living cells are performed
by protein-protein interactions since they form cells and control
function mechanisms. Thus, identifying pairs of protein-protein
interactions would be very useful, but it is not an easy task. But,
doing a wet lab consumes huge amount of resources whereas
using computational methods is highly challenging since they
may introduce high false positives. Since a protein is a sequence
of amino acids, a protein interaction would be influenced by
some interactions of amino acids, and the identification of
outstanding interacting pairs would give insightful meaning into
how a pair of proteins interacts.

This paper proposes a novel method to analyze a set of wellknown protein-protein interactions for identifying a set of strong
amino acid pairs that may influence the interaction. We calculate
amino acid correlation values via Pearson’s correlation, and use
K-means clustering to group a set of outstanding amino acid
pairs based on correlation values. The experimental results for 10
sets of protein interaction networks can identify a number of
strong amino acid pairs among them.
",16.613394197324528,16.070963081861958,180
HPCC_17_038.txt,13.217900304503921,11.224853386372612,HPCC,4023,"
 — Ethernet is being expanded to many industrial
automation applications, and the demand for high availability
drives the adoption of High Availability Seamless
Redundancy (HSR) due to its capability to achieve zero
recovery time. A major challenge of HSR is long delay of
large HSR network due to its ring architecture. A solution to
this issue is to use QuadBox to design multiple rings which
raise many design questions because of different architecture
options. Our approach is to apply the concept of SS7
signaling to classify the links as A, B, C, and D from which
we present a flexible and scalable architecture. We
developed a simulation model to validate this architecture,
and the results show that one-way delay is not negatively
affected by network growth. Another contribution of this
paper is to use a central controller, like Software Defined
Network (SDN), to create MAC forwarding tables on
individual HSR nodes. The proposed scheme prevents
flooding and broadcast storm in this loop topology.
",15.719379583869454,14.557686240140228,164
HPCC_17_039.txt,14.67012119588654,12.358647485989426,HPCC,4887,"
Virtual Data Center (VDC) is becoming the real
world tendency of IT model that offers flexible and on-demand
service, most of which adopt Software-Defined Networking (SDN)
as their network architecture to simplify network management
and ensure network isolation. In our paper, by following our
Just-in-Time Architecture (JITA) design, we further studied the
composable architecture in optical domain. For this architecture
to work efficiently, Terabit per second (Tbps) bandwidth and
negligible latency (i.e., less than microsecond latency) are needed
to support disaggregated infrastructures. Therefore, in this paper,
we present an all-optical packets switch solution with proposed
SDN controlling scheme. The modeling of the all-optical switch is
also proposed, implemented and studied. Based on the modeling
of the switch, the output Optical Signal-to-Noise Ratio (OSNR)
and latency performance are studied. With the result of the
simulation, we find the limits and dominated factors for designing
the large scale all-optical switch in the composable data centers.
An example of analyzing the maximum throughput and the
latency of the all-optical switch is given at the end.

",16.827784334635936,15.002280219780221,184
HPCC_17_040.txt,14.753311665403604,13.501118963951516,HPCC,2625,"
This paper presents a model of a queueing system
with multi-service adaptive traffic. This type of low-latency traffic
is one of the most dominant type of traffic in modern packet
networks. The idea and operation of the model is discussed
with the example of a queueing system with the FIFO service
discipline.

",11.855464076750408,10.222592592592594,55
HPCC_17_041.txt,16.470633551282624,14.618614308644805,HPCC,4414,"
With the vigorous development of network applications, typical SDN (Software Defined Networks) such as data
centers are gradually carrying more and more complex network
traffic. This poses a great challenge for network monitoring —
how to realize real-time, high-accuracy capture of traffic changes
at low cost. In this paper, we propose a trigger-based monitoring
approach called EffiView. This approach provides three ways to
monitor flow statistics, including flow-stat triggering, FlowRemoved parsing and active polling. The flow-stat triggering can
occur on all multiples of the presupposed flow-stat threshold
for each flow entry. The latter two ways are complementary to
the flow-stat triggering. FlowRemoved parsing is used to acquire
flow statistics from FlowRemoved messages and active polling
is conditionally carried out by the controller at the expiration
of monitoring period. EffiView achieves low-cost monitoring
by combining the three ways efficiently, while ensuring high
accuracy and fine granularity. Based on the NetMagic platform,
We implement EffiView and evaluate its monitoring performance.
The experimental results show that EffiView can reach great
advantages over traditional monitoring approaches.

",16.03029750255766,14.06809497206704,180
HPCC_17_042.txt,15.554680366399808,13.735345071478744,HPCC,6138,"
Deep and shallow convection calculations occupy
significant times in atmosphere models. These calculations also
present significant load imbalances due to varying cloud covers
over different regions of the grid. In this work, we accelerate
these calculations on Intel® Xeon Phi™ Coprocessor Systems.
By employing dynamic scheduling in OpenMP, we demonstrate
large reductions in load imbalance and about 10% increase in
speedups. By careful categorization of data as private, firstprivate
and shared, we minimize data copying overheads for the coprocessors. We identify regions of false sharing among threads and
eliminate them by loop rearrangements. We also employ proportional partitioning of independent column computations across
both the CPU and coprocessor cores based on the performance
ratio of the computations on the heterogeneous resources. These
techniques along with various vectorization strategies resulted in
about 30% improvement in convection calculations.
",16.827784334635936,15.421557971014494,137
HPCC_17_043.txt,15.468714802085817,13.85038864242409,HPCC,8437,"
Memory size and the number of cores inside a chip are increasing
with the advancement of technology. With 3D stacked IC technology,
memory size becomes even larger inside the chip-multiprocessor. So memory
mapping, task scheduling and along with communication optimization
of the network-on-chip become crucial. Hence operating system memory
management and scheduling approach need to consider this advancement
of the on-chip memory size and on-chip network architecture for efficient
mapping.

In this paper, we have proposed an efficient virtual page to memory slice
mapping and used simulated annealing based thread to core mapping of
multi-threaded application onto 3D stacked memory chip-multiprocessor.
Our experimental result shows, the thread to core mapping reduces the overall on-chip communication cost up to 26% and an average of 12%. Moreover,
our proposed virtual pages to DRAM slices mapping reduces the overall onchip communication cost up to a maximum of 86% and an average of 56%
for many real multi-threaded benchmarks and multi-benchmark workloads.
Results also show that overall on-chip communication cost does not improve
much due to the thread only mapping, but when it is combined with the
virtual page to DRAM slice mapping then improvements are significant.
Further, we extended the approach to do dynamic virtual page mapping
at runtime, which reduces overall communication cost up to 78% with
incurring a negligible amount of overhead.

",17.251386760058843,15.774540229885062,233
HPCC_17_044.txt,14.77260973159996,12.978764701546762,HPCC,5210,"
 — Container technology has been prevalent and
widely-adopted in production environment considering the huge
benefits to application packing, deploying and management.
However, the deployment process is relatively slow by using
conventional approaches. In large-scale concurrent deployments,
resource contentions on the central image repository would
aggravate such situation. In fact, it is observable that the image
pulling operation is mainly responsible for the degraded
performance. To this end, we propose Cider - a novel deployment
system to enable rapid container deployment in a_ high
concurrent and scalable manner at scale. Firstly, on-demand
image data loading is proposed by altering the local Docker
storage of worker nodes into all-nodes-sharing network storage.
Also, the local copy-on-write layer for containers can ensure
Cider to achieve the scalability whilst improving the costeffectiveness during the holistic deployment. Experimental
results reveal that Cider can shorten the overall deployment time
by 85% and 62% on average when deploying one container and
100 concurrent containers respectively.

",17.267425705330172,15.761820987654325,163
HPCC_17_045.txt,17.127938897349082,16.132667542877723,HPCC,5114,"
Traditional data centers are designed with a rigid
architecture of fit-for-purpose servers that provision resources
beyond the average workload in order to deal with occasional
peaks of data. Heterogeneous data centers are pushing towards more cost-efficient architectures with better resource
provisioning. In this paper we study the feasibility of using
disaggregated architectures for intensive data applications, in
contrast to the monolithic approach of server-oriented architectures. Particularly, we have tested a proactive network analysis
system in which the workload demands are highly variable.
In the context of the dReDBox disaggregated architecture, the
results show that the overhead caused by using remote memory
resources is significant, between 66% and 80%, but we have
also observed that the memory usage is one order of magnitude
higher for the stress case with respect to average workloads.
Therefore, dimensioning memory for the worst case in conventional systems will result in a notable waste of resources.
Finally, we found that, for the selected use case, parallelism is
limited by memory. Therefore, using a disaggregated architecture will allow for increased parallelism, which, at the same
time, will mitigate the overhead caused by remote memory.

",16.975882523387877,16.456328534031417,192
HPCC_17_046.txt,16.89890334481686,15.855031869759454,HPCC,3451,"
In recent years, fuzzy portfolio selection theory has
been well developed and widely applied. Based on the credibility
theory, several fuzzy portfolio selection models have been
proposed. The fuzzy Mean-CVaR portfolio model is one of the
state-of-the-art. However, its’ fuzzy nature which increases the
computational complexity makes it take a long time to solve. In
order to solve the fuzzy Mean-CVakR portfolio model efficiently, a
hybrid intelligent algorithm is designed by integrating Genetic
Algorithm(GA) with adaptive penalty function, Simulated
Annealing Resilient Back Propagation (SARPROP) neural
network and fuzzy simulation techniques, and to accelerate the
computation speed further, we parallelize the hybrid intelligent
algorithm with MPI technology. In order to demonstrate its
validity and efficiency, we achieve numerical experiments on the
Era supercomputer, and the results are compared with the
method which is obtained by integrating traditional GA and fuzzy
simulation directly. The results show that hybrid intelligent
algorithm can get better performance. Experiments under
different processor cores also achieved on the Era supercomputer
demonstrate the scalability of the parallel hybrid intelligent
algorithm.

",17.971250198000288,17.61558988764045,179
HPCC_17_047.txt,15.05964202507364,12.877539370329892,HPCC,5566,"
Effective design of concurrent tree implementation
plays a major role in improving the scalability of applications
in a multicore environment. We introduce a concurrent binary
search tree with fatnodes (FatCBST) and present algorithms to
perform basic operations on it. Unlike a simple node with single
value, a fatnode consists of a set of values. FatCBST concept
allows a thread to perform update operations on an existing
fatnode without changing the tree structure, making it less
disruptive to other threads’ operations. Fatnodes help to take
advantage of the spatial locality in the cache hierarchy and also
reduce the height of the concurrent binary search tree. Our
FatCBST implementation allows multiple threads to perform
update operations on the same existing fatnode at the same time.
Experimental results show that the FatCBST implementations
that have small fatnode sizes provide better throughput for
high and medium contention workloads; and large fatnode sizes
provide better throughput for low contention workloads, as
compared to the current state-of-the-art implementations.
",16.613394197324528,15.414978354978356,166
HPCC_17_048.txt,15.22359621728826,13.549466850055953,HPCC,5266,"
In prior-research the authors have demonstrated
that, for stencil-based numerical solvers for Partial Differential
Equations (PDEs), the parallel performance can be significantly
improved by selecting sub-domains that are not cubic in shape
(Saxena et. al., HPCS 2016, pp. 875-885). This is achieved through
accounting for cache utilization in both the message passing
and the computational kernel, where it is demonstrated that
the optimal domain decompositions not only depend on the
communication and load balance but also on the cache-misses,
amongst other factors. In this work we demonstrate that those
conclusions may also be extended to more advanced numerical
discretizations, based upon Adaptive Mesh Refinement (AMR).
In particular, we show that when basing our AMR strategy
on the local refinement of patches of the mesh, the optimal
patch shape is not typically cubic. We provide specific examples,
with accompanying explanation, to show that communication
minimizing strategies are not necessarily the best choice when
applying AMR in parallel. All numerical tests undertaken in this
work are based upon the open source BoxLib library.

",18.062587368997235,17.505681818181817,177
HPCC_17_049.txt,14.681155712647804,12.727153143620537,HPCC,4813,"
Depicting a complex system like social networks as a
graph helps understand its structure and relation. As advances in
technology increase the amount of data, simplifying a large-scale
graph has attracted interests. Simplification reduces the size of
a graph while preserving its important properties.

In this paper, we propose the summarization algorithm to
simplify a graph focusing on degree correlation and clustering
coefficient. The degree correlation is a measure to assess the
influence of each vertex and their connections. The clustering
coefficient estimates latent connections between two distinct
vertices. To this end, we first separate a graph into communities.
Looking at groups instead of a graph itself allows us to extract
important vertices and edges more easily. We then categorize
communities into four cases and simplify them in different
ways to preserve the innate characteristics. The quantitative and
qualitative evaluations demonstrate how effectively our algorithm
serves the goal.

Overall, our contributions are as follows: (a) unique pattern:
We found that hubs are connected indirectly via low-degree
vertices. Sigcon preserves these vertices during simplification.
(b) efficient algorithm: Sigcon identifies influential vertices and
edges to simplify a graph on the basis of degree correlation and
clustering coefficient connecting vertices effectively.

",16.11434528070225,14.510000000000002,201
HPCC_17_050.txt,14.058024009251781,12.469520572134915,HPCC,5559,"
In this paper, we show how to integrate user-item
scoring into a graph-based tag-aware item recommender system.
Building upon the ProbS and PLIERS methods, we introduce
refined formulas for affinity and similarity scoring taking into
account user-item preference in the mass diffusion of
recommender systems. Additionally, we propose a two-step
similarity score that recommends items based on a repeated mass
diffusion on the item-tag graph. We denote the proposed method
as User Preference-based Probability Spreading for content
recommendation, UPPS. UPPS relies on the notion that the
influence of current user items on the recommendation process
should depend on the user item preference. To evaluate the
proposed approach, we employ the well-known MovieLens
dataset. In comparison to ProbS and PLIERS, UPPS yields an
improvement in both the NDCG@10 and P@10 measures by more
than 25% over ProbS and more than 100% over PLIERS.

",13.925175675911131,13.456476190476192,153
HPCC_17_051.txt,14.82206552363181,12.742837361413091,HPCC,5753,"
Nowadays Video-on-Demand are widely used and
number of customers increases, consequently, many network
problems must be solved to ensure the quality of service. One
of the solutions that has proved its efficiency consists in using
Peer-to Peers architecture. However, they raise challenges such as
peers resource allocation. Most literature on tackle the problem
with optimal static rules found at off-line study of the system. In
this paper, we propose to formulate this problem as a dynamic
optimization problem, then, solve it using a dynamic learning
based optimization algorithm. The obtained results show that
using a dynamic resource allocation reduces the rejection rate
while enhancing the entropy of the system, in the face of a
dynamically changing title demand.

",15.247664890283005,13.425245901639347,123
HPCC_17_052.txt,16.222070456875528,15.12059959118783,HPCC,5001,"
The Shopping Mall is in many countries a
focus point for the lives of the people. It is the place where
people go and shop, and in a certain sense it has become
the modern equivalent of a square. Along the years, this
attraction center has followed a definite evolutionary path,
becoming wider, bigger, looking to maximize the comfort
of the customers: a better place, for better sales. In this
paper, we analyze the shopping mall under a different
perspective, and check if this evolutionary line is in fact
the best possible one, or if instead the mall could be shaped
differently. We propose a structural modification of the
mall (halving), and analyze its impact on the efficiency
(the attractiveness of the shops). Counterintuitively, we
find that this structural modification leads to a significant
increase in the overall mall performance, a result that
impacts the recent evolution of the mall, and that suggests
that the mall as we know it might be far from being the
best one, and some radical changes could be in order.

",15.021129683784007,14.622727272727271,177
HPCC_17_053.txt,13.621798314149924,11.270304218657163,HPCC,5510,"
In a RFID enabled system, tagged products is
ordinarily required to be transferred from an owner to another,
which makes the ownership of corresponding tags change for
several times. To overcome problem of temporary ownership
transfer, a lightweight anonymous group ownership transfer
protocol is proposed in this paper. It involves ownership transfer
and ownership recovery for group tags, which are all based upon
mutual authentication. Compared with most of previous
protocols using hash functions and cryptographic suites, our
scheme only utilizes simple operations, which are completely
compliant with EPC Class-1 Generation-2 standard. Through
our scheme, the window problem in multi-owner environment is
first solved. Security analysis shows that our protocol can
provide security and privacy preservation. It is also
demonstrated that it resists various attacks. Performance
comparisons indicate that our scheme can meet necessary
security requirements and achieve better performance.
",16.218646115125612,14.486145104895105,144
HPCC_17_054.txt,13.954856865014328,12.1430187961938,HPCC,4539,"
Vehicular ad hoc networks play an important role
in current intelligent transportation networks which have gained
much attention from academia and industry. The vehicular
networks can be implemented by the LTE/LTE-A networks,
which have been formally defined in a series of standards by the
third generation partnership projects (3GPP). There are lots of
challenges in the authentication processes in the LTE/LTE-A. In
this paper, aimed to improve the security functionality of the
vehicular networks, a new secure and efficient group
authentication and key agreement scheme for the vehicular
networks has been proposed, named as the GAKAV. Compared
with the existing schemes, the proposed scheme can greatly
reduce the amount of control message transmission from mass of
vehicular equipment (VEs) to the network and avoid much
overhead in the LTE/LTE-A networks. Besides, it has the
following security functions including privacy preservation, nonframeability and non-repudiation verification.
",16.92669308720184,15.793211920529803,152
HPCC_17_055.txt,15.485773399422676,13.000375728902018,HPCC,4523,"
Session hijacking of controller attack is one of the
most common ways of information leakage Software-Defined
Networking is facing, which brings a serious threat to cyber
security. However, the existing defense technologies mainly focus
on how to detect attacks and reduce the attack success rate. The
paper proposes a method from another perspective to minimize
the cost that the network undertakes and find an optimal
defender’s strategy when an attack is unavoidable in some case.
The main work is as follows. First of all, we models the scenario
of attack and defense as a Stackelberg Games, and prove the
optimal strategy is equal to the SSE (Strong Stackelberg
Equilibrium). Furthermore, we design an algorithm to solve the
equilibrium of the game in the case of infinite strategic space.
Finally, the experimental results show that the proposed
algorithm is feasible, and then the optimal defense strategy we
obtained is significantly superior to other general strategies and
cut down the total cost of the whole network in practical.

",16.785175570968402,14.699914456800688,169
HPCC_17_056.txt,14.818844353412025,12.982104984827071,HPCC,4216,"
This work studies the problem of GPU thread
mapping for a Sierpitski gasket fractal embedded in a discrete
Euclidean space of n x n. A block-space map \ : Zz +> Z? is
proposed, from Euclidean parallel space E to embedded fractal
space F, that maps in O(log, log.(n)) time and uses no more
than O(n™) threads with H ~ 1.58... being the Hausdorff
dimension, making it parallel space efficient. When compared to
a bounding-box map, \(w) offers a sub-exponential improvement
in parallel space and a monotonically increasing speedup once
nm > no. Experimental performance tests show that in practice
A(w) can produce performance improvement at any block-size
once n > no = 2°, reaching approximately 10x of speedup for
n = 2'© under typical block configurations.

",15.381575749822971,13.614248175182482,129
HPCC_17_057.txt,15.368551322841661,13.936144271207542,HPCC,6526,"
The emergence of Next Generation Sequencing
(NGS) platforms has increased the throughput of genomic sequencing and in turn the amount of data that needs to be
processed, requiring highly efficient computation for its analysis.
In this context, modern architectures including accelerators and
non-volatile memory are essential to enable the mass exploitation
of these bioinformatics workloads. This paper presents a redesign
of the main component of a state-of-the-art reference-free method
for variant calling, SMUFIN, which has been adapted to make
the most of GPUs and NVM devices. SMUFIN relies on counting the frequency of k-mers (substrings of length k) in DNA
sequences, which also constitutes a well-known problem for many
bioinformatics workloads, such as genome assembly. We propose
techniques to improve the efficiency of k-mer counting and to
scale-up workloads like SMUFIN that used to require 16 nodes
of Marenostrum 3 to a single machine with a GPU and NVM
drives. Results show that although the single machine is not able
to improve the time to solution of 16 nodes, its CPU time is 7.5x
shorter than the aggregate CPU time of the 16 nodes, with a
reduction in energy consumption of 5.5x.

",17.879347455551382,16.965547263681596,204
HPCC_17_058.txt,14.725506899497937,13.07530260917223,HPCC,5408,"
Graphics processing units (GPUs) provide high performance at low power consumption as long as resources are
well utilized. Thread block size is one factor in determining
a kernel’s occupancy, which is a metric for measuring GPU
utilization. A general guideline is to find the block size that leads
to the highest occupancy. However, many combinations of block
and grid sizes can provide highest occupancy, but performance
can vary significantly between different configurations. This is
because variation in thread structure yields different utilization
of hardware resources. Thus, optimizing for occupancy alone is
insufficient and thread structure must also be considered. It is
the programmer’s responsibility to set block size, but selecting
the right size is not always intuitive. In this paper, we propose
using machine learning to automatically select profitable block
sizes. Additionally, we show that machine learning techniques
coupled with performance counters can provide insight into the
underlying reasons for performance variance between different
configurations.

",15.021129683784007,14.041505376344087,158
HPCC_17_059.txt,14.295452379269392,12.591364575561041,HPCC,4282,"
Finite Element Analysis (FEA) is a numerical
method for solving engineering problem. Implementations of FEA
are common high performance computing (HPC) applications.
As a typical application of FEA, surface reconstruction of large
model, proves to be a computationally expensive task as well.
Poisson Surface Reconstruction, a cutting-edge surface reconstruction algorithm, is a commonly used method for efficiently
solving linear systems and it creates watertight surfaces from
oriented point sets. Poisson Surface Reconstruction leverages
an octree-based Marching Cubes (MC) method for isosurface
extraction. Hierarchical octree structure avoids unnecessary cells
visiting and prevents cracks arising. In this paper, we integrate
several quality improved methods with MC and coordinate unrelated components, obtaining a better quality mesh implementation. We improve mesh quality based on an extended lookup table
and modify the connectivity of some fundamental patterns in MC,
which effectively remove the reconstruction holes, thus improving
overall surface quality. As for the relative value between each
vertex and the average isovalue, the extended table explicitly
differentiates between “strictly larger” and “equal to”. Newly
introduced patterns in MC statistically prevent poor quality
triangles production. Moreover, a decision making algorithm is
proposed to eliminate ambiguity problems. We adapt SnapMC
algorithm to avoid non manifold triangles to a certain extent.
Comparisons with traditional Poisson algorithm and Smooth
Signed Distance (SSD) highlight the capability in quality mesh
generation and efficacy in handling high computational demand.

",16.941846955676066,16.528908296943232,230
HPCC_17_060.txt,17.192909130983228,16.152452827595685,HPCC,5464,"
Since the Local Outlier Factor (LOF) was first
proposed, there is a large family of approaches that is derived
from it. For the reason that the existing local outliers detection
approaches only focus on the extent of overall separation between
an object and its neighbors, and do not pay attention to the degree of dispersion between them, the precision of these approaches will be seriously affected in the scattered data sets for outlier
detection. In this paper, we redefine the local outliers by combining the degree of dispersion of the object and its neighbors, and
propose a new local outlier detection approach (N2DLOF). Compared to conventional approaches, the outliers obtained by
N2DLOF are more sensitive to the degree of anomaly of the scattered data sets. Experiments show that our approach has a significant improvement on outlier detection precision in the case of
scattered datasets with similar time complexity. In short, we extend the ecosystem of the local outlier detection approaches from
a new perspective.
",18.243605946275583,15.935853658536587,165
HPCC_17_061.txt,14.433388466209305,12.206276391512638,HPCC,5839,"
Bitcoin introduces a revolutionary decentralized
consensus mechanism. However, Bitcoin-derived consensus mechanisms applied to public blockchain are inadequate for the
deployment scenarios of budding consortium blockchain. We
propose a new consensus algorithm, Proof of Vote (POV). The
consensus is coordinated by the distributed nodes controlled
by consortium partners which will come to a decentralized
arbitration by voting. The key idea is to establish different
security identity for network participants, so that the submission
and verification of the blocks are decided by the agencies’ voting
in the league without the depending on a third-party intermediary or uncontrollable public awareness. Compared with
the fully decentralized consensus—Proof of Work (POW), POV
has controllable security, convergence reliability, only one block
confirmation to achieve the transaction finality, and low-delay
transaction verification time.

",18.7741,18.184615384615388,131
HPCC_17_062.txt,15.431234336397452,13.176009436976237,HPCC,5819,"
Public safety officials want to have maximum
situational-awareness though real-time information, such as video
content, for natural disaster management. The video content
can be generated by surveillance cameras or crowd-sourced
(e.g., using smart-phones) and live-streamed to the Incident
Commander. Such video contents need to be processed to adapt
the characteristics of the specialized multi-view display devices.
When a disaster occurs, there is a surge in the number of videos
streamed to the Incident Commander that oversubscribes the
processing servers and the network load. Incident Commanders,
however, need a smooth and uninterrupted viewing experience
specifically for the important events of interest that can change
over time. The challenge is how to enable the Incident Commander to interactively prioritize important video streams to receive
them uninterruptedly while the system is oversubscribed. In such
a system, normal video streams (i.¢., non-prioritized ones) should
not be interrupted at the expense of prioritization. To address
this challenge, in this research, we propose a stream-priority
aware resource allocation mechanism to enable interactive video
prioritization without a major impact on the flow of nonprioritized video streams. The mechanism includes a method to
select appropriate tasks from the arriving ones and a method
to map the selected task to the appropriate video server. Our
simulation results express that the percentage of normal and
prioritized video streaming tasks that have completed on-time
are improved, when compared with baseline scheduling methods.

",17.228024813938504,15.428404375707284,242
HPCC_17_063.txt,16.441279058678656,16.182660186803684,HPCC,5243,"
An increasing number of scientific applications
carry out big data transfer through dedicated networks for
global collaboration, where bandwidth scheduling plays a critical
role in improving the utilization of network resources and
meeting diverse user requests. In this paper, we formulate a
periodic bandwidth scheduling problem to maximize the number
of satisfied user requests for profiling-based floating-window
bandwidth reservations under deadline constraint on a network
path, referred to as PFWR-DC. We prove that PFWR-DC is
NP-complete, and propose an integrated bandwidth scheduling
algorithm based on the product of floating-window size and
dynamic transport throughput, referred to as ProductWT-BS.
Extensive simulation results shed light on the performance
superiority of ProductWT-BS in terms of scheduling success ratio
over five heuristic algorithms designed for performance comparison. The proposed scheduling algorithm has great potential to
improve the performance of collaborative scientific applications
that require the floating-window bandwidth provisioning service
for coordinated network-based operations.

",21.640513641318698,23.735141509433962,160
HPCC_17_064.txt,16.10923599650654,14.284895118668548,HPCC,5840,"
 Energy efficiency is considered a challenging
problem in modern multicore systems. Partitioning the cores into
multiple voltage and frequency islands (VFI) provides a
compromise between simple global Dynamic Voltage Frequency
Scaling (DVFS) and fine-grain per-core, per-task DVFS. This
paper formulates the optimization problem of scheduling tasks
statically on multiple VFIs as a Mixed Integer Linear
Programming (MILP) such that for a given energy budget, the
program execution time (makespan) is minimized. Our proposed
solution consists of two steps. In the first step, we use an Integer
Linear Programming (ILP)-based algorithm, from our previous
work, to assign per-core fine-grain dynamic Voltage/Frequency
(V/F) levels to each task in a task set (program) to minimize the
makespan for a given energy budget. In the second step, which is
the focus of this paper, we use the MILP framework to schedule
this task set, with the given V/F levels provided in step one, on the
islands of a VFI-enabled multicore system to again minimize the
makespan subject to (1) the energy budget and (2) the task set’s
precedence (dependency) constraints. Together with the solutions
obtained by MILP, a round-robin algorithm is used to compare
these two methodologies to [LP that provides the best solution.
Our experimental results show that across all the benchmarks
considered, the MILP-based and round-robin makespan
solutions are on average 1.2 and 2.28 times slower than the ILPbased makespan solutions, respectively.
",18.243605946275583,16.74843106995885,247
HPCC_17_065.txt,15.873273068229356,15.159437288350158,HPCC,4891,"
Recently, several studies have considered applications with a single time constraint (i.e., deadline) running on
cloud systems. In this work, to effectively support user requests
with flexible timing constraints (e.g., users may prefer expedited services and are willing to pay extra for getting their job
processed at earlier times), we consider applications with multiple deadlines for being processed in resource-constrained cloud
systems and investigate corresponding virtual machine (VM)
provisioning schemes. Specifically, by considering the multiple
deadline-bid pairs of user requests, we propose a Slope-based
Time-Sensitive Resource Factor with Dominant Resource being
considered to prioritize such requests. In addition, we study
the mapping schemes that allocate multiple VMs of a user
request to only one or multiple computing nodes, which are
denoted as Bundled and Distributed mappings, respectively. The
evaluation results show that, compared to the single deadline
schemes, the proposed VM provisioning schemes that consider
multiple deadlines and distributed mapping can significantly
improve the achieved system benefit and resource utilization.

",19.083932058031824,19.79468292682927,167
HPCC_17_066.txt,16.260532769752828,15.113864235500877,HPCC,5624,"
That the sum of guarantees on bandwidth for any
link should be smaller than the link capacity, a prerequisite
of offering bandwidth guarantees for tenants in the cloud, is
satisfied by virtual machine placement algorithms. The current
ones, however, either know nothing about bandwidth guarantees
or employ a coarse-grained model for resource abstraction. To
solve the problem, this paper first proposes a fine-grained virtual
machine placement algorithm which is formulated as a nonlinear
program of which the objective is to minimize the number of
physical machines used. Specifically, apart from constraints for
server resources, we add an additional one for each link to ensure
the sum of offered guarantees for each of those links is not greater
than its capacity. Further, we devise a heuristic algorithm to
address the nonlinear programming problem. Through extensive
simulations, we show that our approach is cost-effective, which
can reduce the number of physical machines required by 26.17%
on average compared with the most recent one.

",16.728156217252725,15.43219512195122,166
HPCC_17_067.txt,17.8199060379095,17.335746509058286,HPCC,5814,"
Collaborative cloud storage environment, which
share resources of multiple geographically distributed datacenters
owned by different providers enable scientific workflow from
different locations to process large scale big intermediate data
through the Internet. Distributed datacenters are federated and
each member can collaborate with each other to efficiently share
and process the intermediate data from distributed workflow
instances. This paper focuses on the storage cost minimization
of intermediate data placement in federated cloud datacenters.
Through collaborative and federation mechanisms, we propose an
exact federation data placement algorithm based on integer linear
programming model (ILP) to assist multiple datacenters hosting
intermediate data files generated from a scientific workflow.
Under the constraints of the problem, the proposed algorithm
finds an optimal intermediate data placement with a cost saving
over the federated cloud datacenters, taking into account scientific
user requirements, data dependency and size. Experimental
results show the cost-efficiency of the proposed cloud storage
federation algorithm.

",20.10790988173199,20.995263157894737,153
HPCC_17_068.txt,15.846826538257016,13.888220090163315,HPCC,6685,"
Cloud instances are usually virtual machines hosted
on shared hardware. Containers are often used to deploy services
in cloud instances. However, excessive consumption of shared
hardware resources by some VMs may lead to unpredictability in
the performance of containers running in co-located VMs. Existing techniques to detect performance interference in applications
are either too expensive in terms of profiling or applicable only
from the perspective of the infrastructure owner. In this paper, we
propose Sherlock, a lightweight subscriber-centric mechanism to
detect performance interference and estimate its impact on cloud
services. Sherlock does not require access to hardware counters
and can work on unmodified clouds without any support from
the cloud provider or changes to the hypervisor. Sherlock uses a
simple profiling technique which is performed only for a short
duration before deployment. When interference from co-resident
VMs is observed, Sherlock notifies the cloud subscriber, so that
any remedial actions can be taken. We also define a metric Score,
which is an estimate of the impact of interference on a service.
Experiments on the real-world web benchmark CloudSuite show
that our approach is able to detect interference with accuracy
ranging from 89% to 98.4%, and with very less false positives
(< 8%).

",16.32212239822248,14.648455284552846,206
HPCC_17_069.txt,15.430449953263086,13.389146024879782,HPCC,4933,"
Non-Uniform Memory Access (NUMA) architecture
has become the dominant architecture and is widely used in virtualization platforms. In NUMA-based cloud computing platform,
arbitrary topology of vCPUs and memory may cause significant
performance degradation for VMs, which introduces great challenges for virtual machine monitors (VMMs) to efficiently manage
the vCPUs and memory. Previous studies mainly sample the
characteristics of the vCPUs to indicate the optimizing strategies
to reduce the NUMA overheads in virtualization platforms. But
the typical periodical sampling methods have some deviations
with the real vCPU characteristics. This leads to the inaccurate
sampling and scheduling decisions for the optimizing strategies.

Motivated by the inaccuracy in sampling methods and scheduling decisions, we propose a fine-grained scheduler, named vScope,
which makes accurate scheduling decisions according to the
guest OS processes in the vCPUs, to improve the performance
of memory-intensive workloads in cloud platforms. In vScope,
the VMM identifies the guest OS processes in the vCPUs and
calculates the NUMA affinity of each process from the PMU
data. At the end of vCPU’s scheduling cycle, the scheduler
appropriately schedules the vCPUs to their local NUMA node
to alleviate the unnecessary NUMA overhead. We implement
vScope in Xen-4.5.1 VMM and evaluate its effectiveness with some
memory-intensive benchmarks. The experimental results shows
that vScope can achieve up to 11.5% performance improvement
for these workloads when compared with the Credit scheduler
in Xen. Moreover, vScope only introduces limited overhead into
the system.

",17.64278748958908,15.761074380165294,247
HPCC_17_070.txt,15.59512522859632,12.944651374582786,HPCC,5907,"
Diskless checkpointing is an effective solution to
avoid the I/O bottleneck in disk-based checkpointing for
tolerating a small number of node failures in large distributed
systems. However, the existing encoding schemes used by diskless
checkpointing lead to high communication overheads because of
cross node encoding. This negates the advantages of diskless
checkpointing, especially in scenarios with a limited network
bandwidth. This paper proposes a diskless checkpointing scheme
with vertical encoding to address this problem. Vertical encoding
eliminates the dependency among nodes and also facilitates a
balanced communication. Moreover, an analysis model is
developed to obtain an optimal configuration parameter.
Experimental results show that the proposed scheme reduces
significantly the communication overhead of both checkpointing
and fault recovery, with no encoding overhead introduced.

",17.613555460941566,15.775023041474658,125
HPCC_17_071.txt,15.611409494744176,14.448247501988039,HPCC,2795,"
In today’s large-scale High Performance Computing (HPC) systems, an increasing portion of the computing
capacity is wasted due to failures and recoveries. It is expected
that exascale machines will decrease the mean time between
failures to a few hours, making fault tolerance a major
challenge. This work explores novel methodologies to fault
tolerance that achieve forward recovery, power-awareness, and
scalability. The proposed model, referred to as Rejuvenating
Shadows, is able to deal with multiple types of failure and
maintain consistent level of resilience. An implementation is
provided for MPI, and empirically evaluated with various
benchmark applications that represent a wide range of HPC
workloads. The results demonstrate Rejuvenating Shadows’
ability to tolerate high failure rates, and to outperform inmemory checkpointing/restart in both execution time and
resource utilization.

",16.728156217252725,16.006153846153847,132
HPCC_17_072.txt,14.329049965797331,12.095926879093284,HPCC,5335,"
Phase Change Memory (PCM) is considered as one
of the most popular candidates to replace flash memory in mobile
consumer systems. PCM has many superior performance characteristics, including non-volatility, byte-addressability, low access
latency and power consumption. However, it also suffers from
finite program counts like flash memory. Prior researches used
PCM as a black box, and implemented the wear leveling schemes
in device controller, which failed to utilize file attributes in host
side and result in poor efficiency of wear evenness. In this paper,
we propose a file aware wear leveling algorithm (called FAWL)
for PCM-based storage system in mobile consumer electronics.
FAWL is designed in the host side, which combines file attributes
and statistical information of PCM. It exploits rich attributes of
files to divide files into different categories and distribute them
in suitable pages to avoid extra swap overhead. In addition, by
utilizing an adjust management in FAWL, the wear imbalance
can be greatly mitigated. Experimental results show that FAWL
effectively improves the lifetime of PCM compared with existing
wear leveling algorithms, including random swapping, start-gap
and segment swapping.

",16.32212239822248,14.04936936936937,186
HPCC_17_073.txt,16.287551027696072,14.714432754907541,HPCC,5387,"
The large-scale surveillance video processing workloads are gradually migrated to cloud computing platforms.
Meanwhile, the hybrid storage architecture, integrating both
HDD and SSD storage devices, is increasingly used in the current
cloud platforms. However, the computing and storage capabilities
of the nodes are constantly changing, and this requires the
delicate design of the data layout strategy for avoiding the serious
load skew in the distributed computing nodes with the hybrid
storage architecture. In this paper, we propose a Load-Aware
Data Migration (LADM) scheme for distributed surveillance
video processing with hybrid storage architecture. Specifically,
according to the proposed the load estimation model and the
storage capacity constraint, the Node-Level Data Migration
(NLDM) strategy is used to periodically migrate the appropriate
video chunks from the local HDD to the local SSD for improving
the node processing performance, and the Cluster-Level Data
Migration (CLDM) strategy is used to periodically migrate the
appropriate video chunks from the high load nodes to the low
load nodes for achieving the overall load balance of cluster.
We conduct the extensive experiments based on the distributed
surveillance video processing platform we developed, and the
experimental results show that the proposed LADM scheme
outperforms the current methods.

",20.581828153500815,19.79643564356436,203
HPCC_17_074.txt,15.283360298639305,13.139628602907205,HPCC,4931,"
 In real-time distributed transaction processing,
deadlocks must be detected and resolved. Timeouts are not a
viable option because they lead to lost work and missed deadlines.
We have proposed a suite of deadlock detection protocols and a
resolution protocol which carry a varying degree of (generally low)
overhead. The protocols behave differently under varying load
conditions and transaction characteristics. Further, the invocation
period of these protocols can be controlled to improve
performance when the overhead tends to become large. The
performance of these protocols has been demonstrated using a
distributed real-time transaction processing simulator which
provides an interactive interface to set parameters, and also has
provisions to select from a variety of concurrency control, priority
assignment, workload distribution and other protocols. The
impact of transaction workload, underlying system configuration,
resource availability, detection rates, and congestion on each of the
proposed protocols is observed and presented. In general, the
multi-cycle detection protocol demonstrated the most superior
performance over a broad range of parameters. The results
presented in this paper were obtained from over 147,000
simulations.

",17.5058628484301,15.54666666666667,179
HPCC_17_075.txt,14.630688737672983,12.435875813073768,HPCC,5646,"
Real-time systems need reliable guarantees for the
satisfaction of their timing constraints. However, novel speed-up
hardware architectures and software mechanism, which target
improving average-case performances, ignore and sometimes
worsen the ability to obtain guarantees. An alternative approach
is the Logical Execution Time (LET) model, but there are some
deficiencies in existing LET-based development tools. In this
paper, we propose a novel LET-based time-aware programming
framework called TipFrame. The framework introduces Servants
to improve the responsiveness of LET-based periodic tasks further. The runtime makes behaviors in the system level consistent
with the semantics of LET model for predictability. TipFrame
implements in C language providing time-aware programming
interfaces called TipFrame-C. The programming paradigm of
TipFrame-C is described using an autopilot avionic control
system. Evaluation results demonstrate that our approach is
effective and efficient to construct LET-based real-time systems.

",15.470042427545799,13.865225225225227,149
HPCC_17_076.txt,14.610463261953743,12.18788392196669,HPCC,5742,"
Position prediction of moving object has become a
reality utilizing the vast amount of location data acquired by
positioning devices embedded in mobile phones and cars. In this
paper, we proposed a position prediction system which focuses on
the time regularity of object moving. Historical location data of the
object is used to extract personal trajectory patterns to obtain
candidate next positions. Each of the candidate positions is scored
by the proposed Time Mode-based Prediction (TMP) algorithm
according to the proximity between the time component of
patterns and current time. The position with the highest score is
regarded as predicted next position. Furthermore, a hybrid B/S
and C/S architecture is employed to perform the real-time
prediction and results display. An evaluation based on a public
trajectory data set of 12 objects demonstrates that the proposed
TMP algorithm can realize position prediction with high accuracy.
Moreover, the average accuracy rate of our prediction algorithm
is about 85.5%, which is 33.7% greater than the Markov-based
algorithm with one known position.

",17.122413403193683,14.738547687861274,176
HPCC_17_077.txt,15.750415652941605,13.645407166123778,HPCC,6629,"
Internet of Things (IoT), one of the key elements
of a smart factory, is dubbed as Industrial IoT (IloT). Software
defined networking is a technique that benefits network management in IIoT applications by providing network reconfigurability.
In this way, controllers are integrated within the network
to advertise routing rules dynamically based on network and
link changes. We consider controllers within Wireless Sensor
Networks (WSNs) for IloT applications in such a way to provide
reliability and timeliness. Network reliability is addressed for the
case of node failure by considering multiple sinks and multiple
controllers. Real-time requirements are implicitly applied by
limiting the number of hops (maximum path-length) between
sensors and sinks/controllers, and by confining the maximum
workload on each sink/controller. Deployment planning of sinks
should ensure that when a sink or controller fails, the network
is still connected. In this paper, we target the challenge of
placement of multiple sinks and controllers, while ensuring that
each sensor node is covered by multiple sinks (k sinks) and
multiple controllers (’ controllers). We evaluate the proposed
algorithm using the benchmark GRASP-MSP through extensive
experiments, and show that our approach outperforms the
benchmark by lowering the total deployment cost by up to
24%. The reduction of the total deployment cost is fulfilled not
only as the result of decreasing the number of required sinks
and controllers but also selecting cost-effective sinks/controllers
among all candidate sinks/controllers.

",17.00531248756302,14.763428571428573,239
HPDC_17_001.txt,16.059235859764016,14.598679296826983,HPDC,4088,"

Scientific workflows are increasingly common in the workloads of
current High Performance Computing (HPC) systems. However,
HPC schedulers do not incorporate workflow-specific mechanisms
beyond the capacity to declare dependencies between their jobs.
Thus, workflows are run as sets of batch jobs with dependencies,
which induces long intermediate wait times and, consequently, long
workflow turnaround times. Alternatively, to reduce their turnaround time, workflows may be submitted as single pilot jobs that
are allocated their maximum required resources for their entire
runtime. Pilot jobs achieve shorter turnaround times but reduce
the HPC system’s utilization because resources may idle during
the workflow’s execution. We present a workflow-aware scheduling (WoAS) system that enables existing scheduling algorithms
to exploit fine-grained information on a workflow’s resource requirements and structure without modification. The current implementation of WoAS is integrated into Slurm, a widely used HPC
batch scheduler. We evaluate the system using a simulator using
real and synthetic workflows and a synthetic baseline workload
that captures job patterns observed over three years of workload
data from Edison, a large supercomputer hosted at the National Energy Research Scientific Computing Center. Our results show that
WoAS reduces workflow turnaround times and improves system
utilization without significantly slowing down conventional jobs.

",17.251386760058843,16.676470588235293,208
HPDC_17_002.txt,15.696214376419114,13.894613626288379,HPDC,6323,"

Key-based workload partitioning is a common strategy used in
parallel stream processing engines, enabling effective key-value
tuple distribution over worker threads in a logical operator. It is
likely to generate poor balancing performance when workload variance occurs on the incoming data stream. This paper presents a
new key-based workload partitioning framework, with practical
algorithms to support dynamic workload assignment for stateful
operators. The framework combines hash-based and explicit keybased routing strategies for workload distribution, which specifies
the destination worker threads for a handful of keys and assigns
the other keys with the hash function. When short-term distribution fluctuations occur to the incoming data stream, the system
adaptively updates the routing table containing the chosen keys, in
order to rebalance the workload with minimal migration overhead
within the stateful operator. We formulate the rebalance operation
as an optimization problem, with multiple objectives on minimizing
state migration costs, controlling the size of the routing table and
breaking workload imbalance among worker threads. Despite of the
NP-hardness nature behind the optimization formulation, we carefully investigate and justify the heuristics behind key (re)routing
and state migration, to facilitate fast response to workload variance
with ignorable cost to the normal processing in the distributed
system. Empirical studies on synthetic data and real-world stream
applications validate the usefulness of our proposals.

",19.53771442962202,18.563761210762333,224
HPDC_17_003.txt,14.651420488808167,12.828815810114687,HPDC,9009,"

We present NICE, a key-value storage system design that
leverages new software-defined network capabilities to build
cluster-based network-efficient storage system. NICE presents
novel techniques to co-design network routing and multicast with
storage replication, consistency, and load balancing to achieve
higher efficiency, performance, and scalability.

We implement the NICEKV prototype. NICEKV follows the
NICE approach in designing four essential network-centric
storage mechanisms: request routing, replication, consistency, and.
load balancing. Our evaluation shows that the proposed approach
brings significant performance gains compared to the current keyvalue systems design: up to 7x put/get performance improvement,
up to 2x reduction in network load, 3x to 9x load reduction on the
storage nodes, and the elimination of scalability bottlenecks
present in current designs.

",16.887214914478655,17.0992,126
HPDC_17_004.txt,14.674485576119398,13.61602335012904,HPDC,5117,"

Scientific workflows are increasingly used in High Performance
Computing (HPC) environments to manage complex simulation
and analyses, often consuming and generating large amounts of
data. However, workflow tools have limited support for managing
the input, output and intermediate data. The data elements of a
workflow are often managed by the user through scripts or other
ad-hoc mechanisms. Technology advances for future HPC systems
is redefining the memory and storage subsystem by introducing
additional tiers to improve the I/O performance of data-intensive
applications. These architectural changes introduce additional complexities to managing data for scientific workflows. Thus, we need
to manage the scientific workflow data across the tiered storage
system on HPC machines. In this paper, we present the design and
implementation of MaDaTS (Managing Data on Tiered Storage for
Scientific Workflows), a software architecture that manages data
for scientific workflows. We introduce Virtual Data Space (VDS), an
abstraction of the data in a workflow that hides the complexities of
the underlying storage system while allowing users to control data
management strategies. We evaluate the data management strategies with real scientific and synthetic workflows, and demonstrate
the capabilities of MaDaTS. Our experiments demonstrate the flexibility, performance and scalability gains of MaDaTS as compared to
the traditional approach of managing data in scientific workflows.

",17.238542476582836,16.631700934579445,215
HPDC_17_005.txt,14.690345738582344,12.779606029174666,HPDC,10622,"

User-Defined Functions (UDF) allow application programmers to
specify analysis operations on data, while leaving the data management tasks to the system. This general approach enables numerous
custom analysis functions and is at the heart of the modern Big
Data systems. Even though the UDF mechanism can theoretically
support arbitrary operations, a wide variety of common operations —
such as computing the moving average of a time series, the vorticity
of a fluid flow, etc., — are hard to express and slow to execute. Since
these operations are traditionally performed on multi-dimensional
arrays, we propose to extend the expressiveness of structural locality for supporting UDF operations on arrays. We further propose an
in situ UDF mechanism, called ArrayUDF, to implement the structural locality. ArrayUDF allows users to define computations on
adjacent array cells without the use of join operations and executes
the UDF directly on arrays stored in data files without requiring
to load their content into a data management system. Additionally,
we present a thorough theoretical analysis of the data access cost
to exploit the structural locality, which enables ArrayUDF to automatically select the best array partitioning strategy for a given UDF
operation. In a series of performance evaluations on large scientific
datasets, we have observed that — using the generic UDF interface ArrayUDF consistently outperforms Spark, SciDB, and RasDaMan.

",18.643177196211187,18.52460616438356,220
HPDC_17_006.txt,15.456261359676443,13.838102478868219,HPDC,5955,"

k-means is one of the most influential and utilized machine learning
algorithms. Its computation limits the performance and scalability
of many statistical analysis and machine learning tasks. We rethink
and optimize k-means in terms of modern NUMA architectures
to develop a novel parallelization scheme that delays and minimizes synchronization barriers. The k-means NUMA Optimized
Routine (knor) library has (i) in-memory (knori), (ii) distributed
memory (knord), and (ii) semi-external memory (knors) modules
that radically improve the performance of k-means for varying
memory and hardware budgets. knori boosts performance for single machine datasets by an order of magnitude or more. knors
improves the scalability of k-means on a memory budget using
SSDs. knors scales to billions of points on a single machine, using a fraction of the resources that distributed in-memory systems
require. knord retains knori’s performance characteristics, while
scaling in-memory through distributed computation in the cloud.
knor modifies Elkan’s triangle inequality pruning algorithm such
that we utilize it on billion-point datasets without the significant
memory overhead of the original algorithm. We demonstrate knor
outperforms distributed commercial products like HO, Turi (formerly Dato, GraphLab) and Spark’s MLlib by more than an order
of magnitude for datasets of 107 to 10? points.

",20.890749979661237,20.522272727272732,213
HPDC_17_007.txt,13.07923805802399,10.790899606466411,HPDC,9556,"

Stochastic gradient descent (SGD) is widely used by many machine
learning algorithms. It is efficient for big data applications due to
its low algorithmic complexity. SGD is inherently serial and its
parallelization is not trivial. How to parallelize SGD on many-core
architectures (e.g. GPUs) for high efficiency is a big challenge.

In this paper, we present cuaMF_SGD, a parallelized SGD solution
for matrix factorization on GPUs. We first design high-performance
GPU computation kernels that accelerate individual SGD updates by
exploiting model parallelism. We then design efficient schemes
that parallelize SGD updates by exploiting data parallelism. Finally, we scale cuMF_SGD to large data sets that cannot fit into one
GPU’s memory. Evaluations on three public data sets show that
cuMF_SGD outperforms existing solutions, including a 64-node
CPU system, by a large margin using only one GPU card.

",13.81666964889586,12.327402877697843,142
HPDC_17_008.txt,13.56184437578753,11.227192719506679,HPDC,10594,"

We reduce the cost of communication and synchronization in graph
processing by analyzing the fastest way to process graphs: pushing
the updates to a shared state or pulling the updates to a private
state. We investigate the applicability of this push-pull dichotomy to
various algorithms and its impact on complexity, performance, and
the amount of used locks, atomics, and reads/writes. We consider
11 graph algorithms, 3 programming models, 2 graph abstractions,
and various families of graphs. The conducted analysis illustrates
surprising differences between push and pull variants of different algorithms in performance, speed of convergence, and code
complexity; the insights are backed up by performance data from
hardware counters. We use these findings to illustrate which variant
is faster for each algorithm and to develop generic strategies that
enable even higher speedups. Our insights can be used to accelerate
graph processing engines or libraries on both massively-parallel
shared-memory machines as well as distributed-memory systems.
Site: https://spcl.inf.ethz.ch/Research/Parallel_Programming/PushPull

",17.77360955136429,15.953913043478263,169
HPDC_17_009.txt,15.72613890767878,14.245643996564677,HPDC,8828,"

Providing fault-tolerance is of major importance for data analytics frameworks such as Hadoop and Spark, which are typically
deployed in large clusters that are known to experience high failures rates. Unexpected events such as compute node failures are
in particular an important challenge for in-memory data analytics
frameworks, as the widely adopted approach to deal with them is
to recompute work already done. Recomputing lost work, however,
requires allocation of extra resource to re-execute tasks, thus increasing the job runtimes. To address this problem, we design a
checkpointing system called pANDa that is tailored to the intrinsic
characteristics of data analytics frameworks. In particular, PANDA
employs fine-grained checkpointing at the level of task outputs and
dynamically identifies tasks that are worthwhile to be checkpointed
rather than be recomputed. As has been abundantly shown, tasks
of data analytics jobs may have very variable runtimes and output sizes. These properties form the basis of three checkpointing
policies which we incorporate into PANDA.

We first empirically evaluate PANDA on a multicluster system
with single data analytics applications under space-correlated failures, and find that PANDA is close to the performance of a fail-free
execution in unmodified Spark for a large range of concurrent failures. Then we perform simulations of complete workloads, mimicking the size and operation of a Google cluster, and show that PANDA
provides significant improvements in the average job runtime for
wide ranges of the failure rate and system load.

",17.37919286519448,16.407103825136613,245
HPDC_17_010.txt,16.245393345835957,15.178693920038892,HPDC,7486,"

Requirements for reliability, low power consumption, and performance place complex and conflicting demands on the design of
high-performance computing (HPC) systems. Fault-tolerance techniques such as checkpoint/restart (C/R) protect HPC applications
against hardware faults. These techniques, however, have non negligible overheads particularly when the fault rate exposed by the
hardware is high: it is estimated that in future HPC systems, up
to 60% of the computational cycles/power will be used for fault
tolerance.

To mitigate the overall overhead of fault-tolerance techniques,
we propose LeiGo, an approach that attempts to continue the execution of a HPC application when crashes would otherwise occur. Our
hypothesis is that a class of HPC applications have good enough
intrinsic fault tolerance so that its possible to re-purpose the default
mechanism that terminates an application once a crash-causing
error is signalled, and instead attempt to repair the corrupted application state, and continue the application execution. This paper
explores this hypothesis, and quantifies the impact of using this
observation in the context of checkpoint/restart (C/R) mechanisms.

Our fault-injection experiments using a suite of five HPC applications show that, on average, LetGo is able to elide 62% of the
crashes encountered by applications, of which 80% result in correct
output, while incurring a negligible performance overhead. As a
result, when LetGo is used in conjunction with a C/R scheme, it
enables significantly higher efficiency thereby leading to faster time
to solution.

",18.377959752453624,17.728770491803278,245
HPDC_17_011.txt,16.127109704585074,14.241815718157184,HPDC,3398,"

With the rate of errors that can silently effect an application’s
state/output expected to increase on future HPC machines, numerous application-level detection and recovery schemes have been
proposed. Recovery is more efficient when errors are contained and
affect only part of the computation’s state. Containment is usually
achieved by verifying all information leaking out of a statically
defined containment domain, which is an expensive procedure. Alternatively, error propagation can be analyzed to bound the domain
that is affected by a detected error. This paper investigates how
silent data corruption (SDC) due to soft errors propagates through
three HPC applications: HPCCG, Jacobi, and CoMD. To allow for
more detailed view of error propagation, the paper tracks propagation at the instruction and application variable level. The impact
of detection latency on error propagation is shown along with an
application’s ability to recover. Finally, the impact of compiler optimizations are explored along with the impact of local problem size
on error propagation.

",16.373557378465907,14.83266304347826,165
HPDC_17_012.txt,15.430802447674107,13.940568420679828,HPDC,8328,"

We present the Hippo system to enable the diagnosis of distributed
machine learning (ML) pipelines by leveraging fine-grained data
lineage. Hippo exposes a concise yet powerful API, derived from
primitive lineage types, to capture fine-grained data lineage for
each data transformation. It records the input datasets, the output
datasets and the cell-level mapping between them. It also collects
sufficient information that is needed to reproduce the computation.
Hippo efficiently enables common ML diagnosis operations such
as code debugging, result analysis, data anomaly removal, and
computation replay. By exploiting the metadata separation and
high-order function encoding strategies, we observe an O(10°)x
total improvement in lineage storage efficiency vs. the baseline of
cell-wise mapping recording while maintaining the lineage integrity.
Hippo can answer the real use case lineage queries within a few
seconds, which is low enough to enable interactive diagnosis of ML
pipelines.

",17.28802050969988,15.973809523809525,150
HPDC_17_013.txt,15.755195620906417,13.624290185517314,HPDC,8135,"

Highly-parallel, high-performance scientific applications must maximize performance inside of a power envelope while maintaining
scalability. Emergent parallel and distributed systems offer a growing number of operating modes that provide unprecedented control of processor speed, memory latency, and memory bandwidth.
Optimizing these systems for performance and power requires an
understanding of the combined effects of these modes and thread
concurrency on execution time. In this paper, we describe how
an analytical performance model that separates pure computation
time (C) and pure stall time (S) from computation-memory overlap
time (O) can accurately capture these combined effects. We apply
the COS model to predict the performance of thread and power
mode combinations to within 7% and 17% for parallel applications
(e.g. LULESH) on Intel x86 and IBM BG/Q architectures, respectively. The key insight of the COS model is that the combined
effects of processor and memory throttling and concurrency on
overlap trend differently than the combined effects on pure computation and pure stall time. The COS model is novel in that it
enables independent approximation of overlap which leads to capabilities and accuracies that are as good or better than the best
available approaches.

",17.971250198000288,16.24548076923077,197
HPDC_17_014.txt,15.42675592999532,14.145982737592647,HPDC,3690,"

Disk-to-disk wide-area file transfers involve many subsystems and
tunable application parameters that pose significant challenges
for bottleneck detection, system optimization, and performance
prediction. Performance models can be used to address these challenges but have not proved generally usable because of a need for
extensive online experiments to characterize subsystems. We show
here how to overcome the need for such experiments by applying
machine learning methods to historical data to estimate parameters
for predictive models. Starting with log data for millions of Globus
transfers involving billions of files and hundreds of petabytes, we
engineer features for endpoint CPU load, network interface card
load, and transfer characteristics; and we use these features in both
linear and nonlinear models of transfer performance, We show that
the resulting models have high explanatory power. For a representative set of 30,653 transfers over 30 heavily used source-destination
pairs (“edges”), totaling 2,053 TB in 46.6 million files, we obtain
median absolute percentage prediction errors (MdAPE) of 7.0% and
4.6% when using distinct linear and nonlinear models per edge,
respectively; when using a single nonlinear model for all edges, we
obtain an MdAPE of 7.8%. Our work broadens understanding of
factors that influence file transfer rate by clarifying relationships
between achieved transfer rates, transfer characteristics, and competing load. Our predictions can be used for distributed workflow
scheduling and optimization, and our features can also be used for
optimization and explanation.

",20.130776976110326,20.208571428571435,243
HPDC_17_015.txt,15.174074067571151,13.531611405340893,HPDC,6578,"

In this paper, we develop a predictive model useful for output performance prediction of supercomputer file systems under production
load. Our target environment is Titan—the 3rd fastest supercomputer in the world—and its Lustre-based multi-stage write path.
We observe from Titan that although output performance is highly
variable at small time scales, the mean performance is stable and
consistent over typical application run times. Moreover, we find
that output performance is non-linearly related to its correlated parameters due to interference and saturation on individual stages on
the path. These observations enable us to build a predictive model
of expected write times of output patterns and I/O configurations,
using feature transformations to capture non-linear relationships.
We identify the candidate features based on the structure of the
Lustre/Titan write path, and use feature transformation functions
to produce a model space with 135,000 candidate models. By searching for the minimal mean square error in this space we identify a
good model and show that it is effective.

",16.439396014739867,15.1909440267335,173
HPDC_17_016.txt,15.969376441374191,15.184133553164017,HPDC,6284,"

Wide-area data transfers in high-performance computing infrastructures are increasingly being carried over dynamically provisioned
dedicated network connections that provide high capacities with
no competing traffic. We present extensive TCP throughput measurements and time traces over a suite of physical and emulated
10 Gbps connections with 0-366 ms round-trip times (RTTs). Contrary to the general expectation, they show significant statistical
and temporal variations, in addition to the overall dependencies on
the congestion control mechanism, buffer size, and the number of
parallel streams. We analyze several throughput profiles that have
highly desirable concave regions wherein the throughput decreases
slowly with RTTs, in stark contrast to the convex profiles predicted
by various TCP analytical models. We present a generic throughput
model that abstracts the ramp-up and sustainment phases of TCP
flows, which provides insights into qualitative trends observed in
measurements across TCP variants: (i) slow-start followed by wellsustained throughput leads to concave regions; (ii) large buffers and
multiple parallel streams expand the concave regions in addition
to improving the throughput; and (iii) stable throughput dynamics,
indicated by a smoother Poincaré map and smaller Lyapunov exponents, lead to wider concave regions. These measurements and
analytical results together enable us to select a TCP variant and its
parameters for a given connection to achieve high throughput with
statistical guarantees.

",21.043213290922328,21.237272727272728,221
HPDC_17_017.txt,13.884643708131001,11.645361325025814,HPDC,7954,"

Many distributed systems require coordination between the components involved. With the steady growth of such systems, the probability of failures increases, which necessitates scalable fault-tolerant
agreement protocols. The most common practical agreement protocol, for such scenarios, is leader-based atomic broadcast. In this
work, we propose ALLCONCUR, a distributed system that provides
agreement through a leaderless concurrent atomic broadcast algorithm, thus, not suffering from the bottleneck of a central coordinator.
In ALLCONCUR, all components exchange messages concurrently
through a logical overlay network that employs early termination
to minimize the agreement latency. Our implementation of ALLCONCUR supports standard sockets-based TCP as well as highperformance InfiniBand Verbs communications. ALLCONCUR can
handle up to 135 million requests per second and achieves 17x higher
throughput than today’s standard leader-based protocols, such as
Libpaxos. Thus, ALLCONCUR is highly competitive with regard to
existing solutions and, due to its decentralized approach, enables
hitherto unattainable system designs in a variety of fields.

",17.833180683606166,16.53401898734177,160
HPDC_17_018.txt,14.582176120602245,12.568092985837712,HPDC,7015,"

Graphs have become increasingly important in many applications
and domains such as querying relationships in social networks or
managing rich metadata generated in scientific computing. Many
of these use cases require high-performance distributed graph
databases for serving continuous updates from clients and, at the
same time, answering complex queries regarding the current graph.
These operations in graph databases, also referred to as online
transaction processing (OLTP) operations, have specific design and
implementation requirements for graph partitioning algorithms. In
this research, we argue it is necessary to consider the connectivity
and the vertex degree changes during graph partitioning. Based
on this idea, we designed an Incremental Online Graph Partitioning (IOGP) algorithm that responds accordingly to the incremental
changes of vertex degree. IOGP helps achieve better locality, generate balanced partitions, and increase the parallelism for accessing high-degree vertices of the graph. Over both real-world and
synthetic graphs, IOGP demonstrates as much as 2x better query
performance with a less than 10% overhead when compared against
state-of-the-art graph partitioning algorithms.

",18.08858127442927,17.171114781172587,174
HPDC_17_019.txt,17.14579406010497,15.51326392800257,HPDC,7110,"

Load balancing and partitioning are critical when it comes to parallel computations. Popular partitioning strategies based on space
filling curves focus on equally dividing work. The partitions produced are independent of the architecture or the application. Given
the ever-increasing relative cost of data movement and increasing
heterogeneity of our architectures, it is no longer sufficient to only
consider an equal partitioning of work. Minimizing communication
costs are equally if not more important. Our hypothesis is that an
unequal partitioning that minimizes communication costs significantly can scale and perform better than conventional equal-work
partitioning schemes. This tradeoff is dependent on the architecture as well as the application. We validate our hypothesis in the
context of a finite-element computation utilizing adaptive meshrefinement. Our central contribution is a new partitioning scheme
that minimizes the overall runtime of subsequent computations by
performing architecture and application-aware non-uniform work
assignment in order to decrease time to solution, primarily by minimizing data-movement. We evaluate our algorithm by comparing it
against standard space-filling curve based partitioning algorithms
and observing time-to-solution as well as energy-to-solution for
solving Finite Element computations on adaptively refined meshes.
We demonstrate excellent scalability of our new partition algorithm up to 262, 144 cores on ORNL’s Titan and demonstrate that
the proposed partitioning scheme reduces overall energy as well as
time-to-solution for application codes by up to 22.0%.

",18.821437475804725,17.277272727272727,239
ICAC_17_001.txt,15.867852595115348,14.047377846200252,ICAC,7704,"
Many online application services are now provided
by cloud-deployed VM clusters. Although economical, VMs in the
cloud are prone to interference due to contention for physical
resources among colocated users. Worse, this interference is
dynamic and unpredictable. Current provider-centric solutions
are application-oblivious and are thus not always aware of the
user’s SLO requirements or application bottlenecks. Further,
such solutions rely on VM scheduling and migration, approaches
that are not agile enough to mitigate volatile interference.

This paper presents DIAL, an interference-aware load balancer that can be employed by cloud users without requiring any assistance from the provider. DIAL addresses timevarying interference by dynamically shifting load away from
compromised VMs without violating the application’s tail latency
SLOs. The key idea behind DIAL is to infer the demand for
contended resources on the physical hosts, which is otherwise
hidden from users. Estimates of the colocated load are then
used to drive the load distribution for the application VMs.
Our experimental results on OpenStack and AWS clouds show
that DIAL can reduce application tail latencies by as much as
70% and 48% compared to interference-oblivious and existing
interference-aware load balancers, respectively.

",17.631426480028413,15.869243986254293,197
ICAC_17_002.txt,15.114689081267553,13.285531972229602,ICAC,6932,"
With the rapid growth of social media and
Internet-of-Things, real-time processing of big data has
become a core operation in various business areas. It is of
paramount importance that big-data analyses are executed
timely with specified accuracy guarantees. However, workloads in the wild are highly bursty with skewed contents
and often present the conundrum of meeting latency and
accuracy requirements simultaneously. In this paper we
propose AccStream, which selectively samples and processes data tuples and blocks on emerging batch streaming
platforms with a special focus on analysis of aggregation,
e.g., counts, and top-k. AccStream dynamically learns the
latency model of analysis jobs via on-line probing technique
and employs sample theory to determine the lower limit of
data so as to fulfill given accuracy targets. A unique feature
of AccStream ensuring strong latency-accuracy fulfillment
even under conflicts is the hybrid windowing that trades off
data freshness via a combination of tumbling and rolling
windows. We evaluate the prototype of AccStream on Spark
Streaming, analyzing Twitter data. Our extensive results
confirm that AccStream is able to achieve the latency and
accuracy target against a wide range of conditions, i.e., slow
and fast dynamic load intensities and content skewnesses,
even when facing conflicting latency and accuracy targets.
All in all, the effectiveness of AccStream in delivering
timely, accurate, and (partial) fresh streaming analytics lies
in shedding the adequate amount of input data at the right
time and place.

",17.122413403193683,16.790055096418737,245
ICAC_17_003.txt,15.14240042594501,13.17389936946277,ICAC,7358,"
Independent applications co-scheduled on the same
hardware will interfere with one another, affecting performance
in complicated ways. Predicting this interference is key to efficiently scheduling applications on shared hardware, but forming
accurate predictions is difficult because there are many shared
hardware features that could lead to the interference. In this
paper we investigate machine learning approaches (specifically,
regularization) to understand the relation between those hardware features and application interference. We propose ESP, a
highly accurate and fast regularization technique for application
interference prediction. To demonstrate this practicality, we
implement ESP and integrate it into a scheduler for both single
and multi-node Linux/x86 systems and compare the scheduling
performance to state-of-the-art heuristics. We find that ESP-based
schedulers increase throughput by 1.25-1.8x depending on the
scheduling scenario. Additionally, we find that ESP’s accurate
predictions allow schedulers to avoid catastrophic decisions,
which heuristic approaches fundamentally cannot detect.

",18.3970566412798,16.789097744360905,156
ICAC_17_004.txt,14.46726266371375,12.492896697059624,ICAC,8554,"
Due to the increasing popularity of smartphones,
many people are now equipped with both a smartphone and
at least one desktop (or laptop) computer. Although the two
computing devices are used for similar purposes (e.g., email, web
browsing), they are often both kept on for the user’s convenience,
despite only one device is actively used at a time. Therefore, if
one of the two computing devices can be put into an energysaving mode when the other one is in use, a significant amount
of energy can be saved for both the phone and the desktop.

In this paper, we propose CoSmart, a light-weight solution
that coordinates the smartphone with the desktop for joint
energy savings. CoSmart dynamically degrades the smartphone
to a feature phone with only basic GSM functions when the
user is detected to be with the desktop, in order to save both
computation and idle energy. The desktop is then put into sleep
for energy savings when the user leaves it, while the phone can
be turned back to a smartphone, such that the user can continue
the operation with seamless task migration. There are several
research challenges in the design of CoSmart, which include 1)
predicting whether the user would stay long enough with the
desktop to offset the migration overheads, and 2) determining
the best time point for task migration that can result in the
most energy savings. To this end, we propose a novel algorithm
for dynamic idle time length prediction, and model joint energy
savings as an optimization problem for the most energy savings. A
prototype of CoSmart is implemented in Android and evaluated
using different real user traces and popular apps. Results show
CoSmart can achieve, on average, 61.3% energy savings for the
smartphone and 46.7% energy savings for the desktop, which
outperforms other baselines by as much as 17.2% to 19.0%.

",16.404322709996244,15.68451612903226,317
ICAC_17_005.txt,15.071582011989126,13.305088974800807,ICAC,7659,"
Many big-data processing applications use dataanalytics frameworks such as Apache Hadoop. Such frameworks
have tunable configuration parameters set by experienced system
administrators and/or application developers. However, tuning
parameters manually can be hard and time-consuming because it
requires domain-specific knowledge and understanding of
complex inter-dependencies amongst parameters. Most of the
frameworks seek efficient resource management by using slots or
containers as resource units to be assigned to jobs or tasks, the
maximum number of slots or containers in a system being part of
the static configuration of the system. This static resource
management has limited effectiveness in coping with jobs’
diversity and workload dynamics, even in the case of a single job.
Seeking to improve performance (e.g., multiple-jobs makespan
and job completion time) without modification of the framework,
this paper proposes a hierarchical approach using a fuzzy-logic
controller to dynamically adjust the number of concurrent jobs
and additional controllers (one for each cluster node) to adjust the
number of resource units assigned to jobs on each node. The fuzzylogic controller uses fuzzy rules based on a concave downward
relationship between aggregate CPU usage and the number of
concurrent jobs. The other controllers use a simple heuristic
algorithm to adjust the number of resource units on the basis of
resource usage by job tasks. A prototype of our approach was
implemented for Apache Hadoop on a cluster running at
CloudLab. The prototype was evaluated using realistic workloads
generated by SWIM, a statistical workload injector for
MapReduce. The evaluation shows that the proposed approach
yields up to a 42% reduction of the jobs makespan that results
from using Hadoop default settings.

",17.015998829145012,16.515636363636364,277
ICAC_17_006.txt,15.18595314665481,13.459036559181992,ICAC,7264,"
The hybrid runtime (HRT) model offers a path
towards high performance and efficiency. By integrating the OS
kernel, runtime, and application, an HRT allows the runtime
developer to leverage the full feature set of the hardware and
specialize OS services to the runtime’s needs. However, conforming to the HRT model currently requires a port of the runtime to
the kernel level, for example to the Nautilus kernel framework,
and this requires knowledge of kernel internals. In response, we
developed Multiverse, a system that bridges the gap between a
built-from-scratch HRT and a legacy runtime system. Multiverse
allows unmodified applications and runtimes to be brought into
the HRT model without any porting effort whatsoever by splitting
the execution of the application between the domains of a legacy
OS and an HRT environment. We describe the design and
implementation of Multiverse and illustrate its capabilities using
the massive, widely-used Racket runtime system.

",17.693802365651003,16.335392156862746,155
ICAC_17_007.txt,16.720747305660982,16.33052387417968,ICAC,4370,"
Scientific workflows have become a popular computational model in a variety of application domains, such as
astronomy, material science, physics, and biology. As scientific
applications are moving to the cloud to take advantage of
the elasticity and service level agreement of resources, there
has been a number of recent research efforts on cloudbased workflow systems that support various types of performance guarantees under resource cost constraints. However,
most of the related work often requires advanced knowledge
about workflow structures to perform scheduling and resource
optimization. In addition, existing workflow systems usually
employ a monolithic approach in workflow implementation
and execution, which makes them inefficient in dealing with
heterogeneous types of workflows. In this paper, we present
MONAD, a self-adaptive micro-service infrastructure for heterogeneous scientific workflows. Specifically, our micro-service
architecture helps improve the flexibility of workflow composition and execution, and enables fine-grained scheduling at task
level, considering task sharing across different workflows. In
addition, we employ a feedback control approach with artificial
neural network-based system identification to provide resource
adaptation without any advanced knowledge of workflow
structures. Our evaluation on multiple realistic heterogeneous
workflows demonstrates that our system is robust and efficient
in dealing with dynamic scientific workloads.

",18.243605946275583,19.31789603960397,203
ICAC_17_008.txt,16.558217237472377,15.656921231943311,ICAC,7187,"
One of the challenges in self-adaptive systems concerns how to make adaptation to themselves at runtime in
response to possible and even unexpected changes from the environment and/or user goals. A feasible solution to this challenge is
rule-based adaptation, in which, adaptation decisions are made
according to predefined rules that specify what particular actions
should be performed to react to different changing events from
the environment. Although it has the characteristic of highlyefficient decision making for adaptation, rule-based adaptation
has two limitations: 1. no guarantee that those predefined rules
will lead to optimal or nearly-optimal adaptation results; 2.
weak support to evolve these rules to cope with non-stationary
environment and changeable user goals at runtime. In this
paper, we propose a reinforcement learning-based framework to
the generation and evolution of software adaptation rules. This
framework manifests two key capabilities for self-adaptation: 1.
the capability of automatically learning adaptation rules from
different goal settings at the offline phase; 2. the capability of automatically evolving adaptation rules from real-time information
about the environment and user goals at the online phase. The
two capabilities are built on the combination of reinforcement
learning and case-based reasoning techniques. This framework
improves the existing rule-based adaptation from two points:
the flexibility of adaptation logic, and the quality of adaptation
rules. We evaluate this framework through a case study of an
E-commerce web application, which shows that this framework
improves both the efficiency and effectiveness of self-adaptation.

",19.412932280823824,18.944540513833996,254
ICAC_17_009.txt,16.01427013425303,14.476901511997664,ICAC,4905,"
Self-adaptation can be realized in various ways.
Rule-based approaches prescribe the adaptation to be executed if
the system or environment satisfy certain conditions and result in
scalable solutions, however, with often only satisfying adaptation
decisions. In contrast, utility-driven approaches determine optimal adaptation decisions by using an often costly optimization
step, which typically does not scale well for larger problems. We
propose a rule-based and utility-driven approach that achieves
the beneficial properties of each of these directions such that
the adaptation decisions are optimal while the computation
remains scalable since an expensive optimization step can be
avoided. The approach can be used for the architecture-based
self-healing of large software systems. We define the utility for
large dynamic architectures of such systems based on patterns
capturing issues the self-healing must address and we use patternbased adaptation rules to resolve the issues. Defining the utility as
well as the adaptation rules pattern-based allows us to compute
the impact of each rule application on the overall utility and
to realize an incremental and efficient utility-driven self-healing.
We demonstrate the efficiency and optimality of our scheme in
comparative experiments with a static rule-based scheme as a
baseline and a utility-driven approach using a constraint solver.
",18.511140095513987,17.285768779342728,214
ICAC_17_010.txt,16.37521324611987,14.89843260006376,ICAC,4342,"
The feasibility of large-scale decentralized networks
for local computations, as an alternative to big data systems
that are often privacy-intrusive, expensive and serve exclusively
corporate interests, is usually questioned by network dynamics
such as node leaves, failures and rejoins in the network. This is
especially the case when decentralized computations performed
in a network, such as the estimation of aggregation functions,
e.g. summation, are linked to the actual nodes connected in
the network, for instance, counting the sum using input values
from only connected nodes. Reverse computations are required
to maintain a high aggregation accuracy when nodes leave or
fail. This paper introduces an autonomic agent-based model
for highly dynamic self-corrective networks using decentralized
reverse computations. The model is generic and equips the nodes
with the capability to disseminate connectivity status updates
in the network. Highly resilient agents to the dynamic network
migrate to remote nodes and orchestrate reverse computations
for each node leave or failure. In contrast to related work, no
other computational resources or redundancy are introduced.
The self-corrective model is experimentally evaluated using realworld data from a smart grid pilot project under highly dynamic
network adjustments that correspond to catastrophic events with
up to 50% of the nodes leaving the network. The model is highly
agile and modular and is applied to the large-scale decentralized
aggregation network of DIAS, the Dynamic Intelligent Aggregation Service, without major structural changes in its design and
operations. Results confirm the outstanding improvement in the
aggregation accuracy when self-corrective actions are employed
with a minimal increase in communication overhead.

",17.80541091248751,17.501454545454546,266
ICAC_17_011.txt,17.21607215924645,16.15882749950374,ICAC,6430,"
Epilepsy is a chronic brain disorder characterized
by the occurrence of spontaneous seizures of which about
30 percent of patients remain medically intractable and may
undergo surgical intervention; despite the latter, some may
still fail to attain a seizure-free outcome. Functional changes
may precede structural ones in the epileptic brain and may
be detectable using existing noninvasive modalities. Functional
connectivity analysis through electroencephalography (EEG) and
resting state-functional magnetic resonance imaging (rs-[MRD),
complemented by diffusion tensor imaging (DTI), has provided
such meaningful input in cases of temporal lobe epilepsy (TLE).
Recently, the emergence of edge computing has provided competent solutions enabling context-aware and real-time response
services for users. By leveraging the potential of autonomic edge
computing in epilepsy, we develop and deploy both noninvasive and invasive methods for the monitoring, evaluation and
regulation of the epileptic brain, with responsive neurostimulation (RNS; Neuropace). First, an autonomic edge computing
framework is proposed for processing of big data as part of
a decision support system for surgical candidacy. Second, an
optimized model for estimation of the epileptogenic network using
independently acquired EEG and rs-fMRI is presented. Third,
an unsupervised feature extraction model is developed based
on a convolutional deep learning structure for distinguishing
interictal epileptic discharge (IED) periods from nonIED periods
using electrographic signals from electrocorticography (ECoG).
Experimental and simulation results from actual patient data
validate the effectiveness of the proposed methods.

",20.68540608451941,20.31837606837607,235
ICAC_17_012.txt,15.397872554742385,13.433320646595437,ICAC,7040,"
Distributed micro-batch streaming systems, such as
Spark Streaming, employ backpressure mechanisms to maintain
a stable, high throughput stream of results that is robust to
runtime dynamics. Checkpointing in stream processing systems is
a process that creates periodic snapshots of the data flow for fault
tolerance. These checkpoints can be expensive to produce and
add significant delay to the data processing. The checkpointing
latencies are also variable at runtime, which in turn compounds
the challenges for the backpressure mechanism to maintain
stable performance. Consequently, the interferences caused by
the checkpointing may degrade system performance significantly,
even leading to exhaustion of resources or system crash.

This paper describes GOVERNOR, a controller that factors
the checkpointing costs into the backpressure mechanism. It not
only guarantees a smooth execution of the stream processing
but also reduces the throughput loss caused by interferences
of the checkpointing. Our experimental results on four stateful
streaming operators with real-world data sources demonstrate
that Governor implemented in Spark Streaming can achieve 26%
throughput improvement, and lower the risk of system crash,
with negligible overhead.

",16.975882523387877,15.638750000000002,178
ICAC_17_013.txt,15.631015597178365,13.622717998239171,ICAC,8653,"
A major concern for today’s smartphones is their
much faster battery drain than traditional feature phones,
despite their greater battery capacities. The difference is mainly
contributed by those more powerful but also much more powerconsuming smartphone components, such as the multi-core application processor. While the application processor must be active
when any smart apps are being used, it is also unnecessarily
waken up, even during idle periods, to perform operations related
to basic phone functions (i.e., incoming calls and text messages).

In this paper, we investigate how to increase the battery life of
smartphones by minimizing the use of the application processor
during idle periods. We find that the application processor is often
waken up by a process running on it, called the Radio Interface
Layer Daemon (RILD), which interfaces the user and apps to the
GSM/LTE cellular network. In particular, we demonstrate that a
great amount of energy could be saved if RILD is stopped, such
that the application processor can sleep more often. Based on this
key finding, we design a Smart On Demand (SOD) configuration
that reduces smartphone idle energy consumption by running
RILD operations on a secondary low-power microcontroller. As
a result, RILD operations can be handled at much lower energy
costs and the application processor is waken up only when one
needs to use any smart apps, in an on-demand manner. We have
built a hardware prototype of SOD and evaluated it with real
user traces. Our results show that SOD can increase its battery
life by up to 2.5 more days.

",16.404322709996244,15.069459770114943,265
ICAC_17_014.txt,14.916563500837746,13.105721077487825,ICAC,5471,"
This paper develops an algorithm to identify and
geo-locate real world events that may be present as social activity
signals in two different social networks. Specifically, we focus on
content shared by users on Twitter and Instagram in order to
design a system capable of fusing data across multiple networks.
Past work has demonstrated that it is indeed possible to detect
physical events using various social network platforms. However,
many of these signals need corroboration in order to handle
events that lack proper support within a single network. We
leverage this insight to design an unsupervised approach that can
correlate event signals across multiple social networks. location
of the event occurrence. We evaluate our algorithm using both
simulations and real world datasets collected using Twitter and
Instagram. The results indicate that our algorithm siginificantly
improves false positive elimination and attains high precision
compared to baseline methods on real world datasets.

",15.719379583869454,15.719545884579002,152
ICDM_17_001.txt,16.497336213698524,15.769190894372766,ICDM,5215,"
Collecting labeling information of time-to-event
analysis is naturally very time consuming, i.e., one has to wait
for the occurrence of the event of interest, which may not
always be observed for every instance. By taking advantage of
censored instances, survival analysis methods internally consider
more samples than standard regression methods, which partially
alleviates this data insufficiency problem. Whereas most existing
survival analysis models merely focus on a single survival prediction task, when there are multiple related survival prediction
tasks, we may benefit from the tasks relatedness. Simultaneously
learning multiple related tasks, multi-task learning (MTL) provides a paradigm to alleviate data insufficiency by bridging data
from all tasks and improves generalization performance of all
tasks involved. Even though MTL has been extensively studied,
there is no existing work investigating MTL for survival analysis.
In this paper, we propose a novel multi-task survival analysis
framework that takes advantage of both censored instances and
task relatedness. Specifically, based on two common used task
relatedness assumptions, i.e., low-rank assumption and cluster
structure assumption, we formulate two concrete models, COXTRACE and COX-cCMTL, under the proposed framework, respectively. We develop efficient algorithms and demonstrate the
performance of the proposed multi-task survival analysis models
on the The Cancer Genome Atlas (TCGA) dataset. Our results
show that the proposed approaches can significantly improve the
prediction performance in survival analysis and can also discover
some inherent relationships among different cancer types.

",19.51006720791134,18.465411436541142,242
ICDM_17_002.txt,13.949060027652596,12.65022437183162,ICDM,6162,"
Recommender systems have attracted much attention in last decades, which can help the users explore new items
in many applications. As a popular technique in recommender
systems, item recommendation works by recommending items to
users based on their historical interactions. Conventional item
recommendation methods usually assume that users and items
are stationary, which is not always the case in real-world applications. Many time-aware item recommendation models have been
proposed to take the temporal effects into the considerations
based on the absolute time stamps associated with observed
interactions. We show that using absolute time to model temporal
effects can be limited in some circumstances. In this work, we
propose to model the temporal dynamics of both users and
items in item recommendation based on their life cycles. This
problem is very challenging to solve since the users and items
can co-evolve in their life cycles and the sparseness of the data
become more severe when we consider the life cycles of both
users and items. A novel time-aware item recommendation model
called BiCycle is proposed to address these challenges. BiCycle
is designed based on two important observations: 1) correlated
users or items usually share similar patterns in the similar stages
of their life cycles. 2) user preferences and item characters can
evolve gradually over different stages of their life cycles. Extensive
experiments conducted on three real-world datasets demonstrate
the proposed approach can significantly improve the performance
of recommendation tasks by considering the inner life cycles of
both users and items.
",16.404322709996244,16.15064566929134,255
ICDM_17_003.txt,18.443566009382838,16.58909598906941,ICDM,6924,"
Due to the sparse distribution of road video surveillance cameras, precise trajectory tracking for hit-and-run vehicles remains a challenging task. Previous research on vehicle
trajectory recovery mostly focuses on recovering trajectory with
low-sampling-rate GPS coordinates by retrieving road traffic flow
patterns from collected GPS information. However, to the best of
our knowledge, none of them considered using on-road taxicabs
as mobile video surveillance cameras as well as the time-varying
characteristics of vehicle traveling and road traffic flow patterns,
therefore not suitable for recovering trajectories of hit-and-run
vehicles. With this insight, we model the travel time-cost of a
road segment during various time periods precisely with LNDs
(Logarithmic Normal Distributions), then use LSNDs (Log Skew
Normal Distributions) to approximate the time-cost of an urban
trip during various time periods. We propose a novel approach
to calculate possible location and time distribution of the hit-andrun vehicle in parallel, select the optimal taxicab to verify the
distribution by uploading and checking video clips of this taxicab,
finally refine the restoring trajectory in a parallel and recursive
manner. We evaluate our solution on real-world taxicab and road
surveillance system datasets. Experimental results demonstrate
that our approach outperforms alternative solutions in terms of
accuracy ratio of vehicle tracking.

",21.703370914358928,19.659269102990034,216
ICDM_17_004.txt,15.136549531436724,13.708839285714287,ICDM,6447,"
Driven by the dramatic growth of data both in terms
of the size and sources, learning from heterogeneous data is
emerging as an important research direction for many real
applications. One of the biggest challenges of this type of
problem is how to meaningfully integrate heterogeneous data to
considerably improve the generality and quality of the learning
model. In this paper, we first present a unified learning framework that aims to leverage the structural information from two
types of data heterogeneity: view heterogeneity (as in multiview learning) and worker heterogeneity (as in crowdsourcing).
The objective follows the principles of view consistency and
worker consensus by minimizing the loss term with a regularized prediction tensor. We then propose to relax and solve the
optimization framework with an iterative updating method.
We also prove that the gradient of the most time-consuming
updating block is separable with respect to the workers, which
leads to a randomized algorithm with faster speed and better
convergence. Finally, we compare the proposed method with
several state-of-the-arts and demonstrate its effectiveness on
various data sets.

",17.451712890111917,16.529644830307813,182
ICDM_17_005.txt,12.966002169164554,10.701427491631573,ICDM,6129,"
Local community detection (or local clustering) is
of fundamental importance in large network analysis. Random
walk based methods have been routinely used in this task. Most
existing random walk methods are based on the single-walker
model. However, without any guidance, a single-walker may
not be adequate to effectively capture the local cluster. In this
paper, we study a multi-walker chain (MWC) model, which
allows multiple walkers to explore the network. Each walker is
influenced (or pulled back) by all other walkers when deciding
the next steps. This helps the walkers to stay as a group and
within the cluster. We introduce two measures based on the
mean and standard deviation of the visiting probabilities of the
walkers. These measures not only can accurately identify the
local cluster, but also help detect the cluster center and boundary,
which cannot be achieved by the existing single-walker methods.
We provide rigorous theoretical foundation for MWC, and
devise efficient algorithms to compute it. Extensive experimental
results on a variety of real-world networks demonstrate that
MWC outperforms the state-of-the-art local community detection
methods by a large margin.

",13.172668634160413,11.703205741626796,191
ICDM_17_006.txt,15.168826796108004,13.186455502070924,ICDM,6886,"
We describe a dynamic graph generator with overlapping communities that is capable of simulating community
scale events while at the same time maintaining crucial graph
properties. Such a benchmark generator is useful to measure and
compare the responsiveness and efficiency of dynamic community
detection algorithms. Since the generator allows the user to
tune multiple parameters, it can also be used to test the
robustness of a community detection algorithm across a spectrum
of inputs. In an experimental evaluation, we demonstrate the
generator’s performance and show that graph properties are
indeed maintained over time. Further, we show that standard
community detection algorithms are able to find the generated
community structure.

To the best of our knowledge, this is the first time that
all of the above have been combined into one benchmark
generator, and this work constitutes an important building block
for the development of efficient and reliable dynamic, overlapping
community detection algorithms.

",18.7741,17.424210526315793,154
ICDM_17_007.txt,15.632329498897391,14.194592567788561,ICDM,5668,"
Many real-world applications are characterized by
temporal data collected from multiple modalities, each sampled
with a different resolution. Examples include manufacturing
processes and financial market prediction. In these applications,
an interesting observation is that within the same modality, we
often have data from multiple views, thus naturally forming a 2level hierarchy: with the multiple modalities on the top, and the
multiple views at the bottom. For example, in aluminum smelting
processes, the multiple modalities include power, noise, alumina
feed, etc; and within the same modality such as power, the different views correspond to various voltage, current and resistance
control signals and measured responses. For such applications, we
aim to address the following challenge, i.e., how can we integrate
such multi-modality multi-resolution data to effectively predict
the targets of interest, such as bath temperature in aluminum
smelting cell and the volatility in financial market.

In this paper, for the first time, we simultaneously model the
hierarchical data structure and the multi-resolution property via
a novel framework named HiMuV. Different from existing work
based on multiple views on a single level or a single resolution,
the proposed framework is based on the key assumption that the
information from different modalities is complementary, whereas
the information within the same modality (across different views)
is redundant in terms of predicting the targets of interest. Therefore, we introduce an optimization framework where the objective
function contains both the prediction loss and a novel regularizer
enforcing the consistency among different views within the same
modality. To solve this optimization framework, we propose
an iterative algorithm based on randomized block coordinate
descent. Experimental results on synthetic data, benchmark data,
and various real data sets from aluminum smelting processes,
and stock market prediction demonstrate the effectiveness and
efficiency of the proposed algorithm.

",20.267338824336647,19.711194630872487,300
ICDM_17_008.txt,16.41741730751504,14.894185774850374,ICDM,7374,"
With the rapid rise of various e-commerce and
social network platforms, users are generating large amounts of
heterogenous behavior data, such as purchase history, adding-tofavorite, adding-to-cart and click activities, and this kind of user
behavior data is usually binary, only reflecting a user’s action
or inaction (i.e., implicit feedback data). Tensor factorization
is a promising means of modeling heterogenous user behaviors
by distinguishing different behavior types. However, ambiguity
arises in the interpretation of the unobserved user behavior
records that mix both real negative examples and _ potential
positive examples. Existing tensor factorization models either
ignore unobserved examples or treat all of them as negative
examples, leading to either poor prediction performance or
huge computation cost. In addition, the distribution of positive
examples w.r.t. behavior types is heavily skewed. Existing tensor
factorization models would bias towards the type of behaviors
with a large number of positive examples.

In this paper, we propose a scalable probabilistic tensor
factorization model (SPTF) for heterogenous behavior data, and
develop a novel negative sampling technique to optimize SPTF
by leveraging both observed and unobserved examples with
much lower computational costs and higher modeling accuracy.
To overcome the issue of the heavy skewness of the behavior
data distribution, we propose a novel adaptive ranking-based
positive sampling approach to speed up the model convergence
and improve the prediction accuracy for sparse behavior types.
Our proposed model optimization techniques enable SPTF to be
scalable to large-scale behavior datasets. Extensive experiments
have been conducted on a large-scale e-commerce dataset, and
the experimental results show superiority of our proposed SPTF
model in terms of prediction accuracy and scalability.

",20.17186042781802,19.385051094890517,280
ICDM_17_009.txt,14.92859688622616,13.913729093080565,ICDM,6205,"
Learning to optimize AUC performance for classifying label imbalanced data in online scenarios has been extensively
studied in recent years. Most of the existing work has attempted
to address the problem directly in the original feature space,
which may not be suitable for non-linearly separable datasets.
To solve this issue, some kernel-based learning methods are
proposed for non-linearly separable datasets. However, such
kernel approaches have been shown to be inefficient and failed
to scale well on large datasets in practice. Taking this cue, in
this work, we explore the use of scalable kernel-based learning
techniques as surrogates to existing approaches: random Fourier
features and Nystrém method, for tackling the problem and
bringing new insights to the differences between the two methods
based on their online performance. In contrast to the conventional
kernel-based learning methods which suffer from high computational complexity of the kernel matrix, our proposed approaches
elevate this issue with linear features that approximate the kernel
function/matrix. Specifically, two different surrogate kernel-based
learning models are presented for addressing the online AUC
maximization task: (i) the Fourier Online AUC Maximization
(FOAM) algorithm that samples the basis functions from a dataindependent distribution to approximate the kernel function; and
(ii) the Nystrém Online AUC Maximization (NOAM) algorithm
that samples a subset of instances from the training data to
approximate the kernel matrix by a low rank matrix. Another
novelty of the present work is the proposed mini-batch Online
Gradient Descent method for model updating to control the noise
and reduce the variance of gradients. We provide theoretical
analyses for the two proposed algorithms. Empirical studies on
commonly used large scale datasets show that the proposed
algorithms outperformed existing state-of-the-art methods in
terms of both AUC performance and computational efficiency.

",18.1352568364455,17.889718120805373,299
ICDM_17_010.txt,14.005381812920906,11.579785319034329,ICDM,6473,"
Large-scale optimization problems abound in data
mining and machine learning applications, and the computational
challenges they pose are often addressed through parallelization.
We identify structural properties under which a convex optimization problem can be massively parallelized via map-reduce
operations using the Frank-Wolfe (FW) algorithm. The class of
problems that can be tackled this way is quite broad and includes
experimental design, AdaBoost, and projection to a convex hull.
Implementing FW via map-reduce eases parallelization and
deployment via commercial distributed computing frameworks.
We demonstrate this by implementing FW over Spark, an engine
for parallel data processing, and establish that parallelization
through map-reduce yields significant performance improvements: we solve problems with 10 million variables using 350
cores in 44 minutes; the same operation takes 133 hours when
executed serially.

",18.243605946275583,18.395393939393937,133
ICDM_17_011.txt,15.33954311908391,13.899644285887469,ICDM,6079,"
Linear Discriminant Analysis (LDA) is widely-used
for supervised dimension reduction and linear classification. Classical LDA, however, suffers from the i//-posed estimation problem
on data with high dimension and low sample size (HDLSS). To
cope with this problem, in this paper, we propose an Adaptive
Wishart Discriminant Analysis (AVVD.A) for classification, that
makes predictions in an ensemble way. Comparing to existing
approaches, AWD.A has two advantages: 1) leveraging the
Wishart distribution, AVWVD.A ensembles multiple LDA classifiers
parameterized by the sampled covariance matrices via a Bayesian
Voting Scheme, which theoretically improves the robustness of
classification, compared to LDA classifiers using a single (probably ill-posed) covariance matrix estimator; 2) AWD.A updates
the weights for voting optimally to adapt the local information
of each new input data, so as to enable the nonlinear classification. Theoretical analysis indicates that AVVDA guarantees a
close approximation to the optimal Bayesian inference and thus
achieves robust performance on high dimensional data. Extensive
experiments on real-world datasets show that our approach
outperforms state-of-the-art algorithms by a large margin.

",20.581828153500815,19.986363636363638,181
ICDM_17_012.txt,14.049035280873305,11.867877879050589,ICDM,6298,"
Bayesian optimization (BO) has recently emerged
as a powerful and flexible tool for hyper-parameter tuning and
more generally for the efficient global optimization of expensive
black-box functions. Systems implementing BO has successfully
solved difficult problems in automatic design choices and machine
learning hyper-parameters tunings. Many recent advances in
the methodologies and theories underlying Bayesian optimization
have extended the framework to new applications and provided
greater insights into the behavior of these algorithms. Still, these
established techniques always require a user-defined space to
perform optimization. This pre-defined space specifies the ranges
of hyper-parameter values. In many situations, however, it can
be difficult to prescribe such spaces, as a prior knowledge is
often unavailable. Setting these regions arbitrarily can lead to
inefficient optimization - if a space is too large, we can miss the
optimum with a limited budget, on the other hand, if a space is
too small, it may not contain the optimum point that we want to
get. The unknown search space problem is intractable to solve in
practice. Therefore, in this paper, we narrow down to consider
specifically the setting of “weakly specified” search space for
Bayesian optimization. By weakly specified space, we mean that
the pre-defined space is placed at a sufficiently good region so that
the optimization can expand and reach to the optimum. However,
this pre-defined space need not include the global optimum.
We tackle this problem by proposing the filtering expansion
strategy for Bayesian optimization. Our approach starts from
the initial region and gradually expands the search space. We
develop an efficient algorithm for this strategy and derive its
regret bound. These theoretical results are complemented by an
extensive set of experiments on benchmark functions and two
real-world applications which demonstrate the benefits of our
proposed approach.

",15.202697889610203,13.875867109634552,302
ICDM_17_013.txt,14.96829667596596,12.871811832186264,ICDM,5738,"
Network clustering is an essential approach to finding latent clusters in real-world networks. As the scale of realworld networks becomes increasingly larger, the existing network
clustering algorithms fail to discover meaningful clusters efficiently. In this paper, we propose a framework called AnySCAN,
which applies anytime theory to the structural clustering algorithm for networks (SCAN). Moreover, an active learning strategy
is proposed to advance the refining procedure in AnySCAN
framework. AnySCAN with the active learning strategy is able
to find the exactly same clustering result on large-scale networks
as the original SCAN in a significantly more efficient manner.
Extensive experiments on real-world and synthetic networks
demonstrate that our proposed method outperforms existing
network clustering approaches.

",16.728156217252725,14.909017094017099,118
ICDM_17_014.txt,15.25663828276786,13.549235773616783,ICDM,6940,"
The contents generated from different data sources
are usually non-uniform, such as long texts produced by news
websites and short texts produced by social media. Uncovering
topics over large-scale non-uniform texts becomes an important
task for analyzing network data. However, the existing methods
may fail to recognize the difference between long texts and
short texts. To address this problem, we propose a novel topic
modeling method for non-uniform text topic modeling referred
to as self-adaptive sliding window based topic model (SSWTM).
Specifically, in all kinds of texts, relevant words have a closer
distance to each other than irrelevant words. Based on this
assumption, SSWTM extracts relevant words by using a selfadaptive sliding window and models on the whole corpus. The
self-adaptive sliding window can filter noisy information and
change the size of a window according to different text contents.
Experimental results on short texts from Twitter and long texts
from Chinese news articles demonstrate that our method can
discover more coherent topics for non-uniform texts compared
with state-of-the-art methods.

",14.731742533061166,13.242395251396648,180
ICDM_17_015.txt,15.189819381529446,13.377877406196067,ICDM,4975,"
Given an heterogeneous social network, can we
forecast its future? Can we predict who will start using a given
hashtag on twitter? Can we leverage side information, such as
who retweets or follows whom, to improve our membership
forecasts? We present TENSORCAST, a novel method that forecasts time-evolving networks more accurately than current state
of the art methods by incorporating multiple data sources in
coupled tensors. TENSORCAST is (a) scalable, being linearithmic
on the number of connections; (b) effective, achieving over
20% improved precision on top-1000 forecasts of community
members; (c) general, being applicable to data sources with
different structure. We run our method on multiple real-world
networks, including DBLP and a Twitter temporal network with
over 310 million non-zeros, where we predict the evolution of the
activity of the use of political hashtags.

",15.021129683784007,14.585797101449277,139
ICDM_17_016.txt,14.044539233453587,12.197835899236026,ICDM,6537,"
Detecting fraudulent users in online social networks
is a fundamental and urgent research problem as adversaries can
use them to perform various malicious activities. Global social
structure based methods, which are known as guilt-by-association,
have been shown to be promising at detecting fraudulent users.
However, existing guilt-by-association methods either assume
symmetric (i.e., undirected) social links, which oversimplifies the
asymmetric (i.e., directed) social structure of real-world online
social networks, or only leverage labeled fraudulent users or
labeled normal users (but not both) in the training dataset, which
limits detection accuracies.

In this work, we propose GANG, a guilt-by-association
method on directed graphs, to detect fraudulent users in OSNs.
GANG is based on a novel pairwise Markov Random Field
that we design to capture the unique characteristics of the
fraudulent-user-detection problem in directed OSNs. In the basic
version of GANG, given a training dataset, we leverage Loopy
Belief Propagation (LBP) to estimate the posterior probability
distribution for each user and uses it to predict a user’s label.
However, the basic version is not scalable enough and not
guaranteed to converge because it relies on LBP. Therefore,
we further optimize GANG and our optimized version can
be represented as a concise matrix form, with which we are
able to derive conditions for convergence. We compare GANG
with various existing guilt-by-association methods on a largescale Twitter dataset and a large-scale Sina Weibo dataset with
labeled fraudulent and normal users. Our results demonstrate
that GANG substantially outperforms existing methods, and that
the optimized version of GANG is significantly more efficient than
the basic version.

",17.238542476582836,16.96792988929889,275
ICDM_17_017.txt,15.5787274180514,14.05912273393692,ICDM,6492,"
The methodology of community detection can be
divided into two principles: imposing a network model on a
given graph, or optimizing a designed objective function. The
former provides guarantees on theoretical detectability but falls
short when the graph is inconsistent with the underlying model.
The latter is model-free but fails to provide quality assurance
for the detected communities. In this paper, we propose a novel
unified framework to combine the advantages of these two
principles. The presented method, SGC-GEN, not only considers
the detection error caused by the corresponding model mismatch
to a given graph, but also yields a theoretical guarantee on
community detectability by analyzing Spectral Graph Clustering
(SGC) under GENerative community models (GCMs). SGC-GEN
incorporates the predictability on correct community detection
with a measure of community fitness to GCMs. It resembles
the formulation of supervised learning problems by enabling
various community detection loss functions and model mismatch
metrics. We further establish a theoretical condition for correct
community detection using the normalized graph Laplacian
matrix under a GCM, which provides a novel data-driven loss
function for SGC-GEN. In addition, we present an effective algorithm to implement SGC-GEN, and show that the computational
complexity of SGC-GEN is comparable to the baseline methods.
Our experiments on 18 real-world datasets demonstrate that
SGC-GEN possesses superior and robust performance compared
to 6 baseline methods under 7 representative clustering metrics.

",18.599290044081553,17.992482269503544,236
ICDM_17_018.txt,13.313935327660339,10.79269459245251,ICDM,6253,"
In this paper, we study the problem of using representation learning to assist information diffusion prediction on
graphs. In particular, we aim at estimating the probability of an
inactive node to be activated next in a cascade. Despite the success
of recent deep learning methods for diffusion, we find that they
often underexplore the cascade structure. We consider a cascade
as not merely a sequence of nodes ordered by their activation
time stamps; instead, it has a richer structure indicating the
diffusion process over the data graph. As a result, we introduce
a new data model, namely diffusion topologies, to fully describe
the cascade structure. We find it challenging to model diffusion
topologies, which are dynamic directed acyclic graphs (DAGs),
with the existing neural networks. Therefore, we propose a novel
topological recurrent neural network, namely Topo-LSTM, for
modeling dynamic DAGs. We customize Topo-LSTM for the
diffusion prediction task, and show it improves the state-of-theart baselines, by 20.1%-56.6% (MAP) relatively, across multiple
real-world data sets.

",14.90622815163357,13.729264705882354,173
ICDM_17_019.txt,14.361689110485614,12.248266545819188,ICDM,6803,"
In large-scale data classification tasks, it is becoming
more and more challenging in finding a true class from a
huge amount of candidate categories. Fortunately, a hierarchical
structure usually exists in these massive categories. The task
of utilizing this structure for effective classification is called
hierarchical classification. It usually follows a top-down fashion
which predicts a sample from the root node with a coarse-grained
category to a leaf node with a fine-grained category. However,
misclassification is inevitable if the information is insufficient or
large uncertainty exists in the prediction process. In this scenario,
we can design a stopping strategy to stop the sample at an internal
node with a coarser category, instead of predicting a wrong
leaf node. Several studies address the problem by improving
performance in terms of hierarchical accuracy and informative
prediction. However, all of these researches ignore an important
issue: when predicting a sample at the current node, the error is
inclined to occur if large uncertainty exists in the next lower level
children nodes. In this paper, we integrate this uncertainty into a
risk problem: when predicting a sample at a decision node, it will
take precipitance risk in predicting the sample to a children node
in the next lower level on one hand, and take conservative risk
in stopping at the current node on the other. We address the risk
problem by designing a Local Bayes Risk Minimization (LBRM)
framework, which divides the prediction process into recursively
deciding to stop or to go down at each decision node by balancing
these two risks in a top-down fashion. Rather than setting a
global loss function in the traditional Bayes risk framework,
we replace it with different uncertainty in the two risks for
each decision node. The uncertainty on the precipitance risk and
the conservative risk are measured by information entropy on
children nodes and information gain from the current node to
children nodes, respectively. We propose a Weighted Tree Induced
Error (WTIE) to obtain the predictions of minimum risk with
different emphasis on the two risks. Experimental results on
various datasets show the effectiveness of the proposed LBRM
algorithm.

",16.351538315227643,15.049337589784518,359
ICDM_17_020.txt,14.43842620921151,12.775137820665936,ICDM,6891,"
In this paper, we focus on developing a novel
mechanism to preserve differential privacy in deep neural networks, such that: (1) The privacy budget consumption is totally
independent of the number of training steps; (2) It has the ability
to adaptively inject noise into features based on the contribution
of each to the output; and (3) It could be applied in a variety
of different deep neural networks. To achieve this, we figure
out a way to perturb affine transformations of neurons, and
loss functions used in deep neural networks. In addition, our
mechanism intentionally adds “more noise” into features which
are “less relevant” to the model output, and vice-versa. Our
theoretical analysis further derives the sensitivities and error
bounds of our mechanism. Rigorous experiments conducted on
MNIST and CIFAR-10 datasets show that our mechanism is
highly effective and outperforms existing solutions.

",17.353723509956247,17.357277777777778,145
ICDM_17_021.txt,13.394188425929748,11.430843943781152,ICDM,5673,"
Most state-of-the-art graph kernels only take local
graph properties into account, i.e, the kernel is computed
with regard to properties of the neighborhood of vertices or
other small substructures. On the other hand, kernels that do
take global graph properties into account may not scale well
to large graph databases. Here we propose to start exploring
the space between local and global graph kernels, so called
glocalized graph kernels, striking the balance between both
worlds. Specifically, we introduce a novel graph kernel based on
the k-dimensional Weisfeiler-Lehman algorithm. Unfortunately,
the k-dimensional Weisfeiler-Lehman algorithm scales exponentially in &. Consequently, we devise a stochastic version of the
kernel with provable approximation guarantees using conditional
Rademacher averages. On bounded-degree graphs, it can even
be computed in constant time. We support our theoretical results
with experiments on several graph classification benchmarks,
showing that our kernels often outperform the state-of-the-art
in terms of classification accuracies.

",15.247664890283005,14.335,162
ICDM_17_022.txt,13.810625212754609,11.977172614914597,ICDM,7135,"
In the big data era, the information about the
same object collected from multiple sources is inevitably
conflicting. The task of identifying true information (i.e., the
truths) among conflicting data is referred to as truth discovery,
which incorporates the estimation of source reliability degrees
into the aggregation of multi-source data. However, in many
real-world applications, large-scale data are distributed across
multiple servers. Traditional truth discovery approaches cannot
handle this scenario due to the constraints of communication
overhead and privacy concern. Another limitation of most
existing work is that they ignore the differences among objects,
i.e., they treat all the objects equally. This limitation would
be exacerbated in distributed environments where significant
differences exist among the objects. To tackle the aforementioned issues, in this paper, we propose a novel distributed
truth discovery framework (DTD), which can effectively and
efficiently aggregate conflicting data stored across distributed
servers, with the differences among the objects as well as
the importance level of each server being considered. The
proposed framework consists of two steps: the local truth
computation step conducted by each local server and the
central truth estimation step taking place in the central server.
Specifically, we introduce the uncertainty values to model the
differences among objects, and propose a new uncertaintybased truth discovery method (UbTD) for calculating the true
information of objects in each local server. The outputs of the
local truth computation step include the estimated local truths
and the variances of objects, which are the input information
of the central truth estimation step. To infer the final true
information in the central server, we propose a new algorithm
to aggregate the outputs of all the local servers with the quality
of different local servers taken into account. The proposed
distributed truth discovery framework can infer object truths
without delivering any raw data to the central server, and
thus can reduce communication overhead as well as preserve
data privacy. Experimental results on three real world datasets
show that the proposed DTD framework can efficiently estimate
object truths with accuracy guarantee, and the proposed UbTD
algorithm significantly outperforms the state-of-the-art batch
truth discovery approaches.

",17.650647597118724,17.959775910364147,360
ICDM_17_023.txt,14.039111514368571,12.099156116137056,ICDM,7636,"
 Unsupervised semantic segmentation in the time
series domain is a much-studied problem due to its potential to
detect unexpected regularities and regimes in poorly understood
data. However, the current techniques have several
shortcomings, which have limited the adoption of time series
semantic segmentation beyond academic settings for three
primary reasons. First, most methods require setting/learning
many parameters and thus may have problems generalizing to
novel situations. Second, most methods implicitly assume that all
the data is segmentable, and have difficulty when that
assumption is unwarranted. Finally, most research efforts have
been confined to the batch case, but online segmentation is
clearly more useful and actionable. To address these issues, we
present an algorithm which is domain agnostic, has only one
easily determined parameter, and can handle data streaming at a
high rate. In this context, we test our algorithm on the largest
and most diverse collection of time series datasets ever
considered, and demonstrate our algorithm’s superiority over
current solutions. Furthermore, we are the first to show that
semantic segmentation may be possible at superhuman
performance levels.

",16.975882523387877,15.945188547486033,181
ICDM_17_024.txt,16.554821507878724,14.882625678794682,ICDM,6443,"
Literature based discovery (LBD) is a task that
aims to uncover hidden associations between non-interacting
scientific concepts by rationally connecting independent nuggets
of information. Broadly, prior approaches to LBD include use of:
a) distributional statistics and explicit representation, b) graphtheoretic measures, and c) supervised machine learning methods
to find associations. However, purely distributional approaches
may not necessarily entail semantically meaningful association
and graph-theoretic approaches suffer from scalability issues.
While supervised machine learning based approaches have the
potential to elucidate associations, the training data required is
too expensive to generate. In this paper we propose a novel
dynamic Medical Subject Heading (MeSH) embedding model
which is able to model the evolutionary behavior of medical concepts to uncover latent associations between them. The proposed
model allows us to learn the evolutionary trajectories of MeSH
embeddings and detect informative terms. Hence, based on the
dynamic MeSH embeddings, meaningful medical hypotheses can
be efficiently generated. To evaluate the efficacy of the proposed
model, we perform both qualitative and quantitative evaluation.
The results demonstrate that leveraging the evolutionary features
of MeSH concepts is an effective way for predicting novel
associations.

",18.599290044081553,18.574295900178253,188
ICDM_17_025.txt,16.13431791707234,14.924851581022462,ICDM,5677,"
In recent years, the importance of feature engineering has been confirmed by the exceptional performance of deep
learning techniques, that automate this task for some applications.
For others, feature engineering requires substantial manual effort
in designing and selecting features and is often tedious and
non-scalable. We present AutoLearn, a regression-based feature
learning algorithm. Being data-driven, it requires no domain
knowledge and is hence generic. Such a representation is learnt
by mining pairwise feature associations, identifying the linear or
non-linear relationship between each pair, applying regression
and selecting those relationships that are stable and improve the
prediction performance. Our experimental evaluation on 18 UC
Irvine and 7 Gene expression datasets, across different domains,
provides evidence that the features learnt through our model can
improve the overall prediction accuracy by 13.28%, compared
to original feature space and 5.87% over other top performing
models, across 8 different classifiers without using any domain
knowledge.

",18.7741,17.866753246753245,157
ICDM_17_026.txt,14.492999149102289,12.850430107526883,ICDM,6164,"
CANDECOMP/PARAFAC Decomposition (CPD) is
one of the most popular tensor decomposition methods that
has been extensively studied and widely applied. In recent
years, sparse tensors that contain a huge portion of zeros
but a limited number of non-zeros have attracted increasing
interest. Existing techniques are not directly applicable to sparse
tensors, since they mainly target dense ones and usually have
poor efficiency. Additionally, specific issues also arise for sparse
tensors, depending on different data sources and applications: the
role of zero entries can be different; incorporating constraints
like non-negativity and sparseness might be necessary; the ability
to learn on-the-fly is a must for dynamic scenarios that new data
keeps arriving at high velocity. However, state-of-art algorithms
only partially address the above issues. To fill this gap, we propose
a general framework for finding the CPD of sparse tensors. Modeling the sparse tensor decomposition problem by a generalized
weighted CPD formulation and solving it efficiently, our proposed
method is also flexible to handle constraints and dynamic data
streams. Through experiments on both synthetic and real-world
datasets, for the static case, our method demonstrates significant
improvements in terms of effectiveness, efficiency and scalability.
Moreover, under the dynamic setting, our method speeds up
current technology by hundreds to thousands times, without
sacrificing decomposition quality.

",17.37919286519448,16.637899543378996,220
ICDM_17_027.txt,15.579741850924794,14.302287497782512,ICDM,5560,"
Post-traumatic stress disorder (PTSD) is a
traumatic-stressor related disorder developed by exposure to a
traumatic or adverse environmental event that caused serious
harm or injury. Structured interview is the only widely accepted
clinical practice for PTSD diagnosis but suffers from several
limitations including the stigma associated with the disease.
Diagnosis of PTSD patients by analyzing speech signals has
been investigated as an alternative since recent years, where
speech signals are processed to extract frequency features and
these features are then fed into a classification model for PTSD
diagnosis. In this paper, we developed a deep belief network
(DBN) model combined with a transfer learning (TL) strategy
for PTSD diagnosis. We computed three categories of speech
features and utilized the DBN model to fuse these features. The
TL strategy was utilized to transfer knowledge learned from a
large speech recognition database, TIMIT, for PTSD detection
where PTSD patient data is difficult to collect. We evaluated the
proposed methods on two PTSD speech databases, each of which
consists of audio recordings from 26 patients. We compared the
proposed methods with other popular methods and showed that
the state-of-the-art support vector machine (SVM) classifier only
achieved an accuracy of 57.68%, and TL strategy boosted the
performance of the DBN from 61.53% to 74.99%. Altogether,
our method provides a pragmatic and promising tool for PTSD
diagnosis.

",17.879347455551382,16.755036710719533,231
ICDM_17_028.txt,16.685469227759484,15.071015056105367,ICDM,5649,"
Histogram-based similarity has been widely adopted
in many machine learning tasks. However, measuring histogram
similarity is a challenging task for streaming data, where the
elements of a histogram are observed in a streaming manner.
First, the ever-growing cardinality of histogram elements makes
any similarity computation inefficient. Second, the concept-drift
issue in the data streams also impairs the accurate assessment
of the similarity. In this paper, we propose to overcome the
above challenges with HistoSketch, a fast similarity-preserving
sketching method for streaming histograms with concept drift.
Specifically, HistoSketch is designed to incrementally maintain a
set of compact and fixed-size sketches of streaming histograms to
approximate similarity between the histograms, with the special
consideration of gradually forgetting the outdated histogram
elements. We evaluate HistoSketch on multiple classification tasks
using both synthetic and real-world datasets. The results show
that our method is able to efficiently approximate similarity
for streaming histograms and quickly adapt to concept drift.
Compared to full streaming histograms gradually forgetting the
outdated histogram elements, HistoSketch is able to dramatically
reduce the classification time (with a 7500x speedup) with only
a modest loss in accuracy (about 3.5%).

",17.631426480028413,16.82166666666667,194
ICDM_17_029.txt,12.212978012342486,9.72231938029321,ICDM,9815,"
In the age of big data, many graph algorithms
are now required to operate in external memory and deliver
performance that does not significantly degrade with the scale
of the problem. One particular area that frequently deals with
graphs larger than RAM is triangle listing, where the algorithms
must carefully piece together edges from multiple partitions to
detect cycles. In recent literature, two competing proposals (i.e.,
Pagh and PCF) have emerged; however, neither one is universally
better than the other. Since little is known about the I/O cost of
PCF or how these methods compare to each other, we undertake
an investigation into the properties of these algorithms, model
their I/O cost, understand their shortcomings, and shed light on
the conditions under which each method defeats the other. This
insight leads us to develop a novel framework we call Trigon
that surpasses the I/O performance of both previous techniques
in all graphs and under all RAM conditions.

",18.026119701940384,16.876250000000002,162
ICDM_17_030.txt,14.52644396638923,12.433461143444024,ICDM,5293,"
Given an undirected network where some of the
nodes are labeled, how can we classify the unlabeled nodes with
high accuracy? Loopy Belief Propagation (LBP) is an inference
algorithm widely used for this purpose with various applications
including fraud detection, malware detection, web classification,
and recommendation. However, previous methods based on LBP
have problems in modeling complex structures of attributed
networks because they manually and heuristically select the most
important parameter, the propagation strength.

In this paper, we propose Supervised Belief Propagation (SBP),
a scalable and novel inference algorithm which automatically
learns the optimal propagation strength by supervised learning.
SBP is generally applicable to attributed networks including
weighted and signed networks. Through extensive experiments,
we demonstrate that SBP generalizes previous LBP-based methods and outperforms previous LBP and RWR based methods in
real-world networks.

",18.422482065455632,17.04722222222222,136
ICDM_17_031.txt,15.592634721574996,14.900717110067884,ICDM,6103,"
Due to the recent vast availability of transportation
traffic data, major research efforts have been devoted to traffic
prediction, which is useful in many applications such as urban
planning, traffic management and navigations systems. Current
prediction methods that independently train a model per traffic
sensor cannot accurately predict traffic in every situation (e.g.,
rush hours, constructions and accidents) because there may not
exist sufficient training samples per sensor for all situations. To
address this shortcoming, our core idea is to explore the commonalities of prediction tasks across multiple sensors who behave
similarly in a specific traffic situation. Instead of building a model
independently per sensor, we propose a Multi-Task Learning
(MTL) framework that aims to first automatically identify the
traffic situations and then simultaneously build one forecasting
model for similar-behaving sensors per traffic situation. The key
innovation here is that instead of the straightforward application
of MTL where each “task” corresponds to a sensor, we relate each
MTL’s “task” to a traffic situation. Specifically, we first identify
these traffic situations by running clustering algorithms on all
sensors’ data. Subsequently, to enforce the commonalities under
each identified situation, we use the group Lasso regularization
in MTL to select a common set of features for the prediction
tasks, and we adapt efficient FISTA algorithm with guaranteed
convergence rate. We evaluated our methods with a large volume
of real-world traffic sensor data; our results show that by
incorporating traffic situations, our proposed MTL framework
performs consistently better than naively applying MTL per
sensor. Moreover, our holistic approach, under different traffic
situations, outperforms all the best traffic prediction approaches
for a given situation by up to 18% and 30% in short and long
term predictions, respectively.

",18.946978176291534,19.780350877192983,288
ICDM_17_032.txt,16.518332778003437,15.055885864422176,ICDM,6769,"
Entity resolution in settings with rich relational
structure often introduces complex dependencies between coreferences. Exploiting these dependencies is challenging – it
requires seamlessly combining statistical, relational, and logical
dependencies. One task of particular interest is entity resolution
in familial networks. In this setting, multiple partial representations of a family tree are provided, from the perspective of
different family members, and the challenge is to reconstruct a
family tree from these multiple, noisy, partial views. This reconstruction is crucial for applications such as understanding genetic
inheritance, tracking disease contagion, and performing census
surveys. Here, we design a model that incorporates statistical
signals, such as name similarity, relational information, such
as sibling overlap, and logical constraints, such as transitivity
and bijective matching, in a collective model. We show how to
integrate these features using probabilistic soft logic, a scalable
probabilistic programming framework. In experiments on realworld data, our model significantly outperforms state-of-theart classifiers that use relational features but are incapable of
collective reasoning.
",18.903936251131103,18.019845679012345,163
ICDM_17_033.txt,17.178724751044467,15.901734394637622,ICDM,4533,"
The task of community detection over complex
networks is of paramount importance in a multitude of applications. The present work puts forward a_ top-to-bottom
community identification approach, termed DC-EgoTen, in which
an egonet-tensor (EgoTen) based algorithm is developed in a
divide-and-conquer (DC) fashion for breaking the network into
smaller subgraphs, out of which the underlying communities
progressively emerge. In particular, each step of DC-EgoTen
forms a multi-dimensional egonet-based representation of the
graph, whose induced structure enables casting the task of
overlapping community identification as a constrained PARAFAC
decomposition. Thanks to the higher representational capacity of
tensors, the novel egonet-based representation improves the quality of detected communities by capturing multi-hop connectivity
patterns of the network. In addition, the top-to-bottom approach
ensures successive refinement of identified communities, so that
the desired resolution is achieved. Synthetic as well as real-world
tests corroborate the effectiveness of DC-EgoTen.

",19.117987234576393,18.87746835443038,159
ICDM_17_034.txt,14.276558368476785,12.865701120079066,ICDM,5854,"
The problem of automated discovery of process
models from event logs has been intensively researched in the
past two decades. Despite a rich field of proposals, state-ofthe-art automated process discovery methods suffer from two
recurrent deficiencies when applied to real-life logs: (i) they
produce large and spaghetti-like models; and (ii) they produce
models that either poorly fit the event log (low fitness) or highly
generalize it dow precision). Striking a tradeoff between these
quality dimensions in a robust and scalable manner has proved
elusive. This paper presents an automated process discovery
method that produces simple process models with low branching
complexity and consistently high and balanced fitness, precision
and generalization, while achieving execution times 2-6 times
faster than state-of-the-art methods on a set of 12 real-life logs.
Further, our approach guarantees deadlock-freedom for cyclic
process models and soundness for acyclic. Our proposal combines
a novel approach to filter the directly-follows graph induced by
an event log, with an approach to identify combinations of split
gateways that accurately capture the concurrency, conflict and
causal relations between neighbors in the directly-follows graph.

",16.728156217252725,17.90875,193
ICDM_17_035.txt,14.758202413565954,12.558033813962002,ICDM,5634,"
Clustering results are often affected by covariates
that are independent of the clusters one would like to discover.
Traditionally, Alternative Clustering algorithms can be used to
solve such a problem. However, these suffer from at least one
of the following problems: i) continuous covariates or nonlinearly separable clusters cannot be handled; ii) assumptions
are made about the distribution of the data; iii) one or
more hyper-parameters need to be set. Here we propose a
novel algorithm, named Kernel Conditional Clustering (KCC),
whose objective is derived from a kernel based conditional
dependence measure. KCC is parameter-light and makes no
assumptions about the cluster structure, the covariates, or the
distribution of the data. On both simulated and real-world
datasets, the proposed KCC algorithm detects the ground
truth cluster structures more accurately than state-of-the-art
alternative clustering methods.

",16.728156217252725,15.171428571428574,141
ICDM_17_036.txt,15.194113744984573,13.752306081896247,ICDM,5381,"
Time series motifs are approximately repeating
patterns in real-valued time series data. They are useful for
exploratory data mining and are often used as inputs for various
time series clustering, classification, segmentation, rule discovery,
and visualization algorithms. Since the introduction of the first
motif discovery algorithm for univariate time series in 2002,
multiple efforts have been made to generalize motifs to the
multidimensional case. In this work, we show that these efforts,
which typically attempt to find motifs on all dimensions, will not
produce meaningful motifs except in the most contrived situations.
We explain this finding and introduce mSTAMP, an algorithm
that allows meaningful discovery of multidimensional motifs.
Beyond producing objectively and subjectively meaningful results,
our algorithm has a host of additional advantages, including being
much faster, requiring fewer parameters and supporting
streaming data. We demonstrate the utility of our mSTAMPbased motif discovery framework on domains as diverse as audio
processing, industry, and sports analytics.
",17.613555460941566,17.533479853479857,157
ICDM_17_037.txt,13.080247692603098,11.574427396254134,ICDM,6159,"
Precipitation prediction, such as short-term rainfall
prediction, is a very important problem in the field of meteorological service. In practice, most of recent studies focus on leveraging
radar data or satellite images to make predictions. However, there
is another scenario where a set of weather features are collected
by various sensors at multiple observation sites. The observations
of a site are sometimes incomplete but provide important clues for
weather prediction at nearby sites, which are not fully exploited
in existing work yet. To solve this problem, we propose a multitask convolutional neural network model to automatically extract
features from the time series measured at observation sites and
leverage the correlation between the multiple sites for weather
prediction via multi-tasking. To the best of our knowledge, this
is the first attempt to use multi-task learning and deep learning
techniques to predict short-term rainfall amount based on multisite features. Specifically, we formulate the learning task as
an end-to-end multi-site neural network model which allows to
leverage the learned knowledge from one site to other correlated
sites, and model the correlations between different sites. Extensive
experiments show that the learned site correlations are insightful
and the proposed model significantly outperforms a broad set
of baseline models including the European Centre for Mediumrange Weather Forecasts system (ECMWF).

",16.827784334635936,16.482272727272733,221
ICDM_17_038.txt,13.464907435552753,11.678726578904055,ICDM,8488,"
 Since their introduction over a decade ago, time
series motifs have become a fundamental tool for time series
analytics, finding diverse uses in dozens of domains. In this work
we introduce Time Series Chains, which are related to, but distinct
from, time series motifs. Informally, time series chains are a
temporally ordered set of subsequence patterns, such that each
pattern is similar to the pattern that preceded it, but the first and
last patterns are arbitrarily dissimilar. In the discrete space, this
is similar to extracting the text chain “hit, hot, dot, dog” from a
paragraph. The first and last words have nothing in common, yet
they are connected by a chain of words with a small mutual
difference. Time series chains can capture the evolution of systems,
and help predict the future. As such, they potentially have
implications for prognostics. In this work, we introduce a robust
definition of time series chains, and a scalable algorithm that
allows us to discover them in massive datasets.

",13.81666964889586,11.69527108433735,167
ICDM_17_039.txt,15.810568938915129,13.906181490286496,ICDM,5956,"
Besides the text content, documents and their
associated words usually come with rich sets of meta information, such as categories of documents and semantic/syntactic
features of words, like those encoded in word embeddings. Incorporating such meta information directly into the generative
process of topic models can improve modelling accuracy and
topic quality, especially in the case where the word-occurrence
information in the training data is insufficient. In this paper, we
present a topic model, called MetaLDA, which is able to leverage either document or word meta information, or both of them
jointly. With two data argumentation techniques, we can derive
an efficient Gibbs sampling algorithm, which benefits from the
fully local conjugacy of the model. Moreover, the algorithm is
favoured by the sparsity of the meta information. Extensive
experiments on several real world datasets demonstrate that
our model achieves comparable or improved performance in
terms of both perplexity and topic quality, particularly in
handling sparse texts. In addition, compared with other models
using meta information, our model runs significantly faster.

",18.08858127442927,17.11262458471761,173
ICDM_17_040.txt,16.171058159701523,14.924164359222718,ICDM,6143,"
With the rapid development of location-based social
networks, Point-of-Interest (POI) recommendation has played an
important role in helping people discover attractive locations.
However, existing POI recommendation methods assume a flat
structure of POIs, which are better described in a hierarchical
structure in reality. Furthermore, we discover that both users’
content and spatial preferences exhibit hierarchical structures. To
this end, in this paper, we propose a hierarchical geographical
matrix factorization model (HGMEF) to utilize the hierarchical
structures of both users and POIs for POI recommendation.
Specifically, we first describe the POI influence degrees over
regions with two-dimensional normal distribution, and learn the
influence areas of different layers of POIs as the input of HGMF.
Then, we perform matrix factorization on user content preference
matrix, user spatial preference matrix, and POIs characteristic
matrix jointly with the modeling of implicit hierarchical structures. Moreover, a two-step optimization method is proposed to
learn the implicit hierarchical structure and find the solution
of HGMF efficiently. Finally, we evaluate HGMF on two largescale real-world location-based social networks datasets. Our
experimental results demonstrate that it outperforms the stateof-the-art methods in terms of precision and recall.

",17.833180683606166,17.083367346938775,197
ICDM_17_041.txt,13.828265149375767,12.31387236191885,ICDM,6574,"
Matrix Factorization (MF) is a very popular method
for recommendation systems. It assumes that the underneath
rating matrix is low-rank. However, this assumption can be
too restrictive to capture complex relationships and interactions
among users and items. Recently, Local LOw-Rank Matrix
Approximation (LLORMA) has been shown to be very successful
in addressing this issue. It just assumes the rating matrix is
composed of a number of low-rank submatrices constructed
from subsets of similar users and items. Although LLORMA
outperforms MF, how to construct such submatrices remains
a big problem. Motivated by the availability of rich social
connections in today’s recommendation systems, we propose a
novel framework, i.e., Social LOcal low-rank Matrix Approximation (SLOMA), to address this problem. To the best of
our knowledge, SLOMA is the first work to incorporate social
connections into the local low-rank framework. Furthermore, we
enhance SLOMA by applying social regularization to submatrices
factorization, denoted as SLOMA++. Therefore, the proposed
model can benefit from both social recommendation and the local
low-rank assumption. Experimental results from two real-world
datasets, Yelp and Douban, demonstrate the superiority of the
proposed models over LLORMA and ME. !

",13.747043046817236,13.592354312354317,196
ICDM_17_042.txt,15.081958876993959,13.47735768816855,ICDM,6452,"
In recent years, deep discriminative models have
achieved extraordinary performance on supervised learning
tasks, significantly outperforming their generative counterparts.
However, their success relies on the presence of a large amount
of labeled data. How can one use the same discriminative
models for learning useful features in the absence of labels?
We address this question in this paper, by jointly modeling
the distribution of data and latent features in a manner that
explicitly assigns zero probability to unobserved data. Rather
than maximizing the marginal probability of observed data,
we maximize the joint probability of the data and the latent
features using a two step EM-like procedure. To prevent the
model from overfitting to our initial selection of latent features,
we use adversarial regularization. Depending on the task, we
allow the latent features to be one-hot or real-valued vectors,
and define a suitable prior on the features. For instance, onehot features correspond to class labels, and are directly used
for unsupervised and semi-supervised classification task, whereas
real-valued feature vectors are fed as input to simple classifiers
for auxiliary supervised discrimination tasks. The proposed
model, which we dub dicriminative encoder (or DisCoder), is
flexible in the type of latent features that it can capture. The
proposed model achieves state-of-the-art performance on several
challenging tasks. Qualitative visualization of the latent features
shows that the features learnt by the DisCoder are indeed
meaningful.

",15.186304673781336,14.877272727272729,237
ICDM_17_043.txt,14.48295866126578,12.101291296625224,ICDM,7556,"
Product bundling is widely adopted for information
goods and online services because it can increase profit for
companies. For example, cable companies often bundle Internet
access and video streaming services together. However, it is
challenging to obtain an optimal bundling strategy, not only
because it is computationally expensive, but also that customers’
private information (e.g., valuations for products) is needed for
the decision, and we need to infer it from accessible datasets.
As customers’ purchasing data are getting richer due to the
popularity of online shopping, doors are open for us to infer
this information. This paper aims to address: (1) How to infer
customers’ valuations from the purchasing data? (2) How to determine the optimal product bundle to maximize the profit? We first
formulate a profit maximization framework to select the optimal
bundle set. We show that finding the optimal bundle set is NPhard. We then identify key factors that impact the profitability
of product bundling. These findings give us insights to develop a
computationally efficient algorithm to approximate the optimal
product bundle with a provable performance guarantee. To obtain the input of the bundling algorithm, we infer the distribution
of customers’ valuations from their purchasing data, based on
which we run our bundling algorithm and conduct experiments
on an Amazon co-purchasing dataset. We extensively evaluate the
accuracy of our inference and the bundling algorithm. Our results
reveal conditions under which bundling is highly profitable and
provide insights to guide the deployment of product bundling.

",16.666482175067898,15.037813765182186,249
ICDM_17_044.txt,15.100848662350156,14.055328808567527,ICDM,9792,"
Network embedding aims at projecting the network data
into a low-dimensional feature space, where the nodes are represented
as a unique feature vector and network structure can be effectively
preserved. In recent years, more and more online application service
sites can be represented as massive and complex networks, which are
extremely challenging for traditional machine learning algorithms to
deal with. Effective embedding of the complex network data into lowdimension feature representation can both save data storage space and
enable traditional machine learning algorithms applicable to handle the
network data. Network embedding performance will degrade greatly
if the networks are of a sparse structure, like the emerging networks
with few connections. In this paper, we propose to learn the embedding
representation for a target emerging network based on the broad learning
setting, where the emerging network is aligned with other external mature
networks at the same time. To solve the problem, a new embedding
framework, namely “Deep allgned autoencoder based eMbEdding”
(DIME), is introduced in this paper. DIME handles the diverse link and
attribute in a unified analytic based on broad learning, and introduces
the multiple aligned attributed heterogeneous social network concept to
model the network structure. A set of meta paths are introduced in the
paper, which define various kinds of connections among users via the
heterogeneous link and attribute information. The closeness among users
in the networks are defined as the meta proximity scores, which will be
fed into DIME to learn the embedding vectors of users in the emerging
network. Extensive experiments have been done on real-world aligned
social networks, which have demonstrated the effectiveness of DIME in
learning the emerging network embedding vectors.

",16.404322709996244,16.43257553956835,279
ICDM_17_045.txt,14.408155226819819,12.543885702910206,ICDM,7603,"
We consider the fundamental problem of inferring
the causal direction between two univariate numeric random
variables X and Y from observational data. The two-variable
case is especially difficult to solve since it is not possible to use
standard conditional independence tests between the variables.

To tackle this problem, we follow an information theoretic
approach based on Kolmogorov complexity and use the Minimum
Description Length (MDL) principle to provide a_ practical
solution. In particular, we propose a compression scheme to
encode local and global functional relations using MDL-based
regression. We infer X causes Y in case it is shorter to describe
Y as a function of X than the inverse direction. In addition, we
introduce SLOPE, an efficient linear-time algorithm that through
thorough empirical evaluation on both synthetic and real world
data we show outperforms the state of the art by a wide margin.

",17.693802365651003,15.56027397260274,147
ICDM_17_046.txt,16.314705170223927,15.340649686736775,ICDM,6622,"
Building effective recommender systems for domains
like fashion is challenging due to the high level of subjectivity
and the semantic complexity of the features involved (i.e., fashion
styles). Recent work has shown that approaches to ‘visual’ recommendation (e.g. clothing, art, etc.) can be made more accurate
by incorporating visual signals directly into the recommendation
objective, using ‘off-the-shelf’ feature representations derived
from deep networks. Here, we seek to extend this contribution by
showing that recommendation performance can be significantly
improved by learning ‘fashion aware’ image representations
directly, i.e., by training the image representation (from the pixel
level) and the recommender system jointly; this contribution
is related to recent work using Siamese CNNs, though we are
able to show improvements over state-of-the-art recommendation
techniques such as BPR and variants that make use of pretrained visual features. Furthermore, we show that our model
can be used generatively, i.e., given a user and a product category,
we can generate new images (i.e., clothing items) that are most
consistent with their personal taste. This represents a first step
towards building systems that go beyond recommending existing
items from a product corpus, but which can be used to suggest
styles and aid the design of new products.
",19.882160675589997,22.187708737864074,212
ICDM_17_047.txt,15.814785946413558,13.640939469872635,ICDM,6877,"
In today’s era of big data, robust least-squares
regression becomes a more challenging problem when considering the adversarial corruption along with explosive growth of
datasets. Traditional robust methods can handle the noise but
suffer from several challenges when applied in huge dataset
including 1) computational infeasibility of handling an entire
dataset at once, 2) existence of heterogeneously distributed
corruption, and 3) difficulty in corruption estimation when
data cannot be entirely loaded. This paper proposes online and
distributed robust regression approaches, both of which can
concurrently address all the above challenges. Specifically, the
distributed algorithm optimizes the regression coefficients of each
data block via heuristic hard thresholding and combines all the
estimates in a distributed robust consolidation. Furthermore,
an online version of the distributed algorithm is proposed to
incrementally update the existing estimates with new incoming
data. We also prove that our algorithms benefit from strong
robustness guarantees in terms of regression coefficient recovery
with a constant upper bound on the error of state-of-the-art batch
methods. Extensive experiments on synthetic and real datasets
demonstrate that our approaches are superior to those of existing
methods in effectiveness, with competitive efficiency.

",20.670646682091633,19.62901785714286,194
ICDM_17_048.txt,14.62263096281443,12.51500820288815,ICDM,6433,"
Motivated by the relevance of clustering or transitivity to a variety of network applications, we study the Triangle
Interdiction Problem (TIP), which is to find a minimum-size set
of edges that intersects all triangles of a network. As existing
approximation algorithms for this NP-hard problem either do
not scale well to massive networks or have poor solution quality,
we formulate two algorithms, TARL and DART, with worstcase guarantees 5/2 and 3 with respect to optimal, respectively.
Furthermore, DART is able to efficiently maintain its worstcase guarantee under dynamic edge insertion and removal to
the network. In our comprehensive experimental evaluation, we
demonstrate that DART is able to run on networks with billions
of triangles within 2 hours and is able to dynamically update its
solution in microseconds.

",20.503739492662863,19.32346153846154,131
ICDM_17_049.txt,14.118286838603545,11.542286054878772,ICDM,5635,"
We consider the problem of modeling data matrices
with locally low rank (LLR) structure, a generalization of the
popular low rank structure widely used in a variety of real
world application domains ranging from medical imaging to
recommendation systems. While LLR modeling has been found to
be promising in real world application domains, limited progress
has been made on the design of scalable algorithms for such
structures. In this paper, we consider a convex relaxation of
LLR structure, and propose an efficient algorithm based on
dual projected gradient descent (D-PGD) for computing the
proximal operator. While the original problem is non-smooth,
so that primal (sub)gradient algorithms will be slow, we show
that the proposed D-PGD algorithm has geometrical convergence
rate. We present several practical ways to further speed up
the computations, including acceleration and approximate SVD
computations. With experiments on both synthetic and real data
from MRI (magnetic resonance imaging) denoising, we illustrate
the superior performance of the proposed D-PGD algorithm
compared to several baselines.

",19.78447435784618,17.949411764705882,171
ICDM_17_050.txt,16.411884785636573,15.12399584494553,ICDM,7786,"
Given the soaring amount of data being generated
daily, graph mining tasks are becoming increasingly challenging,
leading to tremendous demand for summarization techniques.
Feature selection is a representative approach that simplifies
a dataset by choosing features that are relevant to a specific
task, such as classification, prediction, and anomaly detection.
Although it can be viewed as a way to summarize a graph in
terms of a few features, it is not well-defined for exploratory
analysis, and it operates on a set of observations jointly rather
than conditionally (i.e., feature selection from many graphs vs.
selection for an input graph conditioned on other graphs).
In this work, we introduce E AGLE (Exploratory Analysis of
Graphs with domain knowLEdge), a novel method that creates interpretable, feature-based, and domain-specific graph summaries
in a fully automatic way. That is, the same graph in different
domains—e.g., social science and neuroscience—will be described
via different E AGLE summaries, which automatically leverage the
domain knowledge and expectations. We propose an optimization
formulation that seeks to find an interpretable summary with
the most representative features for the input graph so that
it is: diverse, concise, domain-specific, and efficient. Extensive
experiments on synthetic and real-world datasets with up to
∼ 1M edges and ∼ 400 features demonstrate the effectiveness
and efficiency of E AGLE and its benefits over existing methods.
We also show how our method can be applied to various graph
mining tasks, such as classification and exploratory analysis.
",18.643177196211187,18.577419354838714,249
ICDM_17_051.txt,15.560165357918276,13.992623482712187,ICDM,6028,"
Subgroup discovery is a local pattern mining technique to find interpretable descriptions of sub-populations that
stand out on a given target variable. That is, these subpopulations are exceptional with regard to the global distribution.
In this paper we argue that in many applications, such as scientific discovery, subgroups are only useful if they are additionally
representative of the global distribution with regard to a control
variable. That is, when the distribution of this control variable
is the same, or almost the same, as over the whole data.

We formalise this objective function and give an efficient
algorithm to compute its tight optimistic estimator for the
case of a numeric target and a binary control variable. This
enables us to use the branch-and-bound framework to efficiently
discover the top-/ subgroups that are both exceptional as well
as representative. Experimental evaluation on a wide range of
datasets shows that with this algorithm we discover meaningful
representative patterns and are up to orders of magnitude faster
in terms of node evaluations as well as time.

",16.785175570968402,16.006857142857143,176
ICDM_17_052.txt,15.298949778282154,13.253069315106263,ICDM,5414,"
We introduce a dynamical spatio-temporal model
formalized as a recurrent neural network for forecasting time
series of spatial processes, i.e. series of observations sharing
temporal and spatial dependencies. The model learns these dependencies through a structured latent dynamical component, while a
decoder predicts the observations from the latent representations.
We consider several variants of this model, corresponding to
different prior hypothesis about the spatial relations between the
series. The model is evaluated and compared to state-of-the-art
baselines, on a variety of forecasting problems representative
of different application areas: epidemiology, geo-spatial statistics
and car-traffic prediction. Besides these evaluations, we also
describe experiments showing the ability of this approach to
extract relevant spatial relations.

",18.878054631974784,18.914000000000005,120
ICDM_17_053.txt,14.5698299324781,12.326043523143102,ICDM,5808,"
An interesting observation about the well-known
ADABOOST algorithm is that, though theory suggests it should
overfit when applied to noisy data, experiments indicate it often
does not do so in practice. In this paper, we study the behavior
of ADABOOST on datasets with one-sided uniform class noise
using linear classifiers as the base learner. We show analytically
that, under some ideal conditions, this approach will not overfit,
and can in fact recover a zero-error concept with respect to the
true, uncorrupted instance labels. We also analytically show that
ADABOOST increases the margins of predictions over boosting
iterations, as has been previously suggested in the literature.
We then compare the empirical behavior of ADABOOST using
real world datasets with one-sided noise derived from multipleinstance data. Although our assumptions may not hold in a
practical setting, our experiments show that standard ADABOOST
still performs well, as suggested by our analysis, and often
outperforms baseline variations in the literature that explicitly
try to account for noise.
",18.062587368997235,17.169191616766465,168
ICDM_17_054.txt,14.709093010309285,12.998123577553685,ICDM,6275,"
Understanding newly emerging events or topics
associated with a particular region of a given day can provide
deep insight on the critical events occurring in highly evolving
metropolitan cities. We propose herein a novel topic modeling
approach on text documents with spatio-temporal information
(e.g., when and where a document was published) such as
location-based social media data to discover prevalent topics
or newly emerging events with respect to an area and a time
point. We consider a map view composed of regular grids
or tiles with each showing topic keywords from documents
of the corresponding region. To this end, we present a tilebased spatio-temporally exclusive topic modeling approach called
STExNMEF, based on a novel nonnegative matrix factorization
(NMF) technique. STEXxNMF mainly works based on the two
following stages: (1) first running a standard NMF of each tile
to obtain general topics of the tile and (2) running a spatiotemporally exclusive NMF on a weighted residual matrix. These
topics likely reveal information on newly emerging events or
topics of interest within a region. We demonstrate the advantages
of our approach using the geo-tagged Twitter data of New
York City. We also provide quantitative comparisons in terms of
the topic quality, spatio-temporal exclusiveness, topic variation,
and qualitative evaluations of our method using several usage
scenarios. In addition, we present a fast topic modeling technique
of our model by leveraging parallel computing.

",17.122413403193683,16.990170940170945,236
ICDM_17_055.txt,13.797220006483208,11.498649424258137,ICDM,5483,"
We deal with online learning of acyclic Conditional
Preference networks (CP-nets) from data streams, possibly corrupted with noise. We introduce a new, efficient algorithm relying
on (i) information-theoretic measures defined over the induced
preference rules, which allow us to deal with corrupted data in
a principled way, and on (ii) the Hoeffding bound to define an
asymptotically optimal decision criterion for selecting the best
conditioned variable to update the learned network. This is the
first algorithm dealing with online learning of CP-nets in the
presence of noise. We provide a thorough theoretical analysis
of the algorithm, and demonstrate its effectiveness through an
empirical evaluation on synthetic and on real datasets.

",19.032712561301913,17.565553097345134,114
ICDM_17_056.txt,14.948183534974046,13.035141387628197,ICDM,4996,"
One of the most current challenging problems in
Gaussian process regression (GPR) is to handle large-scale
datasets and to accommodate an online learning setting where
data arrive irregularly on the fly. In this paper, we introduce
a novel online Gaussian process model that could scale with
massive datasets. Our approach is formulated based on alternative representation of the Gaussian process under geometric
and optimization views, hence termed geometric-based online GP
(GoGP). We developed theory to guarantee that with a good
convergence rate our proposed algorithm always produces a
(sparse) solution which is close to the true optima to any arbitrary
level of approximation accuracy specified a priori. Furthermore,
our method is proven to scale seamlessly not only with large-scale
datasets, but also to adapt accurately with streaming data. We
extensively evaluated our proposed model against state-of-the-art
baselines using several large-scale datasets for online regression
task. The experimental results show that our GoGP delivered
comparable, or slightly better, predictive performance while
achieving a magnitude of computational speedup compared with
its rivals under online setting. More importantly, its convergence
behavior is guaranteed through our theoretical analysis, which
is rapid and stable while achieving lower errors.

",17.693802365651003,17.052000000000003,201
ICDM_17_057.txt,16.02580242352731,14.359429391192343,ICDM,4362,"
Discovering and analyzing networks from nonnetwork data is a task with applications in fields as diverse as
neuroscience, genomics, energy, economics, and more. In these
domains, networks are often constructed out of multiple time
series by computing measures of association or similarity between
pairs of series. The nodes in a discovered graph correspond to
time series, which are linked via edges weighted by the association
scores of their endpoints. After graph construction, the network
may be thresholded such that only the edges with stronger
weights remain and the desired sparsity level is achieved.

While this approach is feasible for small datasets, its quadratic
time complexity does not scale as the individual time series
length and the number of compared series increase. Thus, to
avoid the costly step of building a fully-connected graph before
sparsification, we propose a fast network discovery approach
based on probabilistic hashing of randomly selected time series
subsequences. Evaluation on real data shows that our methods
construct graphs nearly 15 times as fast as baseline methods,
while achieving both network structure and accuracy comparable
to baselines in task-based evaluation.

",16.26309291913925,15.952732919254661,185
ICDM_17_058.txt,16.417486047598107,14.963913949568632,ICDM,7206,"
Detection of interesting (e.g., coherent or anomalous)
clusters has been studied extensively on plain or univariate
networks, with various applications. Recently, algorithms have
been extended to networks with multiple attributes for each node
in the real-world. In a multi-attributed network, often, a cluster of
nodes is only interesting for a subset (subspace) of attributes, and
this type of clusters is called subspace clusters. However, in the
current literature, few methods are capable of detecting subspace
clusters, which involves concurrent feature selection and network
cluster detection. These relevant methods are mostly heuristicdriven and customized for specific application scenarios.

In this work, we present a generic and theoretical framework
for detection of interesting subspace clusters in large multiattributed networks. Specifically, we propose a subspace graphstructured matching pursuit algorithm, namely, SG—Pursuit,
to address a broad class of such problems for different score
functions (e.g., coherence or anomalous functions) and topology
constraints (e.g., connected subgraphs and dense subgraphs). We
prove that our algorithm 1) runs in nearly-linear time on the
network size and the total number of attributes and 2) enjoys
rigorous guarantees (geometrical convergence rate and tight
error bound) analogous to those of the state-of-the-art algorithms
for sparse feature selection problems and subgraph detection
problems. As a case study, we specialize SG—Pursuit to optimize
a number of well-known score functions for two typical tasks,
including detection of coherent dense and anomalous connected
subspace clusters in real-world networks. Empirical evidence
demonstrates that our proposed generic algorithm SG-Pursuit
is superior over state-of-the-art methods that are designed
specifically for these two tasks.

",18.026119701940384,17.054074074074077,274
ICDM_17_059.txt,13.184767865910658,11.316255230044188,ICDM,5686,"
Time series classification has attracted much attention due to the ubiquity of time series. With the advance of technologies, the volume of available time series data becomes huge
and the content is changing rapidly. This requires time series
data mining methods to have low computational complexities. In
this paper, we propose a parameter-free time series classification
method that has a linear time complexity. The approach is
evaluated on all the 85 datasets in the well-known UCR time
series classification archive. The results show that the new method
achieves better overall classification accuracy performance than
the widely used benchmark, i.e. 1-nearest neighbor with dynamic
time warping, while consuming orders of magnitude less running
time. The proposed method is also applied on a large real-world
bird sounds dataset to verify its effectiveness.

",14.348710955821954,13.362281449893391,136
ICDM_17_060.txt,13.95054839169497,11.7765651874785,ICDM,5807,"
Given a contact network and coarse-grained diagnostic information like electronic Healthcare Reimbursement
Claims (eHRC) data, can we develop efficient intervention policies
to control an epidemic? Immunization is an important problem
in multiple areas especially epidemiology and public health.
However, most existing studies focus on developing pre-emptive
strategies assuming prior epidemiological models. In practice,
disease spread is usually complicated, hence assuming an underlying model may deviate from true spreading patterns, leading
to possibly inaccurate interventions. Additionally, the abundance
of health care surveillance data (like eHRC) makes it possible
to study data-driven strategies without too many restrictive
assumptions. Hence, such an approach can help public-health
experts take more practical decisions.

In this paper, we take into account propagation log and contact
networks for controlling propagation. We formulate the novel and
challenging Data-Driven Immunization problem without assuming
classical epidemiological models. To solve it, we first propose
an efficient sampling approach to align surveillance data with
contact networks, then develop an efficient algorithm with the
provably approximate guarantee for immunization. Finally, we
show the effectiveness and scalability of our methods via extensive
experiments on multiple datasets, and conduct case studies on
nation-wide real medical surveillance data.

",17.693802365651003,17.341090909090912,199
ICDM_17_061.txt,14.824878098063802,13.167354277190189,ICDM,6723,"
The need for short-text classification arises in many
text mining applications particularly health care applications.
In such applications shorter texts mean linguistic ambiguity
limits the semantic expression, which in turns would make
typical methods fail to capture the exact semantics of the scarce
words. This is particularly true in health care domains when the
text contains domain-specific or infrequently appearing words,
whose embedding can not be easily learned due to the lack of
training data. Deep neural network has shown great potentials
in boost the performance of such problems according to its
strength on representation capacity. In this paper, we propose
a bidirectional long short-term memory (BI-LSTM) recurrent
network to address the short-text classification problem that can
be used in two settings. Firstly when a knowledge dictionary
is available we adopt the well-known attention mechanism to
guide the training of network using the domain knowledge in
the dictionary. Secondly, to address the cases when domain
knowledge dictionary is not available, we present a multi-task
model to jointly learn the domain knowledge dictionary and do
the text classification task simultaneously. We apply our method
to a real-world interactive healthcare system and an extensively
public available ATIS dataset. The results show that our model
can positively grasp the key point of the text and significantly
outperforms many state-of-the-art baselines.

",15.903189008614273,15.302280701754388,229
ICDM_17_062.txt,13.823984058089383,12.444445040972706,ICDM,6561,"
Rapid development of bike-sharing systems has
brought people enormous convenience during the past decade.
On the other hand, high transport flexibility comes with dynamic
distribution of shared bikes, leading to an unbalanced bike
usage and growing maintenance cost. In this paper, we consider
to rebalance bicycle utilization by means of directing users to
different stations. For the first time, we devise a trip advisor
that recommends bike check-in and check-out stations with joint
consideration of service quality and bicycle utilization. From
historical data, we firstly identify that biased bike usage is
rooted from circumscribed bicycle circulation among few active
stations. Therefore, with defined station activeness, we optimize
the bike circulation by leading users to shift bikes between
highly active stations and inactive ones. We extensively evaluate
the performance of our design through real-world datasets.
Evaluation results show that the percentage of frequent used
bikes decreases by 33.6% on usage number and 28.6% on usage
time.

",15.247664890283005,13.83169025157233,162
ICDM_17_063.txt,14.92095877421638,13.370077179808096,ICDM,3794,"
Building an ideal graph which reveals the exact
intrinsic structure of the data is critical in graph-based clustering.
There have been a lot of efforts to construct an affinity matrix
satisfying such a need in terms of a similarity measure. A
recent approach attracting attention is on using doubly stochastic
normalization of the affinity matrix to improve the clustering
performance. In this paper, we propose a novel method to
build a high-quality affinity matrix via incorporating DavisKahan theorem of matrix perturbation theory in the doubly
stochastic normalization problem. We interpret the goal of
the doubly stochastic normalization problem as minimizing the
relative distance between the eigenspaces of the corresponding
matrices. Also, for the doubly stochastic normalization problem
we include an additional constraint that each eigenvalue be on
the unit interval to fully conform to the spectral graph theory.
Experiments on our framework present superior performance
over various datasets.

",16.439396014739867,16.05247619047619,151
ICDM_17_064.txt,16.223228487440018,14.772234818923504,ICDM,5834,"
Given a collection of basic customer demographics
(e.g., age and gender) and their behavioral data (e.g., item
purchase histories), how can we predict sensitive demographics
(e.g., income and occupation) that not every customer makes
available?

This demographics prediction problem is modeled as a classification task in which a customer’s sensitive demographic y
is predicted from his feature vector zx. So far, two lines of
work have tried to produce a “good” feature vector z« from
the customer’s behavioral data: (1) application-specific feature
engineering using behavioral data and (2) representation learning
(such as singular value decomposition or neural embedding) on
behavioral data. Although these approaches successfully improve
the predictive performance, (1) designing a good feature requires
domain experts to make a great effort and (2) features obtained
from representation learning are hard to interpret.

To overcome these problems, we present a Relational Infinite
Support Vector Machine (R-iSVM), a mixture-of-experts model
that can leverage behavioral data. Instead of augmenting the
feature vectors of customers, R-iSVM uses behavioral data to
find out behaviorally similar customer clusters and constructs a
local prediction model at each customer cluster. In doing so, RiSVM successfully improves the predictive performance without
requiring application-specific feature designing and hard-tointerpret representations.

Experimental results on three real-world datasets demonstrate
the predictive performance and interpretability of R-iSVM.
Furthermore, R-iSVM can co-exist with previous demographics
prediction methods to further improve their predictive performance.

",19.174585242480724,18.41,246
ICDM_17_065.txt,12.972690020922244,10.408100691055324,ICDM,7010,"
The blooming availability of traces for social, biological, and communication networks opens up unprecedented opportunities in analyzing diffusion processes in networks. However,
the sheer sizes of the nowadays networks raise serious challenges
in computational efficiency and scalability.

In this paper, we propose a new hyper-graph sketching
framework for influence dynamics in networks. The core of
our sketching framework, called SKIS, is an efficient importance sampling algorithm that returns only non-singular reverse
cascades in the network. Comparing to previously developed
sketches like RIS and SKIM, our sketch significantly enhances
estimation quality while substantially reducing processing time
and memory-footprint. Further, we present general strategies
of using SKIS to enhance existing algorithms for influence
estimation and influence maximization which are motivated by
practical applications like viral marketing. Using SKIS, we
design high-quality influence oracles for seed sets with average
estimation error up to 10x times smaller than those using RIS
and 6x times smaller than SKIMs. In addition, our influence
maximization using SKIS substantially improves the quality of
solutions for greedy algorithms. It achieves up to 10x times speedup and 4x memory reduction for the fastest RIS-based DSSA
algorithm, while maintaining the same theoretical guarantees.

",19.032712561301913,18.092937817258882,198
ICDM_17_066.txt,15.532255605016982,14.188722253052166,ICDM,6898,"
Ecological inference (EI) is a classical problem from
political science to model voting behavior of individuals given only
aggregate election results. Flaxman et al. recently formulated
EI as machine learning problem using distribution regression,
and applied it to analyze US presidential elections. However,
distribution regression unnecessarily aggregates individual-level
covariates available from census microdata, and ignores known
structure of the aggregation mechanism. We instead formulate
the problem as learning with label proportions (LLP), and develop
a new, probabilistic, LLP method to solve it. Our model is the
straightforward one where individual votes are latent variables.
We use cardinality potentials to efficiently perform exact inference
over latent variables during learning, and introduce a novel
message-passing algorithm to extend cardinality potentials to
multivariate probability models for use within multiclass LLP
problems. We show experimentally that LLP outperforms distribution regression for predicting individual-level attributes, and
that our method is as good as or better than existing state-ofthe-art LLP methods.

",18.848423458724294,19.210535714285715,161
ICDM_17_067.txt,14.091605476429862,11.994202664046927,ICDM,6364,"
The number of triangles in a graph is useful to
deduce a plethora of important features of the network that
the graph is modeling. However, finding the exact value of
this number is computationally expensive. Hence, a number of
approximation algorithms based on random sampling of edges, or
wedges (adjacent edge pairs) have been proposed for estimating
this value. We argue that for large sparse graphs with powerlaw degree distribution, random edge sampling requires sampling
large number of edges before providing enough information
for accurate estimation, and existing wedge sampling methods
lead to biased samplings, which in turn lead to less accurate
estimations. In this paper, we propose a hybrid algorithm between
edge and wedge sampling that addresses the deficiencies of
both approaches. We start with uniform edge sampling and
then extend each selected edge to form a wedge that is more
informative for estimating the overall triangle count. The core
estimate we make is the number of triangles each sampled edge
in the first phase participates in. This approach provides accurate
approximations with very small sampling ratios, outperforming
the state-of-the-art up to 8 times in sample size while providing
estimations with 95% confidence.

",16.061879428645646,14.49902918781726,198
ICDM_17_068.txt,15.086842683497824,13.04459157847155,ICDM,4499,"
Spammers use automated content spinning techniques to evade plagiarism detection by search engines. Text
spinners help spammers in evading plagiarism detectors by
automatically restructuring sentences and replacing words or
phrases with their synonyms. Prior work on spun content
detection relies on the knowledge about the dictionary used by
the text spinning software. In this work, we propose an approach
to detect spun content and its seed without needing the text
spinner’s dictionary. Our key idea is that text spinners introduce
stylometric artifacts that can be leveraged for detecting spun
documents. We implement and evaluate our proposed approach
on a corpus of spun documents that are generated using a popular
text spinning software. The results show that our approach can
not only accurately detect whether a document is spun but also
identify its source (or seed) document — all without needing the
dictionary used by the text spinner.

",15.151101081350806,13.881272015655579,148
ICDM_17_069.txt,15.211485079921605,13.364563946558505,ICDM,4082,"
In recent years, finding repetitive similar patterns
in time series has become a popular problem. These patterns are
called time series motifs. Recent studies show that using grammar
compression algorithms to find repeating patterns from the
symbolized time series holds promise in discovering approximate
motifs with variable length. However, grammar compression
algorithms are traditionally designed for string compression.
Therefore, existing work on grammar induction has not fully
utilized much available information that can be used to enhance
the performance of the algorithms. In this work, an iterative
framework based on grammar induction is proposed. In each
iteration, a revision operator called Noise Reduction Operator is
applied to revise the symbolized time series string based on the
rules returned from a base grammar induction algorithm. In our
experiments, we show that the proposed work can find motifs of
the same quality, with much faster running time compared to the
state-of-the-art variable-length exact motif discovery algorithm in
real world time series data.

",15.414825405933506,13.918414634146345,165
ICDM_17_070.txt,14.036940093169452,11.875943133196426,ICDM,5215,"
This paper proposes a new framework for anomaly
detection when collectively monitoring many complex systems.
The prerequisite for condition-based monitoring in industrial
applications is the capability of (1) capturing multiple operational states, (2) managing many similar but different assets,
and (3) providing insights into the internal relationship of the
variables.
To meet these criteria, we propose a multi-task learning
approach based on a sparse mixture of sparse Gaussian
graphical models (GGMs). Unlike existing fused- and grouplasso-based approaches, each task is represented by a sparse
mixture of sparse GGMs, and can handle multi-modalities.
We develop a variational inference algorithm combined with
a novel sparse mixture weight selection algorithm. To handle
issues in the conventional automatic relevance determination
(ARD) approach, we propose a new `0 -regularized formulation
that has guaranteed sparsity in mixture weights. We show
that our framework eliminates well-known issues of numerical
instability in the iterative procedure of mixture model learning.
We also show better performance in anomaly detection tasks
on real-world data sets. To the best of our knowledge, this is
the first proposal of multi-task GGM learning allowing multimodal distributions.
",17.251386760058843,15.870701754385966,190
ICDM_17_071.txt,13.79165644938697,11.38954315720381,ICDM,5328,"
We consider the semi-supervised dimension
reduction problem: given a high dimensional dataset with
a small number of labeled data and huge number of
unlabeled data, the goal is to find the low-dimensional
embedding that yields good classification results. Most of
the previous algorithms for this task are linkage-based
algorithms. They try to enforce the must-link and cannotlink constraints in dimension reduction, leading to a nearest neighbor classifier in low dimensional space. In this
paper, we propose a new hyperplane-based semi-supervised
dimension reduction method—the main objective is to learn
the low-dimensional features that can both approximate the
original data and form a good separating hyperplane. We
formulate this as a non-convex optimization problem and
propose an efficient algorithm to solve it. The algorithm
can scale to problems with millions of features and can
easily incorporate non-negative constraints in order to learn
interpretable non-negative features. Experiments on real
world datasets demonstrate that our hyperplane-based dimension reduction method outperforms state-of-art linkagebased methods when very few labels are available.

",17.93193317476759,16.33481245011971,180
ICDM_17_072.txt,13.954178793117073,12.324182146542828,ICDM,6161,"
Active learning aims to reduce manual labeling
efforts by proactively selecting the most informative unlabeled
instances to query. In real-world scenarios, it’s often more
practical to query a batch of instances rather than a single
one at each iteration. To achieve this we need to keep not
only the informativeness of the instances but also their diversity.
Many heuristic methods have been proposed to tackle batch
mode active learning problems, however, they suffer from two
limitations which if addressed would significantly improve the
query strategy. Firstly, the similarity amongst instances is simply
calculated using the feature vectors rather than being jointly
learned with the classification model. This weakens the accuracy
of the diversity measurement. Secondly, these methods usually
exploit the decision boundary by querying the data points close
to it. However, this can be inefficient when the labeled set is too
small to reveal the true boundary. In this paper, we address both
limitations by proposing a deep neural network based algorithm.
In the training phase, a pairwise deep network is not only
trained to perform classification, but also to project data points
into another space, where the similarity can be more precisely
measured. In the query selection phase, the learner selects a
set of instances that are maximally uncertain and minimally
redundant (exploitation), as well as are most diverse from the
labeled instances (exploration). We evaluate the effectiveness
of the proposed method on a variety of classification tasks:
MNIST classification, opinion polarity detection and heart failure
prediction. Our method outperforms the baselines with both
higher classification accuracy and faster convergence rate.

",15.705129121369687,14.553992395437263,265
ICPE_17_001.txt,12.928950774520189,10.809746695468277,ICPE,5915,"

Benchmark suites are an indispensable part of scientific research to compare different approaches against each another.
The diversity of benchmarks is an important asset to evaluate novel approaches for effectiveness and weaknesses. In
this paper, we describe the memory characteristics and the
GC behavior of commonly used Java benchmarks, i.e., the
DaCapo benchmark suite, the DaCapo Scala benchmark
suite and the SPECjvm2008 benchmark suite. The paper
can serve as a useful guide to select benchmarks in accordance with desired application characteristics on modern
virtual machines as well as with different compilers and
garbage collectors. It also helps to put results that are
based on these benchmarks into perspective. Additionally,
we compare Java’s current default collector to the G1 GC.

",15.470042427545799,13.860966386554622,122
ICPE_17_002.txt,15.669102729476759,13.21416055999125,ICPE,7836,"

This paper presents an empirical approach to measuring and
modeling the energy consumption of multicore processors.
The modeling approach allows us to find a breakdown of the
energy consumption among a set of key hardware components, also called HW nodes. We explicitly model the frontend and the back-end in terms of the number of instructions
executed. We also model the L1, L2 and L3 caches. Furthermore, we explicitly model the static and dynamic energy
consumed by the the uncore and core components. From a
software perspective, our methodology allows us to correlate
energy to the executed code, which helps find opportunities
for code optimization and tuning.
We use binary analysis and hardware counters for performance characterization. Although, we use the on-chip
counters (RAPL) for energy measurement, our methodology
does not rely on a specific method for energy measurement.
Thus, it is portable and easy to deploy in various computing
environments. We validate our energy model using two Intel
processors with a set of HPC codelets, where data sizes are
varied to come from the L1, L2 and L3 caches and show 3%
average modeling error. We present a comprehensive analysis and show energy consumption differences between kernels
and relate those differences to the algorithms that are implemented. Finally, we discuss how vectorization leads to
energy savings compared to non-vectorized codes.

",15.796290986955235,13.67005605381166,224
ICPE_17_003.txt,16.039701851396753,14.161212541795472,ICPE,7768,"

Modern processors incorporate several performance monitoring units, which can be used to count events that occur within different components of the processor. They
provide access to information on hardware resource usage
and can therefore be used to detect performance bottlenecks. Thus, many performance measurement tools are able
to record them complementary to information about the application behavior. However, the exact meaning of the supported hardware events is often incomprehensible due to the
system complexity and partially lacking or even inaccurate
documentation. For most events it is also not documented
whether a certain rate indicates a saturated resource usage.
Therefore, it is usually difficult to draw conclusions on the
performance impact from the observed event rates. In this
paper, we evaluate whether hardware performance counters
can be used to measure the capacity utilization within the
memory hierarchy and estimate the impact of memory accesses on the achieved performance. The presented approach
is based on a small selection of micro-benchmarks that constantly stress individual components in the memory subsystem, ranging from caches to main memory. These workleads are used to identify hardware performance counters
that provide good estimates for the utilization of individual
components in the memory hierarchy. However, since access
latencies can be interleaved with computing instructions, a
high utilization of the memory hierarchy does not necessarily result in low performance. We therefore also investigate
which stall counters provide good estimates for the number of cycles that are actually spent waiting for the memory
hierarchy.

",17.74463917768843,16.50371057513915,246
ICPE_17_004.txt,13.281810140855267,11.668203046027362,ICPE,8247,"

Many software systems provide configuration options relevant to users, which are often called features. Features
influence functional properties of software systems as well
as non-functional ones, such as performance and memory
consumption. Researchers have successfully demonstrated
the correlation between feature selection and performance.
However, the generality of these performance models across
different hardware platforms has not yet been evaluated.

We propose a technique for enhancing generality of performance models across different hardware environments using
linear transformation. Empirical studies on three real-world
software systems show that our approach is computationally
efficient and can achieve high accuracy (less than 10% mean
relative error) when predicting system performance across
23 different hardware platforms. Moreover, we investigate
why the approach works by comparing performance distributions of systems and structure of performance models across
different platforms.

",17.122413403193683,16.395939849624064,134
ICPE_17_005.txt,16.181675994134054,14.692235186526819,ICPE,8553,"

Video streaming applications generate a large fraction of Internet traffic. Much of this content is delivered over HTTP
using standard web servers. Unlike other types of web workloads, HTTP video streaming workloads are typically disk
bound, and therefore an important problem is that of optimizing disk access.

In this paper we design, implement and evaluate Libception, an application-level shim library that implements techniques for improving disk I/O efficiency. Web servers can
achieve the benefits of these techniques simply by linking
with Libception, without the need to modify source code.
In contrast to making kernel changes or attempting to optimize kernel tuning, Libception provides a portable and relatively simple setting in which techniques for optimizing I/O
in HTTP video streaming servers can be implemented and
evaluated.

We report experimental results evaluating the efficacy of
the aggressive prefetching and disk I/O serialization techniques currently implemented in Libception, for three web
servers (Apache, nginx and the userver) and two operating
systems (FreeBSD, Linux). We find that on FreeBSD, video
streaming throughput with all three web servers can be doubled by linking with Libception. On Linux, performance
similar to that provided with Libception was eventually obtained by examining the kernel source to understand and
tune kernel parameters. With the default kernel parameter
settings, however, and regardless of which Linux disk scheduler is selected, we find that use of Libception can approximately double throughput. We find that both aggressive
prefetching and serialization are necessary to achieve these
benefits.

",17.122413403193683,15.668487302171513,248
ICPE_17_006.txt,15.50417087971923,14.52189318532989,ICPE,9676,"

Many cost-conscious public cloud workloads (“tenants”) are turning to Amazon EC2’s spot instances because, on average, these
instances offer significantly lower prices (up to 10 times lower)
than on-demand and reserved instances of comparable advertized
resource capacities. To use spot instances effectively, a tenant must
carefully weigh the lower costs of these instances against their poorer
availability. Towards this, we empirically study four features of
EC2 spot instance operation that a cost-conscious tenant may find
useful to model. Using extensive evaluation based on both historical and current spot instance data, we show shortcomings in the
state-of-the-art modeling of these features that we overcome. Our
analysis reveals many novel properties of spot instance operation
some of which offer predictive value while others do not. Using
these insights, we design predictors for our features that offer a
balance between computational efficiency (allowing for online resource procurement) and cost-efficacy. We explore “case studies”
wherein we implement prototypes of dynamic spot instance procurement advised by our predictors for two types of workloads.
Compared to the state-of-the-art, our approach achieves (i) comparable cost but much better performance (fewer bid failures) for a
latency-sensitive in-memory Memcached cache, and (ii) an additional 18% cost-savings with comparable (if not better than) performance for a delay-tolerant batch workload.

",16.975882523387877,16.717499999999998,225
ICPE_17_007.txt,15.431940100861606,13.769373864349905,ICPE,10215,"

Simplifying the task of resource management and scheduling for
customers, while still delivering complex Quality-of-Service (QoS),
is key to cloud computing. Many autoscaling policies have been
proposed in the past decade to decide on behalf of cloud customers
when and how to provision resources to a cloud application utilizing
cloud elasticity features. However, in prior work, when a new policy
is proposed, it is seldom compared to the state-of-the-art, and is often
compared only to static provisioning using a predefined QoS target.
This reduces the ability of cloud customers and of cloud operators to
choose and deploy an autoscaling policy. In our work, we conduct an
experimental performance evaluation of autoscaling policies, using
as application model workflows, a commonly used formalism for
automating resource management for applications with well-defined
yet complex structure. We present a detailed comparative study of
general, state-of-the-art, generic autoscaling policies, along with
two new workflow-specific policies. To understand the performance
differences between the 7 policies, we conduct various forms of
pairwise and group comparisons. We report both individual and
aggregated metrics. Our results highlights the trade-offs between
the suggested policies, and thus enable a better understanding of the
current state-of-the-art.

",17.37919286519448,15.718525641025646,209
ICPE_17_008.txt,14.84438055546049,12.961960004486386,ICPE,9035,"

Elastic scaling of event stream processing systems has gained significant attention recently due to the prevalence of cloud computing technologies. We investigate on the complexities associated
with elastic scaling of an event processing system in a private/public cloud scenario. We develop an Elastic Switching Mechanism
(ESM) which reduces the overall average latency of event processing jobs by significant amount considering the cost of operating the
system. ESM is augmented with adaptive compressing of upstream
data. The ESM conducts one of the two types of switching where
either part of the data is sent to the public cloud (data switching)
or a selected query is sent to the public cloud (query switching)
based on the characteristics of the query. We model the operation
of the ESM as the function of two binary switching functions. We
show that our elastic switching mechanism with compression is capable of handling out-of-order events more efficiently compared
to techniques which does not involve compression. We used two
application benchmarks called EmailProcessor and a Social Networking Benchmark (SNB2016) to conduct multiple experiments
to evaluate the effectiveness of our approach. In a single query
deployment with EmailProcessor benchmark we observed that our
elastic switching mechanism provides 1.24 seconds average latency
improvement per processed event which is 16.70% improvement
compared to private cloud only deployment. When presented the
option of scaling EmailProcessor with four public cloud VMs ESM
further reduced the average latency by 37.55% compared to the
single public cloud VM. In a multi-query deployment with both
EmailProcessor and SNB2016 we obtained a reduction of average
latency of both the queries by 39.61 seconds which is a decrease
of 7% of overall latency. These performance figures indicate that
our elastic switching mechanism with compressed data streams can
effectively reduce the average elapsed time of stream processing
happening in private/public clouds.

",17.219254097808864,16.065675895765477,312
ICPE_17_009.txt,14.486380044095064,12.039839480612429,ICPE,5366,"

Moving averages (MAs) are often used in adaptive systems
to monitor the state during operation. Their output is used
as input for control purposes. There are multiple methods with different ability, complexity, and parameters. We
propose a framework for the definition of MAs and develop
performance criteria, e.g., the concept of memory, that allow to parameterize different methods in a comparable way.
Moreover, we identify deficiencies of frequently used methods and propose corrections. We extend MAs to moying histograms which facilitate the approximation of timedependent quantiles. We further extend the framework to
rate measurement, discuss various approaches, and propose
a novel method which reveals excellent properties. The proposed concepts help to visualize time-dependent data and to
simplify design, parametrization, and evaluation of technical
control systems.

",15.742502247213078,14.433452380952378,128
ICPE_17_010.txt,15.827298239773445,14.40900532879596,ICPE,7405,"

A multi-tier Internet server application needs to be analyzed
for its performance before it is released. Performance analysis is usually done by (a) load testing of the application on
a testbed and (b) building a performance model of the application. While there are a plethora of Web load-generator
tools available, there are two problems with these tools: one,
the tests have to be configured manually, which can lead to a
time-consuming trial-and-error process until the desired performance charts in the appropriate load ranges are obtained;
and two, the load generator tools do not produce output
that is directly useful for creating a performance model of
the application. In this paper, we present AutoPerf, a load
generator tool designed to meet two distinct goals, named
capacity analysis and profiling. The goal of capacity analysis
is to run a comprehensive load test on a Web application, in
an appropriately chosen range, at a minimal number of load
levels, while still producing an accurate graph of throughput
and response time vs load levels. The goal of profiling is to
generate a detailed server resource usage profile per request
type, without instrumenting the application code. This data
(e.g. CPU execution time by Web server for one request) is
crucial for parameterizing performance models of the application. AutoPerf intelligently plans and configures its load
tests by using analytical results from queuing theory along
with some heuristics. Results show that AutoPerf is able to
run performance tests very efficiently while still producing
an accurate chart of performance metrics.

",16.76809479433877,15.324050583657588,259
ICPE_17_011.txt,14.94712753587542,14.014967709765887,ICPE,6549,"

Black-box modeling techniques are used when modeling computer systems with unknown internal structure or behavior
and/or when it is not feasible or too time consuming to
monitor a running computer system. The main challenge in
these situations lies in estimating values for the parameters
of these models, especially the values of service demands
at the various devices for each transaction class. These estimates have to be compliant with the input-output relationships observed through measurements. This means that
solving a model of the system with the estimated parameters
should yield the same outputs (e.g., response times) for the
same inputs (e.g., arrival rates or concurrency level). This
paper presents a method for automatically estimating service demands for open, closed, single and multiclass queuing
networks (QN). The method is based on casting the estimation problem as a non-linear optimization problem. However, because the solution of closed QNs does not have a
closed form, we need to resort to black-box optimization
techniques. The parameter estimation method presented
here is part of «Model, a framework for automatically deriving performance models of systems whose detailed characteristics (structure and behavior) are unknown. Other portions of the framework were discussed in detail in previous
publications by the authors. This paper illustrates the ideas
through several numerical examples and then applies them
to a multi-tiered operational system running OFBiz. The
estimated service demands closely satisfy the input-output
relationships at various workload intensity levels and can be
used for prediction purposes.
",16.018793980428352,14.753041255932825,252
ICPE_17_012.txt,16.001832087453263,14.475478335941297,ICPE,5973,"

Benchmarking is a widely-used technique to quantify the
performance of software systems. However, the design and
implementation of a benchmarking study can face several
challenges. In particular, the time required to perform a
benchmarking study can quickly spiral out of control, owing to the number of distinct variables to systematically examine. In this paper, we propose IRIS, an IteRative and
Intelligent Experiment Selection methodology, to maximize
the information gain while minimizing the duration of the
benchmarking process. IRIS selects the region to place the
next experiment point based on the variability of both dependent, i.e., response, and independent variables in that
region. It aims to identify a performance function that minimizes the response variable prediction error for a constant
and limited experimentation budget. We evaluate IRIS for a
wide selection of experimental, simulated and synthetic systems with one, two and three independent variables. Considering a limited experimentation budget, the results show
IRIS is able to reduce the performance function prediction
error up to 4.3 times compared to equal distance experiment
point selection. Moreover, we show that the error reduction
can further improve through system-specific parameter tuning. Analysis of the error distributions obtained with IRIS
reveals that the technique is particularly effective in regions
where the response variable is sensitive to changes in the
independent variables.

",17.353723509956247,16.744889400921668,220
ICPE_17_013.txt,17.561525718206685,16.533741631070274,ICPE,6873,"
Many situations in the security domain require decisionmaking based on complex data, i.e., many variables which
need to be taken into account before adequate decisions can
be made. For example, in a surveillance scenario, the size
and complexity of the area of interest, the mix of objects,
and the unexpected behavior of suspects are just a few examples of complex variables to be analyzed in the process.
Existing decision support systems provide some analysis,
but are typically limited in the complexity they can handle. Therefore, users end up with simplified models which
often suffer in the accuracy of their decisions and, ultimately,
may lead to incorrect decisions. In this work, we present a
framework that can scale to cope with the complexity and
time requirements of real-world scenarios, while remaining
flexible to handle the ad-hoc adaptation to the situation.
We discuss the challenges and solutions for such a scalable
and flexible system, and validate it using a target tracking
scenario in urban environments of different sizes.
",18.7741,17.103809523809527,170
ICPE_17_014.txt,16.90489592811387,15.850650524704399,ICPE,9306,"

The evolution of cloud-computing imposes many challenges
on performance testing and requires not only a different
approach and methodology of performance evaluation and
analysis, but also specialized tools and frameworks to support such work. In traditional performance testing, typically a single workload was run against a static test configuration. The main metrics derived from such experiments
included throughput, response times, and system utilization
at steady-state. While this may have been sufficient in the
past, where in many cases a single application was run on
dedicated hardware, this approach is no longer suitable for
cloud-based deployments. Whether private or public cloud,
such environments typically host a variety of applications on
distributed shared hardware resources, simultaneously accessed by a large number of tenants running heterogeneous
workloads. The number of tenants as well as their activity and resource needs dynamically change over time, and
the cloud infrastructure reacts to this by reallocating existing or provisioning new resources. Besides metrics such as
the number of tenants and overall resource utilization, performance testing in the cloud must be able to answer many
more questions: How is the quality of service of a tenant impacted by the constantly changing activity of other tenants?
How long does it take the cloud infrastructure to react to
changes in demand, and what is the effect on tenants while it
does so? How well are service level agreements met? What
is the resource consumption of individual tenants? How can
global performance metrics on application- and system-level
in a distributed system be correlated to an individual tenant’s perceived performance?

In this paper we present CloudPerf, a performance test
framework specifically designed for distributed and dynamic
multi-tenant environments, capable of answering all of the
above questions, and more. CloudPerf consists of a distributed harness, a protocol-independent load generator and
workload modeling framework, an extensible statistics framework with live-monitoring and post-analysis tools, interfaces
for cloud deployment operations, and a rich set of both lowlevel as well as high-level workloads from different domains.

",17.03242331605538,16.544082840236687,340
ICPE_17_015.txt,15.719907069031464,14.508268031303306,ICPE,8539,"

Video game crash events are characterized primarily by large
media payloads and by highly bursty traffic patterns, with
hundreds of thousands or millions of reports being issued in
only a few minutes. These events are invaluable in quickly
responding to game breaking issues that directly impact user
experience. Even the slightest delay in capturing, processing
and reporting these events can lead to user abandonment
and significant financial cost.

A traditional standalone RESTful service, backed by a
vertically scaled SQL database is neither a reliable nor costeffective solution to this problem. An architecture that decouples capture and persistence and uses a horizontally scalable NoSQL database is not only easier to provision, but also
uses fewer cpu and memory resources to provide the same
end to end latency and throughput.

By replacing our RESTful implementation with one that
takes advantage both of the aforementioned design and multitenant provisioning, we have reduced our dedicated cpu footprint by 63% and memory footprint by 59%. Additionally,
we have decreased our data loss during spikes to essentially
0, maintained sub-second persistence latency and improved
query latency in the average case by 54% with only a 3%
sacrifice for worst case queries.

",18.3970566412798,17.786122448979594,197
ICPE_17_016.txt,15.514038796780547,14.183047923890815,ICPE,4848,"

The detection of early-warning signals of performance deterioration can help technical support teams in taking swift
remedial actions, thus ensuring rigor in production support
operations of large scale software systems. Performance
anomalies or deterioration, if left unattended, often result
in system slowness and unavailability. In this paper, we
presents a simple, intuitive and low-overhead technique for
recognizing the early warning signs in near real time before they impact the system The technique is based on the
inverse relationship which exists between throughput and
average response time in a closed system. Because of this
relationship, a significant increase in the average system response time causes an abrupt fall in system throughput.
To identify such occurrences automatically, Individuals and
Moving Range (XmR) control charts are used. We also provide a case study from a real-world production system, in
which the technique has been successfully used. The use of
this technique has reduced the occurrence of performance
related incidents significantly in our daily operations. The
technique is tool agnostic and can also be easily implemented
in popular system monitoring tools by building custom extensions.

",15.742502247213078,15.633260869565223,185
ICPE_17_017.txt,15.73668398856724,14.37301820671685,ICPE,5986,"
Tactical Operations Center (TOC) system in military field is
an advanced computer system composed of multiple servers
and desktops to interlock internal/external weapon systems
processing mission-critical applications in combat situation.
However, the current TOC system has several limitations
such as difficulty of integrating tactical weapon systems including missile launch system and radar system into the
single TOC system due to the heterogeneity of HW and
SW between systems, and an inefficient computing resource
management for the weapon systems.
In this paper, we proposed a novel HPC supported missioncritical Cloud architecture as TOC for Surface-to-Air-Missile
(SAM) system with OpenStack Cloud OS, Data Distribution
Service (DDS), and GPU virtualization techniques. With
this approach, our system provides elastic resource management over the weapon systems with virtual machines,
integration of heterogeneous systems with different kinds of
guest OS, real-time, reliable, and high-speed communication
between the virtual machines and virtualized GPU resource
over the virtual machines. Evaluation of our TOC system includes DDS performance measurement over 10Gbps
Ethernet and QDR InfiniBand networks on the virtualized
environment with OpenStack Cloud OS, and GPU virtualization performance evaluation with two different methods,
PCI pass-through and remote-API. With the evaluation results, we conclude that our system provides reasonable performance in the combat situation compared to the previous
TOC system while additionally supports scalable and elastic
use of computing resource through the virtual machines.
",21.640513641318698,23.256287553648075,234
ICPE_17_018.txt,14.350926956427372,12.416523809523813,ICPE,8165,"

Quantitative aspects of modern IT systems are often specified by service level agreements (SLAs) which relate the
maximal load of a system with guaranteed bounds for response times and delays. These quantities are specified for
single services which are combined in a service oriented architecture (SOA) to composed services offered to potential
users or other service providers. To derive SLAs for composed services and to plan the required capacity to guarantee SLAs, appropriate methods and tools have to be used
that compute results based on information given in SLAs.
In this paper it is argued that most available approaches
are not sufficient to analyze systems based on SLA information. A new method and a tool are presented that support
the efficient calculation of bounds for delays in composed
systems based on bounds for the load and the delay of the
individual components which are specified in the SLAs of
the components. Furthermore, the presented tool can be
used to generate bounds for the required processing capacity which a provider has to provide in order to guarantee the
quality of service defined in the SLAs.

The presented approach is in some sense a counterpart
to mean value analysis for queueing networks but rather
than mean values, worst case bounds for different quantities
like response times or departure processes are computed.
Analysis is based on min/+ algebra but the mathematical
approach is hidden from the user by a graphical interface
allowing a simple graphical specification and result representation for networks of composed services.

",19.5731925562951,18.959523809523812,252
ICPE_17_019.txt,15.794232070221945,13.690991107206095,ICPE,8032,"

Checkpointing and rollback is a key mechanism used to improve the reliability of software systems. The benefits of
this mechanism can be offset by the overhead of checkpointing when the failure rate is low. The problem of developing analytic models of rollback and checkpointing has been
continuously addressed for over four decades using different assumptions. This paper examines the problem under
a more realistic angle, i.e., one in which there are several
software components sharing resources (e.g., processors and
I/O devices) among themselves and with the checkpointing
processes. Additionally, the paper allows for different components to have different computing, rollback, and checkpointing demands, as well as different failure distributions.
Our models also allow for various checkpointing processes
to be executing concurrently to checkpoint the state of different software components. The analytic models developed
here combine Markov Chains and Queuing Networks and allow us to compute the following metrics: (1) average time
needed by a component to complete its execution, (2) average throughput of a component, (3) availability of a component, and (4) checkpointing overhead. The models were validated through extensive simulation and experimentation.

",17.833180683606166,16.46695652173913,187
ICPE_17_020.txt,14.584939196764577,13.197386911669891,ICPE,4074,"

A number of analytical models have been proposed to estimate the write amplification of the Flash storage to obtain
the expected lifespan. This work is dedicated to examining
the practical implication of the four existing analytical models for estimating the write amplification: Coupon Collector,
Uniform Distribution, Expected Value and Markov model.
Since the models assume uniform random workload in full
utilization of an SSD to predict write amplification, they are
not applicable in predicting write amplification in general
workload. Moreover, the existing models have not been verified with the real SSD. In this work, we compare the write
amplification of the models with that of a real SSD. When
we use 0.147 as the overprovisioning factor of an SSD while
running uniform random workload, the write amplification
of Uniform Distribution, Expected Value, Markov model is
3.90, 4.08, and 4.08, respectively. However, write amplification of the real SSD shows 1.19, which is very different
from that of the prediction models. Through experiment,
we found that write amplification is closely related to the
value of overprovisioning factor. To improve the accuracy
of existing prediction models, we update the overprovisioning factor to take account of the ratio of a hot file and the
utilization of the storage. We also find that by setting the
overprovisioning factor to 1.15, we can obtain write amplification of 1.2 which is close to the write amplification of
general workload in a real SSD.
",16.15616582465906,15.264000000000003,244
ICPE_17_021.txt,15.675352579470843,14.035102985848713,ICPE,8862,"

Today’s embedded systems demand increasing computing
power to accommodate the ever-growing software functionality. Automotive and avionic systems aim to leverage the
high performance capabilities of multicore platforms, but are
faced with challenges with respect to temporal predictability. Multicore designers have achieved much progress on
improvement of memory-dependent performance in caching
systems and shared memories in general. However, having
applications running simultaneously and requesting the access to the shared memories concurrently leads to interference. The performance unpredictability resulting from interference at any shared memory level may lead to violation
of the timing properties in safety-critical real-time systems.
In this paper, we introduce a formal analysis framework for
the schedulability and memory interference of multicore systems with shared caches and DRAM. We build a multicore
system model with a fine grained application behavior given
in terms of periodic preemptible tasks, described with explicit read and write access numbers for shared caches and
DRAM. We also provide a method to analyze and recommend candidates for task-to-core reallocation with the goal
to find schedulable configurations if a given system is not
schedulable. Our model-based framework is realized using
Uppaal and has been used to analyze a case study.

",17.631426480028413,16.66129353233831,203
ICPE_17_022.txt,15.25407513399512,13.952240103270224,ICPE,9767,"

Systems deployed in mobile environments are typically char
acterized by intermittent connectivity and asynchronous send
ing/reception of data. To create effective mobile systems
for such environments, it is essential to guarantee acceptable levels of timeliness between sending and receiving mobile users. In order to provide QoS guarantees in different application scenarios and contexts, it is necessary to
model the system performance by incorporating the intermittent connectivity. Queueing Network Models (QNMs)
offer a simple modeling environment, which can be used to
represent various application scenarios, and provide accurate analytical solutions for performance metrics, such as
system response time. In this paper, we provide an analytical solution regarding the end-to-end response time between
users sending and receiving data by modeling the intermittent connectivity of mobile users with QNMs. We utilize
the publish /subscribe (pub/sub) middleware as the underlying communication infrastructure for mobile users. To represent the user’s connections/disconnections, we model and
solve analytically an ON/OFF queueing system by applying
a mean value approach. Finally, we validate our model using
simulations with real-world workload traces. The deviations
between the performance results foreseen by the analytical
model and the ones provided by the simulator are shown to
be less than 5% for a variety of scenarios.

",18.7741,18.424587320574165,211
ICPE_17_023.txt,17.203000818800227,15.412025142592846,ICPE,4613,"

Previous work has shown that benchmark and application performance in public cloud computing environments can be highly variable. Utilizing Amazon EC2 traces that include measurements affected by CPU, memory, disk, and network performance, we study
commonly used methodologies for comparing performance measurements in cloud computing environments. The results show
considerable flaws in these methodologies that may lead to incorrect conclusions. For instance, these methodologies falsely report
that the performance of two identical systems differ by 38% using a confidence level of 95%. We then study the efficacy of the
Randomized Multiple Interleaved Trials (RMIT) methodology using the same traces. We demonstrate that RMIT could be used
to conduct repeatable experiments that enable fair comparisons in
this cloud computing environment despite the fact that changing
conditions beyond the user’s control make comparing competing
alternatives highly challenging.

",18.7741,17.04722222222222,137
ICPE_17_024.txt,15.606329165446102,14.743722115516114,ICPE,8213,"

The increasing demand for real-time analytics requires the
fusion of Transactional (OLTP) and Analytical (OLAP) systems, eschewing ETL processes and introducing a plethora
of proposals for the so-called Hybrid Analytical and Transactional Processing (HTAP) systems.

Unfortunately, current benchmarking approaches are not
able to comprehensively produce a unified metric from the
assessment of an HTAP system. The evaluation of both
engine types is done separately, leading to the use of disjoint
sets of benchmarks such as TPC-C or TPC-H.

In this paper we propose a new benchmark, HTAPBench,
providing a unified metric for HTAP systems geared toward
the execution of constantly increasing OLAP requests limited by an admissible impact on OLTP performance. To
achieve this, a load balancer within HTAPBench regulates
the coexistence of OLTP and OLAP workloads, proposing
a method for the generation of both new data and requests,
so that OLAP requests over freshly modified data are comparable across runs.

We demonstrate the merit of our approach by validating
it with different types of systems: OLTP, OLAP and HTAP;
showing that the benchmark is able to highlight the differences between them, while producing queries with comparable complexity across experiments with negligible variability.

",18.946978176291534,19.19774111675127,198
ICPE_17_025.txt,16.015661993022032,14.45038578603442,ICPE,6300,"

Computer architects have increased hardware parallelism
and power efficiency by integrating massively parallel hardware accelerators (coprocessors) into compute systems. Many
modern HPC clusters now consist of multi-CPU nodes along
with additional hardware accelerators in the form of graphics
processing units (GPUs). Each CPU and GPU is integrated
with system memory via communication links (QPI and
PCIe) and multi-channel memory controllers. The increasing density of these heterogeneous computing systems has
resulted in complex performance phenomena including nonuniform memory access (NUMA) and resource contention
that make application performance hard to predict and tune.
This paper presents the Topology Aware Resource Usability
and Contention (TARUC) benchmark. TARUC is a modular, open-source, and highly configurable benchmark useful
for profiling dense heterogeneous systems to provide insight
for developers who wish to tune application codes for specific systems. Analysis of TARUC performance profiles from
a multi-CPU, multi-GPU system is also presented.

",17.77360955136429,17.70447619047619,151
ICPE_17_026.txt,15.074533637471436,13.236305777327054,ICPE,7886,"

The process of performance tuning is time consuming and
costly even if it is carried out automatically. It is crucial to
learn from the experience of experts. Our long-term goal is
to construct a database of facts extracted from specific performance tuning histories of computation-intensive applications such that we can search the database for promising
optimization patterns that fit a given kernel.

In this study, as a significant step toward our goal, we
explored a thousand computation-intensive applications in
terms of the distribution of kernel classes, each of which is related to expected efficiency and specific tuning patterns. To
statistically estimate the distribution of the kernel classes,
100 loops were randomly sampled and then manually classified by experienced performance engineers. The result indicates that 50-70% of the kernels are memory-bound and
hence difficult to run efficiently on modern scalar processors. In addition, based on the classification results, we
constructed experimental classifiers for identifying loop kernels and for predicting kernel classes, which achieved crossvalidated classification accuracy of 81% and 65%, respectively.

",18.243605946275583,16.88342857142857,176
ICPE_17_027.txt,15.661803597450092,14.232360552041406,ICPE,8665,"

The usage of open source (OS) software is wide-spread across
many industries. While the functional quality of OS projects
is considered to be similar to closed-source software, much
is unknown about the quality in terms of performance. One
challenge for OS developers is that, unlike for functional
testing, there is a lack of accepted best practices for performance testing. To reveal the state of practice of performance
testing in OS projects, we conduct an exploratory study on
111 Java-based OS projects from GitHub. We study the
performance tests of these projects from five perspectives:
(1) developers, (2) size, (3) test organization, (4) types of
performance tests and (5) used tooling. We show that writing performance tests is not a popular task in OS projects:
performance tests form only a small portion of the test suite,
are rarely updated, and are usually maintained by a small
group of core project developers. Further, even though many
projects are aware that they need performance tests, developers appear to struggle implementing them. We argue
that future performance testing frameworks should provider
better support for low-friction testing, for instance via nonparameterized methods or performance test generation, as
well as focus on a tight integration with standard continuous
integration tooling.

",15.247664890283005,14.462500000000002,209
ICPE_17_028.txt,16.336786204458555,14.254598765512451,ICPE,5423,"

Collection data structures have a major impact on the performance of applications, especially in languages such as
Java, C#, or C++. This requires a developer to select an
appropriate collection from a large set of possibilities, including different abstractions (e.g. list, map, set, queue),
and multiple implementations. In Java, the default implementation of collections is provided by the standard Java
Collection Framework (JCF). However, there exist a large
variety of less known third-party collection libraries which
can provide substantial performance benefits with minimal
code changes.

In this paper, we first study the popularity and usage
patterns of collection implementations by mining a code
corpus comprised of 10,986 Java projects. We use the results to evaluate and compare the performance of the six
most popular alternative collection libraries in a large variety of scenarios. We found that for almost every scenario
and JCF collection type there is an alternative implementation that greatly decreases memory consumption while offering comparable or even better execution time. Memory
savings range from 60% to 88% thanks to reduced overhead
and some operations execute 1.5x to 50x faster.

We present our results as a comprehensive guideline to
help developers in identifying the scenarios in which an alternative implementation can provide a substantial performance improvement. Finally, we discuss how some coding
patterns result in substantial performance differences of collections.

",17.916177094544413,15.955821428571433,226
ICPE_17_029.txt,16.37991970643937,15.299521410579349,ICPE,8750,"

Although methods and tools for unit testing of performance
exist for over a decade, anecdotal evidence suggests unit testing of performance is not nearly as common as unit testing
of functionality. We examine this situation in a study of
GitHub projects written in Java, looking for occurrences of
performance evaluation code in common performance testing
frameworks. We quantify the use of such frameworks, identifying the most relevant performance testing approaches, and
describe how we adjust the design of our SPL performance
testing framework to follow these conclusions.

",17.122413403193683,17.96367816091954,88
ICPP_17_001.txt,15.941549722622792,14.04817522174822,ICPP,7170,"
 Production-quality parallel applications are often
a mixture of diverse operations, such as computation- and
communication-intensive, regular and irregular, tightly coupled
and loosely linked operations. In conventional construction of
parallel applications, each process performs all the operations,
which might result inefficient and seriously limit scalability,
especially at large scale. We propose a decoupling strategy to
improve the scalability of applications running on large-scale
systems. Our strategy separates application operations onto
groups of processes and enables a dataflow processing paradigm
among the groups. This mechanism is effective in reducing the
impact of load imbalance and increases the parallel efficiency
by pipelining multiple operations. We provide a proof-of-concept
implementation using MPI, the de-facto programming system
on current supercomputers. We demonstrate the effectiveness of
this strategy by decoupling the reduce, particle communication,
halo exchange and I/O operations in a set of scientific and dataanalytics applications. A performance evaluation on 8,192 processes of a Cray XC40 supercomputer shows that the proposed
approach can achieve up to 4x performance improvement.

",19.032712561301913,17.86437865497076,173
ICPP_17_002.txt,14.051333972674552,11.790730544647435,ICPP,5670,"
Parallel and distributed processing is employed to
accelerate training for many deep-learning applications with
large models and inputs. As it reduces synchronization and
communication overhead by tolerating stale gradient updates,
asynchronous stochastic gradient descent (ASGD), derived from
stochastic gradient descent (SGD), is widely used. Recent theoretical analyses show ASGD converges with linear asymptotic
speedup over SGD.

Oftentimes glossed over in theoretical analysis are communication overhead and practical learning rates that are critical
to the performance of ASGD. After analyzing the communication performance and convergence behavior of ASGD using
the Downpour algorithm as an example, we demonstrate the
challenges for ASGD to achieve good practical speedup over
SGD. We propose a distributed, bulk-synchronous stochastic
gradient descent algorithm that allows for sparse gradient
aggregation from individual learners. The communication cost
is amortized explicitly by a gradient aggregation interval, and
global reductions are used instead of a parameter server for
gradient aggregation. We prove its convergence and show that
it has superior communication performance and convergence
behavior over popular ASGD implementations such as Downpour
and EAMSGD for deep-learning applications.

",19.53771442962202,18.318372905027932,180
ICPP_17_003.txt,15.596188217789148,13.71827643475585,ICPP,4698,"
This paper implements a Smoothed Particle
Hydrodynamics simulation code and distributes it on a
heterogeneous cluster. The theoretical analysis results show
that treating GPU as equivalent peer of CPU rather than an
assistant or a substitute is the most efficient way of using a
CPU+GPU compute node. However, it raises complex
challenges of heterogeneous cooperation. Our strategies of
hybrid-level domain decomposition, multi-stage thread pool
and data transfer optimization are addressed and the
respective effects are showed. Steady balance ratio of 1.1
between 6 nodes and GPU workload ratio of 0.43 between
2CPU+1GPU cards are observed. Average speedups of 2.9x,
1.81x, 1.1x over the single GPU version are obtained on three
heterogeneous systems. The weak scaling tests are carried out
on 1 to 16384 CPU-GPU nodes and the overall parallel
efficiency of 70.3% is achieved.

",14.955608360458719,13.52245187436677,146
ICPP_17_004.txt,15.228077735262268,13.891022821829768,ICPP,8061,"
HPCG and Graph500 can be regarded as the two
most relevant benchmarks for high-performance computing systems. Existing supercomputer designs, however, tend to focus on
floating-point peak performance, a metric less relevant for these
two benchmarks, leaving resources underutilized, and resulting
in little performance improvements, for these benchmarks, over
time. In this work, we analyze the implementation of both
benchmarks on a novel shared-memory near-data processing
architecture. We study a number of aspects: 1. a system parameter design exploration, 2. software optimizations, and 3. the
exploitation of unique architectural features like user-enhanced
coherence as well as the exploitation of data-locality for inter
near-data processor traffic.

For the HPCG benchmark, we show a factor 2.5x application
level speedup with respect to a CPU, and a factor 2.5x powerefficiency improvement with respect to a GPU. For the Graph500
benchmark, we show up to a factor 3.5x speedup with respect toa
CPU. Furthermore, we show that, with many of the existing datalocality optimizations for this specific graph workload applied,
local memory bandwidth is not the crucial parameter, and a
high-bandwidth as well as low-latency interconnect are arguably
more important, shining a new light on the near-data processing
characteristics most relevant for this type of heavily optimized
graph processing.

",18.3970566412798,18.05809078771696,218
ICPP_17_005.txt,15.28751558938782,14.244276737779678,ICPP,5810,"
Hyperspectral image classification has been proved
significant in remote sensing field. Traditional classification methods have meet bottlenecks due to the lack of remote sensing
background knowledge or high dimensionality. Deep learning
based methods, such as deep convolutional neural network
(CNN), can effectively extract high level features from raw data.
But the training of deep CNN is rather time-consuming. The
general purpose graphic processing units (GPUs) have been
considered as one of the most common co-processors that can help
accelerate deep learning applications. In this paper we propose
a GPU-based Cube CNN (GCN) framework for hyperspectral
image classification. First, a Parallel Neighbor Pixels Extraction
(PNPE) algorithm is designed to enable the framework directly
loading raw hyperspectral image into GPU’s global memory, and
extracting samples into data cube. Then, based on the peculiarity
of hyperspectral image and cube convolution, we propose a
novel Cube CNN-to-GPU mapping mechanism that transfers the
training of Cube CNN to GPU effectively. Finally, the mini-batch
gradient descent(MBGD) algorithm is improved with Computing
United Device Architecture(CUDA) multi-streaming technique,
which further speeds up network training in GCN framework.
Experiments on KSC dataset, PU dataset and SA dataset show
that, compared with state-of-art framework Caffe, we achieve
up to 83% and 67% reduction in network training time without
losing accuracy, when using SGD (Stochastic Gradient Descent)
and MBGD algorithm respectively. Experiments across different
GPUs show the same performance trend, which demonstrates the
good extendibility of GCN framework.

",16.133371291317395,15.422888563049856,250
ICPP_17_006.txt,14.054081349126118,12.29180613687679,ICPP,7277,"
The architectural trend towards heterogeneity has
pushed heterogeneous computing to the fore of parallel computing research. Heterogeneous algorithms, often carefully handcrafted, have been designed for several important problems from
parallel computing such as sorting, graph algorithms, matrix
computations, and the like. A majority of these algorithms follow
a work partitioning approach where the input is divided into
appropriate sized parts so that individual devices can process
the “right” parts of the input. However, arriving at a good work
partitioning is usually non-trivial and may require extensive empirical search. Such an extensive empirical search can potentially
offset any gains accrued out of heterogeneous algorithms. Other
recently proposed approaches too are in general inadequate.

In this paper, we propose a simple and effective technique for
work partitioning in the context of heterogeneous algorithms.
Our technique is based on sampling and therefore can adapt to
both the algorithm used and the input instance. Our technique
is generic in its applicability as we will demonstrate in this
paper. We validate our technique on three problems: finding
the connected components of a graph (CC), multiplying two
unstructured sparse matrices (spmm), and multiplying two scalefree sparse matrices. For these problems, we show that using our
method, we can find the required threshold that is under 10%
away from the best possible thresholds.

",16.35954948731386,14.887968160871392,218
ICPP_17_007.txt,14.57215672987235,12.377459852216752,ICPP,6437,"
OpenMP is the de facto standard application programming interface (API) for on-node parallelism. The most
popular OpenMP runtimes rely on POSIX threads (pthreads)
implementations that offer an excellent performance for coarsegrained parallelism and match perfectly with the current hardware. However, a recent trend in runtimes/applications points in
the direction of leveraging massive on-node parallelism in conjunction with fine-grained and dynamic scheduling paradigms. It
has been demonstrated that lightweight thread CLWT) solutions
are more appropriate for these new parallel paradigms. We have
developed GLTO, an OpenMP implementation over the recentlyemerged Generic Lightweight Threads (GLT) API. GLT exports
a common API for LWT libraries that offers the possibility of
running the same application over different native LWT solutions.
In this paper we use GLTO to analyze different scenarios where
OpenMP implementations may benefit from the use of either
LWT or pthreads. Our study reveals that none of the threading
approaches obtains the best performance in all the scenarios, but
that there are important gaps among them.

",17.122413403193683,14.935714285714287,169
ICPP_17_008.txt,14.859952420007676,13.27518475904619,ICPP,7942,"
Dynamic task graph schedulers automatically balance work across processor cores by scheduling tasks among
available threads while preserving dependences. In this paper,
we design NABBITC, a provably efficient dynamic task graph
scheduler that accounts for data locality on NUMA systems.
NABBITC allows users to assign a color to each task representing
the location (e.g., a processor core) that has the most efficient
access to data needed during that node’s execution. NABBITC
then automatically adjusts the scheduling so as to preferentially
execute each node at the location that matches its color—leading
to better locality because the node is likely to make local rather
than remote accesses. At the same time, NABBITC tries to
optimize load balance and not add too much overhead compared
to the vanilla NABBIT scheduler that does not consider locality.
We provide a theoretical analysis that shows that NABBITC does
not asymptotically impact the scalability of NABBIT.

We evaluated the performance of NABBITC on a suite of
memory intensive benchmarks. Our experiments indicates that
adding locality awareness has a considerable performance advantage compared to the vanilla NABBIT scheduler. In addition, we
also compared NABBITC to OpenMP programs for both regular
and irregular applications. For regular applications, OpenMP
achieves perfect locality and perfect load balance statically. For
these benchmarks, NABBITC has a small performance penalty
compared to OpenMP due to its dynamic scheduling strategy. For
irregular applications, where OpenMP can not achieve locality
and load balance simultaneously, we find that NABBITC performs
better. Therefore, NABBITC combines the benefits of localityaware scheduling for regular applications (the forte of static
schedulers such as those in OpenMP) and dynamically adapting
to load imbalance (the forte of dynamic schedulers such as Cilk
Plus, TBB, and Nabbit).

",16.941846955676066,15.865211267605634,287
ICPP_17_009.txt,16.13527236161354,14.24261495093414,ICPP,5237,"

Transactional Memory (TM) promises both to provide a scalable
mechanism for synchronization in concurrent programs, and to offer ease-of-use benefits to programmers. To date, TM’s biggest successes have been as a mechanism for achieving Transactional Lock
Elision (TLE). In TLE, critical sections are attempted as transactions, with a fall-back to the original lock if conflicts manifest. Thus
TLE expects to improve scalability, but not ease of programming.
Still, until TLE can deliver performance improvements, transactional styles of programming are unlikely to gain popularity.

In this paper, we describe our experiences employing TLE in
two real-world programs: the PBZip2 file compression tool, and
the x265 video encoder/decoder. We discuss the obstacles we encountered, propose solutions to those obstacles, and introduce open
challenges. In experiments using the GCC compiler’s hardware and
software support for TM, we observe that both are able to outperform the original lock-based code, potentially heralding the readiness of TM to be used more broadly in TLE, if not for truly transactional styles of programming.

",17.267425705330172,14.199651162790698,175
ICPP_17_010.txt,17.055298803336388,16.122216167753567,ICPP,5658,"
We present a set of new batched CUDA kernels
for the LU factorization of a large collection of independent
problems of different size, and the subsequent triangular
solves. All kernels heavily exploit the registers of the graphics
processing unit (GPU) in order to deliver high performance for
small problems. The development of these kernels is motivated
by the need for tackling this embarrasingly-parallel scenario
in the context of block-Jacobi preconditioning that is relevant
for the iterative solution of sparse linear systems.

",18.599290044081553,17.09397590361446,84
ICPP_17_011.txt,14.670898414483307,12.74134569660917,ICPP,5702,"
Sparse general matrix-matrix multiplication
(SpGEMM) is one of the key kernels of preconditioners such
as algebraic multigrid method or graph algorithms. However,
the performance of SpGEMM is quite low on modern
processors due to random memory access to both input and
output matrices. As well as the number and the pattern of
non-zero elements in the output matrix, important for
achieving locality, are unknown before the _ execution.
Moreover, the state-of-the-art GPU implementations of
SpGEMM requires large amounts of memory for temporary
results, limiting the matrix size computable on fast GPU
device memory. We propose a new fast SpGEMM< algorithm
requiring small amount of memory and achieving high
performance. Calculation of the pattern and value in output
matrix is optimized by using GPU’ s on-chip shared memory
and a hash table. Additionally, our algorithm launches
multiple kernels running concurrently to improve the
utilization of GPU resources. The kernels for the calculation
of each row of output matrix are chosen based on the
number of non-zero elements. Performance evaluation using
matrices from the Sparse Matrix Collection of University
Florida on NVIDIA’ s Pascal generation GPU shows that our
approach achieves speedups of up to x4.3 in single precision
and x4.4 in double precision compared to existing SpGEMM
libraries. Furthermore, the memory usage is reduced by
14.7% in single precision and 10.9% in double precision on
average, allowing larger matrices to be computed.
",17.00531248756302,15.457546218487398,243
ICPP_17_012.txt,14.691626622321433,13.010738690172655,ICPP,4652,"
Low-rank sparse tensor factorization is a popular
tool for analyzing multi-way data and is used in domains
such as recommender systems, precision healthcare, and cybersecurity. Imposing constraints on a factorization, such as
non-negativity or sparsity, is a natural way of encoding prior
knowledge of the multi-way data. While constrained factorizations are useful for practitioners, they can greatly increase
factorization time due to slower convergence and computational
overheads. Recently, a hybrid of alternating optimization and
alternating direction method of multipliers (AO-ADMM) was
shown to have both a high convergence rate and the ability to
naturally incorporate a variety of popular constraints. In this
work, we present a parallelization strategy and two approaches
for accelerating AO-ADMM. By redefining the convergence
criteria of the inner ADMM iterations, we are able to split
the data in a way that not only accelerates the per-iteration
convergence, but also speeds up the execution of the ADMM
iterations due to efficient use of cache resources. Secondly,
we develop a method of exploiting dynamic sparsity in the
factors to speed up tensor-matrix kernels. These combined
advancements achieve up to 8x speedup over the state-of-theart on a variety of real-world sparse tensors.

",17.693802365651003,16.451078431372554,205
ICPP_17_013.txt,16.555474125569543,14.890937901248979,ICPP,8533,"
General-purpose computing on GPUs has become
more accessible due to features such as shared virtual memory
and demand paging. Unfortunately it comes at a price, and that
is performance. Automatic memory management is convenient
but suffers from many drawbacks, preventing heterogeneous
systems from achieving their full potential. In this work we
analyze the challenges and inefficiencies of demand paging in
GPUs, in particular on collaborative computations where data
migrates multiple times between host and device. We establish
that demand paging on GPUs introduces significant overheads for
these kind of computations, and identify the issues of false sharing
and unnecessary data transfers derived from the granularity at
which data is migrated. In order to alleviate these problems
we propose a memory organization and dynamic migration
scheme to efficiently share data between host and device at fine
granularities and without software intervention. We evaluate our
design with a set of collaborative heterogeneous benchmarks and
find it achieves 15% lower execution times on average with cache
line-sized migrations, but severely degrading performance on
benchmarks that access large blocks of contiguous memory. Pagesized migrations, although inefficient, provide on average a 47%
execution time reduction with our design over a baseline system
implementing demand paging. Our results suggest that cache linesized migrations are not feasible in systems using a PCI-Express
interconnect. In order to understand how future interconnect
technologies will impact the feasibility of fine-grained migrations,
we evaluate our scheme with various link latencies. We find
interconnect latencies four to five times lower than PCI-Express
are sufficient to effectively share data at finer granularities.

",17.946242345024807,17.101818181818185,265
ICPP_17_014.txt,14.864390974463273,12.74398341271782,ICPP,5449,"
As we move towards an era of hundreds of cores,
the research community has witnessed the emergence of optoelectronic network on-chip designs based on nanophotonics, in
order to achieve higher network throughput, lower latencies,
and lower dynamic power. However, traditional nanophotonics
options face limitations such as large device footprints compared with electronics, higher static power due to continuous
laser operation, and an upper limit on achievable data rates due
to large device capacitances. Nanoplasmonics is an emerging
technology that has the potential for providing transformative
gains on multiple metrics due to its potential to increase
the light-matter interaction. In this paper, we propose and
analyze a hybrid opto-electric NoC that incorporates Hybrid Plasmonics Photonics Interconnect (HyPPI), an optical
interconnect that combines photonics with plasmonics. We
explore various opto-electronic network hybridization options
by augmenting a mesh network with HyPPI links, and compare
them with the equivalent options afforded by conventional
nanophotonics as well as pure electronics. Our design space
exploration indicates that augmenting an electronic NoC with
HyPPI gives a performance to cost ratio improvement of up
to 1.8x. To further validate our estimates, we conduct trace
based simulations using the NAS Parallel Benchmark suite.
These benchmarks show latency improvements up to 1.64x,
with negligible energy increase. We then further carry out
performance and cost projections for fully optical NoCs, using
HyPPI as well as conventional nanophotonics. These futuristic
projections indicate that all-HyPPI NoCs would be two orders
more energy efficient than electronics, and two orders more
area efficient than all-photonic NoCs.
",18.458006810337128,17.573856031128404,260
ICPP_17_015.txt,15.585357415451881,13.865879604672056,ICPP,7891,"
Improving the performance of I/O virtualization is a
key issue for cloud and datacenter infrastructures, especially with
the rapid increase of network interconnection speeds. Previous
efforts have made the performance overhead associated with
the virtual I/O data path largely negligible. The remaining
bottlenecks mainly lie in the event path: hypervisor interventions
trigger costly virtual machine (VM) exits and lead to dramatical
performance degradation. Aiming at an optimal virtual I/O
event path, we propose ES2, a comprehensive scheme that
simultaneously improves bidirectional I/O event delivery between
guest VMs and their devices. ES2 can provide efficient I/O
request delivery, non-exit interrupt delivery and enhanced I/O
responsiveness. Moreover, it does not require any modification
to guest operating system (OS) or compromise any virtualization
benefit. We demonstrate that ES2 greatly reduces VM exit rate
with the time in guest (TIG) for I/O processing above 96%
for TCP streams and 99% for UDP streams, increases guest
throughput by 1.8x for Memcached and 2x for Apache, and keeps
guest latency at a very low level.

",17.93193317476759,16.33809523809524,179
ICPP_17_016.txt,16.40608486872306,15.208182878106701,ICPP,6963,"
While GPUs are becoming common in HPC systems,
the CPU is still responsible for managing both GPU-side and
CPU-side compute, communication, and synchronization operations. For instance, if a result from a GPU-side computation is to
be transferred to a remote destination, then the CPU must synchronize on GPU compute completion issuing a communication
operation. Both CPU cycles and energy are consumed waiting
for synchronization. In turn, this significantly affects overall
application time and scalability (eg: strong scaling applications).

In this work, we present techniques to decouple communication control flow between CPU and GPU on GPU-enabled systems
with MPI+CUDA applications using the novel GPUDirect-aSync
(GDS) mechanism. GDS allows the GPU to progress network
communication with the goal of placing the CPU away from the
critical path. To take advantage of GDS in MPI+CUDA applications, we introduce the notion of offloading MPI operations
to CUDA streams (referred as MPI-GDS) which subsequently
allow the GPU and the NIC to progress MPI communication
in stream-order either before or after a CUDA operation. We
also propose efficient designs/protocols to realize point-to-point
communication operations that guarantee stream-ordering while
achieving good performance. The proposed methods show good
benefits with micro-benchmarks and up to 30% improvement in
application-kernel pattern mimicking benchmark and up to 36%
improvement with broadcast application-pattern simulation (in
medium message range with 8 GPU nodes) in comparison with
a pure MPI+CUDA application.

",18.001758247042904,17.089041835357623,245
ICPP_17_017.txt,16.232189363564807,15.169243786526238,ICPP,6680,"
Broadcast operations (e.g. MPI_Bcast) have been
widely used in deep learning applications to exchange a
large amount of data among multiple graphics processing
units (GPUs). Recent studies have shown that leveraging the
InfiniBand hardware-based multicast (IB-MCAST) protocol
can enhance scalability of GPU-based broadcast operations.
However, these initial designs with IB-MCAST are not optimized for multi-source broadcast operations with large messages, which is the common communication scenario for deep
learning applications. In this paper, we first model existing
broadcast schemes and analyze their performance bottlenecks
on GPU clusters. Then, we propose a novel broadcast design
based on message streaming to better exploit IB-MCAST and
NVIDIA GPUDirect RDMA (GDR) technology for efficient
large message transfer operation. The proposed design can
provide high overlap among multi-source broadcast operations.
Experimental results show up to 68% reduction of latency
compared to state-of-the-art solutions in a benchmark-level
evaluation. The proposed design also shows near-constant
latency for a single broadcast operation as a system grows.
Furthermore, it yields up to 24% performance improvement in
the popular deep learning framework, Microsoft CNTK, which
uses multi-source broadcast operations; notably, the performance gains are achieved without modifications to applications.
Our model validation shows that the proposed analytical model
and experimental results match within a 10% range. Our model
also predicts that the proposed design outperforms existing
schemes for multi-source broadcast scenarios with increasing
numbers of broadcast sources in large-scale GPU clusters.
",15.021129683784007,14.326138211382116,248
ICPP_17_018.txt,15.205132819127673,13.530584845275115,ICPP,4628,"
GPUs are employed to accelerate scientific applications however they require much more programming effort from
the programmers particularly because of the disjoint address
spaces between the host and the device. OpenACC and OpenMP
4.0 provide directive based programming solutions to alleviate
the programming burden however synchronous data movement
can create a performance bottleneck in fully taking advantage
of GPUs. We propose a tiling based programming model and
its library that simplifies the development of GPU programs and
overlaps the data movement with computation. The programming
model decomposes the data and computation into tiles and treats
them as the main data transfer and execution units, which enables
pipelining the transfers to hide the transfer latency. Moreover,
partitioning application data into tiles allows the programmer to
still take advantage of GPU even though application data cannot
fit into the device memory. The library leverages C++ lambda
functions, OpenACC directives, CUDA streams and tiling API
from TiDA to support both productivity and performance. We
show the performance of the library on a data transfer-intensive
and a compute-intensive kernels and compare its speedup against
OpenACC and CUDA. The results indicate that the library can
hide the transfer latency, handle the cases where there is no
sufficient device memory, and achieves reasonable performance.

",18.511140095513987,17.28745283018868,212
ICPP_17_019.txt,14.41868550815511,12.301812298251281,ICPP,6741,"
This paper investigates how to improve the memory
locality of graph-structured analytics on large-scale shared memory systems. We demonstrate that a graph partitioning where all
in-edges for a vertex are placed in the same partition improves
memory locality. However, realising performance improvement
through such graph partitioning poses several challenges and
requires rethinking the classification of graph algorithms and
preferred data structures. We introduce the notion of mediumdense frontiers, a type of frontier that is sufficiently dense for a
bitmap representation, yet benefits from an indexed graph layout.
Using three types of frontiers, and three graph layout schemes
optimized to each frontier type, we design an edge traversal
algorithm that autonomously decides which type to use. The
distinction of forward vs. backward graph traversal folds into
this decision and need no longer be specified by the programmer.

We have implemented our techniques in a NUMA-aware graph
analytics framework derived from Ligra and demonstrate a
speedup of up to 4.34x over Ligra and up to 2.93 over Polymer.

",16.613394197324528,15.121428571428574,173
ICPP_17_020.txt,15.95605968804415,14.04979134637777,ICPP,4737,"
We present parallel algorithms for computing
cycle orders and cycle perimeters in relative neighborhood
graphs. This parallel algorithm has wide-ranging applications from microscopic to macroscopic domains, e.g.,
in histopathological image analysis and wireless network
routing. Our algorithm consists of the following steps (subalgorithms): (1) Uniform partitioning of the graph vertices
across processes, (2) Parallel Delaunay triangulation and
(3) Parallel computation of the relative neighborhood
graph and the cycle orders and perimeters. We evaluated
our algorithm on a large dataset with 6.5 Million points
and demonstrate excellent fixed-size scalability. We also
demonstrate excellent isogranular scalability up to 131K
processes. Our largest run was on a dataset with 13 billion
points on 131K processes on ORNL’s Cray XK7 “Titan”
supercomputer.

",18.243605946275583,17.285,124
ICPP_17_021.txt,13.414496140553652,11.810263768115945,ICPP,7332,"
To overcome scalability and performance issues for
process queries over a web-scale RDF data, various studies have
proposed RDF SPARQL query processing algorithm using
parallel processing manners. However, it is hard to resolve the
scalability and performance issues together because the
problem of communication overhead between nodes is closely
related to the data distribution for parallel processing. For
efficient RDF query parallel processing, it is essential to
distribute and process data evenly while reducing
communication overhead. In this paper, we propose RDF query
parallel processing algorithms with RDF data partitioning
technique to guarantee evenly distributed data over the cluster.
We also propose our in-memory RDF query processing system
as a form of Bulk Synchronization Parallel system to reduce
network overhead. Our empirical evaluation results show that
the proposed system outperforms a popular RDF-3X on LUBM
benchmark and UniProt queries from 2.20 to 43.08 times.
Especially, the effectiveness of the system improves significantly
when the SPARQL queries are complex with high input and
select.

",17.93193317476759,17.04531841652324,169
ICPP_17_022.txt,14.415001700794669,13.122112163698372,ICPP,7436,"
To reduce energy consumption and carbon emission, many data centers have deployed (or anticipate to build)
their own renewable-energy power plants. However, the renewable energy (such as wind, tide, and solar energy) has the serious
issues of intermittency and variability that prevent the green
energy from being utilized effectively in practice. To cope with
the issues, new power-supply management policies and workload
scheduling algorithms have been designed. However, most existing
work focuses on power optimization on computation only. In
this paper, we introduce a novel scheme called OptiMatch to
optimize the match between the power supply and the userworkload demand for massive storage systems that are mostly
powered by renewable energy sources. OptiMatch has a hierarchical architecture, which consists of a number of heterogeneous storage devices. OptiMatch systematically utilizes the
performance disparities between heterogeneous storage devices
(i.e., performance per watt, IOPS/watt) to split the process for
every write request into two stages: an on-line stage and a
deferred off-line stage. The deferred off-line requests are used
to match the green energy supplies. To maximize green energy
utilization and minimize power budget without sacrificing quality
of service, the fundamental methodology is to make the aggregate
power supplies be proportional to the I/O workload demand
at any time. To this end, our OptiMatch employs novel codesign optimizations. (1) We propose a dual-drive power control
approach that makes the number of active nodes proportional to
the workload demand when the green power supply is insufficient,
meanwhile be proportional to the green power supply when
green power is sufficient. (2) During periods of insufficient
green supplies, we exploit virtualization consolidation schemes
which enable a fine-grained power control to minimize the grid
budgets. (3) During the periods of sufficient green supplies, we
design an intelligent workload scheduling scheme which enables a
near-optimal off-line requests assignment to maximize the green
utilization. The experimental results demonstrate that the new
OptiMatch framework can achieve high green utilization (up to
94.9%) with a minor performance degradation (less than 9.8%).

",17.122413403193683,16.263537913699206,345
ICPP_17_023.txt,15.086042034136167,12.844704975473018,ICPP,5638,"
With the development of cloud computing, disk
arrays tolerating triple disk failures (3DFTs) are receiving more
attention nowadays because they can provide high data reliability
with low monetary cost. However, a challenging issue in these
arrays is how to efficiently reconstruct the lost data, especially
for partial stripe errors (e.g., sector and chunk errors). It is one
of the most significant scenarios in practice. However, existing
cache strategies are not efficient for partial stripe reconstruction
in 3DFTs, which is because the complex relationships among data
and parities are usually ignored during the recovery process.

To address this problem, in this paper, we proposed a comprehensive cache policy called Favorable Block First (FBF), which
can speed up the partial stripe reconstruction of 3DFTs. FBF
investigates the relationships among parity chains via allocating
various priorities of shared chunks. Thus in the recovery process,
by giving higher priorities to the chunks which are shared by
more parities chains, FBF can dynamically hold the significant
data in buffer cache for partial stripe reconstruction. Obviously,
it increases the cache hit ratio and reduces the reconstruction
time. To demonstrate the effectiveness of FBF, we conduct several
simulations via Disksim. The results show that, compared to
typical recovery schemes by combining with classic cache policies
(e.g., LRU, LFU and ARC), FBF improves hit ratio by up to
2.47x and accelerates the reconstruction process by 14.90%,
respectively.

",16.76809479433877,14.77680786026201,234
ICPP_17_024.txt,13.81035871262139,11.31226126898365,ICPP,6556,"
Modern distributed storage systems often store
redundant data in multiple replications or erasure coding
according to their access frequencies. Multiple replications
scheme is well-performance for hot data while erasure coding
scheme is storage-efficient for warm and cold data. When hot
data turn cold, an encoding procedure starts to do the conversion. However, due to sequential striping, current conversion
methods do not perform well for different data layouts, and
cause risky blocks and expensive network consumption.

In this paper, we propose Sice, a new encoder which deploys
non-sequential striping. It constructs non-sequential stripes
according to the data layout, performs conversion quickly with
low overheads and ends to no reduction of system reliability.
The results of both simulation and evaluation show that
Sice gains almost the same good performance for different
data layouts and has a great scalability. Sice helps HDFSRAID reduce network consumption by about 65% and reduce
influence on concurrent I/O-intensive applications by about
60%.

",15.742502247213078,13.87986801242236,162
ICPP_17_025.txt,14.27023069675817,12.874996553742367,ICPP,7229,"
As a fundamental cloud service for modern Web
applications, the cloud object storage system stores and retrieves millions or even billions of read-heavy data objects.
Serving for a massive amount of requests each day makes the
response latency be a vital component of user experiences. Due
to the lack of suitable understanding on the response latency
distribution, current practice is to use overprovision resources
to meet Service Level Agreement (SLA). Hence we build a
performance model for the cloud object storage system to predict the percentiles of requests meeting SLA (response latency
requirement), in the context of complicated disk operations
and event-driven programming model. Furthermore, we find
that the waiting time for being accept(-ed at storage servers
may introduce significant delay. And we quantify the impacts
on system response latency, due to requests waiting for being
accept()-ed. In a variety of scenarios, our model reduces the
prediction errors by up to 73% compared to baseline models,
and the prediction error of our model is 4.44% on average.

",14.955608360458719,14.776908939014202,173
ICPP_17_026.txt,17.144892636002933,15.959955949888691,ICPP,7411,"
Data staging has been shown to be very
effective for supporting data intensive in-situ workflows
and coupling of applications. Experimental sciences
are increasingly becoming collaborative among geographically distributed teams, and include experimental instruments and HPC facilities. This new way of
doing science poses new challenges due to data sizes,
complexity of computation, and the use of wide area
networks between couplings. In this paper, we explore
how the staging abstraction can be extended to support
such workflows. Specifically, we develop a NUMA-like
abstraction that orchestrates multiple distributed localarea staging abstractions, and provides asynchronous
data put/get semantics to enable data sharing across
them. To mask data movement overhead and provide
in-time data access, we propose the use of predictive
prefetching approaches that leverage the iterative nature of the coupling. We evaluate our prototype implementation using a fusion workflow and show that our
design can effectively and transparently support widearea coupled workflows. Additionally, results show that
the use of prefetching techniques leads to significant
gains in data access times of data that needs to be
moved over the wide area network.
",16.975882523387877,16.415403005464487,184
ICPP_17_027.txt,15.632568617437677,14.226479117998267,ICPP,5263,"

Enterprise storage systems must scale to increasing core
counts to meet stringent performance requirements. Both
the NetApp® Data ONTAP® storage operating system and
its WAFL” file system have been incrementally parallelized over the years, but some components remain singlethreaded. The WAFL write allocator, which is responsible
for assigning blocks on persistent storage to dirty data in a
way that maximizes write throughput to the storage media,
is single-threaded and has become a major scalability
bottleneck. This paper presents a new write allocation
architecture, White Alligator, for the WAFL file system that
scales performance on many cores. We also place the new
architecture in the context of the historical parallelization
of WAFL and discuss the architectural decisions that have
facilitated this parallelism. The resulting system demonstrates increased scalability that results in throughput
gains of up to 274% on a many-core storage system.

",16.11434528070225,15.644655172413795,144
ICPP_17_028.txt,15.33054807823573,12.96848668306544,ICPP,6850,"
String pattern matching with finite automata
(FAs) is a well-established method across many areas in
computer science. Until now, data dependencies inherent in
the pattern matching algorithm have hampered effective parallelization. To overcome the dependency-constraint between
subsequent matching steps, simultaneous deterministic finite
automata (SFAs) have been recently introduced. Although an
SFA facilitates parallel FA matching, SFA construction itself is
limited by the exponential state-growth problem, which makes
sequential SFA construction intractable for all but the smallest
problem sizes.

In this paper, we propose several optimizations to leverage
parallelism, improve cache and memory utilization and greatly
reduce the processing steps required to construct an SFA. We
introduce fingerprints and hashing for efficient comparisons of
SFA states. Kernels of x86 SIMD-instructions facilitate cachelocality and leverage data-parallelism with the construction of
SFA states. Our parallelization for shared-memory multicores
employs lock-free synchronization to minimize cache-coherence
overhead. Our dynamic work-partitioning scheme employs
work-stealing with thread-local work-queues. The structural
properties of FAs allow efficient compression of SFA states.
Our construction algorithm dynamically switches to in-memory
compression of SFA states for problem sizes which approach
the main memory size limit of a given system.

We evaluate our approach with patterns from the PROSITE
protein database. We achieve speedups of up to 312x on a 64core AMD system and 193x on a 44-core (88 hyperthreads) Intel
system. Our SFA construction algorithm shows scalability on
both evaluation platforms.

Keywords-SFA construction; fingerprints; hashing; workstealing; lock-free programming; compression; parallelization;
multicores;

",16.15616582465906,15.056470588235296,256
ICPP_17_029.txt,16.73621778427604,14.94103925325254,ICPP,5419,"
Fast, accurate three dimensional reconstructions
of plasma equilibria, crucial for physics interpretation of
fusion data generated within confinement devices like stellarators/tokamaks, are computationally very expensive and
routinely require days, even weeks, to complete using serial
approaches. Here, we present a parallel implementation of the
three dimensional plasma reconstruction code, V3FIT. A formal
analysis to identify the performance bottlenecks and scalability
limits of this new parallel implementation, which combines
both task and data parallelism, is presented. The theoretical
findings are supported by empirical performance results on
several thousands of processor cores of a Cray XC30 supercomputer. Parallel V3FIT is shown to deliver over 40X speedup,
enabling fusion scientists to carry out three dimensional plasma
equilibrium reconstructions at unprecedented scales in only a
few hours (instead of in days/weeks) for the first time.

",22.076135915942103,22.485770676691732,134
ICPP_17_030.txt,16.42755727183461,15.11729992979993,ICPP,7459,"
This paper presents a low-overhead optimizer for
the ubiquitous sparse matrix-vector multiplication (SpMV) kernel. Architectural diversity among different processors together
with structural diversity among different sparse matrices lead
to bottleneck diversity. This justifies an SpMV optimizer that
is both matrix- and architecture-adaptive through runtime specialization. To this direction, we present an approach that first
identifies the performance bottlenecks of SpMV for a given
sparse matrix on the target platform either through profiling
or by matrix property inspection, and then selects suitable
optimizations to tackle those bottlenecks. Our optimization pool is
based on the widely used Compressed Sparse Row (CSR) sparse
matrix storage format and has low preprocessing overheads,
making our overall approach practical even in cases where
fast decision making and optimization setup is required. We
evaluate our optimizer on three x86-based computing platforms
and demonstrate that it is able to distinguish and appropriately
optimize SpMV for the majority of matrices in a representative
test suite, leading to significant speedups over the CSR and
Inspector-Executor CSR SpMV kernels available in the latest
release of the Intel MKL library.

",20.267338824336647,19.711557377049186,184
ICPP_17_031.txt,14.872794079484962,13.011325250962148,ICPP,7063,"
Mobile edge cloud has been increasingly concerned
by researchers due to its closer distance to mobile users than
the traditional cloud on Internet. Offloading computations from
mobile devices to the nearby edge cloud is an effective technique
to accelerate the applications and/or save energy on the mobile
devices. However, the mobile edge cloud usually has limited
computation resources and constrained access bandwidth shared
by multiple users in its proximity. Thus, allocation of resources
and bandwidth among the users is significant to the overall
application performance. In this paper, we study network aware
multi-user computation partitioning problem in mobile edge clouds,
i.e., to decide for each user which parts of the application should
be offload onto the edge cloud, and which others should be
executed locally, and meanwhile to allocate the access bandwidth
among the users, such that the average application performance
of the users is maximized.

This problem is novel in that we consider the competition
among users for both computing resources and bandwidth, and
jointly optimizes the partitioning decisions with the allocation
of resources and bandwidths among users, while most existing
works either focus on the single user computation partitioning
or study the multiple user computation partitioning without
regard of the constrained network bandwidth. We first formulate
the problem, and then transform it into the classic Multi-class
Multi-dimensional Knapsack Problem and develop an effective
algorithm, namely Performance Function Matrix based Heuristic
(PFM-H), to solve it. Comprehensive simulations show that
our proposed algorithm outperforms the benchmark algorithms
significantly in the average application performance.

",19.53771442962202,19.627034883720928,260
ICPP_17_032.txt,14.524549130348174,12.657210533184024,ICPP,5325,"
In this paper, we study the link scheduling problem
considering the fluctuating fading effect in transmissions. We
extend the previous deterministic physical interference model
to the Rayleigh-fading model that uses the stochastic propagation to address fading effects. Based on this model, we
formulate a problem called Fading-Resistant Link Scheduling
(Fading-R-LS) problem, which aims to maximize the throughput of all links in a single time slot. We prove that this problem
is NP-hard. Based on the geometric structure of Fading-R-LS,
we then propose two centralized schemes with O(g(L)) and
O(1) performance guarantee, respectively, where g(L) is the
number of magnitudes of transmission link lengths. We further
propose a distributed scheme based on game theory, which has
O(A“®) performance guarantee, where A is the ratio between
the maximum and the minimum distances between nodes, and
a: is the path loss exponent. Our experimental results show that
the superior performance of our proposed schemes compared
to a previous scheme.

",15.532846611407376,13.725122569737952,169
ICPP_17_033.txt,13.089014105749225,10.710074269297412,ICPP,6229,"
We introduce a novel graph called a host-switch
graph, which consists of host vertices and switch vertices. Using
host-switch graphs, we formulate a graph problem called an
order/radix problem (ORP) for designing low end-to-end latency
interconnection networks. Our focus is on reducing the host-tohost average shortest path length (h-ASPL), since the shortest
path length between hosts in a host-switch graph corresponds
to the end-to-end latency of a network. We hence define ORP
as follows: given order (the number of hosts) and radix (the
number of ports per switch), find a host-switch graph with the
minimum h-ASPL. We demonstrate that the optimal number
of switches can mathematically be predicted. On the basis of
the prediction, we carry out a randomized algorithm to find a
host-switch graph with the minimum h-ASPL. Interestingly, our
solutions include a host-switch graph such that switches have
the different number of hosts. We then apply host-switch graphs
to interconnection networks and evaluate them practically. As
compared with the three conventional interconnection networks
(the torus, the dragonfly, and the fat-tree), we demonstrate that
our networks provide higher performance while the number of
switches can decrease.

",13.731508374201272,11.933333333333334,205
ICPP_17_034.txt,15.576679339046976,14.16183975091532,ICPP,7182,"
Lambda architecture is a novel event-driven
serverless paradigm that allows companies to build scalable
and reliable enterprise applications. As an attractive alternative
to traditional service oriented architecture (SOA), Lambda
architecture can be used in many use cases including BI
tools, in-memory graph databases, OLAP, and streaming
data processing. In practice, an important aim of Lambda’s
service providers is devising an efficient way to co-locate
multiple Lambda functions with different attributes into a set
of available computing resources. However, previous studies
showed that consolidated workloads can compete fiercely for
shared resources, resulting in severe performance variability/degradation. This paper proposes a resource allocation
mechanism for a Lambda platform based on the model predictive control framework. Performance evaluation is carried
out by comparing the proposed solution with multiple resource
allocation heuristics, namely enhanced versions of spread and
binpack, and best-effort approaches. Results confirm that the
proposed controller increases the overall resource utilization
by 37% on average and achieves a significant improvement in
preventing QoS violation incidents compared to others.

",19.5731925562951,18.641236424394325,173
ICPP_17_035.txt,16.959505483330318,15.406664673904384,ICPP,5882,"
 Clouds offer great flexibility for scaling applications due to the wide spectrum of resources with different
cost-performance, inherent resource elasticity and pay-peruse charging. However, determining cost-time-efficient cloud
configurations to execute a given application in the large
resource configuration space remains a key challenge. The
growing importance of elastic applications for which the
accuracy is a function of resource consumption introduces new
opportunities to exploit resource elasticity on clouds. In this
paper, we introduce CELIA, a measurement-driven analytical modeling approach to determine cost-time-optimal cloud
resource configurations to execute a given elastic application
with a time deadline and a cost budget. We evaluate CELIA
with three representative elastic applications on more than ten
million configurations consisting of Amazon EC2 resource types
with different cost-performance. Using CELIA, we show that
multiple cost-time Pareto-optimal configurations exist among
feasible cloud configurations that execute an elastic application
within a time deadline and cost budget. These Pareto-optimal
configurations exhibit up to 30% cost savings for an elastic
application representing n-body simulation. We investigate the
impact of fixed-time scaling on the cost of executing elastic
applications on cloud. We show that cost gradient with respect
to resource demand is smaller when cloud resources with better
cost-performance are used. Furthermore, we show that the
relative increase in cost is always smaller compared to the
relative reduction of execution time deadline. For example,
tightening the execution time deadline by two-thirds incurs only
40% increase in cost for the n-body simulation application.

",18.046022208747292,16.708588609833743,258
ICPP_17_036.txt,17.080201200220547,16.025956838342385,ICPP,4496,"
Computation offloading is a programming model in
which program fragments (e.g. hot loops) are annotated so that
their execution is performed in dedicated hardware or accelerator devices. Although offloading has been extensively used to
move computation to GPUs, through directive-based annotation
standards like OpenMP, offloading computation to very large
computer clusters can become a complex and cumbersome task.
It typically requires mixing programming models (e.g. OpenMP
and MPI) and languages (e.g. C/C++ and Scala), dealing with
various access control mechanisms from different clouds (e.g.
AWS and Azure), and integrating all this into a single application.
This paper introduces the cloud as a computation offloading
device. It integrates OpenMP directives, cloud based mapreduce Spark nodes and remote communication management
such that the cloud appears to the programmer as yet another
device available in its local computer. Experiments using LLVM,
OpenMP 4.5 and Amazon EC2 show the viability of the proposed
approach and enable a thorough analysis of the performance and
costs involved in cloud offloading. The results show that although
data transfers can impose overheads, cloud offloading can still
achieve promising speedups of up to 86x in 256 cores for the
2MM benchmark using 1GB matrices.

",15.774802946060372,13.754919597989947,203
ICPP_17_037.txt,13.736674896851198,10.83166368228207,ICPP,6792,"
The complete Voronoi map of a binary image with
black and white pixels is a matrix of the same size such that each
element is the closest black pixel of the corresponding pixel. The
complete Voronoi map visualizes the influence region of each
black pixel. However, each region may not be connected due
to exclave pixels. The connected Voronoi map is a modification
of the complete Voronoi map so that all regions are connected.
The Euclidean distance map of a binary image is a matrix, in
which each element is the distance to the closest black pixel.
It has many applications of image processing such as dilation,
erosion, deblurring effects, skeletonizing and matching. The main
contribution of this paper is to present simple and fast parallel
algorithms for computing the complete/connected Voronoi maps
and the Euclidean distance map and implement them in the
GPU. Our parallel algorithm first computes the mixed Voronoi
map, which is a mixture of the complete and connected Voronoi
maps, and then converts it into the complete/connected Voronoi
by exposing/hiding all exclave pixels. After that, the complete
Voronoi map is converted into the Euclidean distance map by
computing the distance to the closest black pixel for every pixel in
an obvious way. The experimental results on GeForce GTX 1080
GPU show that the computing time for these conversions is
relatively small. The throughput of our GPU implementation
for computing the Euclidean distance maps of 2K x 2.K binary
images is up to 2.08 times larger than the previously published
best GPU implementation, and up to 172 times larger than CPU
implementation using Intel Core i7-4790.

",16.00919986374329,13.598941605839418,276
ICPP_17_038.txt,15.160022188383103,13.76249040793579,ICPP,5127,"
Recommender systems are becoming the crystal ball
of the Internet because they can anticipate what the users may
want, even before the users know they want it. However, the
machine-learning algorithms typically involved in the training of
such systems can be computationally expensive, and often may
require several days for retraining. Here, we present a distributed
approach for load-balancing the training of a recommender
system based on state-of-art non-negative matrix factorization
principles. The approach can exploit the presence of a cluster of
mixed CPUs and GPUs, and results in a 466-fold performance
improvement compared with the serial CPU implementation,
and a 15-fold performance improvement compared with the best
previously reported results for the popular Netflix data set.

",18.243605946275583,17.530645161290327,125
ICPP_17_039.txt,16.032535933649818,14.430462824032965,ICPP,5164,"
Network analysis software relies on graph layout
algorithms to enable users to visually explore network data.
Nowadays, networks easily consist of millions of nodes and
edges, resulting in hours of computation time to obtain a
readable graph layout on a typical workstation. Although these
machines usually do not have a very large number of CPU
cores, they can easily be equipped with Graphics Processing
Units (GPUs), opening up the possibility of exploiting hundreds
or even thousands of cores to counter the aforementioned
computational challenges. In this paper we introduce a novel
GPU framework for visualizing large real-world network data.
The main focus is on a GPU implementation of force-directed
graph layout algorithms, which are known to create high quality network visualizations. The proposed framework is used
to parallelize the well-known ForceAtlas2 algorithm, which
is widely used in many popular network analysis packages
and toolkits. The different procedures and data structures of
the algorithm are adjusted to the CUDA GPU architecture’s
specifics in terms of memory coalescing, shared memory usage
and thread workload balance. To evaluate its performance, the
GPU implementation is tested using a diverse set of 38 different
large-scale real-world networks. This allows for a thorough
characterization of the parallelizable components of both forcedirected layout algorithms in general as well as the proposed
GPU framework as a whole. Experiments demonstrate how
the approach can efficiently process very large real-world
networks, showing overall speedup factors between 40x and
123 compared to existing CPU implementations. In practice,
this means that a network with 4 million nodes and 120 million
edges can be visualized in 14 minutes rather than 9 hours.

",16.581925556534415,15.957818181818183,277
ICPP_17_040.txt,15.556553193586684,14.674930924707933,ICPP,5019,"
Efficient execution of distributed database operators such as joining and aggregating is critical for the
performance of big data analytics. With the increase of the
compute speedup of modern CPUs, reducing the network
communication time of these operators in large systems is
becoming increasingly important, and also challenging current
techniques. Significant performance improvements have been
achieved by using state-of-the-art methods, such as reducing
network traffic designed in the data management domain,
and data flow scheduling in the data communications domain.
However, the proposed techniques in both fields just view
each other as a black box, and performance gains from a
co-optimization perspective have not yet been explored.

In this paper, based on current research in coflow scheduling,
we propose a novel Coflow-based Co-optimization Framework
(CCF), which can co-optimize application-level data movement
and network-level data communications for distributed operators, and consequently contribute to their performance in
large distributed environments. We present the detailed design
and implementation of CCF, and conduct an experimental
evaluation of CCF using large-scale simulations on large data
joins. Our results demonstrate that CCF can always perform
faster than current approaches on network communications in
large-scale distributed scenarios.

",18.08858127442927,18.21056147832267,202
ICPP_17_041.txt,12.43973731371274,10.491508774160735,ICPP,7716,"
Parity declustering is widely deployed in erasure
coded storage systems so as to provide fast recovery and high
data availability. However, to perform scaling on such RAIDs,
it is necessary to preserve the parity declustered data layout
so as to guarantee the RAID performance after scaling. Unfortunately, existing scaling algorithms fail to achieve this goal so
they can not be applied for scaling RAIDs which have deployed
parity declustering. To address this challenge, we develop
an efficient scaling algorithm called PDS (Parity Declustering
Scaling). In particular, we first employ an auxiliary Balanced
Incomplete Block Design (BIBD) to define the data migrations
during scaling so as to preserve parity declustered data layout,
and then define the addressing algorithm in the scaled system
based on the migrations. We provide theoretical proofs to show
that PDS preserves the parity declustered data layout, which
is the basis for scaling RAIDs with parity declustering, and
also theoretically prove that PDS achieves the even distribution
of data/parity blocks after scaling and requires only the
minimal data migrations. To show the performance of PDS, we
implement it in MD in Linux Kernel, and conduct experiments
with real-world traces. Results show PDS can reduce 89.70% of
data migration time and 24.44% of user response time during
scaling on average, compared with the round-robin scheme.

",16.827784334635936,16.638761415525114,222
ICPP_17_042.txt,13.475913121715347,11.879786192793528,ICPP,7171,"
In this paper we consider the data caching problem
in next generation data services in the cloud, which is characterized by using monetary cost and access trajectory information
to control cache replacements, instead of exploiting capacityoriented strategies as in traditional research. In particular, given
a stream of requests to a shared data item with respect to
a homogeneous cost model, we first propose a fast off-line
algorithm using dynamic programming techniques. The proposed
algorithm can generate optimal schedule within O(mn) timespace complexity to cache, migrate as well as replicate the
shared data item to serve an n-length request sequence with
minimum cost in a fully connected m-node network, substantially
improving the previous results. Additionally, we also study this
problem in its online form, and present a 3-competitive online
algorithm by leveraging a speculative caching idea. The algorithm
can serve an online request in constant time, and is space efficient
in O(m) as well, rendering it to be more practical in reality.
Our research complements the shortage of similar research in
literature on this problem.

",18.243605946275583,18.005555555555556,181
ICPP_17_043.txt,15.030165495196158,13.321813701758064,ICPP,5923,"
The matrix-matrix multiplication is an essential
building block that can be found in various scientific and
engineering applications. High-performance implementations
of the matrix-matrix multiplication on state-of-the-art processors may be of great importance for both the vendors
and the users. In this paper, we present a detailed methodology of implementing and optimizing the double-precision
general format matrix-matrix multiplication (DGEMM) kernel
on the emerging SW26010 processor, which is used to build
the Sunway TaihuLight supercomputer. We propose a threelevel blocking algorithm to orchestrate data on the memory
hierarchy and expose parallelism on different hardware levels,
and design a collective data sharing scheme by using the
register communication mechanism to exchange data efficiently
among different cores. On top of those, further optimizations
are done based on a data-thread mapping method for efficient
data distribution, a double buffering scheme for asynchronous
DMA data transfer, and an instruction scheduling method
for maximizing the pipeline usage. Experiment results show
that the proposed DGEMM implementation can fully exploit
the unique hardware features provided by SW26010 and can
sustain up to 95% of the peak performance.

",18.946978176291534,19.281577540106955,188
ICPP_17_044.txt,15.381575749822971,13.46330189561527,ICPP,6083,"
 The home-grown SW26010 many-core processor
enabled the production of China’s first independently developed number-one ranked supercomputer -— the Sunway
TaihuLight. The design of the limited off-chip memory bandwidth, however, renders the SW26010 a highly memory-bound
processor. To compensate for this limitation, the processor
was designed with a unique hardware feature, “Register Level
Communication” (RLC), to share register data among its
8 x 8 computing processing elements (CPEs) via a 2D onchip network. Such a radical architecture has sparked global
researchers’ concerns regarding the programming challenges
this may cause. To address these concerns, we adopted two
compute-bound scientific kernels as benchmarks to identify the
potential programming challenges. The first kernel is doubleprecision general matrix-multiplication (DGEMM). An RLCfriendly algorithm was designed for this kernel to reuse the data
that already reside in the registers of 64 CPEs. This novel optimization enables the kernel to achieve up to 88.7% efficiency
in one core group of the SW26010. This paper reveals, for
the first time, the details of how the highly efficient DGEMM
is implemented on the home-grown processor. The second
kernel that we used is N-body. Due to the inefficient hardware
support for transcendental operations on the SW26010, we
replaced the reciprocal square root (rsqrt) instruction of N-body
with a software routine to tackle the problem. Based on the
programming challenges identified through these two optimized
kernels, we proposed a three-level programming guideline for
the SW26010. The paper concludes with our crucial finding
that the critical step towards bridging the ninja performance
gap on the SW26010 is to design an RLC-friendly algorithm
to increase arithmetic intensity.

",15.402047736947555,13.646058394160587,277
ICPP_17_045.txt,15.577731575324233,13.515350549641642,ICPP,6417,"
Customizing the precision of data can provide
attractive trade-offs between accuracy and hardware resources.
Custom hardware and FPGA designs allow bit-level control
over precision, but software is typically limited by the range of
types supported by the underlying processor. We propose a new
form of vector computing aimed at arrays of custom-precision
data on general-purpose processors with SIMD extensions.
We represent these vectors in bitslice format and use bitwise
instructions to build arithmetic operators that operate on
the customized bit precision. We construct a domain-specific
code generator that builds bit-level customizable floatingpoint and integer operators for our vector types. Using a
hardware circuit optimization tool we optimize our logical
expressions, and synthesize fast software arithmetic operators
for bitslice vector types. We evaluate the resulting code and
find that advanced logic optimization significantly improves
performance. Experiments on a platform with Intel AVX2
SIMD extensions show that this approach is efficient for vectors
of low-precision custom floating-point types, while providing
arbitrary bit precision.

",17.553077303434723,15.759992603550298,170
ICPP_17_046.txt,13.029362365210329,10.922727272727276,ICPP,5418,"
Machine Learning (ML) approaches are widelyused classification/regression methods for data mining applications. However, the time-consuming training process greatly
limits the efficiency of ML approaches. We use the example
of SVM (traditional ML algorithm) and DNN (state-of-the-art
ML algorithm) to illustrate the idea in this paper. For SVM,
a major performance bottleneck of current tools is that they
use a unified data storage format because the data formats
can have a significant influence on the complexity of storage
and computation, memory bandwidth, and the efficiency of
parallel processing. To address the problem above, we study the
factors influencing the algorithm’s performance and conduct
auto-tuning to speed up SVM training. DNN training is even
slower than SVM. For example, using a 8-core CPUs to train
AlexNet model by CIFAR-10 dataset costs 8.2 hours. CIFAR-10
is only 170 MB, which is not efficient for distributed processing.
Moreover, due to the algorithm limitation, only a small batch of
data can be processed at each iteration. We focus on finding the
right algorithmic parameters and using auto-tuning techniques
to make the algorithm run faster. For SVM training, our
implementation achieves 1.7—16.3x speedup (6.8 x on average)
against the non-adaptive case (using the worst data format) for
various datasets. For DNN training on CIFAR-10 dataset, we
reduce the time from 8.2 hours to only roughly 1 minute. We
use the benchmark of dollars per speedup to help the users to
select the right deep learning hardware.

",14.106358169646779,12.61501976284585,260
ICPP_17_047.txt,18.73362127757611,17.42302102563467,ICPP,7360,"
Parallel computing architectures like GPUs have
traditionally been used to accelerate applications with dense and
highly-structured workloads; however, many important applications in science and engineering are irregular and dynamic in
nature, making their effective parallel implementation a daunting
task. Numerical simulation of charged particle beam dynamics is
one such application where the distribution of work and data in
the accurate computation of collective effects at each time step is
irregular and exhibits control-flow and memory access patterns
that are not readily amenable to GPU’s architecture. Algorithms
with these properties tend to present both significant branch and
memory divergence on GPUs which leads to severe performance
bottlenecks.

We present a novel cache-aware algorithm that uses machine
learning to address this problem. The algorithm presented here
uses supervised learning to adaptively model and track irregular
access patterns in the computation of collective effects at each
time step of the simulation to anticipate the future control-flow
and data access patterns. Access pattern forecast are then used to
formulate runtime decisions that minimize branch and memory
divergence on GPUs, thereby improving the performance of
collective effects computation at a future time step based on
the observations from earlier time steps. Experimental results on
NVIDIA Tesla K40 GPU shows that our approach is effective
in maximizing data reuse, ensuring workload balance among
parallel threads, and in minimizing both branch and memory
divergence. Further, the parallel implementation delivers up to
485 Gflops of double precision performance, which translates to
a speedup of up to 2.5X compared to the fastest known GPU
implementation.

",20.385943968408593,19.77730769230769,263
ICPP_17_048.txt,15.307591657268308,14.123787872763419,ICPP,7978,"
Pattern matching is a key building block of
Intrusion Detection Systems and firewalls, which are deployed
nowadays on commodity systems from laptops to massive web
servers in the cloud. In fact, pattern matching is one of their
most computationally intensive parts and a bottleneck to their
performance. In Network Intrusion Detection, for example,
pattern matching algorithms handle thousands of patterns and
contribute to more than 70% of the total running time of the
system.

In this paper, we introduce efficient algorithmic designs
for multiple pattern matching which (a) ensure cache locality
and (b) utilize modern SIMD instructions. We first identify
properties of pattern matching that make it fit for vectorization
and show how to use them in the algorithmic design. Second,
we build on an earlier, cache-aware algorithmic design and we
show how cache-locality combined with SIMD gather instructions, introduced in 2013 to Intel’s family of processors, can
be applied to pattern matching. We evaluate our algorithmic
design with open data sets of real-world network traffic: Our
results on two different platforms, Haswell and Xeon-Phi, show
a speedup of 1.8x and 3.6x, respectively, over Direct Filter
Classification (DFC), a recently proposed algorithm by Choi
et al. for pattern matching exploiting cache locality, and a
speedup of more than 2.3x over Aho-Corasick, a widely used
algorithm in today’s Intrusion Detection Systems.

",17.613555460941566,17.975936507936506,231
ICPP_17_049.txt,15.198644201494684,12.792343332666075,ICPP,5967,"
The exponential growth of available data has
increased the need for interactive exploratory analysis. Dataset
can no longer be understood through manual crawling and
simple statistics. In Geographical Information Systems (GIS),
the dataset is often composed of events localized in space and
time; and visualizing such a dataset involves building a map
of where the events occurred.

We focus in this paper on events that are localized among
three dimensions (latitude, longitude, and time), and on computing the first step of the visualization pipeline, space-time
kernel density estimation (STKDE), which is most computationally expensive. Starting from a gold standard implementation, we show how algorithm design and engineering, parallel
decomposition, and scheduling can be applied to bring near
real-time computing to space-time kernel density estimation.
We validate our techniques on real world datasets extracted
from infectious disease, social media, and ornithology.

",17.879347455551382,16.479825174825177,144
ICPP_17_050.txt,13.586105778512884,10.833715729641138,ICPP,6473,"
Many real-world networks, including online social
networks and communication networks, are commonly modeled
as temporal graphs. Answering earliest-arrival queries in temporal graphs is one of the most fundamental studies with numerous
applications, such as information diffusion and measuring temporal closeness centrality. As graph sizes are growing rapidly,
speedup of query execution time becomes even more important.

In this paper, we propose a novel edge-centric parallel algorithm for solving single-source earliest-arrival problem in
temporal graphs based on a new data structure named EdgeScan-Dependency Graph (ESD-Graph). We evaluate the proposed parallel algorithm by theoretical analysis as well as
by empirical experiments on real-world temporal graphs and
synthetic graphs. Empirical results show that the new parallel
algorithm outperforms the existing serial algorithm by up to 8.2
and 9.5 times on multi-core processors for real-world data and
synthetic data respectively.

",18.7741,17.01506849315069,149
ICPP_17_051.txt,15.960344054690541,14.263423822248726,ICPP,5714,"
In parallel computing, a valid graph coloring
yields a lock-free processing of the colored tasks, data points,
etc., without expensive synchronization mechanisms. However,
coloring is not free and the overhead can be significant. In
particular, for the bipartite-graph partial coloring (BGPC)
and distance-2 graph coloring (D2GC) problems, which have
various use-cases within the scientific computing and numerical
optimization domains, the coloring overhead can be in the
order of minutes with a single thread for many real-life graphs.

In this work, we propose parallel algorithms for bipartitegraph partial coloring on shared-memory architectures. Compared to the existing shared-memory BGPC algorithms, the
proposed ones employ greedier and more optimistic techniques
that yield a better parallel coloring performance. In particular,
on 16 cores, the proposed algorithms are more than 4x faster
than their counterparts in the ColPack library which is, to
the best of our knowledge, the only publicly-available coloring
library for multicore architectures. In addition to BGPC, the
proposed techniques are employed to devise parallel distance-2
graph coloring algorithms and similar performance improvements have been observed. Finally, we propose two costless
balancing heuristics for BGPC that can reduce the skewness
and imbalance on the cardinality of color sets (almost) for free.
The heuristics can also be used for the D2GC problem and in
general, they will probably yield a better color-based parallelization performance especially on many-core architectures.

",18.599290044081553,16.68695035460993,236
ICPP_17_052.txt,15.489471894985723,14.155790060418788,ICPP,4620,"
We present a scalable distributed memory
library for generating and computations involving structured dense matrices, such as those produced by boundary
integral equation formulations. Such matrices are dense,
but have special structure that can be exploited to obtain
efficient storage and matrix-vector product evaluations and
consequently the fast solution of linear systems. At the core
of the methods we use is the observation that off-diagonal
matrix blocks of such matrices have a low numerical rank,
and that this property can be exploited in a multi-level
fashion. In this work we focus on the Hierarchically SemiSeparable (HSS) representation. We present algorithms for
building and using HSS representations that are parallelized using MPI and CUDA to leverage state-of-the-art
heterogeneous clusters. The efficiency of our methods and
implementation is demonstrated on large dense matrices
obtained from a boundary integral equation formulation of
the Laplace equation with Dirichlet boundary conditions.
We demonstrate excellent (linear) scalability on up to 128
GPUs on 128 nodes. Our codes will lay the foundation for
fast direct solvers for elliptic problems.

",17.693802365651003,15.825702247191014,179
ICPP_17_053.txt,15.607016016907629,13.815382876057328,ICPP,6014,"
Optimizing the performance of GPU kernels is
challenging for both human programmers and code generators.
For example, CUDA programmers must set thread and block
parameters for a kernel, but might not have the intuition to
make a good choice. Similarly, compilers can generate working
code, but may miss tuning opportunities by not targeting GPU
models or performing code transformations. Although empirical
autotuning addresses some of these challenges, it requires extensive experimentation and search for optimal code variants. This
research presents an approach for tuning CUDA kernels based
on static analysis that considers fine-grained code structure and
the specific GPU architecture features. Notably, our approach
does not require any program runs in order to discover nearoptimal parameter settings. We demonstrate the applicability
of our approach in enabling code autotuners such as Orio to
produce competitive code variants comparable with empiricalbased methods, without the high cost of experiments.

",17.28802050969988,15.798639455782315,148
ICPP_17_054.txt,15.404528540342557,13.768208665043815,ICPP,7483,"
Distributed algorithms for data analytics partition
their input data across many machines for parallel execution. At
scale, it is likely that some machines will perform worse than
others because they are slower, power constrained or dependent
on undesirable, dirty energy sources. It is challenging to balance
analytics workloads across heterogeneous machines, because the
algorithms are sensitive to statistical skew in data partitions. A
skewed partition can slow down the whole workload or degrade
the quality of results. Sizing partitions in proportion to each
machine’s performance may introduce or further exacerbate
skew. In this paper, we propose a scheme that controls the
statistical distribution of each partition and sizes partitions
according to the heterogeneity of the computing environment.
We model heterogeneity as a multi-objective optimization, with
the objectives being functions for execution time and dirty energy
consumption. We use stratification to control skew. Experiments
show that our computational heterogeneity aware (Het-Aware)
partitioning strategy speeds up running time by up to 51%
over the stratified partitioning scheme baseline. We also have a
heterogeneity and energy aware (Het-Energy-Aware) partitioning
scheme which is slower than the Het-Aware solution, but can
lower the dirty energy footprint by up to 26%. For some analytic
tasks there is also a significant qualitative benefit when using such
partitioning strategies.

",16.24694786949722,15.668181818181822,218
ICPP_17_055.txt,14.429377891979794,12.22974863372048,ICPP,8268,"
Energy consumption has become a major concern
in the recent years and Green computing has arisen as one of the
challenges in order to reduce CO2 emissions in the computing
domain. Many efforts have been made to make hardware less
energy consuming, reduce cooling energy of data and computing
centers by relocating those facilities to cool regions and other. A
novel approach to make the computing domain greener is to add
renewable energy sources for the power supply. The challenge
of this work is to consider computing facilities which are solely
run by renewable energy sources such as solar panels and wind
turbines. In this work we tackle the problem of scheduling
independent tasks within a predicted power envelope that varies
during the time. First we evaluate different instances of the
problem from a theoretical point of view. Then we propose several
heuristics for the case of multi-core architectures and we assess
their performance on synthetic workloads and power envelopes.

",16.26309291913925,14.268198757763972,162
ICPP_17_056.txt,18.345227261341282,18.015225895316807,ICPP,6027,"
A novel efficient inplace, multithreaded, and cachefriendly parallel 2-D wavelet transform algorithm based on
the lifting transform is introduced. In order to maximize the
cache utilization and consequently minimize the memory bus
bandwidth use, the threads compete to work on a small memory
area maximizing the chance of finding it in the cache and
their synchronization is done with very low overhead without
the use of any locks and relying solely on the basic compareand-swap (CAS) atomic primitive. An implementation in the
C programming language with and without the use of vector
(single instruction multiple data - SIMD) instructions is provided
for both single (serial) and multi (parallel) threaded singleloop DWT implementations as well as serial and parallel naive
implementations using linear (row order) and strided (column
order) memory access patterns for comparison. Results show
a significant improvement over the single-threaded optimized
implementation and a much greater improvement over both
the single and multi threaded naive implementations, reaching
minimum running time depending on the number of processor
cores and the available memory bus bandwidth, i.e., it becomes
memory bound using the minimum number of memory accesses.
Given the simplicity and high speed of the lifting steps, an analysis
based on the number of memory bus operations (read and write)
is done for images that are larger than twice the shared cache
size which establishes a lower bound for the running time of
all linear memory access algorithms and also determines the
maximum speed gains to be expected in relation to currently
implemented parallel schemes based on the parallel execution
of independent lifting steps. It also shows the optimality of
the parallel algorithm presented. Finally, a comparison with
currently available implementations shows the gains achieved by
the proposed algorithm.

",22.80048607216075,23.06852216748769,292
ICPP_17_057.txt,15.226385567649594,13.48592394947724,ICPP,4528,"
We propose a heuristic for parallel partitioning of
graphs into equi-sized components. In particular, we identify a
relationship between the graph partitioning problem (GPP) and
the traveling saleman problem (TSP), and use that to reduce
partitioning to TSP. Given that better performing heuristics are
known for TSP than are for GPP, this reduction also leads to
improved GPP heuristics. What is more, a good GPP solution
can also be used to speed up computation of TSP.

We first derive a good bi-partition from a cut of the TSP cycle
in time proportional to the number of edges in the graph. We
then continue this bi-partitioning recursively until the required
number of partitions are left. Further, in order to speed up the
computation of TSP, which we use as a subroutine, we perform an
initial rough partitioning of the graph into K parts, compute TSP
tours in each of these smaller partitions and then merge these
local tours to solve the full TSP. We then use this full TSP solution
to obtain the final partitioning in parallel. Our empirical analysis
shows that for partition count k > 32, our parallel algorithm
gives a cut better in size than that of algorithms known for low
cut-size (e.g., KaBaPE), and when time is of concern, it finishes
in significantly less time with comparable cuts. We also show that
our algorithm gives much smaller cuts in comparable time than
those known for fast computation (e.g., PT-Scotch).

",15.247664890283005,12.817898785425104,249
ICPP_17_058.txt,16.908762484321528,15.380282304568023,ICPP,7232,"
Apache Storm is a fault-tolerant, distributed inmemory computation system for processing large volumes of
high-velocity data in real-time. As an integral part of the faulttolerance mechanism, Storm’s state management is achieved by
a checkpointing framework, which commits states regularly and
recovers lost states from the latest checkpoint. However, this
method involves a remote data store for state preservation and
access, resulting in significant overheads to the performance of
error-free execution.

In this paper, we propose E-Storm, a replication-based state
management system that actively maintains multiple state backups on different worker nodes. We build a prototype on top of
Storm by extending it with monitoring and recovery modules to
support inter-task state transfer whenever needed. The experiments carried out on synthetic and real-world streaming applications confirm that E-Storm outperforms the existing checkpointing method in terms of the resulting application performance,
obtaining as much as 9.44 times throughput improvement while
reducing the application latency down to 9.8%.

",18.7741,16.727317073170735,168
ICPP_17_059.txt,14.955016428309872,12.774490203337908,ICPP,5998,"
Projections and measurements of error rates in
near-exascale and exascale systems suggest a dramatic growth,
due to extreme scale (10° cores), concurrency, software complexity, and deep submicron transistor scaling. Such a growth makes
resilience a critical concern, and may increase the incidence
of errors that “escape”, silently corrupting application state.
Such errors can often be revealed by application software tests
but with long latencies, and thus are known as Jatent errors.
We explore how to efficiently recover from latent errors, with
an approach called application-based focused recovery (ABFR).
Specifically we present a case study of stencil computations,
a widely useful computational structure, showing how ABFR
focuses recovery effort where needed, using intelligent testing
and pruning to reduce recovery effort, and enables recovery
effort to be overlapped with application computation. We analyze
and characterize the ABFR approach on stencils, creating a
performance model parameterized by error rate and detection
interval (latency). We compare projections from the model
to experimental results with the Chombo stencil application,
validating the model and showing that ABFR on stencil can
achieve a significant reductions in error recovery cost (up to
400x) and recovery latency (up to 4x). Such reductions enable
efficient execution at scale with high latent error rates.

",17.971250198000288,16.906868932038837,206
ICPP_17_060.txt,17.089526604725066,16.081163928332213,ICPP,6072,"
Power is a critical factor that limits the performance
and scalability of modern high performance computer systems.
Considering power as a first-order constraint and a scarce system
resource, power-bounded computing represents a new perspective
to address the power challenge in HPC.

In this work we present an application-aware, miultidimensional power allocation framework to support powerbounded parallel computing on NUMA-enabled multicore systems. This framework utilizes multiple complementary software
and hardware power management mechanisms to manage power
distribution among sockets, cores, and NUMA memory nodes
under a total power budget. More importantly, this framework
implements a hierarchical power coordination method that
leverages applications’ performance and power scalability to
efficiently identify an ideal power distribution.

We describe the design of the framework and evaluate its
performance on a NUMA-enabled multicore system with 24
cores. Experimental results show that the proposed framework
performs close to the oracle solution for parallel programs with
various power budgets.

",17.451712890111917,17.76040293040293,157
IGSC_17_001.txt,15.364716193906144,12.991430435201494,IGSC,6704,"
Unintended smartphone rebooting and unexpected
shutdown (due to reasons like battery run outs, overheating, or
automatic app upgrades) is annoying. What can be even worse
is that a phone user has to restart, from the very beginning, the
apps he or she was using when the phone got rebooted, because
all the app states would be lost, especially when the number of
apps in use is large. Hence, a recovery service is sorely needed
for today’s smartphones where apps are becoming increasingly
complex. While checkpointing has long been used for desktop and
laptop computers, such whole-system state preserving techniques
cannot be applied to smartphones directly, due to the constraints
of smartphones on energy, delay, and storage space.

In this paper, we propose SmartCP, an intelligent checkpointing methodology, in order to reduce the energy required
by a smartphone and the amount of efforts required by a
user to recover the app states after the smartphone restarts.
SmartCP selectively checkpoints the most important apps based
on the apps’ characteristics and predicted future usage, under
the resource constraints of the phone. We propose a novel
model that quantitatively analyzes the recovery energy and
efforts of each category of smartphone apps and formulate
selective checkpointing as a constrained optimization problem.
We prototype SmartCP on Android and evaluate it using realworld traces as well as real user feedback. The results show
that SmartCP outperforms two alternative app selection schemes
by saving 28% more energy and 39% more recovery efforts on
average.

",16.99224021665606,15.464750337381918,249
IGSC_17_002.txt,15.268299274186287,14.345315416675515,IGSC,4183,"
Aggressive network densification in next generation
cellular networks is accompanied by an increase of the system energy consumption and calls for more advanced power
management techniques in base stations. In this paper, we
present a novel proactive and decentralized power management
method for small cell base stations in a cache-enabled multitier heterogeneous cellular network. User contexts are utilized
to drive the decision of dynamically switching a small cell base
station between the active mode and the sleep mode to minimize
the total energy consumption. The online control problem is
formulated as a contextual multi-armed bandit problem. A
variational inference based Bayesian neural network is proposed
as the solution method, which implicitly finds a proper balance
between exploration and exploitation. Experimental results show
that the proposed solution can achieve up to 46.9% total energy
reduction compared to baseline algorithms in the high density
deployment scenario and has comparable performance to an
offline optimal solution.

",18.062587368997235,17.17145161290323,157
IGSC_17_003.txt,15.570936240876026,13.88157606356371,IGSC,6273,"
Many smartphone apps can consume an unnecessarily high amount of energy, shortening battery life. Although
users can easily notice the undesired fast battery drain, it is
almost impossible for them to precisely remember how the
abnormal battery drain (ABD) is triggered, making it difficult for
developers to fix the problem. Therefore, app developers are in an
urgent need for a tool that can provide them helpful information.

In this paper, we propose eDelta, a framework that assists
developers in pinpointing the APIs with high energy deviation,
which usually have a high probability of being relevant to the nondeterministic ABD. Specifically, eDelta performs comparative
trace analysis to identify APIs that have significant energy consumption deviation in different user traces. With the information
provided by eDelta, developers can substantially reduce the
time they spend searching for the ABD root causes. We have
prototyped eDelta in Android 4.4 and evaluated it with twenty
real-world apps. Our results show that eDelta can effectively
pinpoint the APIs with high energy deviation and those APIs
indeed cause ABD. Specifically, it reduces, on average, 94.6% of
the amount of code that the developers would need to search for
ABD root causes.

",17.5058628484301,15.733846153846155,198
IGSC_17_004.txt,15.346271600324059,13.159360740064713,IGSC,2824,"
As technology nodes continue to scale, main memories experience both increasing energy consumption as well as
reliability challenges. In order to address rising failure rates due
to problems with yield and runtime effects, such as crosstalk, due
to process variation in small feature sizes, improved correction
capabilities at the bit-level are increasingly essential. To address
this challenge, we propose a sustainable approach to error
correction in deeply scaled memories. In particular, we propose
a novel area-efficient and sustainable fault map (SFaultMap)
which targets holistic energy considerations to improve reliability
while minimizing both operational and embodied energy. To
demonstrate the effectiveness of SFaultMap we conduct a sustainability study, based on holistic energy consumption, to evaluate
under which scenarios different solutions should be employed.
In all cases and scenarios with moderate to high fault rates,
SFaultMap has reduced energy over Error Correcting Pointers
(ECP) for a five year lifetime. Moreover, as fault rate increases,
the indifference time for ECP to recover upfront manufacturing
energy increases from years to decades.

",18.548980349730343,17.42571428571429,170
IGSC_17_005.txt,17.35252225812706,16.10631716402462,IGSC,4666,"
With the increase in number of processing chips in
platform based computation intensive systems such as servers, a
seamless, scalable, energy efficient and high bandwidth
interconnection network is required. Newly envisioned silicon
interposers with Network-on-Chip (NoC) interconnection
framework have emerged as an energy efficient technology for
2.5D integration of multiple processor and memory chips, where
multiple chips are mounted on another die called the interposer
and are interconnected using the metal layers of the interposer die.
However, conventional interposer based multichip integration is
limited to edge-to-edge connections between the adjacent dies
leaving the interposer’s routing resources underutilized. In this
paper, we propose large scale utilization of the available abundant
interposer resources for multichip integration by implementing a
hypercube interconnection architecture in an interposer for chipto-chip communication. Through system level simulations, we
demonstrate that such multichip system integrated with
interposer can provide high bandwidth and energy-efficient
communication under various traffic patterns.

",22.41755141528993,21.993384615384617,159
IGSC_17_006.txt,16.29233646866138,14.972504790156957,IGSC,4006,"
 — As big data emerges, the complexity of database
workloads and database systems has increased significantly. It
is no longer possible for one type of database to efficiently
handle all big data applications. NoSQL databases are widely
used to complement conventional SQL databases. In addition to
traditional metrics such as response time and throughput, large
scale NoSQL database systems pose higher requirements on
energy efficiency due to the incredible volume of data (and the
associated cost) that need to be stored and processed.
Unfortunately, research on optimizations for energy efficiency
in database systems has been historically overlooked. In this
paper, we investigate numerous optimizations for two NoSQL
databases (MongoDB and Cassandra) and conduct a
comprehensive study on the impact of these optimizations on
performance and energy efficiency. Our experimental results
derived from 100GB of Twitter data reveal that 1) energy
efficiency can be improved significantly for both MongoDB and
Cassandra via query optimizations without degrading
performance; and 2) energy efficiency does not always scale
linearly with performance improvement.

",18.699421769314853,17.95021385799829,168
IGSC_17_007.txt,17.17353429123476,15.500445877634089,IGSC,6585,"
 Composite Cores Architecture (CCA), a class of
dynamic heterogeneous architectures, enables the system to
construct the right core at run-time for each application by
composing cores together to build larger core or decomposing a
large core into multiple smaller cores. While this architecture
provides more flexibility for the running application to find the
best run-time settings to maximize energy-efficiency, due to
interdependence of various tuning parameters such as the type of
the core, run-time voltage and frequency and the number of
threads, it makes it more challenging for scheduling. Past
research mainly addressed the scheduling problem in composite
cores architecture by looking at one or two of these tuning
parameters. However, as we will show in this paper, it is
important to concurrently optimize and fine-tune these
parameters to harness the power of heterogeneity in this
emerging class of architecture. In addition, most previous work
on CCA mainly studied traditional single threaded CPU
applications. In this work, we investigate the scheduling
challenges for multithreaded applications in CCA. First, through
methodical investigation of power and performance results, we
characterize various multithreaded applications on a CCA which
can be composed into few big or many little cores and
demonstrate how the interplay among various application,
system, and architecture level parameters affect the performance
and energy-efficiency. Furthermore, based on characterization
results, a highly accurate regression-based model for energyefficiency prediction is developed to guide the scheduling
decision. Using the predictive model, we developed a scheduling
scheme for effective mapping of multithreaded applications onto
CCA. The results show that the proposed scheduling scheme on
average achieves close to 94% efficiency as compared to the
Oracle scheduling.

",19.38786093064905,17.833294964028777,279
IGSC_17_008.txt,15.708894963382935,14.339500318022235,IGSC,4462,"
Heterogeneous compute nodes have become an integral component of today’s HPC systems. Recent research has
established the importance of data layout and placement on such
systems. This paper explores the power and energy aspects of
data layout and placement on heterogeneous systems. We present
results of an experimental study that evaluates the impact of data
layout and placement on candidate HPC node architectures for
kernels that exhibit a wide variety of performance characteristics.
The results of the study show that data layout and placement can have a significant impact on the energy efficiency
of heterogeneous applications. On some platforms, selecting
the appropriate layout can yield up to an order-of-magnitude
improvement in energy efficiency. The study shows that the
conventional approach of using a structure-of-arrays for devicemapped data structures is not always profitable and that in
addition to memory divergence, data layout choices are impacted
by a variety of factors including arithmetic intensity and task
granularity. The results of the study are used to establish a set
of energy imperatives to guide data layout and placement across
different architectures.
",16.373557378465907,15.974807692307696,184
IGSC_17_009.txt,14.967197298428442,12.394925230044333,IGSC,5790,"
In this paper, we consider energy-efficient and faulttolerant scheduling of real-time tasks on heterogeneous multicore
systems. Each task consists of a main copy and a backup
copy which are scheduled on different cores, for fault tolerance
purposes. Our framework deliberately delays the backup tasks
in order to cancel them dynamically when the main task
copies complete successfully (without faults). We identify and
address two dimensions of the problem, i.e., partitioning tasks
and determining processor voltage/frequency levels to minimize
energy consumption. Our experimental results show that our
proposed algorithms’ performance levels are close to that of
an ideal solution with optimal (but computationally prohibitive)
partitioning and frequency assignment components.

",17.80541091248751,16.774306306306304,113
IGSC_17_010.txt,16.061152602589004,14.391112588789579,IGSC,4459,"
Advanced power measurement capabilities are becoming available on large scale High Performance Computing
(HPC) deployments. There exist several approaches to providing
power measurements today, primarily through in-band (e.g.
RAPL) and out-of-band measurements (e.g. power meters). Both
types of measurement can be augmented with application-level
profiling, however it can be difficult to assess the type and detail
of measurement needed to obtain insight from the application
power profile. This paper presents a taxonomy for classifying
power profiling techniques on modern HPC platforms. Three
HPC mini-applications are analyzed across three production HPC
systems to examine the level of detail, scope, and complexity
of these power profiles. We demonstrate that a combination
of out-of-band measurement with in-band application region
profiling can provide an accurate, detailed view of power usage
without introducing overhead. This work also provides a set of
recommendations for how to best profile HPC workloads.

",15.579741850924794,13.634473684210526,155
IGSC_17_011.txt,14.899811845980729,12.503164893617022,IGSC,4297,"
There has been a 10,000-fold increase in performance of supercomputers since 1992 but only 300-fold improvement in performance per watt. Dynamic adaptation of hardware
techniques such as fine-grain clock gating, power gating and
dynamic voltage/frequency scaling, are used for many years
to improve the computer’s energy efficiency. However, recent
demands of exascale computation, as well as the increasing
carbon footprint, require new breakthrough to make ICT systems
more energy efficient. Energy efficient software has not been
well studied in the last decade. In this paper, we take an
early step to investigate the energy efficiency of Java which is
one of the most common languages used in ICT systems. We
evaluate energy consumption of data types, operators, control
statements, exception, and object in Java at a granular level. Intel
Running Average Power Limit (RAPL) technology is applied to
measure the relative power consumption of small code snippets.
Several observations are found, and these results will help in
standardizing the energy consumption traits of Java which can
be leveraged by software developers to generate energy efficient
code in future.

",16.52667757954773,14.483598901098905,185
IGSC_17_012.txt,15.512922181460532,13.335368463219798,IGSC,4305,"
Smartphone users spend more than 80% of their
phone time accessing web information, which could cause undesirably large energy drain. To provide web information, a web
activity may invoke asynchronous execution in different hardware
devices. Thus, traditional energy estimation methods based on
system statistics are usually insufficient to capture the secluded
energy cost. In this paper, we propose REEWA, a runtime energy
estimation framework for web activities on smartphones. In sharp
contrast to the traditional modeling methods, REEWA features
a design to provide highly accurate and low-overhead energy
estimation based on hardware performance counters that can
accurately record hardware-level events. Specifically, REEWA
features (1) a set of energy models for smartphone hardware
components involved in web activities, which are built based on
their respective performance counters; (2) a correlation study
on the counter selection process that provides the best tradeoff
between the estimation accuracy and overhead; (3) a performance
counter management mechanism for activity deployment. We
prototyped and evaluated REEWA in two real android smartphones. The results show that, compared to traditional estimation
methods, REEWA achieves an average 33% higher estimation
accuracy with a negligible overhead (less than 1%, worst-case).
We applied REEWA to support heterogeneous core scheduling for
web activities, which can help reduce 40% energy consumption.

",18.36309006607702,17.15333333333334,212
IGSC_17_013.txt,15.062152180223528,13.183717334494773,IGSC,4582,"
Application level power budget allocation is one
way to overcome the power constraint problem in future HPC
systems. This technique mainly depends on finding an optimal
number of compute nodes and power level for each node.
However, utilizing that power at node level requires optimization
of the underlying programming model. OpenMP is the defacto standard for intra-node parallelism. In this paper, we
investigate the impact of OpenMP runtime environment on the
performance of OpenMP code at the different power level. We
studied 28 OpenMP parallel regions from five NAS Parallel
Benchmark (NPB) applications. Based on the study we show that
for a given power level, a suitable selection of OpenMP runtime
parameters can improve the execution time and energy consumption of a parallel region up to 67% and 72%, respectively. We
also show that these fine grain improvements resulted in upto
26% execution time and 38% energy consumption improvement
for a given OpenMP application.

",15.247664890283005,13.73915322580645,156
IGSC_17_014.txt,14.659898854468938,13.080794732061765,IGSC,5496,"
In this paper, we present an enhanced and holistic
energy model for the widely used Network Simulator, NS3. As
computing becomes more energy efficient, data movement, and
hence data center networks consume an increasing percentage
of the total energy consumption and it is important to provide
energy management capabilities in the network simulators. Our
enhanced NS3 simulator supports the use of different sleep
modes for each network port on a switch/router and endpoint, and backplane on switch/router in order to reduce the
network energy consumption. It also supports two port level
power management mechanisms - unidirectional (transmitter
only) and bidirectional (transmitter and receiver). Moreover, the
simulator supports local consolidation that consolidates traffic
across multiple outgoing/incoming links into a node, and a global
controller that monitors all links in the network via a SDN-like
mechanism and helps make better local consolidation decisions.
The implemented capabilities are illustrated by applying them
to the popular fat-tree based data center network. It is shown
that the local consolidation mechanism coupled with bidirection
port-level energy management can reduce the network energy
consumption substantially.

",18.848423458724294,18.141233411397348,184
IGSC_17_015.txt,13.623524946454776,11.086227556770357,IGSC,5449,"
Colocation data centers (or colocations, for short)
are important participants in emergency demand response (EDR)
programs. One key challenge in colocations is that tenants control
their own servers, thus, may not coordinate to reduce their power
consumption. In this paper, we propose a joint truthful incentive
mechanism Co-Colo to encourage tenants joining EDR programs,
which includes a local optimization mechanism (LocalOpt) and
a global optimization mechanism (GlobalOpt). In LocalOpt, tenants are motivated to improve the energy efficiency locally. In
GlobalOpt, tenants can request some public server resources to
improve the energy efficiency. By jointly considering the two
mechanisms, Co-Colo effectively reduces the energy-saving cost.
A (1 + €)-approximation algorithm is proposed to obtain the
asymptotic optimal energy-saving scheme. We also consider a
special case when the public resources are sufficient, and design
a 2-approximation algorithm. Furthermore, the robustness of the
proposed algorithms are proved. Trace-driven simulations verify
the effectiveness and feasibility of Co-Colo.

",15.6451,14.80204347826087,160
IGSC_17_016.txt,16.376576746424536,14.121890971896171,IGSC,4382,"
Energy efficiency is considered to be a critical
concern for modern hardware and a variety of hardware
features have been developed to improve the energy balance
for executing applications. This article focuses on the dynamic
voltage frequency scaling (DVFS) technique, which is available
for many platforms, including CPUs and GPUs. Analytical
models for capturing the energy efficiency are considered and
it is investigated whether such an analytical model is able to
support an a priori selection of the operational frequency that
leads to a near optimal energy consumption for the application
code to be executed. Also the energy-delay product (EDP) is
investigated, weighting the power against the square of execution
time. The experimental evaluation is performed on the basis
of the multi-threaded PARSEC benchmarks. We show that the
operational frequency selected according to the analytical models
leads to an energy consumption that is near the minimum energy
consumption over all frequencies available.

",19.287186520377343,18.09662337662338,155
IGSC_17_017.txt,15.789266202346163,14.032325844997295,IGSC,4993,"
In today’s multicore systems, depending on an
application’s computational demand, cores are either operated
individually at different Voltage/Frequency (V/F) levels or
grouped into multiple Voltage-Frequency Islands (VFIs) to
reduce system energy consumption. This paper formulates a
task scheduling and VFI partitioning problem whose
optimization goal is to minimize the task set (application)
execution time (makespan) for a given energy budget. First, the
combinatorial optimization problem is formulated with Integer
Linear Programming (ILP) to obtain per-core, per-task
dynamic V/F levels in a fine-grain VFI-based system with
single-core islands. Next, static task scheduling on coarse-grain
VFI-based systems, where an island can contain several cores
operated at the same V/F level, is formulated with Mixed
Integer Linear Programming (MILP), considering the energy
budget and task set’s precedence constraints. The experimental
results show that under different energy budget constraints,
fine-grain, dynamic task allocations provide on average 1.35x
speedup over static coarse grain scheduling and partitioning
methods.

",20.456717378047102,20.093333333333337,170
IGSC_17_018.txt,16.21514389536657,14.325083141554988,IGSC,3965,"
The task-to-core scheduling problem using
Dynamic Voltage and Frequency Scaling (DVFS) for achieving
three objectives of performance, energy, and temperature (PET),
poses algorithmic challenges as it involves conflicting goals and
trade-offs. Some myriad static algorithms have been proposed for
solving this problem which can be roughly categorized into three
groups: approaches for generating optimal solutions (for smaller
sizes problems), complex optimization techniques, and fast
heuristics. These algorithms generate multi-dimensional results
which can be hardly intelligible. The assessment of these results
requires new comparison methods and concise evaluation
measures. This paper proposes a set of benchmarks and
evaluation procedures for carrying out methodical comparisons
of various algorithms for solving the PET-aware task-to-core
scheduling problem. The proposed performance measures assist
in judiciously comparing these different algorithms and
analyzing their results on a unified basis. The goal is also to seek
answers as to how good the Pareto-optimal algorithms are
compared to fast heuristics tackling the same problem with the
same assumptions. At the same time, we are interested in
knowing how good both the groups of algorithms are compared
to the absolute optimal (at least for small sets of problems). In
addition, the paper provides methods for evaluating trade-offs
and determining which application and target parameters affect
the results (performance, energy consumed and temperature
achieved) of these algorithms. Extensive experimentations
facilitate a comprehensive comparison of different kinds of
algorithms amongst themselves as well as with optimal solutions
obtained through Integer Linear Programming as a reference.

",18.7741,17.32292063492064,253
IGSC_17_019.txt,16.24595585922095,15.628990361278692,IGSC,2980,"
Low-energy computing in the use phase is compelling because it helps to address thermal density issues of
deeply scaled CMOS, maximizes battery-life of mobile computing
platforms, while also addressing sustainability. Unfortunately,
environmental impacts of fabricating CMOS integrated circuits
(Cs) is increasing and rapidly catching the operational phase
of computing systems, particularly for low-energy and mobile
computing products. This is due to trends in fabrication techniques for increasingly small geometries, such as increasing
photo-lithography and metrology costs. Without attention, IC
fabrication will likely become the dominant energy consumer
and source of carbon emissions over an IC’s lifetime. We propose
a scaled parameterized model for evaluating the environmental
impacts of IC fabrication, which can scale from 130nm to
32nm technology and account for stepwise changes in process
technologies. As an example of the type of analysis possible
using this model we demonstrate the environmental impacts
of changing the metal stack at these technology nodes. Our
results indicate that based on the die area calculated from a
commercial design flow and our parameterized model, changing
the number of metal layers from eight to six layers results in an
average savings in manufacturing energy of 9.5%, 13.8%, and
13% for 130nm, 90nm, and 65nm technologies, respectively, and,
depending on scenario, it can take years for operational energy
savings to makeup this difference.

",19.854308518879517,19.953346203346204,226
IGSC_17_020.txt,15.068877525733747,13.829670431414677,IGSC,4674,"
To save the electrical energy in a household, it is
essential to monitor where and how the power is consumed. To
maximize the efficiency of energy conservation, it is necessary
to make the running power low in the power monitor system,
which the tradition systems pay less attention to. This paper
presents PowerAnalyzer, an energy-aware system for monitoring
running states and power of each household appliance plugged
into power line from a single point detection. PowerAnalyzer
takes steady-state current waveforms as the appliances signature,
and uses the deep neural network (DNN) models to infer the
running states and running power of household appliances.
We focus on the energy consumption of PowerAnalyzer itself.
The energy efficiency of PowerAnalyzer is optimized from these
aspects: Using dynamic time intervals to collect electric data,
replacing a cloud server with an edge node to process data, and
transmitting differential data over a low power wireless protocol.
The evaluation results show that PowerAnalyzer offers 3.45%
average power metering error and 98.38% average accuracy of
inferring running states of appliances. PowerAnalyzer draws less
than 247mW static power and 304mW peak power.

",16.52667757954773,16.125887096774196,189
IGSC_17_021.txt,15.14364094964802,13.400637300843488,IGSC,6620,"
Designing 3D systems with on-chip DRAM is a
promising solution to improve memory bandwidth and reduce
memory access latency. However, 3D chips exacerbate the chip
thermal problem due to their longer heat dissipation path, as well
as the tight thermal coupling between logic and memory layers. In
this paper, we are interested in studying thermal aware resource
management strategies for both CPUs and memory systems when
realizing hard real-time systems on 3D platforms under given peak
temperature constraints. Given the dramatically increased power
density not only from CPUs but also from memory systems as well,
we believe that a joint CPU and memory system resource management is highly desired for 3D platforms to effectively deal with the
heat dissipation confined in a small package. In addition, different
from many existing thermal management strategies, which are
reactive and best-effort in nature, we are more interested in ones
that can ensure the strong guarantee for real-time applications.
To this end, we introduce a novel approach that incorporates the
periodic resource model to guarantee timing constraints for hard
real-time systems under thermal constraints. In the meantime,
by periodically (deterministically) throttling the accesses of CPUs
and memory resources, our approach can effectively guarantee
the thermal constraints imposed on both CPUs and memory
systems. We use simulation results to demonstrate the effectiveness
of our proposed approach in guaranteeing both the timing and
temperature constraints for hard real-time tasks on 3D platforms.

",17.833180683606166,17.662045454545453,243
IGSC_17_022.txt,17.626424815289198,16.306842474454573,IGSC,3561,"
 Perpetual awareness systems are sensing systems characterized by continuous monitoring
and ubiquitous sensing; they are essential to many
safety and mission-critical applications, e.g. assisted
living, healthcare and public safety. In this paper,
we present SAFER, a perpetual heterogeneous IoT
system; deployed in homes to detect critical events
(injury, hazardous-environment) that must trigger
immediate action and response. A key challenge here
is the energy consumption associated with perpetual operations. We propose a novel energy-aware
perpetual home IoT system where battery-operated
and wall-powered IoT devices co-execute to ensure
safety of occupants. We use a semantic approach that
extracts activities-of-daily-living from device data to
drive energy-optimized sensor activations. To validate
our approach, we developed an elderly fall detection
system using multi-personal and in-situ sensing devices. Using initial measurements to drive larger simulations, we show that our Cost-Function-Gradient
algorithm can achieve greater than 4X reductions in
energy dissipation without loss of sensing accuracy.
",18.243605946275583,17.815354951796667,165
IGSC_17_023.txt,14.783125029745705,13.315313459644248,IGSC,4144,"
Cloud computing has largely replaced dedicated and
physical computing systems by providing critical features such
as elasticity and on-demand access to resources. However,
despite its many benefits, the cloud does have its limitations,
such as limited or no control over the hardware and limited
customization options. Users who deploy applications on the
cloud only have control over software tuning and optimizations
since the infrastructure is managed by the provider.

In this paper, we analyze cloud-deployed Web applications
that are multi-tiered and employ Memcached as the object
caching layer. Memcached is a high performance memory
caching system and, if there are no other bottlenecks in the
system, the overall application performance should be dictated
by Memcached. However, we show that other components of
the system such as web servers, load balancers, and some
underlying system configurations, severely impact application performance. We analyze these components and provide
guidelines on their implementation and parameter tuning to
minimize resource waste in the cloud.

",17.28802050969988,15.797442680776019,163
IISWC_17_001.txt,16.73505054277406,14.66993077763907,IISWC,5762,"
Scale-out infrastructure such as Cloud is built
upon a large network of multi-core processors. Performance,
power consumption, and capital cost of such infrastructure
depend on the overall system configuration including number of
processing cores, core frequency, memory hierarchy and
capacity, number of memory channels, and memory data rate.
Among these parameters, memory subsystem is known to be one
of the performance bottlenecks, contributing significantly to the
overall capital and operational cost of the server. Also, given the
rise of Big Data and analytics applications, this could potentially
pose an even bigger challenge to the performance of cloud
applications and cost of cloud infrastructure. Hence it is
important to understand the role of memory subsystem in cloud
infrastructure and in particular for this emerging class of
applications. Despite the increasing interest in recent years, little
work has been done in understanding memory requirements
trends and developing accurate and effective models to predict
performance and cost of memory subsystem. Currently there is
no well-defined methodology for selecting a memory
configuration that reduces execution time and power
consumption by considering the capital and operational cost of
cloud. In this paper, through a comprehensive real-system
empirical analysis of performance, we address these challenges
by first characterizing diverse types of scale-out applications
across a wide range of memory configuration parameters. The
characterization helps to accurately capture applications’
behavior and derive a model to predict their performance. Based
on the developed predictive model, we propose MeNa, which is a
methodology to maximize the performance/cost ratio of scale-out
applications running in cloud environment. MeNa navigates
memory and processor parameters to find the system
configuration for a given application and a given budget, to
maximum performance. Compared to brute force method, MeNa
achieves more than 90% accuracy for identifying the right
configuration parameters to maximize performance/cost ratio.
Moreover, we show how MeNa can be effectively leveraged for
server designers to find architectural insights or subscribers to
allocate just enough budget to maximize performance of their
applications in cloud.
",19.746749792175326,18.0103550295858,339
IISWC_17_002.txt,15.903189008614273,14.375070121563322,IISWC,7658,"
Datacenters often are a power utility’s largest
consumers, and are expected to participate in several power management scenarios with diverse characteristics in which Energy
Storage Devices (ESDs) are expected to play important roles.
Different ESD technologies exist, including little explored technologies such as flow batteries, that offer different performance
characteristics in cost, size, and environmental impact. While
prior works in datacenter ESD literature have considered one
of usage aspect, technology, performance metric (typically cost),
the whole three-dimensional space is little explored. Towards
understanding this design space, this paper presents first such
study towards joint characterization of ESD usages based on their
provisioning and operating demands, under ideal and realistic
ESD technologies, and quantify their impact on datacenter performance. We expect our work can help datacenter operators to
characterize this three-dimensional space in a systematic manner,
and make design decisions targeted towards cost-effective and
environmental impact aware datacenter energy management.
",21.374148478970945,21.185736842105268,154
IISWC_17_003.txt,17.624192110409517,16.707169811320757,IISWC,5829,"
Datacenters provide flexibility and high performance for
users and cost efficiency for operators. However, the high
computational demands of big data and analytics
technologies such as MapReduce, a dominant programming
model and framework for big data analytics, mean that even
small changes in the efficiency of execution in the data center
can have a large effect on user cost and operational cost.
Fine-tuning configuration parameters of MapReduce
applications at the application, architecture, and system
levels plays a crucial role in improving the energy-efficiency
of the server and reducing the operational cost. In this work,
through methodical investigation of performance and power
measurements, we demonstrate how the interplay among
various MapReduce configurations as well as application
and architecture level parameters create new opportunities
to co-locate MapReduce applications at the node level. We
also show how concurrently fine-tuning optimization
parameters for multiple scheduled MapReduce applications
improves energy-efficiency compared to fine-tuning
parameters for each application separately. In this paper, we
present Co-Located Application Optimization (COLAO)
that co-schedules multiple MapReduce applications at the
node level to enhance energy efficiency. Our results show
that through co-locating MapReduce applications and finetuning configuration parameters concurrently, COLAO
reduces the number of nodes by half to execute MapReduce
applications while improving the EDP by 2.2X on average,
compared to fine-tuning applications individually and run
them serially for a broad range of studied workloads.
",22.076135915942103,22.61039072039073,236
IISWC_17_004.txt,18.847897579345105,17.433823714816118,IISWC,7346,"
Porting sequential applications to heterogeneous
HPC systems requires extensive software and hardware expertise
to estimate the potential speedup and to efficiently use the
available compute resources in such systems. To streamline this
daunting process, researchers have proposed several “black-box”
performance prediction approaches that rely on the performance
of a training set of parallel applications. However, due to the lack
of a diverse set of applications along with their optimized parallel
implementations for each architecture type, the predicted speedup
by these approaches is not the speedup upper-bound, and even
worse it can be misleading, if the reference parallel implementations are not equally-optimized for every target architecture.

This paper presents AutoMatch, an automated framework for
matching of compute kernels to heterogeneous HPC architectures.
AutoMatch uses hybrid (static and dynamic) analysis to find the
best dependency-preserving parallel schedule of a given sequential
code. The resulting operations schedule serves as a basis to
construct a cost function of the optimized parallel execution of
the sequential code on heterogeneous HPC nodes. Since such
a cost function informs the user and runtime system about
the relative execution cost across the different hardware devices
within HPC nodes, AutoMatch enables efficient runtime workload
distribution that simultaneously utilizes all the available devices
in performance-proportional way. For a set of open-source HPC
applications with different characteristics, AutoMatch turns out
to be very effective, identifying the speedup upper-bound of
sequential applications and how close the parallel implementation
is to the best parallel performance across five different HPC
architectures. Furthermore, AutoMatch’s workload distribution
scheme achieves approximately 90% of the performance of a
profiling-driven oracle.
",21.19438992294339,20.05962962962963,272
IISWC_17_005.txt,18.106758349069494,16.925763498084752,IISWC,7813,"
Modern computer systems are accelerator-rich,
equipped with many types of hardware accelerators to speed
up computation. For example, graphics processing units (GPUs)
are a type of accelerators that are widely employed to accelerate
parallel workloads. In order to well utilize different accelerators
to gain better execution time speedup or reduce total energy
consumption, many scheduling algorithms have been proposed
to select the optimal target device to process an OpenCL kernel
according to the kernel’s individual characteristics. However, in
a real computer system, there are a lot of workloads co-located
together on a single machine and would be processed on different
devices simultaneously. The CPU cores and accelerators may contend shared resources, such as the host main memory and shared
last-level cache. Thus, it is not robust to schedule an OpenCL
kernel execution by simply considering the characteristics of the
kernel. To maximize the system throughput, it is important to
consider the execution behavior of all co-located applications
when performing OpenCL kernel execution scheduling.

In this paper, we provide a detailed characterization study
demonstrating that scheduling an OpenCL kernel to run on
different devices can introduce varying performance impact
to itself and the other co-located applications due to memory
interference. Based on the characterization results, we then
develop a light-weight, scalable performance degradation predictor specifically for heterogeneous computer systems, called
HeteroPDP. HeteroPDP aims to dynamically predict and balance
the execution time slowdown of all co-located applications in
a heterogeneous computation environment. Our real system
evaluation results show that comparing with always running an
OpenCL kernel on the host CPU, HeteroPDP is able to achieve
3X execution time speedup when an OpenCL kernel runs alone
and improve the system fairness from 24% to 65% when an
OpenCL kernel is co-located with other applications.
",18.72661885863436,18.255725144420797,301
IISWC_17_006.txt,14.546138613846377,12.365039427219642,IISWC,6723,"
Modern SoCs contain CPU and GPU cores to
execute both general purpose and highly-parallel graphics workloads. While the primary use of the GPU is for rendering
graphics, the effects of graphics workloads on the overall system
have received little attention. The primary reason for this is
the lack of efficient tools and simulators for modern graphics
applications.

In this work, we present GLTraceSim, a new graphics memory
tracing and replay framework for studying the memory behavior
of graphics workloads and how they interact in heterogeneous
CPU/GPU memory systems. GLTraceSim efficiently generates
GPU memory access traces and their corresponding, synchronized, CPU render thread memory traces. The resulting traces
can then be replayed in both high-level models and detailed fullsystem simulators.

We evaluate GLTraceSim on a range of graphics workloads
from browsers to games. Our results show that GLTraceSim
can efficiently generate graphics memory traces, and use these
traces to study graphics performance in heterogeneous CPU/GPU
memory systems. We show that understanding the impact of
graphics workloads is essential, as they can cause slowdowns
in co-running CPU applications of 26-59%, depending on the
memory technology.
",16.45884130781739,14.451851851851853,190
IISWC_17_007.txt,15.34733327068021,13.369854646640366,IISWC,9534,"
Three-dimensional (3D)-stacking technology, which
enables the integration of DRAM and logic dies, offers high
bandwidth and low energy consumption. This technology also
empowers new memory designs for executing tasks not traditionally associated with memories. A practical 3D-stacked memory is
Hybrid Memory Cube (HMC), which provides significant access
bandwidth and low power consumption in a small area. Although
several studies have taken advantage of the novel architecture
of HMC, its characteristics in terms of latency and bandwidth
or their correlation with temperature and power consumption
have not been fully explored. This paper is the first, to the
best of our knowledge, to characterize the thermal behavior of
HMC in a real environment using the AC-510 accelerator and
to identify temperature as a new limitation for this state-ofthe-art design space. Moreover, besides bandwidth studies, we
deconstruct factors that contribute to latency and reveal their
sources for high- and low-load accesses. The results of this paper
demonstrates essential behaviors and performance bottlenecks for
future explorations of packet-switched and 3D-stacked memories.
",17.77360955136429,16.27366883116883,177
IISWC_17_008.txt,14.826543219697577,13.572799273775129,IISWC,9212,"
Large-scale systems with arrays of solid state disks
(SSDs) have become increasingly common in many computing
segments. To make such systems resilient, we can adopt erasure
coding such as Reed-Solomon (RS) code as an alternative to
replication because erasure coding can offer a significantly lower
storage cost than replication. To understand the impact of using
erasure coding on system performance and other system aspects
such as CPU utilization and network traffic, we build a storage
cluster consisting of approximately one hundred processor cores
with more than fifty high-performance SSDs, and evaluate the
cluster with a popular open-source distributed parallel file
system, Ceph. Then we analyze behaviors of systems adopting
erasure coding from the following five viewpoints, compared
with those of systems using replication: (1) storage system //O
performance; (2) computing and software overheads; (3) VO
amplification; (4) network traffic among storage nodes; (5) the
impact of physical data layout on performance of RS-coded SSD
arrays. For all these analyses, we examine two representative
RS configurations, which are used by Google and Facebook
file systems, and compare them with triple replication that a
typical parallel file system employs as a default fault tolerance
mechanism. Lastly, we collect 54 block-level traces from the
cluster and make them available for other researchers.
",18.946978176291534,20.173837209302324,216
IISWC_17_009.txt,15.007303345329245,13.504827605037498,IISWC,7469,"
Block traces are widely used for system studies, model
verifications, and design analyses in both industry and academia.
While such traces include detailed block access patterns, existing
trace-driven research unfortunately often fails to find truenorth due to a lack of runtime contexts such as user idle
periods and system delays, which are fundamentally linked to
the characteristics of target storage hardware. In this work, we
propose TraceTracker, a novel hardware/software co-evaluation
method that allows users to reuse a broad range of the existing
block traces by keeping most their execution contexts and user
scenarios while adjusting them with new system information.
Specifically, our TraceTracker’s software evaluation model can
infer CPU burst times and user idle periods from old storage
traces, whereas its hardware evaluation method remasters the
storage traces by interoperating the inferred time information,
and updates all inter-arrival times by making them aware of
the target storage system. We apply the proposed co-evaluation
model to 577 traces, which were collected by servers from
different institutions and locations a decade ago, and revive
the traces on a high-performance flash-based storage array.
The evaluation results reveal that the accuracy of the execution
contexts reconstructed by TraceTracker is on average 99% and
96% with regard to the frequency of idle operations and the total
idle periods, respectively.
",18.422482065455632,21.217477477477477,224
IISWC_17_010.txt,15.510175679589125,14.501830071719997,IISWC,7093,"
Dynamic programming languages are becoming increasingly popular, and this motivates the need for just-in-time
(JIT) compilation to close the productivity/performance gap. Unfortunately, developing custom JIT-optimizing virtual machines
(VMs) requires significant effort. Recent work has shown the
promise of meta-JIT frameworks, which abstract the language definition from the VM internals. Meta-JITs can enable automatic
generation of high-performance JIT-optimizing VMs from highlevel language specifications. This paper provides a detailed workload characterization of meta-tracing JITs for two different dynamic programming languages: Python and Racket. We propose a
new cross-layer methodology, and then we use this methodology to
characterize a diverse selection of benchmarks at the application,
framework, interpreter, JIT-intermediate-representation, and microarchitecture level. Our work is able to provide initial answers
to important questions about meta-tracing JITs including the potential performance improvement over optimized interpreters, the
source of various overheads, and the continued performance gap
between JIT-compiled code and statically compiled languages.
",18.848423458724294,18.201146384479717,163
IISWC_17_011.txt,14.554592549557764,12.995760283319253,IISWC,1394,"
Graphics rendering is a complex, multi-step process
whose data demands typically dominate memory system design
in SoCs. GPUs create images by merging many, simpler scenes
for each frame. For performance, scenes are tiled into parallel
tasks, each of which produces different parts of the final output.
This execution model results in complex memory behavior, whose
bandwidth demands, reuse and sharing characteristics depend
heavily on the structure and complexity of each application,
frame, scene, and task, and vary over time. To design systems
that can efficiently accommodate and schedule these workloads,
we need to understand their behavior and diversity.

In this work, we explore the data demands of modern graphics
rendering quantitatively, using an architecturally-independent
analysis that identifies the different types of data sharing present
in the applications.
",15.903189008614273,15.114379844961245,130
IISWC_17_012.txt,18.339268331306087,17.02926724364018,IISWC,1271,"
As servers are equipped with more memory modules
each with larger capacity, main-memory systems are now the
second highest energy-consuming component in big-memory
servers and their energy consumption even becomes comparable
to processors in some servers. Meanwhile, it is critical for bigmemory servers and their main-memory systems to offer high
energy efficiency. Prior work exploited mobile LPDDR devices’
advantages (lower power than DDR devices) while attempting
to surmount their limitations (onger latency, lower bandwidth,
or both). However, we demonstrate that such main memory
architectures (based on the latest LPDDR4 devices) are no longer
effective. This is because the power consumption of present DDR4
devices has substantially decreased by adopting the strength of
mobile and graphics memory whereas LPDDR4 has sacrificed
energy efficiency and focused more on increasing data transfer
rates; we also exhibit that the power consumption of DDR4
devices can substantially vary across manufacturers. Moreover,
investigating a new energy-saving feature of DDR4 devices in
depth, we show that activating this feature often hurts overall
energy efficiency of servers due to its performance penalties.
",20.10790988173199,18.722094972067044,180
IISWC_17_013.txt,16.73684848801386,14.916020318986238,IISWC,996,"
Emerging big data frameworks requires
computational resources and memory subsystems that can
naturally scale to manage massive amounts of diverse data.
Given the large size and heterogeneity of the data, it is currently
unclear whether big data frameworks such as Hadoop, Spark,
and MPI will require high performance and large capacity
memory to cope with this change and exactly what role main
memory subsystems will play; particularly in terms of energy
efficiency. The primary purpose of this study is to answer these
questions through empirical analysis of different memory
configurations available on commodity hardware and to assess
the impact of these configurations on the performance and power
of these well-established frameworks. Our results reveal that
while for Hadoop there is no major demand for high-end DRAM,
Spark and MPI iterative tasks (e.g. machine learning) are
benefiting from a high-end DRAM; in particular high frequency
and large numbers of channels. Among the configurable
parameters, our results indicate that increasing the number of
DRAM channels reduces DRAM power and improves the
energy-efficiency across all three frameworks.
",19.48791578843652,20.037820224719102,180
IISWC_17_014.txt,16.647925096878797,15.809118143459916,IISWC,948,"
Energy-use is a key concern when migrating current
deep learning applications onto low power heterogeneous devices
such as a mobile device. This is because deep neural networks
are typically designed and trained on high-end GPUs or servers
and require additional processing steps to deploy them on
low power devices. Such steps include the use of compression
techniques to scale down the network size or the provision of
efficient device-specific software implementations. Migration is
further aggravated by the lack of tools and the inability to
measure power and performance accurately and consistently
across devices. We present a novel evaluation framework for
measuring energy and performance for deep neural networks
using ARMs Streamline Performance Analyser integrated with
standard deep learning frameworks such as Caffe and CuDNNv5.
We apply the framework to study the execution behaviour of
SqueezeNet on the Maxwell GPU of the NVidia Jetson TX1,
on an image classification task (also known as inference) and
demonstrate the ability to measure energy of specific layers of
the neural network.
",17.5058628484301,17.047058823529415,171
IISWC_17_015.txt,14.251653674069555,12.847093723205067,IISWC,1327,"
Approximate computing is getting a lot of traction
especially for its potential in improving power, performance,
and scalability of a computing system. However, prior work
heavily relies upon a programmer to identify code sections
where various approximation techniques can be applied. Such
an approach is error prone and cannot scale well beyond small
applications. In this paper, we contribute with a tool, called
Approximeter, to automatically identify and quantify code sections where approximation can be used and to what extant. The
tool works by first identifying potential approximable functions
and then, injecting errors at appropriate locations. The tool
runs Monte Carlo experiments to quantify statistical relation
between injected error and corresponding output accuracy. The
tool also provides a rough estimate of potential performance
gain from approximating a certain function. Finally, it ranks
the approximable functions based on their error tolerance and
performance gain.
",16.218646115125612,14.816215034965037,144
IISWC_17_016.txt,14.348710955821954,12.179132007233274,IISWC,1183,"
In a closely coupled heterogeneous computing system the work is shared amongst all available computing resources. One challenge is to find an optimal division of work
between the two or more very different kinds of processing units,
each with their own optimal settings. We show that through the
use of statistical techniques, a systematic search of the parameter
space can be conducted. These techniques can be applied to
variables that are categorical or continuous in nature and do not
rely on the standard assumptions of linear models, mainly that
the response variable can be described as a linear combination of
the regression coefficients. Our search technique, when applied
to the HPL benchmark, resulted in a performance gain of 14.5%
over previously reported results.
",16.647925096878797,14.917821138211384,125
IISWC_17_017.txt,16.04894535257393,14.843143334771355,IISWC,1215,"
Programming Micron’s Automata Processor (AP)
requires expertise in both automata theory and the AP architecture, as programmers have to manually manipulate state
transition elements (STEs) and their transitions with a lowlevel Automata Network Markup Language (ANML). When the
required STEs of an application exceed the hardware capacity,
multiple reconfigurations are needed. However, most previous
AP-based designs limit the dataset size to fit into a single AP
board and simply neglect the costly overhead of reconfiguration.
This results in unfair performance comparisons between the AP
and other processors.
To address this issue, we propose a framework for the fast and
fair evaluation of AP devices. Our framework provides a hierarchical approach that automatically generates automata for large
datasets through user-defined paradigms and allows the use of
cascadable macros to achieve highly optimized reconfigurations.
We highlight the importance of counting the configuration time
in the overall AP performance, which in turn, can provide better
insight into identifying essential hardware features, specifically
for large-scale problem sizes. Our framework shows that the AP
can achieve up to 461x overall speedup fairly compared to CPU
counterparts.
",17.267425705330172,17.108260869565218,186
IISWC_17_018.txt,15.92977403010482,13.982585498409335,IISWC,942,,NA,NA,0
IISWC_17_019.txt,14.289928638192109,12.265531932462512,IISWC,3775,"
Key-value stores (e.g., Memcached) and web servers
(e.g., NGINX) are widely used by cloud providers. As interactive
services, they have strict service-level objectives, with typical
99th -percentile tail latencies on the order of a few milliseconds.
Unlike average latency, tail latency is more sensitive to changes
in usage load and traffic patterns, system configurations, and
resource availability. Understanding the sensitivity of tail latency
to application and system factors is critical to efficiently design
and manage systems for these latency-critical services.

We present a comprehensive study of the impact a diverse set of
application, hardware, and isolation configurations have on tail
latency for two representative interactive services, Memcached
and NGINX. Examined factors include input load, thread-level
parallelism, request size, virtualization, and resource partitioning.
We conduct this study on two server platforms with significant
differences in terms of architecture and price points: an Intel
Xeon and an ARM-based Cavium ThunderX server. Experimental results show that latency on both platforms is subject to
changes of several orders of magnitude depending on application
and system settings, with Cavium ThunderX being more sensitive
to configuration parameters.
",18.243605946275583,17.92877112135177,189
IISWC_17_020.txt,14.366576099201222,12.622830716224875,IISWC,6575,"
Designing and optimizing computer systems require
deep understanding of the underlying system. Historically many
important observations that led to the development of essential
hardware and software optimizations were driven by empirical
studies of program behavior. In this paper we report an interesting property of dynamic program execution by viewing it as a
changing (or social) network. In a program social network, two
instructions are friends if there is a producer-consumer relationship between them. One prominent result is that the outdegree of
instructions follow heavy tails or power law distributions, i.e., a
few instructions produce values for many instructions while most
instructions do so for very few instructions. In other words, the
number of instruction dependencies is highly skewed.

In this paper we investigate this curious phenomenon. By
analyzing a large set of workloads under different compilers,
compilation options, ISAs and inputs we find that the dependence
skew is widespread, suggesting that it is fundamental. We
also observe that the skew is fractal across time and space.
Finally, we describe conditions under which skew emerges within
programs and provide evidence that suggests that the heavy-tailed
distributions are a unique program property.
",15.6451,14.023,194
IISWC_17_021.txt,16.87108446685844,15.241657305949847,IISWC,7400,"
he VMware ESXi hypervisor attracts a wide range
of customers and is deployed in domains ranging from desktop
computing to server computing. While the software systems
are increasingly moving towards consolidation, hardware has
already transitioned into multi-socket Non-Uniform Memory
Access (NUMA)-based systems. The marriage of increasing
consolidation and the multi-socket based systems warrants lowoverhead, simple and practical mechanisms to detect and address
performance bottlenecks, without causing additional contention
for shared resources such as performance counters. In this paper,
we propose a simple, practical and highly accurate, dynamic
memory latency probing mechanism to detect memory congestion
in a NUMA system. Using these dynamic probed latencies, we
propose congestion-aware memory allocation, congestion-aware
memory migration, and a combination of these two techniques.
These proposals, evaluated on Intel Westmere (8 nodes) and Intel
Haswell (2 nodes) using various workloads, improve the overall
performance on an average by 7.2% and 9.5% respectively.
",18.946978176291534,18.263496732026145,156
IISWC_17_022.txt,15.808059289997715,14.493141837437616,IISWC,7631,"
Modern multi-core systems employ shared memory
architecture, entailing problems related to the main memory such
as row-buffer conflicts, time-varying hot-spots across memory
channels, and superfluous switches between reads and writes
originating from different cores. There have been proposals to
solve these problems by partitioning main memory across banks
and/or channels such that a DRAM bank is dedicated to a single
core, being free from inter-thread row-buffer conflicts. However,
those studies either focused on only multi-programmed workloads on which cores operate independently, not cooperatively,
or specific hardware configurations with a limited number of
degrees of freedom in the number of main memory banks, ranks,
and channels.

We analyze the influence of memory partitioning on systems with various degrees of banks, ranks, and channels using
multi-threaded and multi-programmed workloads, making the
following key observations. Bank partitioning is beneficial when
memory-intensive applications in a multi-programmed workload
have similar characteristics in bank-level parallelism, bandwidth,
and capacity demands. Any diversity in these demands with a
limited memory capacity greatly diminishes the bank partitioning
benefits. As memory access/usage patterns across cores are
more easily manageable on multi-threaded workloads, bank
partitioning is more often effective with memory intensive multithreaded applications. Channel partitioning becomes effective
when the reduction of the negative impacts of time-varying hotspots across memory channels outweighs the load imbalance
due to partitioning. We also demonstrate the benefits of rank
partitioning with regard to minimizing read-write switches on
multi-threaded applications where cores can coordinate memory
accesses.
",19.061188166129803,18.64116279069768,259
IISWC_17_023.txt,15.225770446220999,13.555693897416866,IISWC,6600,"
Owing to the advantages of low standby power
and high scalability, ReRAM technology is considered as a
promising replacement for conventional DRAM in future manycore systems. In order to make ReRAM highly scalable, the
memory array has to have a crossbar array structure, which
needs a specific access mechanism for activating a row of memory
when reading/writing a data block from/to it. This type of
memory access would cause Sneak Current that would lead to
voltage drop on the memory cells of the activated row, i.e., the
cells which are far from the write drivers experience more voltage
drop compared to those close to them. This results in a nonuniform access latency for the cells of the same row.

To address this problem, we propose and evaluate a scheme
that exploits the non-uniformity of write access pattern of the
workloads. More specifically, based on our extensive characterization of write patterns to the cache lines and memory pages
of 20 CPU workloads, we recognized that (i) on each main
memory access, just a few cache lines of the activated row need
to be updated on a write-back, and more importantly, there is a
temporal and spatial locality of the writes to the cache lines; and
(ii) all pages of the memory footprint of an application do not
see the same write counts during the execution of the workload.
Motivated by these characteristics, we then evaluate different
intra-page memory block permutations in order to improve the
performance of a crossbar ReRAM-based main memory. Our
results collectively show that, by applying some types of intrapage memory block permutation, the access latency to a RERAMbased main memory can be reduced up to 50% when running
the SPEC CPU2006 workloads.
",17.833180683606166,18.181817010309278,293
IISWC_17_024.txt,16.128116888433887,14.81617387421917,IISWC,6440,"
Cameras are the defacto sensor. The growing demand for real-time and low-power computer vision, coupled
with trends towards high-efficiency heterogeneous systems, has
given rise to a wide range of image processing acceleration
techniques at the camera node and in the cloud. In this paper,
we characterize two novel camera systems that use acceleration
techniques to push the extremes of energy and performance
scaling, and explore the computation-communication tradeoffs
in their design. The first case study targets a camera system
designed to detect and authenticate individual faces, running
solely on energy harvested from RFID readers. We design a
multi-accelerator SoC design operating in the sub-mW range,
and evaluate it with real-world workloads to show performance
and energy efficiency improvements over a general purpose
microprocessor. The second camera system supports a 16-camera
rig processing over 32 Gb/s of data to produce real-time 3D-360°
virtual reality video. We design a multi-FPGA processing pipeline
that outperforms CPU and GPU configurations by up to 10x
in computation time, producing panoramic stereo video directly
from the camera rig at 30 frames per second. We find that an
early data reduction step, either before complex processing or
offloading, is the most critical optimization for in-camera systems.
",17.833180683606166,16.84257629107982,213
IISWC_17_025.txt,16.27094395272886,14.511064446447055,IISWC,5298,"
Current handhelds incorporate a variety of accelerators/IPs for improving their performance and energy efficiency.
While these IPs are extremely useful for accelerating parts of a
computation, the CPU still expends a significant amount of time
and energy in the overall execution. Coarse grain customized
hardware of Android APIs and methods, though widely useful,
is also not an option due to the high hardware costs. Instead,
we propose a fine-grain sequence of instructions, called a Loadto-Store (LOST) sequence, for hardware customization. A LOST
sequence starts with a load and ends with a store, including
dependent instructions in between. Unlike prior approaches to
customization, a LOST sequence is defined based on a sequence
of opcodes rather than a sequence of PC addresses or operands.
We identify such commonly occurring LOST sequences within
and across several popular apps and propose a design to integrate
these customized hardware sequences as macro functional units
into the CPU data-path. Detailed evaluation shows that such
customized LOST sequences can provide an average of 25%
CPU speedup, or 12% speedup for the entire system.
",16.373557378465907,14.621666666666666,181
IISWC_17_026.txt,14.602276240619993,12.771616132576558,IISWC,5214,"
Today’s GPUs continue to increase the number of
compute resources with each new generation. Many data-parallel
applications have been re-engineered to leverage the thousands
of cores on the GPU. But not every kernel can fully utilize
all the resources available. Many applications contain multiple
kernels that could potentially be run concurrently. To better
utilize the massive resources on the GPU, device vendors have
started to support Concurrent Kernel Execution (CKE). However,
the application throughput provided by CKE is subject to a
number of factors, including the kernel configuration attributes,
the dynamic behavior of each kernel (e.g., compute-intentive
vs. memory-intensive), the kernel launch order and inter-kernel
dependencies. Minor changes in any of theses factors can have a
large impact on the effectiveness of CKE.
In this paper, we present Moka, an empirical model for tuning
concurrent kernel performance. Moka allows us to accurately
predict the resulting performance and scalability of multi-kernel
applications when using CKE. We consider both static and
dynamic workload characteristics that impact the utility of CKE,
and leverage these metrics to drive kernel scheduling decisions
on NVIDIA GPUs. The underlying data transfer pattern and
GPU resource contention are analyzed in detail. Our model is
able to accurately predict the performance ceiling of concurrent
kernel execution. We validate our model using several real-world
applications that have multiple kernels that can run concurrently,
and evaluate CKE performance on a NVIDIA Maxwell GPU. Our
model is able to predict the performance of CKE applications
accurately, providing estimates that differ by less than 12% as
compared to actual runtime performance. Using our estimates,
we can quickly find the best CKE strategy for our applications
to achieve improved application throughput. We believe we
have developed a useful tool to aid application programmers to
accelerate their applications using CKE.
",15.742502247213078,14.43350165562914,305
IISWC_17_027.txt,17.133693786648443,15.490921929687676,IISWC,7215,"
Floating-point computations produce approximate
results, possibly leading to inaccuracy and reproducibility
problems. Existing work addresses two issues: first, the design of
high precision floating-point representations; second, the study of
methods to trade off accuracy and performance of CPU
applications. However, a comprehensive study of the tradeoffs
between accuracy and performance on modern GPUs is missing.

This study covers the use of different floating-point precisions
(i.e., single and double floating-point precision in IEEE 754
standard, GNU Multiple Precision, and composite floating-point
precision) on GPU using a variety of synthetic and real-world
benchmark applications. First, we analyze the support for single
and double precision floating-point arithmetic on different GPU
architectures, and we characterize the latencies of all floatingpoint instructions on GPU. Second, we study the
performance/accuracy tradeoffs related to the use of different
arithmetic precisions on addition, multiplication, division, and
natural exponential function. Third, we analyze the combined use
of different arithmetic operations on three benchmark
applications characterized by different instruction mixes and
arithmetic intensities. As a result of this analysis, we provide
insights to guide users to the selection of the arithmetic precision
leading to a good performance/accuracy tradeoff depending on
the arithmetic operations and mathematical functions used in
their program and the degree of multithreading of the code.
",19.906493383657665,18.969395161290326,219
IISWC_17_028.txt,15.832023613614833,14.038842000568344,IISWC,7036,"
Although numerous loop optimization techniques
have been designed and deployed in commercial compilers in
the past, virtually no common experimental infrastructure nor
repository exists to help the compiler community evaluate the
effectiveness of these techniques.

This paper describes a repository, LORE, that maintains a
large number of C language for loop nests extracted from popular benchmarks, libraries, and real applications. It also describes
the infrastructure that builds and maintains the repository. Each
loop nest in the repository has been compiled, transformed,
executed, and measured independently. These loops cover a
variety of properties that can be used by the compiler community
to evaluate loop optimizations using a broad and representative
collection of loops.

To illustrate the usefulness of the repository, we also present
two example applications. One is assessing the capabilities of
the auto-vectorization features of three widely used compilers.
The other is measuring the performance difference of a compiler
across different versions. These applications prove that the repository is valuable for identifying the strengths and weaknesses of
a compiler and for quantitatively measuring the evolution of a
compiler.
",17.122413403193683,15.898510242085663,180
IISWC_17_029.txt,14.99867068416609,13.461005805515242,IISWC,5485,"
Understanding the extent to which computational
results can change across platforms, compilers, and compiler flags
can go a long way toward supporting reproducible experiments.
In this work, we offer the first automated testing aid called FLiT
(Floating-point Litmus Tester) that can show how much these
results can vary for any user-given collection of computational
kernels. Our approach is to take a collection of these kernels,
disperse them across a collection of compute nodes (each with
a different architecture), have them compiled and run, and
bring the results to a central SQL database for deeper analysis.
Properly conducting these activities requires a careful selection
(or design) of these kernels, input generation methods for them,
and the ability to interpret the results in meaningful ways.
The results in this paper are meant to inform two different
communities: (a) those interested in seeking higher performance
by considering “IEEE unsafe” optimizations, but then want to
understand how much result variability to expect, and (b) those
interested in standardizing compiler flags and their meanings, so
that one may safely port code across generations of compilers
and architectures. By releasing FLiT, we have also opened up the
possibility of all HPC developers using it as a common resource
as well as contributing back interesting test kernels as well as
best practices, thus extending the floating-point result-consistency
workload we contribute. This is the first such workload and
result-consistency tester underlying floating-point reproducibility
of which we are aware.
",18.848423458724294,19.605145180023232,247
IISWC_17_030.txt,15.636724491199118,14.095550753393713,IISWC,4661,"
Traditionally GPUs focused on streaming, dataparallel applications, with little data reuse or sharing and
coarse-grained synchronization. However, the rise of generalpurpose GPU (GPGPU) computing has made GPUs desirable
for applications with more general sharing patterns and finegrained synchronization, especially for recent GPUs that have
a unified address space and coherent caches. Prior work has
introduced microbenchmarks to measure the impact of these
changes, but each paper uses its own set of microbenchmarks.
In this work, we combine several of these sets together in a single
suite, HeteroSync. HeteroSync includes several synchronization
primitives, data sharing at different levels of the memory
hierarchy, and relaxed atomics. We characterize the scalability
of HeteroSync for different coherence protocols and consistency
models on modern, tightly coupled CPU-GPU systems and show
that certain algorithms, coherence protocols, and consistency
models scale better than others.
",18.062587368997235,17.55435251798561,140
IISWC_17_031.txt,15.086673114142293,13.440074050614967,IISWC,7343,"
The trends of transistor size and system complexity
scaling continue. As a result, soft errors in the system, including
the processor core, are predicted to become one of the major
reliability challenges. A fraction of soft errors at the device level
could become an unmasked error visible to the user. Unmasked
soft errors may manifest as a detectable error, which could be
recoverable (DRE) or unrecoverable (DUE), or a Silent Data
Corruption (SDC). Detecting and recovering from an SDC is
especially challenging since an explicit checker is needed to detect
erroneous state. Predicting when SDCs are more likely could be
valuable in designing resilient systems.

To gain insight, we evaluate the Architectural Vulnerability
Factor (AVF) of all major in-core memory structures of an
out-of-order superscalar processor. In particular, we focus on
the vulnerability factors for detectable and unrecoverable errors
(DUEavr) and silent data corruptions (SDCayr) across windows
of execution to study their characteristics, time-varying behavior,
and their predictability using a linear regression trained offline.
We perform more than 35 million microarchitectural fault
injection simulations and, if necessary, run-to-completion using
functional simulations to determine AVF, DUEavr, and SDCavr.

Our study shows that, similar to AVF, DUEaye and SDCayr
vary over time and across applications. We also find significant differences in DUEgye and SDCayr across the processor
structures we studied. Furthermore, we find that DUEavr can
be predicted using a linear regression with similar accuracy as
AVF estimation. However, SDCavr could not be predicted with
the same level of accuracy. As a remedy, we propose adding a
software vulnerability factor, in the form of SDCpvr, to the linear
regression model for estimating SDCayr. We find that SDCpvr of
the Architectural Register File explains most of the behavior of
SDCayr for the combined microarchitectural structures studied
in this paper. Our evaluation shows that the addition of SDCpvr
improves the accuracy by 5.19x, on average, to a level similar
to DUEavr and AVF estimates. We also evaluate the impact of
limiting software-layer reliability information to only 5 basic
blocks (16x cost reduction, on average), and observe that it
increases error only by 18.7%, on average.
",16.126659408913476,14.805622740716398,361
IMC_17_001.txt,15.858757260049593,13.746584474745344,IMC,9490,"
The Border Gateway Protocol (BGP) has been used for decades as
the de facto protocol to exchange reachability information among
networks in the Internet. However, little is known about how this
protocol is used to restrict reachability to selected destinations, e.g.,
that are under attack. While such a feature, BGP blackholing, has
been available for some time, we lack a systematic study of its
Internet-wide adoption, practices, and network efficacy, as well as
the profile of blackholed destinations.

In this paper, we develop and evaluate a methodology to automatically detect BGP blackholing activity in the wild. We apply
our method to both public and private BGP datasets. We find that
hundreds of networks, including large transit providers, as well as
about 50 Internet exchange points (IXPs) offer blackholing service
to their customers, peers, and members. Between 2014-2017, the
number of blackholed prefixes increased by a factor of 6, peaking
at 5K concurrently blackholed prefixes by up to 400 Autonomous
Systems. We assess the effect of blackholing on the data plane using
both targeted active measurements as well as passive datasets, finding that blackholing is indeed highly effective in dropping traffic
before it reaches its destination, though it also discards legitimate
traffic. We augment our findings with an analysis of the target IP
addresses of blackholing. Our tools and insights are relevant for
operators considering offering or using BGP blackholing services
as well as for researchers studying DDoS mitigation in the Internet.
",16.887214914478655,15.447493827160496,245
IMC_17_002.txt,15.035757032097631,13.12637153652393,IMC,9857,"
Understanding data plane health is essential to improving Internet reliability and usability. For instance, detecting disruptions in
distant networks can identify repairable connectivity problems.
Currently this task is difficult and time consuming as operators
have poor visibility beyond their network’s border. In this paper
we leverage the diversity of RIPE Atlas traceroute measurements to
solve the classic problem of monitoring in-network delays and get
credible delay change estimations to monitor network conditions
in the wild. We demonstrate a set of complementary methods to
detect network disruptions and report them in near real time. The
first method detects delay changes for intermediate links in traceroutes. Second, a packet forwarding model predicts traffic paths
and identifies faulty routers and links in cases of packet loss. In
addition, we define an alarm score that aggregates changes into a
single value per AS in order to easily monitor its sanity, reducing
the effect of uninteresting alarms. Using only existing public data
we monitor hundreds of thousands of link delays while adding no
burden to the network. We present three cases demonstrating that
the proposed methods detect real disruptions and provide valuable
insights, as well as surprising findings, on the location and impact
of the identified events.
",15.514038796780547,14.648182266009858,205
IMC_17_003.txt,14.01227991581925,12.967566381789442,IMC,10534,"
For years, Internet topology research has been conducted through
active measurement. For instance, Campa builds router level topologies on top of IP level traces obtained with traceroute. The resulting graphs contain a significant amount of nodes with a very large
degree, often exceeding the actual number of interfaces of a router.
Although this property may result from inaccurate alias resolution,
we believe that opaque MPLS clouds made of invisible tunnels are
the main cause. Using Layer-2 technologies such as MPLS, routers
can be configured to hide internal IP hops from traceroute. Consequently, an entry point of an MPLS network appears as the neighbor of all exit points and the whole Layer-3 network turns into a
dense mesh of high degree nodes.

This paper tackles three problems: the revelation of IP hops hidden by MPLS tunnels, the MPLS deployment underestimation, and
the overestimation of high degree nodes. We develop new measurement techniques able to reveal the presence and content of
invisible MPLS tunnels. We assess them through emulation and
cross-validation and perform a large-scale measurement campaign
targeting suspicious networks on which we apply statistical analysis. Finally, based on our dataset, we look at basic graph properties
impacted by invisible tunnels.
",14.554592549557764,13.427492610837444,204
IMC_17_004.txt,16.971025423259473,16.259664867935246,IMC,10089,"
We revisit the use of crowdsourced throughput measurements to infer and localize congestion on end-to-end paths, with particular focus on points of interconnections between ISPs. We analyze three
challenges with this approach. First, accurately identifying which
link on the path is congested requires fine-grained network tomography techniques not supported by existing throughput measurement
platforms. Coarse-grained network tomography can perform this
link identification under certain topological conditions, but we show
that these conditions do not always hold on the global Internet. Second, existing measurement platforms provide limited visibility of
paths to popular web content sources, and only capture a small fraction of interconnections between ISPs. Third, crowdsourcing measurements inherently risks sample bias: using measurements from
volunteers across the Internet leads to uneven distribution of samples across time of day, access link speeds, and home network conditions. Finally, it is not clear how large a drop in throughput to
interpret as evidence of congestion. We investigate these challenges
in detail, and offer guidelines for deployment of measurement infrastructure, strategies, and technologies that can address empirical
gaps in our understanding of congestion on the Internet.
",17.833180683606166,15.93556451612903,187
IMC_17_005.txt,15.631866554716563,14.231253129766014,IMC,10658,"
We develop and validate Internet path measurement techniques to
distinguish congestion experienced when a flow self-induces congestion in the path from when a flow is affected by an already congested path. One application of this technique is for speed tests,
when the user is affected by congestion either in the last mile or
in an interconnect link. This difference is important because in the
latter case, the user is constrained by their service plan (ie., what
they are paying for), and in the former case, they are constrained by
forces outside of their control. We exploit TCP congestion control
dynamics to distinguish these cases for Internet paths that are predominantly TCP traffic. In TCP terms, we re-articulate the question:
was a TCP flow bottlenecked by an already congested (possibly interconnect) link, or did it induce congestion in an otherwise idle
(possibly a last-mile) link?

TCP congestion control affects the round-trip time (RTT) of packets within the flow (i.e., the flow RTT): an endpoint sends packets at higher throughput, increasing the occupancy of the bottleneck buffer, thereby increasing the RTT of packets in the flow. We
show that two simple, statistical metrics derived from the flow RTT
during the slow start period—its coefficient of variation, and the
normalized difference between the maximum and minimum RTT—
can robustly identify which type of congestion the flow encounters.
We use extensive controlled experiments to demonstrate that our
technique works with up to 90% accuracy. We also evaluate our
techniques using two unique real-world datasets of TCP throughput measurements using Measurement Lab data and the Ark platform. We find up to 99% accuracy in detecting self-induced congestion, and up to 85% accuracy in detecting external congestion.
Our results can benefit regulators of interconnection markets, content providers trying to improve customer service, and users trying
to understand whether poor performance is something they can fix
by upgrading their service tier.
",17.74463917768843,16.008977626734637,323
IMC_17_006.txt,15.399644182842135,14.46826764080463,IMC,8946,"
IP traffic with forged source addresses (i.e., spoofed traffic) enables a
series of threats ranging from the impersonation of remote hosts to
massive denial-of-service attacks. Consequently, IP address spoofing received considerable attention with efforts to either suppress
spoofing, to mitigate its consequences, or to actively measure the
ability to spoof in individual networks. However, as of today, we
still lack a comprehensive understanding both of the prevalence
and the characteristics of spoofed traffic “in the wild” as well as of
the networks that inject spoofed traffic into the Internet.

In this paper, we propose and evaluate a method to passively
detect spoofed packets in traffic exchanged between networks in
the inter-domain Internet. Our detection mechanism identifies both
source IP addresses that should never be visible in the inter-domain
Internet (i-e., unrouted and bogon sources) as well as source addresses that should not be sourced by individual networks, as inferred from BGP routing information. We apply our method to
classify the traffic exchanged between more than 700 networks
at a large European IXP. We find that the majority of connected
networks do not, or not consistently, filter their outgoing traffic.
Filtering strategies and contributions of spoofed traffic vary heavily
across networks of different types and sizes. Finally, we study qualitative characteristics of spoofed traffic, regarding both application
popularity as well as structural properties of addresses. Combining
our observations, we identify and study dominant attack patterns.
",16.52667757954773,15.89920083682009,241
IMC_17_007.txt,14.380330808106553,12.790959208191001,IMC,10477,"
Denial-of-Service attacks have rapidly increased in terms of frequency and intensity, steadily becoming one of the biggest threats
to Internet stability and reliability. However, a rigorous comprehensive characterization of this phenomenon, and of countermeasures to mitigate the associated risks, faces many infrastructure
and analytic challenges. We make progress toward this goal, by
introducing and applying a new framework to enable a macroscopic
characterization of attacks, attack targets, and DDoS Protection
Services (DPSs). Our analysis leverages data from four independent global Internet measurement infrastructures over the last two
years: backscatter traffic to a large network telescope; logs from
amplification honeypots; a DNS measurement platform covering
60% of the current namespace; and a DNS-based data set focusing
on DPS adoption. Our results reveal the massive scale of the DoS
problem, including an eye-opening statistic that one-third of all
/24 networks recently estimated to be active on the Internet have
suffered at least one DoS attack over the last two years. We also
discovered that often targets are simultaneously hit by different
types of attacks. In our data, Web servers were the most prominent
attack target; an average of 3% of the Web sites in .com, .net, and
.org were involved with attacks, daily. Finally, we shed light on
factors influencing migration to a DPS.
",18.511140095513987,17.22930299539171,218
IMC_17_008.txt,14.58239860808003,12.946559092451462,IMC,11489,"
Understanding the behaviors of, and evading state-level Internetscale censorship systems such as the Great Firewall (GFW) of China,
has emerged as a research problem of great interest. One line of evasion is the development of techniques that leverage the possibility
that the TCP state maintained on the GFW may not represent the
state at end-hosts. In this paper we undertake, arguably, the most extensive measurement study on TCP-level GFW evasion techniques,
with several vantage points within and outside China, and with
clients subscribed to multiple ISPs. We find that the state-of-the
art evasion techniques are no longer very effective on the GFW.
Our study further reveals that the primary reason that causes these
failures is the evolution of GFW over time. In addition, other factors
such as the presence of middleboxes on the route from the client to
the server also contribute to previously unexpected behaviors.

Our measurement study leads us to new understandings of the
GFW and new evasion techniques. Evaluations of our new evasion strategies show that our new techniques provide much higher
success rates of (compared to prior schemes) ~ 90 % or higher.
Our results further validate our new understandings of the GFW’s
evolved behaviors. We also develop a measurement-driven tool
INTANG, that systematically looks for and finds the best strategy
that works with a server and network path. Our measurements
show that INTANG can yield near perfect evasion rates and is extremely effective in aiding various protocols such as HTTP, DNS
over TCP, and Tor in evading the GFW.
",15.308715981407026,13.63905931905932,260
IMC_17_009.txt,15.306299151145826,14.039399172189839,IMC,10724,"
Middleboxes implement a variety of network management policies
(e.g., prioritizing or blocking traffic) in their networks. While such
policies can be beneficial (e.g., blocking malware) they also raise
issues of network neutrality and freedom of speech when used
for application-specific differentiation and censorship. There is
a poor understanding of how such policies are implemented in
practice, and how they can be evaded efficiently. As a result, most
circumvention solutions are brittle, point solutions based on manual
analysis.

This paper presents the design and implementation of lib-erate,
a tool for automatically identifying middlebox policies, reverseengineering their implementations, and adaptively deploying custom circumvention techniques. Unlike previous work, our approach
is application-agnostic, can be deployed unilaterally (i.e., only at
one endpoint) on unmodified applications via a linked library or
transparent proxy, and can adapt to changes to classifiers at runtime. We implemented a lib-erate prototype as a transparent proxy
and evaluate it both in a testbed environment and in operational
networks that throttle or block traffic based on DPI-based classifier
rules, and show that our approach is effective across a wide range
of middlebox deployments.
",19.142268018852484,18.428571428571427,191
IMC_17_010.txt,15.827007803831094,14.90028568736873,IMC,9707,"
Online advertising is progressively moving towards a programmatic
model in which ads are matched to actual interests of individuals
collected as they browse the web. Letting the huge debate around
privacy aside, a very important question in this area, for which little
is known, is: How much do advertisers pay to reach an individual?

In this study, we develop a first of its kind methodology for
computing exactly that — the price paid for a web user by the ad
ecosystem -— and we do that in real time. Our approach is based
on tapping on the Real Time Bidding (RTB) protocol to collect
cleartext and encrypted prices for winning bids paid by advertisers
in order to place targeted ads. Our main technical contribution is a
method for tallying winning bids even when they are encrypted.
We achieve this by training a model using as ground truth prices
obtained by running our own “probe” ad-campaigns. We design our
methodology through a browser extension and a back-end server
that provides it with fresh models for encrypted bids. We validate
our methodology using a one year long trace of 1600 mobile users
and demonstrate that it can estimate a user’s advertising worth
with more than 82% accuracy.
",14.731742533061166,13.801798780487804,207
IMC_17_011.txt,16.496283356255923,14.66748430547548,IMC,7801,"
Most search engines generate significant revenue through search
advertising, wherein advertisements are served alongside traditional
search results. These advertisements are attractive to advertisers
because ads can be targeted and prominently presented to users at
the exact moment that the user is searching for relevant topics.

Deceptive advertising is harmful to all legitimate actors in the
search ad ecosystem: Users are less likely to find what they are
looking for and may lose trust in ads or the search engine, advertisers
lose potential revenue and face unfair competition from advertisers
who are not playing by the rules, and the search engine’s ecosystem
suffers when both users and advertisers are unhappy.

This paper explores search advertiser fraud on Microsoft’s Bing
search engine platform. We characterize three areas: the scale of
search advertiser fraud, the targeting and bidding behavior of fraudulent advertisers, and how fraudulent advertisers impact other advertisers in the ecosystem.
",19.083932058031824,17.968912751677852,152
IMC_17_012.txt,14.034151083348249,12.514291725105192,IMC,9140,"
The increasing popularity of adblockers has prompted online publishers to retaliate against adblock users by deploying anti-adblock
scripts, which detect adblock users and bar them from accessing
content unless they disable their adblocker. To circumvent antiadblockers, adblockers rely on manually curated anti-adblock filter
lists for removing anti-adblock scripts. Anti-adblock filter lists currently rely on informal crowdsourced feedback from users to add/remove filter list rules. In this paper, we present the first comprehensive study of anti-adblock filter lists to analyze their effectiveness
against anti-adblockers. Specifically, we compare and contrast the
evolution of two popular anti-adblock filter lists. We show that
these filter lists are implemented very differently even though they
currently have a comparable number of filter list rules. We then use
the Internet Archive’s Wayback Machine to conduct a retrospective coverage analysis of these filter lists on Alexa top-5K websites
over the span of last five years. We find that the coverage of these
filter lists has considerably improved since 2014 and they detect
anti-adblockers on about 9% of Alexa top-5K websites. To improve
filter list coverage and speedup addition of new filter rules, we
also design and implement a machine learning based method to
automatically detect anti-adblock scripts using static JavaScript
code analysis.
",15.903189008614273,15.239444444444448,218
IMC_17_013.txt,15.736899029732903,14.388408933321223,IMC,9301,"
The impressive growth of the mobile Internet has motivated several
industry reports retelling the story in terms of number of devices
or subscriptions sold per regions, or the increase in mobile traffic,
both WiFi and cellular. Yet, despite the abundance of such reports,
we still lack an understanding of the impact of cellular networks
around the world.

We present the first comprehensive analysis of global cellular
networks. We describe an approach to accurately identify cellular network IP addresses using the Network Information API, a
non-standard Javascript API in several mobile browsers, and show
its effectiveness in a range cellular network configurations. We
combine this approach with the vantage point of one of the world’s
largest CDNs, with servers located in 1,450 networks and clients distributed across across 245 countries, to characterize cellular access
around the globe.

We find that the majority of cellular networks exist as mixed
networks (i.e., networks that share both fixed-line and cellular devices), requiring prefix — not ASN - level identification. We discover
over 350 thousand /24 and 23 thousand /48 cellular IPv4 and IPv6
prefixes respectively. By utilizing addresses level traffic from the
same CDN, we calculate the fraction of traffic coming from cellular
addresses. Overall we find that cellular traffic comprises 16.2% of
the CDN’s global traffic, and that cellular traffic ranges widely in
importance between countries, from capturing nearly 96% of all
traffic in Ghana to just 12.1% in France.
",16.860833078287435,15.840337552742614,244
IMC_17_014.txt,14.230795022380239,12.545826749210477,IMC,10114,"
Devices implementing newer wireless standards continue to displace
older wireless technology. As 802.1lac access points (APs) are
rapidly adopted in enterprise environments, new challenges arise.
This paper first presents an overview of trends in enterprise wireless
networks based on a large-scale measurement study, in which we
collect data from an anonymous subset of millions of radio access
points in hundreds of thousands of real-world deployments. Based on
the observed data and our experience deploying wireless networks
at scale, we then propose two techniques that we have implemented
in Meraki APs to improve both overall network capacity and performance perceived by end users: (i) a dynamic channel assignment
algorithm, TurboCA, that adjusts to frequent RF condition changes,
and (ii) a novel approach, FastACK, that improves the end-to-end performance of TCP traversing high-throughput wireless links. Finally,
we evaluate TurboCA with metrics taken from a variety of real-world
networks and evaluate TCP performance of FastACK with extensive
testbed experiments.
",18.669449996058646,18.460814814814814,164
IMC_17_015.txt,15.362945154245843,13.399768637593699,IMC,11229,"
HTTP Adaptive Streaming (HAS) has emerged as the predominant
technique for transmitting video over cellular for most content
providers today. While mobile video streaming is extremely popular, delivering good streaming experience over cellular networks
is technically very challenging, and involves complex interacting
factors. We conduct a detailed measurement study of a wide crosssection of popular streaming video-on-demand (VOD) services to
develop a holistic understanding of these services’ design and performance. We identify performance issues and develop effective
practical best practice solutions to mitigate these challenges. By extending the understanding of how different, potentially interacting
components of service design impact performance, our findings can
help developers build streaming services with better performance.
",20.075844112070616,18.599221238938053,114
IMC_17_016.txt,15.306518429757684,13.547320141079634,IMC,8201,"
Fast IPv4 scanning has enabled researchers to answer a wealth
of new security and measurement questions. However, while increased network speeds and computational power have enabled
comprehensive scans of the IPv4 address space, a brute-force approach does not scale to IPv6. Systems are limited to scanning a
small fraction of the IPv6 address space and require an algorithmic
approach to determine a small set of candidate addresses to probe.
In this paper, we first explore the considerations that guide designing such algorithms. We introduce a new approach that identifies
dense address space regions from a set of known “seed” addresses
and generates a set of candidates to scan. We compare our algorithm
6Gen against Entropy/IP—the current state of the art—finding that
we can recover between 1-8 times as many addresses for the five
candidate datasets considered in the prior work. However, during
our analysis, we uncover widespread IP aliasing in IPv6 networks.
We discuss its effect on target generation and explore preliminary
approaches for detecting aliased regions.
",15.579741850924794,13.37639534883721,173
IMC_17_017.txt,14.510307528065958,12.737275851432916,IMC,10531,"
Estimating frequencies of items over data streams is a common
building block in streaming data measurement and analysis. Misra
and Gries introduced their seminal algorithm for the problem in
1982, and the problem has since been revisited many times due
its practicality and applicability. We describe a highly optimized
version of Misra and Gries’ algorithm that is suitable for deployment in industrial settings. Our code is made public via an open
source library called Data Sketches that is already used by several
companies and production systems.

Our algorithm improves on two theoretical and practical aspects
of prior work. First, it handles weighted updates in amortized constant time, a common requirement in practice. Second, it uses a
simple and fast method for merging summaries that asymptotically
improves on prior work even for unweighted streams. We describe
experiments confirming that our algorithms are more efficient than
prior proposals.
",14.731742533061166,13.834349315068494,147
IMC_17_018.txt,14.877130331364771,13.293491560768416,IMC,9382,"
Google’s QUIC protocol, which implements TCP-like properties at
the application layer atop a UDP transport, is now used by the vast
majority of Chrome clients accessing Google properties but has
no formal state machine specification, limited analysis, and ad-hoc
evaluations based on snapshots of the protocol implementation in
a small number of environments. Further frustrating attempts to
evaluate QUIC is the fact that the protocol is under rapid development, with extensive rewriting of the protocol occurring over the
scale of months, making individual studies of the protocol obsolete
before publication.

Given this unique scenario, there is a need for alternative techniques for understanding and evaluating QUIC when compared
with previous transport-layer protocols. First, we develop an approach that allows us to conduct analysis across multiple versions
of QUIC to understand how code changes impact protocol effectiveness. Next, we instrument the source code to infer QUIC’s state
machine from execution traces. With this model, we run QUIC in
a large number of environments that include desktop and mobile,
wired and wireless environments and use the state machine to
understand differences in transport- and application-layer performance across multiple versions of QUIC and in different environments. QUIC generally outperforms TCP, but we also identified
performance issues related to window sizes, re-ordered packets,
and multiplexing large number of small objects; further, we identify
that QUIC’s performance diminishes on mobile devices and over
cellular networks.
",19.854308518879517,19.897750759878424,239
IMC_17_019.txt,14.742555808971034,12.939749298737727,IMC,10724,"
Driven by CA compromises and the risk of man-in-the-middle
attacks, new security features have been added to TLS,
HTTPS, and the web PKI over the past five years. These
include Certificate Transparency (CT), for making the CA
system auditable; HSTS and HPKP headers, to harden the
HTTPS posture of a domain; the DNS-based extensions CAA
and TLSA, for control over certificate issuance and pinning;
and SCSV, for protocol downgrade protection.

This paper presents the first large scale investigation of
these improvements to the HTTPS ecosystem, explicitly accounting for their combined usage. In addition to collecting
passive measurements at the Internet uplinks of large University networks on three continents, we perform the largest
domain-based active Internet scan to date, covering 193M
domains. Furthermore, we track the long-term deployment
history of new TLS security features by leveraging passive
observations dating back to 2012.

We find that while deployment of new security features has
picked up in general, only SCSV (49M domains) and CT (7M
domains) have gained enough momentum to improve the overall security of HTTPS. Features with higher complexity, such
as HPKP, are deployed scarcely and often incorrectly. Our empirical findings are placed in the context of risk, deployment
effort, and benefit of these new technologies, and actionable
steps for improvement are proposed. We cross-correlate use
of features and find some techniques with significant correlation in deployment. We support reproducible research and
publicly release data and code.
",15.903189008614273,14.37918106995885,244
IMC_17_020.txt,15.302418726914109,13.768160871128824,IMC,11314,"
Password reuse has been long understood as a problem: credentials
stolen from one site may be leveraged to gain access to another site
for which they share a password. Indeed, it is broadly understood
that attackers exploit this fact and routinely leverage credentials extracted from a site they have breached to access high-value accounts
at other sites (e.g., email accounts). However, as a consequence
of such acts, this same phenomena of password reuse attacks can
be harnessed to indirectly infer site compromises—even those that
would otherwise be unknown. In this paper we describe such a measurement technique, in which unique honey accounts are registered
with individual third-party websites, and thus access to an email
account provides indirect evidence of credentials theft at the corresponding website. We describe a prototype system, called Tripwire,
that implements this technique using an automated Web account
registration system combined with email account access data from a
major email provider. In a pilot study monitoring more than 2,300
sites over a year, we have detected 19 site compromises, including
what appears to be a plaintext password compromise at an Alexa
top-500 site with more than 45 million active users.
",16.92669308720184,17.221091370558373,200
IMC_17_021.txt,14.994964570270469,13.859561355311357,IMC,8788,"
We uncover a thriving ecosystem of large-scale reputation manipulation services on Facebook that leverage the principle of collusion.
Collusion networks collect OAuth access tokens from colluding members and abuse them to provide fake likes or comments to their
members. We carry out a comprehensive measurement study to
understand how these collusion networks exploit popular thirdparty Facebook applications with weak security settings to retrieve
OAuth access tokens. We infiltrate popular collusion networks
using honeypots and identify more than one million colluding Facebook accounts by “milking” these collusion networks. We disclose
our findings to Facebook and collaborate with them to implement a
series of countermeasures that mitigate OAuth access token abuse
without sacrificing application platform usability for third-party
developers. These countermeasures remained in place until April
2017, after which Facebook implemented a set of unrelated changes
in its infrastructure to counter collusion networks. We are the first
to report and effectively mitigate large-scale OAuth access token
abuse in the wild.
",16.785175570968402,16.173788819875778,162
IMC_17_022.txt,15.807176795471939,14.149274099453397,IMC,10573,"
The Domain Name System (DNS) provides a scalable, flexible name
resolution service. Unfortunately, its unauthenticated architecture
has become the basis for many security attacks. To address this, DNS
Security Extensions (DNSSEC) were introduced in 1997. DNSSEC’s
deployment requires support from the top-level domain (TLD) registries and registrars, as well as participation by the organization that
serves as the DNS operator. Unfortunately, DNSSEC has seen poor
deployment thus far: despite being proposed nearly two decades ago,
only 1% of .com, .net, and .org domains are properly signed.

In this paper, we investigate the underlying reasons why DNSSEC
adoption has been remarkably slow. We focus on registrars, as most
TLD registries already support DNSSEC and registrars often serve
as DNS operators for their customers. Our study uses large-scale,
longitudinal DNS measurements to study DNSSEC adoption, coupled with experiences collected by trying to deploy DNSSEC on
domains we purchased from leading domain name registrars and
resellers. Overall, we find that a select few registrars are responsible
for the (small) DNSSEC deployment today, and that many leading
registrars do not support DNSSEC at all, or require customers to take
cumbersome steps to deploy DNSSEC. Further frustrating deployment, many of the mechanisms for conveying DNSSEC information
to registrars are error-prone or present security vulnerabilities. Finally, we find that using DNSSEC with third-party DNS operators
such as Cloudflare requires the domain owner to take a number of
steps that 40% of domain owners do not complete. Having identified
several operational challenges for full DNSSEC deployment, we
make recommendations to improve adoption.
",16.92669308720184,15.242944015444017,261
IMC_17_023.txt,15.649916625154248,13.862910500322254,IMC,8092,"
Machine learning classifiers are basic research tools used in numerous types of network analysis and modeling. To reduce the need for
domain expertise and costs of running local ML classifiers, network
researchers can instead rely on centralized Machine Learning as a
Service (MLaaS) platforms.

In this paper, we evaluate the effectiveness of MLaaS systems
ranging from fully-automated, turnkey systems to fully-customizable
systems, and find that with more user control comes greater risk.
Good decisions produce even higher performance, and poor decisions result in harsher performance penalties. We also find that
server side optimizations help fully-automated systems outperform
default settings on competitors, but still lag far behind well-tuned
MLaaS systems which compare favorably to standalone ML libraries.
Finally, we find classifier choice is the dominating factor in determining model performance, and that users can approximate the
performance of an optimal classifier choice by experimenting with
a small subset of random classifiers. While network researchers
should approach MLaaS systems with caution, they can achieve
results comparable to standalone classifiers if they have sufficient
insight into key decisions like classifiers and feature selection.
",17.28802050969988,16.658167701863352,185
IMC_17_024.txt,14.847965559430229,13.529724839231733,IMC,10538,"
As the number and the diversity of news outlets on the Web grows,
so does the opportunity for “alternative” sources of information
to emerge. Using large social networks like Twitter and Facebook,
misleading, false, or agenda-driven information can quickly and
seamlessly spread online, deceiving people or influencing their
opinions. Also, the increased engagement of tightly knit communities, such as Reddit and 4chan, further compounds the problem,
as their users initiate and propagate alternative information, not
only within their own communities, but also to different ones as
well as various social media. In fact, these platforms have become
an important piece of the modern information ecosystem, which,
thus far, has not been studied as a whole.

In this paper, we begin to fill this gap by studying mainstream and
alternative news shared on Twitter, Reddit, and 4chan. By analyzing
millions of posts around several axes, we measure how mainstream
and alternative news flows between these platforms. Our results
indicate that alt-right communities within 4chan and Reddit can
have a surprising level of influence on Twitter, providing evidence
that “fringe” communities often succeed in spreading alternative
news to mainstream social networks and the greater Web.
",16.26309291913925,16.03018315018315,196
IMC_17_025.txt,14.722376749858796,13.270290837299076,IMC,10460,"
While website domain typosquatting is highly annoying for legitimate domain operators, research has found that it relatively
rarely presents a great risk to individual users. However, any application (e.g., email, ftp,...) relying on the domain name system for
name resolution is equally vulnerable to domain typosquatting, and
consequences may be more dire than with website typosquatting.

This paper presents the first in-depth measurement study of
email typosquatting. Working in concert with our IRB, we registered 76 typosquatting domain names to study a wide variety of user
mistakes, while minimizing the amount of personal information exposed to us. In the span of over seven months, we received millions
of emails at our registered domains. While most of these emails
are spam, we infer, from our measurements, that every year, three
of our domains should receive approximately 3,585 “legitimate”
emails meant for somebody else. Worse, we find, by examining a
small sample of all emails, that these emails may contain sensitive
information (e.g., visa documents or medical records).

We then project from our measurements that 1,211 typosquatting
domains registered by unknown entities receive in the vicinity
of 800,000 emails a year. Furthermore, we find that millions of
registered typosquatting domains have MX records pointing to only
a handful of mail servers. However, a second experiment in which
we send “honey emails” to typosquatting domains only shows very
limited evidence of attempts at credential theft (despite some emails
being read), meaning that the threat, for now, appears to remain
theoretical.
",15.903189008614273,15.493290322580645,254
IMC_17_026.txt,14.508214597708285,13.34175050903237,IMC,9407,"
Doxing is online abuse where a malicious party harms another
by releasing identifying or sensitive information. Motivations for
doxing include personal, competitive, and political reasons, and
web users of all ages, genders and internet experience have been
targeted. Existing research on doxing is primarily qualitative. This
work improves our understanding of doxing by being the first to
take a quantitative approach. We do so by designing and deploying
a tool which can detect dox files and measure the frequency, content,
targets, and effects of doxing on popular dox-posting sites.

This work analyzes over 1.7 million text files posted to past ebin.com, 4chan.org and 8ch.net, sites frequently used to
share doxes online, over a combined period of approximately thirteen weeks. Notable findings in this work include that approximately 0.3% of shared files are doxes, that online social networking
accounts mentioned in these dox files are more likely to close than
typical accounts, that justice and revenge are the most often cited
motivations for doxing, and that dox files target males more frequently than females.

We also find that recent anti-abuse efforts by social networks
have reduced how frequently these doxing victims closed or restricted their accounts after being attacked. We also propose mitigation steps, such a service that can inform people when their
accounts have been shared in a dox file, or law enforcement notification tools to inform authorities when individuals are at heightened
risk of abuse.
",15.172626615295595,15.199803921568634,244
IMC_17_027.txt,15.882057333254185,14.427508881202836,IMC,11285,"
We evaluate the use of data obtained by illicit means against
a broad set of ethical and legal issues. Our analysis covers
both the direct collection, and secondary uses of, data obtained via illicit means such as exploiting a vulnerability, or
unauthorized disclosure. We extract ethical principles from
existing advice and guidance and analyse how they have been
applied within more than 20 recent peer reviewed papers that
deal with illicitly obtained datasets. We find that existing
advice and guidance does not address all of the problems that
researchers have faced and explain how the papers tackle
ethical issues inconsistently, and sometimes not at all. Our
analysis reveals not only a lack of application of safeguards
but also that legitimate ethical justifications for research are
being overlooked. In many cases positive benefits, as well as
potential harms, remain entirely unidentified. Few papers
record explicit Research Ethics Board (REB) approval for
the activity that is described and the justifications given for
exemption suggest deficiencies in the REB process.
",16.439396014739867,15.901112061591103,168
IMC_17_028.txt,14.968090739735256,13.055333386628515,IMC,8177,"
IP anycast provides DNS operators and CDNs with automatic failover and reduced latency by breaking the Internet into catchments,
each served by a different anycast site. Unfortunately, understanding and predicting changes to catchments as anycast sites are added
or removed has been challenging. Current tools such as RIPE Atlas
or commercial equivalents map from thousands of vantage points
(VPs), but their coverage can be inconsistent around the globe.
This paper proposes Verfploeter, a new method that maps anycast
catchments using active probing. Verfploeter provides around 3.8M
passive VPs, 430× the 9k physical VPs in RIPE Atlas, providing
coverage of the vast majority of networks around the globe. We
then add load information from prior service logs to provide calibrated predictions of anycast changes. Verfploeter has been used to
evaluate the new anycast deployment for B-Root, and we also report its use of a nine-site anycast testbed. We show that the greater
coverage made possible by Verfploeter’s active probing is necessary
to see routing differences in regions that have sparse coverage from
RIPE Atlas, like South America and China.
",16.218646115125612,14.095628453038675,183
IPDPS_17_001.txt,13.855498367703632,12.220704227399725,IPDPS,7069,"
The following is a very common question in
numerous theoretical and application-related domains: given
a graph G, does it satisfy some given property? For example,
is G connected? Is its diameter smaller than a given threshold?
Is its average degree larger than a certain threshold?

Traditionally, algorithms to quickly answer such questions
were developed for static and centralized graphs (i.e. G is stored
in a central server and the list of its vertices and edges is
static and quickly accessible). Later, as dictated by practical
considerations, a great deal of attention was given to on-line
algorithms for dynamic graphs (where vertices and edges can
be added and deleted); the focus of research was to quickly
decide whether the new graph still satisfies the given property.

Today, a more difficult version of this problem, referred to
as the distributed monitoring problem, is becoming increasingly
important: large graphs are not only dynamic, but also distributed, that is, G is partitioned between a few servers, none
of which ""sees"" G in its entirety. The question is how to define
local conditions, such that as long as they hold on the local
graphs, it is guaranteed that the desired property holds for the
global G. Such local conditions are crucial for avoiding a huge
communication overhead.

While defining local conditions for linear properties (e.g.
average degree) is relatively easy, they are considerably more
difficult to derive for non-linear functions over graphs. We propose a solution and a general definition of solution optimality,
and demonstrate how to apply it to two important graph properties — the spectral gap and the number of triangles. We also
define an absolute lower bound on the communication overhead
for distributed monitoring, and compare our algorithm to it,
with excellent results. Last but not least, performance improves
as the graph becomes larger and denser — that is, when
distributing it is more important.

",16.666482175067898,14.837006369426756,317
IPDPS_17_002.txt,14.357246225946334,12.459887318359335,IPDPS,8031,"
A Suffix tree is a fundamental and versatile string
data structure that is frequently used in important application
areas such as text processing, information retrieval, and computational biology. Sequentially, the construction of suffix trees takes
linear time, and optimal parallel algorithms exist only for the
PRAM model. Recent works mostly target low core-count sharedmemory implementations but achieve suboptimal complexity, and
prior distributed-memory parallel algorithms have quadratic
worst-case complexity. Suffix trees can be constructed from
suffix and longest common prefix (LCP) arrays by solving the
All-Nearest-Smaller-Values (ANSV) problem. In this paper, we
formulate a more generalized version of the ANSV problem, and
present a distributed-memory parallel algorithm for solving it in
O(2 +p) time. Our algorithm minimizes the overall and per-node
communication volume. Building on this, we present a parallel
algorithm for constructing a distributed representation of suffix
trees, yielding both superior theoretical complexity and better
practical performance compared to previous distributed-memory
algorithms. We demonstrate the construction of the suffix tree
for the human genome given its suffix and LCP arrays in under
2 seconds on 1024 Intel Xeon cores.

",17.971250198000288,17.086184210526316,190
IPDPS_17_003.txt,15.02615483916729,13.165485285519392,IPDPS,5732,"
Ordering vertices of a graph is key to minimize
fill-in and data structure size in sparse direct solvers, maximize
locality in iterative solvers, and improve performance in graph
algorithms. Except for naturally parallelizable ordering methods
such as nested dissection, many important ordering methods have
not been efficiently mapped to distributed-memory architectures.
In this paper, we present the first-ever distributed-memory
implementation of the reverse Cuthill-McKee (RCM) algorithm
for reducing the profile of a sparse matrix. Our parallelization
uses a two-dimensional sparse matrix decomposition. We achieve
high performance by decomposing the problem into a small
number of primitives and utilizing optimized implementations of
these primitives. Our implementation attains up to 38x speedup
on matrices from various applications on 1024 cores of a Cray
XC30 supercomputer and shows strong scaling up to 4096 cores
for larger matrices.

",17.693802365651003,17.194285714285716,141
IPDPS_17_004.txt,13.869797911584097,11.602041343669253,IPDPS,7708,"
Vectorization and GPUs will profoundly change
graph processing. Traditional graph algorithms tuned for
32- or 64-bit based memory accesses will be inefficient on
architectures with 512-bit wide (or larger) instruction units
that are already present in the Intel Knights Landing (KNL)
manycore CPU. Anticipating this shift, we propose SlimSell: a
vectorizable graph representation to accelerate Breadth-First
Search (BFS) based on sparse-matrix dense-vector (SpMV)
products. SlimSell extends and combines the state-of-the-art
SIMD-friendly Sell-C-c matrix storage format with tropical,
real, boolean, and sel-max semiring operations. The resulting
design reduces the necessary storage (by up to 50%) and thus
pressure on the memory subsystem. We augment SlimSell with
the Slim Work and SlimChunk schemes that reduce the amount
of work and improve load balance, further accelerating BFS.
We evaluate all the schemes on Intel Haswell multicore CPUs,
the state-of-the-art Intel Xeon Phi KNL manycore CPUs, and
NVIDIA Tesla GPUs. Our experiments indicate which semiring
offers highest speedups for BFS and illustrate that SlimSell
accelerates a tuned Graph500 BFS code by up to 33%. This
work shows that vectorization can secure high-performance in
BFS based on SpMV products; the proposed principles and
designs can be extended to other graph algorithms.

",15.172626615295595,13.690062893081762,213
IPDPS_17_005.txt,15.304828249441808,13.770205703399949,IPDPS,6059,"
Computer architectures continue to develop rapidly towards massively parallel and heterogeneous sys- tems. Thus, easily extensible yet highly efficient parallelization approaches for a variety of platforms are urgently needed. In this paper, we present SWhybrid, a hybrid computing framework for large-scale biological sequence database search on heterogeneous computing environments with multi-core or many-core processing units (PUs) based on the Smith- Waterman (SW) algorithm. To incorporate a diverse set of PUs such as combinations of CPUs, GPUs and Xeon Phis, we abstract them as SIMD vector execution units with different number of lanes. We propose a machine model, associated with a unified programming interface implemented in C++, to abstract underlying architectural differences. Performance evaluation reveals that SWhybrid (i) outperforms all other tested state-of-the-art tools on both homogeneous and heteroge- neous computing platforms, (ii) achieves an efficiency of over 80% on all tested CPUs and GPUs and over 70% on Xeon Phis, and (iii) achieves utlization rates of over 80% on all tested heterogeneous platforms. Our results demonstrate that there is enough commonality between vector-like instructions across CPUs and GPUs that one can develop higher-level abstractions and still specialize with close-to-peak performance. SWhybrid is open-source software and freely available at https://github.com/turbo0628/swhybrid.
",17.971250198000288,17.39877358490566,215
IPDPS_17_006.txt,15.07506206788188,13.73295729513205,IPDPS,5219,"
The progress of next-generation sequencing has a major impact on medical and genomic research. This technol- ogy can now produce billions of short DNA fragments (reads) in a single run. One of the most demanding computational prob- lems used by almost every sequencing pipeline is short-read alignment; i.e. determining where each fragment originated from in the original genome. Most current solutions are based on a seed-and-extend approach, where promising candidate regions (seeds) are first identified and subsequently extended in order to verify whether a full high-scoring alignment actually exists in the vicinity of each seed. Seed verification is the main bottleneck in many state-of-the-art aligners and thus finding fast solutions is of high importance. We present a parallel ungapped-alignment-featured seed verification (PUNAS) algo- rithm, a fast filter for effectively removing the majority of false positive seeds, thus significantly accelerating the short-read alignment process. PUNAS is based on bit-parallelism and takes advantage of SIMD vector units of modern microprocessors. Our implementation employs a vectorize-and-scale approach supporting multi-core CPUs and many-core Knights Landing (KNL)-based Xeon Phi processors. Performance evaluation reveals that PUNAS is over three orders-of-magnitude faster than seed verification with the Smith-Waterman algorithm and around one order-of-magnitude faster than seed verification with the banded version of Myers bit-vector algorithm. Using a single thread it achieves a speedup of up to 7.3, 27.1, and 11.6 compared to the shifted Hamming distance filter on a SSE, AVX2, and AVX-512 based CPU/KNL, respectively. The speed of our framework further scales almost linearly with the number of cores. ",16.24694786949722,15.446939175931984,283
IPDPS_17_007.txt,15.806779411594135,14.453663371912928,IPDPS,6812,"
Finding regions of local similarity between biological sequences is a fundamental task in computational biology.
BLAST is the most widely-used tool for this purpose, but
it suffers from irregularities due to its heuristic nature. To
achieve fast search, recent approaches construct the index from
the database instead of the input query. However, database
indexing introduces more challenges in the design of index
structure and algorithm, especially for data access through the
memory hierarchy on modern multicore processors.

In this paper, based on existing heuristic algorithms, we
design and develop a database indexed BLAST with the identical sensitivity as query indexed BLAST (i.e., NCBI-BLAST).
Then, we identify that existing heuristic algorithms of BLAST
can result in serious irregularities in database indexed search.
To eliminate irregularities in BLAST algorithm, we propose
muBLASTP, that uses multiple optimizations to improve data
locality and parallel efficiency for multicore architectures and
multi-node systems. Experiments on a single node demonstrate
up to a 5.1-fold speedup over the multi-threaded NCBI BLAST.
For the inter-node parallelism, we achieve nearly linear scaling
on up to 128 nodes and gain up to 8.9-fold speedup over
mpiBLAST.

",17.267425705330172,16.878333333333334,196
IPDPS_17_008.txt,15.306067950941681,13.153122109158186,IPDPS,2750,"
Data movement is increasingly becoming the bottleneck of both performance and energy efficiency in modern
computation. Until recently, it was the case that there is
limited freedom for communication optimization on GPUs,
as conventional GPUs only provide two types of methods for
inter-thread communication: using shared memory or global
memory. However, a new warp shuffle instruction has been
introduced since the Kepler architecture on Nvidia GPUs,
which enables threads within the same warp to directly
exchange data in registers. This brought new performance
optimization opportunities for algorithms with intensive interthread communication. In this work, we deploy register shuffle
in the application domain of sequence alignment (or similarly,
string matching), and conduct a quantitative analysis of the
opportunities and limitations of using register shuffle. We
select two sequence alignment algorithms, Smith-Waterman
(SW) and Pairwise-Hidden-Markov-Model (PairHMM), from
the widely used Genome Analysis Toolkit (GATK) as case
studies. Compared to implementations using shared memory,
we obtain a significant speed-up of 1.2x and 2.1x by using
shuffle instructions for SW and PairHMM. Furthermore,
we develop a performance model for analyzing the kernel
performance based on the measured shuffle latency from a suite
of microbenchmarks. Our model provides valuable insights for
CUDA programmers into how to best use shuffle instructions
for performance optimization.

",17.755912252390015,16.621651090342684,217
IPDPS_17_009.txt,14.54546310938678,12.106387740833501,IPDPS,7171,"
GPUs provide high-bandwidth/low-latency on-chip
shared memory and L1 cache to efficiently service a large
number of concurrent memory requests (to contiguous memory
space). To support warp-wide accesses to L1 cache, GPU L1
cache lines are very wide. However, such L1 cache architecture
cannot always be efficiently utilized when applications generate
many memory requests with irregular access patterns especially due to branch and memory divergences. In this paper,
we propose Elastic-Cache that can efficiently support
both fine- and coarse-grained L1 cache-line management for
applications with both regular and irregular memory access
patterns. Specifically, it can store 32- or 64-byte words in
non-contiguous memory space to a single 128-byte cache line.
Furthermore, it neither requires an extra tag storage structure
nor reduces the capacity of L1 cache since it stores auxiliary
tags for fine-grained L1 cache-line managements in sharedmemory space that is not fully used in many applications.
Our experiment shows that Elastic-Cache improves the
geo-mean performance of applications with irregular memory
access patterns by 58% without degrading performance of
applications with regular memory access patterns.
",18.08858127442927,16.10507936507937,190
IPDPS_17_010.txt,14.331893378675431,12.366508171600145,IPDPS,6343,"
Spin-Transfer Torque Magnetoresistive RandomAccess Memory (STT-MRAM) is a promising memory
technology, which has high density, fast read speed, low
leakage power, and non-volatility, and is suitable for multi-core
on-chip last-level caches. However, the high write energy and
latency, as well as less-than-desirable write endurance of STTMRAM remain challenges. This paper proposes a new encoded
content-aware cache replacement policy to reduce the total
switch bits for write, lower the write energy, and improve write
endurance. Instead of replacing the LRU block under the
conventional pseudo-LRU replacement policy, we select a
replacement block near the LRU position, which has the most
similar content to the missed block. The selected replacement
block can reduce the switch bits without damaging the cache
performance. To avoid fetching and comparing the entire block
contents, we present a novel content encoding method to
encode 64-byte block using just 8 bits, each bit represents 8byte content. The encoded bit is determined by the presence of
a dominant bit value in the 8 bytes. We measure the content
similarity using the Hamming distance between the encoded
bits of the missed block and the replaced block. Performance
evaluation demonstrates that the proposed simple content
encoding method is effective with an average of 20.5%
reduction in total switch bits, which results in improvement on
write endurance and less write energy consumption. These
improvements are accomplished with low overhead and
minimum impact on the cache performance.
",16.76809479433877,14.24173469387755,247
IPDPS_17_011.txt,12.978205425344882,10.56064706564154,IPDPS,7072,"
This paper proposes a new SSD cache architecture,
DEFT-cache, Delayed Erasing and Fast Taping, that maximizes
I/O performance and reliability of RAID storage. First of all,
DEFT-Cache exploits the inherent physical properties of flash
memory SSD by making use of old data that have been overwritten but still in existence in SSD to minimize small write penalty of
RAID5/6. As data pages being overwritten in SSD, old data pages
are invalidated and become candidates for erasure and garbage
collections. Our idea is to selectively delay the erasure of the
pages and let these otherwise useless old data in SSD contribute
to I/O performance for parity computations upon write I/Os.
Secondly, DEFT-Cache provides inexpensive redundancy to the
SSD cache by having one physical SSD and one virtual SSD
as a mirror cache. The virtual SSD is implemented on HDD
but using log-structured data layout, i.e. write data are quickly
logged to HDD using sequential write. The dual and redundant
caches provide a cost-effective and highly reliable write-back
SSD cache. We have implemented DEFT-Cache on Linux system.
Extensive experiments have been carried out to evaluate the
potential benefits of our new techniques. Experimental results on
SPC and Microsoft traces have shown that DEFT-Cache improves
I/O performance by 26.81% to 56.26% in terms of average user
response time. The virtual SSD mirror cache can absorb write
I/Os as fast as physical SSD providing the same reliability as two
physical SSD caches without noticeable performance loss.
",15.903189008614273,13.813707386363635,260
IPDPS_17_012.txt,12.470285537902623,9.88856853995259,IPDPS,7059,"
Non-volatile main memory (NVRAM) enables
data persistence in memory. However, the existence of transient
CPU caches in modern computer architectures brings a serious
performance issue. In particular, cache lines have to be flushed
frequently to guarantee consistent persistent program states.
Hence, persistence and performance cannot be easily obtained
simultaneously.

In this paper, we optimize data persistence by proposing a
software cache. The software cache first buffers lines that need
to be flushed, and then flushes them out at an appropriate later
time. The software cache aims to maximize the combination
of cache line flushes. We designed a new linear-time algorithm
to calculate cache miss ratio curve (MRC) so as to adaptively
select the best cache capacity at run-time based on program
behavior. We evaluated the software cache on a real-world
memory-based database benchmark, the SPLASH2 benchmark
suite and four micro-benchmarks. Results indicate that the
software cache solution reduces cache write backs to persistent
memory by 12x and improves performance over the state-ofthe-art methods by 2.1 x on average, measured on a real system
emulator.

",15.247664890283005,12.190417582417584,184
IPDPS_17_013.txt,15.377788081408546,13.61843697243333,IPDPS,4189,"
With the explosive growth of smartphones and
cloud computing, mobile cloud, which leverages cloud resource
to boost the performance of mobile applications, becomes attractive. Many efforts have been made to improve the performance
and reduce energy consumption of mobile devices by offloading
computational codes to the cloud. However, the offloading cost
caused by the cloud platform has been ignored for many years.
In this paper, we propose Rattrap, a lightweight cloud platform
which improves the offloading performance from cloud side. To
achieve such goals, we analyze the characteristics of typical offloading workloads and design our platform solution accordingly.
Rattrap develops a new runtime environment, Cloud Android
Container, for mobile computation offloading, replacing heavyweight virtual machines (VMs). Our design exploits the idea
of running operating systems with differential kernel features
inside containers with driver extensions, which partially breaks
the limitation of OS-level virtualization. With proposed resource
sharing and code cache mechanism, Rattrap fundamentally
improves the offloading performance. Our evaluation shows
that Rattrap not only reduces the startup time of runtime
environments and shows an average speedup of 16x, but also
saves a large amount of system resources such as 75% memory
footprint and at least 79% disk capacity. Moreover, Rattrap
improves offloading response by as high as 63% over the cloud
platform based on VM, and thus saving the battery life.

",16.52667757954773,14.86700904977376,222
IPDPS_17_014.txt,14.454086463905522,12.677028920535484,IPDPS,6369,"
Abstract— Resource management of modern datacenters
needs to consider multiple competing objectives that involve
complex system interactions. In this work, Linear Temporal
Logic (LTL) is adopted in describing such interactions by
leveraging its ability to express complex properties. Further,
LTL-based constraints are integrated with reinforcement learning according the recent progress on control synthesis theory.
The LTL-constrained reinforcement learning facilitates desired
balance among the competing objectives in managing resources
for datacenters. The effectiveness of this new approach is
demonstrated by two scenarios. In datacenter power management, the LTL-constrained manager reaches the best balance
among power, performance and battery stress compared to
the previous work and other alternative approaches. In multitenant job scheduling, 200 MapReduce jobs are emulated on the
Amazon AWS cloud. The LTL-constrained scheduler achieves
the best balance between system performance and fairness
compared to several other methods including three Hadoop
schedulers.
",17.122413403193683,15.612431506849315,147
IPDPS_17_015.txt,15.033485042632115,13.006256852764029,IPDPS,6864,"
High-speed interconnects (e.g. InfiniBand) have
been widely deployed on modern HPC clusters. With the
emergence of HPC in the cloud, high-speed interconnects
have paved their way into the cloud with recently introduced
Single Root I/O Virtualization (SR-IOV) technology, which is
able to provide efficient sharing of high-speed interconnect
resources and achieve near-native I/O performance. However,
recent studies have shown that SR-IOV-based virtual networks
prevent virtual machine migration, which is an essential virtualization capability towards high availability and resource
provisioning. Although several initial solutions have been proposed in the literature to solve this problem, our investigations
show that there are still many restrictions on these proposed
approaches, such as depending on specific network adapters
and/or hypervisors, which will limit the usage scope of these
solutions on HPC environments. In this paper, we propose a
high-performance virtual machine migration framework for
MPI applications on SR-IOV enabled InfiniBand clusters.
Our proposed method does not need any modification to
the hypervisor and InfiniBand drivers and it can efficiently
handle virtual machine (VM) migration with SR-IOV IB device.
Our evaluation results indicate that the proposed design is
able to not only achieve fast VM migration speed but also
guarantee the high performance for MPI applications during
the migration in the HPC cloud. At the application level, for
NPB LU benchmark running inside VM, our proposed design
is able to completely hide the migration overhead through
the computation and migration overlapping. Furthermore, our
proposed design shows good scaling when migrating multiple
VMs.

",17.467979349516824,16.51640540540541,261
IPDPS_17_016.txt,15.554073337387191,13.653147296979885,IPDPS,7015,"
Exascale systems are expected to feature hundreds of
thousands of compute nodes with hundreds of hardware threads
and complex memory hierarchies with a mix of on-package and
persistent memory modules.

In this context, the Argo project is developing a new operating
system for exascale machines. Targeting production workloads
using workflows or coupled codes, we improve the Linux kernel
on several fronts. We extend the memory management of Linux
to be able to subdivide NUMA memory nodes, allowing better
resource partitioning among processes running on the same node.
We also add support for memory-mapped access to node-local,
PCle-attached NVRAM devices and introduce a new scheduling
class targeted at parallel runtimes supporting user-level load
balancing. These features are unified into compute containers,
a containerization approach focused on providing modern HPC
applications with dynamic control over a wide range of kernel
interfaces. To keep our approach compatible with industrial
containerization products, we also identify contentions points for
the adoption of containers in HPC settings.

Each NodeOS feature is evaluated by using a set of parallel
benchmarks, miniapps, and coupled applications consisting of
simulation and data analysis components, running on a modern
NUMA platform. We observe out-of-the-box performance improvements easily matching, and often exceeding, those observed
with expert-optimized configurations on standard OS kernels.
Our lightweight approach to resource management retains the
many benefits of a full OS kernel that application programmers
have learned to depend on, at the same time providing a set of
extensions that can be freely mixed and matched to best benefit
particular application components.

",17.693802365651003,17.010726235741448,264
IPDPS_17_017.txt,15.24219004704435,14.219509266351725,IPDPS,6736,"
The rational fair consensus problem can be informally defined as follows. Consider a network of 𝑛 (selfish) rational agents, each of them initially supporting a color chosen from a finite set Σ. The goal is to design a protocol that leads the network to a stable monochromatic configuration (i.e. a consensus) such that the probability that the winning color is 𝑐 is equal to the fraction of the agents that initially support 𝑐, for any 𝑐 ∈ Σ. Furthermore, this fairness property must be guaranteed (with high probability) even in presence of any fixed coalition of rational agents that may deviate from the protocol in order to increase the winning probability of their supported colors. A protocol having this property, in presence of coalitions of size at most 𝑡, is said to be a whp - 𝑡-strong equilibrium.
We investigate, for the first time, the rational fair con- sensus problem in the 𝒢𝒪𝒮𝒮I𝒫 communication model where, at every round, every agent can actively contact at most one neighbor via a push/pull operation. We provide a randomized 𝒢𝒪𝒮𝒮I𝒫 protocol that, starting from any initial color configuration of the complete graph, achieves rational fair consensus within 𝑂(log𝑛) rounds using mes- sages of 𝑂(log2 𝑛) size, w.h.p. More in details, we prove that our protocol is a whp 𝑡-strong equilibrium for any 𝑡 = 𝑜(𝑛/ log 𝑛) and, moreover, it tolerates worst-case permanent faults provided that the number of non-faulty agents is Ω(𝑛). As far as we know, our protocol is the first solution which avoids any all-to-all communication, thus resulting in 𝑜(𝑛2) message complexity.
 provide for any central authority, the techniques for studying
",16.99224021665606,16.631418439716317,265
IPDPS_17_018.txt,13.771829352188899,11.5453748860686,IPDPS,11666,"
In this paper, we study the fundamental problem
of leader election in the mobile telephone model: a recently
introduced variation of the classical telephone model modified
to better describe the local peer-to-peer communication services
implemented in many popular smartphone operating systems.
In more detail, the mobile telephone model differs from the
classical telephone model in three ways: (1) each device can
participate in at most one connection per round; (2) the
network topology can undergo a parameterized rate of change;
and (3) devices can advertise a parameterized number of bits
to their neighbors in each round before connection attempts
are initiated. We begin by describing and analyzing a new
leader election algorithm in this model that works under the
harshest possible parameter assumptions: maximum rate of
topology changes and no advertising bits. We then apply this
result to resolve an open question from [1] on the efficiency of
PUSH-PULL rumor spreading under these conditions. We then
turn our attention to the slightly easier case where devices can
advertise a single bit in each round. We demonstrate a large
gap in time complexity between these zero bit and one bit cases.
In more detail, we describe and analyze a new algorithm that
solves leader election with a time complexity that includes the
parameter bounding topology changes. For all values of this
parameter, this algorithm is faster than the previous result,
with a gap that grows quickly as the parameter increases
(indicating lower rates of change). We conclude by describing
and analyzing a modified version of this algorithm that does
not require the assumption that all devices start during the
same round. This new version has a similar time complexity
(the rounds required differ only by a polylogarithmic factor),
but now requires slightly larger advertisement tags.

",18.35118385865746,17.447545454545452,298
IPDPS_17_019.txt,12.536226222900641,10.542010497333703,IPDPS,7055,"
We study (deterministic) leader election in
unidirectional rings of homonym processes that have
no a priori knowledge on the number of processes. In
this context, we show that there is no algorithm that
solves process-terminating leader election for the class
of asymmetric labeled rings. In particular, there is no
process-terminating leader election algorithm in rings in
which at least one label is unique. However, we show
that process-terminating leader election is possible for
the subclass of asymmetric rings, where multiplicity is
bounded. We confirm this positive results by proposing two
algorithms, which achieve the classical trade-off between
time and space.

",15.903189008614273,14.079692307692309,105
IPDPS_17_020.txt,12.242603018883813,9.235952127321589,IPDPS,5947,"
We consider the following balls-into-bins process
with n bins and m balls: Each ball is equipped with a mutually
independent exponential clock of rate 1. Whenever a ball’s
clock rings, the ball samples a random bin and moves there if
the number of balls in the sampled bin is smaller than in its
current bin.

This simple process models a typical load balancing problem where users (balls) seek a selfish improvement of their
assignment to resources (bins). From a game theoretic perspective, this is a randomized approach to the well-known KPmodel [1], while it is known as Randomized Local Search
(RLS) in load balancing literature [2], [3]. Up to now, the
best bound on the expected time to reach perfect balance was
O((Inn)? + In(n) - n?/m) due to [3]. We improve this to
an asymptotically tight O(In(n) + n?/m). Our analysis is
based on the crucial observation that performing destructive
moves (reversals of RLS moves) cannot decrease the balancing
time. This allows us to simplify problem instances and to ignore
“inconvenient moves” in the analysis.

",12.40481918309499,8.95428853754941,184
IPDPS_17_021.txt,17.329120510548567,16.467714625599196,IPDPS,8006,"
We are now developing a manycore-aware implementation of multiprocessed PIC (particle-in-cell) simulation
code with automatic load balancing. A key issue of the
implementation is how to exploit the wide SIMD mechanism
of manycore processors such as Intel Xeon Phi. Our solution is
particle binning to rank all particles in a cell (voxel) in a chunk
of SOA (structure-of-arrays) type one-dimensional arrays so
that particle-push and current-scatter operations on them are
efficiently SIMD-vectorized by our compiler. In addition, our
sophisticated binning mechanism performs sorting of particles
according to their positions on-the-fly, efficiently coping with
occasional bin overflow in a fully multithreaded manner. Our
performance evaluation with up to 64 nodes of Cray XC30
and XC40 supercomputers, equipped with Xeon Phi 5120D
(Knights Corner) and 7250 (Knights Landing) respectively, not
only exhibited good parallel performance, but also proved the
effectiveness of our binning mechanism.

",19.287186520377343,18.787137254901968,154
IPDPS_17_022.txt,14.094110595659462,12.272607073243304,IPDPS,7324,"
B-spline based orbital representations are widely
used in Quantum Monte Carlo (QMC) simulations of solids,
historically taking as much as 50% of the total run time. Random
accesses to a large four-dimensional array make it challenging to
efficiently utilize caches and wide vector units of modern CPUs.
We present node-level optimizations of B-spline evaluations on
multi/many-core shared memory processors. To increase SIMD
efficiency and bandwidth utilization, we first apply data layout
transformation from array-of-structures to structure-of-arrays
(SoA). Then by blocking SoA objects, we optimize cache reuse
and get sustained throughput for a range of problem sizes.
We implement efficient nested threading in B-spline orbital
evaluation kernels, paving the way towards enabling strong
scaling of QMC simulations. These optimizations are portable
on four distinct cache-coherent architectures and result in up to
5.6x performance enhancements on Intel® Xeon Phi™ processor
7250P (KNL), 5.7x on Intel® Xeon Phi™ coprocessor 7120P,
10x on an Intel® Xeon® processor E5v4 CPU and 9.5x on
BlueGene/Q processor. Our nested threading implementation
shows nearly ideal parallel efficiency on KNL up to 16 threads.
We employ roofline performance analysis to model the impacts of
our optimizations. This work combined with our current efforts
of optimizing other QMC kernels, result in greater than 4.5x
speedup of miniQMC on KNL.

",14.975302809339372,14.521298245614034,227
IPDPS_17_023.txt,15.070285020436945,13.018384981226536,IPDPS,6369,"
One-Way Wave Equation Migration (QWEM) is a
depth migration algorithm used for seismic imaging. A parallel
version of this algorithm is widely implemented using MPI.
Heterogenous architectures that use GPUs have become popular
in the Top 500 because of their performance/power ratio. In this
paper, we discuss the methodology and code transformations used
to port OWEM to GPUs using OpenACC, along with the code
changes needed for scaling the application up to 18,400 GPUs
(more than 98%) of the Titan leadership class supercomputer at
Oak Ridget National Laboratory. For the individual OpenACC
kernels, we achieved an average of 3X speedup on a test dataset
using one GPU as compared with an 8-core Intel Sandy Bridge
CPU. The application was then run at large scale on the Titan
supercomputer achieving a peak of 1.2 petaflops using an average
of 5.5 megawatts. After porting the application to GPUs, we
discuss how we dealt with other challenges of running at scale
such as the application becoming more I/O bound and prone to
silent errors. We believe this work will serve as valuable proof
that directive-based programming models are a viable option for
scaling HPC applications to heterogenous architectures.
",16.373557378465907,14.633000000000003,204
IPDPS_17_024.txt,15.913445203769985,14.098910643499753,IPDPS,5302,"
The Ab Initio Molecular Dynamics (AIMD) method
allows scientists to treat the dynamics of molecular and condensed
phase systems while retaining a first-principles-based description
of their interactions. This extremely important method has
tremendous computational requirements, because the electronic
Schrédinger equation, approximated using Kohn-Sham Density
Functional Theory (DFT), is solved at every time step.

With the advent of manycore architectures, application developers have a significant amount of processing power within
each compute node that can only be exploited through massive
parallelism. A compute intensive application such as AIMD forms
a good candidate to leverage this processing power.

In this paper, we focus on adding thread level parallelism
to the plane wave DFT methodology implemented in NWChem.
Through a careful optimization of tall-skinny matrix products,
which are at the heart of the Lagrange multiplier and nonlocal pseudopotential kernels, as well as 3D FFTs, our OpenMP
implementation delivers excellent strong scaling on the latest Intel
Knights Landing (KNL) processor. We assess the efficiency of
our Lagrange multiplier kernels by building a Roofline model
of the platform, and verify that our implementation is close to
the roofline for various problem sizes. Finally, we present strong
scaling results on the complete AIMD simulation for a 64 water
molecules test case, that scales up to all 68 cores of the Knights
Landing processor.

",18.243605946275583,17.5023020361991,222
IPDPS_17_025.txt,13.854561486033958,11.940405764835237,IPDPS,6929,"
Task-based programming models such as
OpenMP, IntelTBB and OmpSs offer the possibility of
expressing dependences among tasks to drive their execution
at runtime. Managing these dependences introduces noticeable
overheads when targeting fine-grained tasks, diminishing the
potential speedups or even introducing performance losses.
To overcome this drawback, we present a general purpose
hardware accelerator, Picost+, to manage the inter-task
dependences efficiently in both time and energy. Our design
also includes a novel nested task support. To this end, a new
hardware/software co-design is presented to overcome the fact
that nested tasks with dependences could result in system
deadlocks due to the limited amount of resources in hardware
task dependence managers.

In this paper we describe a detailed implementation of
this design and evaluate a parallel task-based programming
model using Picos++ in a Linux embedded system with two
ARM Cortex-A9 and a FPGA. The scalability and energy
consumption of the real system implemented have been studied
and compared against a software runtime. Even in a system
limited to 2 threads, using Picos++ results in more than 1.8x
speedup and 40% of energy savings in the most demanding
parallelizations of real benchmarks. As a matter of fact, a
hardware task dependence manager should be able to achieve
much higher speedup and provide more energy savings with
more threads.
",16.04434344847333,15.24754772393539,224
IPDPS_17_026.txt,15.499114181441207,13.728308420953734,IPDPS,6545,"
Shared Memory stands out as a sine qua non for
parallel programming of many commercial and emerging multicore processors. It optimizes patterns of communication that
benefit common programming styles. As parallel programming
is now mainstream, those common programming styles are
challenged with emerging applications that communicate often
and involve large amount of data. Such applications include
graph analytics and machine learning, and this paper focuses
on these domains. We retain the shared memory model and
introduce a set of lightweight in-hardware explicit messaging
instructions in the instruction set architecture (ISA). A set
of auxiliary communication models are proposed that utilize
explicit messages to accelerate synchronization primitives, and
efficiently move computation towards data. The results on a
256-core simulated multicore demonstrate that the proposed
communication models improve performance and dynamic energy by an average of 4x and 42% respectively over traditional
shared memory.

",18.08858127442927,16.442579365079364,145
IPDPS_17_027.txt,14.224596397053613,12.13107127179995,IPDPS,5653,"
Near-threshold computing is emerging as a
promising energy-efficient alternative for power-constrained
environments. Unfortunately, aggressive reduction in supply
voltage to the near-threshold range, albeit effective, faces a
host of challenges. This includes higher relative leakage power
and high error rates, particularly in dense SRAM structures
such as on-chip caches.

This paper presents an architecture that rethinks the cache
hierarchy in near-threshold multiprocessors. Our design uses
STT-RAM to implement all on-chip caches. STT-RAM has
several advantages over SRAM at low voltages including low
leakage, high density, and reliability. The design consolidates
the private caches of near-threshold cores into shared L1
instruction/data caches organized in clusters. We find that
our consolidated cache design can service more than 95% of
incoming requests within a single cycle. We demonstrate that
eliminating the coherence traffic associated with private caches
results in a performance boost of 11%. In addition, we propose
a hardware-based core management system that dynamically
consolidates virtual cores into variable numbers of physical
cores to increase resource efficiency. We demonstrate that this
approach can save up to 33% in energy.

",14.683698258229963,13.294603481624758,189
IPDPS_17_028.txt,14.360563745955481,12.561233411883613,IPDPS,7713,"
Today, machine learning based on neural networks has become mainstream, in many application domains.
A small subset of machine learning algorithms, called Convolutional Neural Networks (CNN), are considered as state-ofthe-art for many applications (e.g. video/audio classification).
The main challenge in implementing the CNNs, in embedded
systems, is their large computation, memory, and bandwidth
requirements. To meet these demands, dedicated hardware accelerators have been proposed. Since memory is the major cost
in CNNs, recent accelerators focus on reducing the memory
accesses. In particular, they exploit data locality using either
tiling, layer merging or intra/inter feature map parallelism to
reduce the memory footprint. However, they lack the flexibility
to interleave or cascade these optimizations. Moreover, most
of the existing accelerators do not exploit compression that
can simultaneously reduce memory requirements, increase
the throughput, and enhance the energy efficiency. To tackle
these limitations, we present a flexible accelerator called
MOCHA. MOCHA has three features that differentiate it
from the state-of-the-art: (i) the ability to compress input/
kernels, (ii) the flexibility to interleave various optimizations,
and (iii) intelligence to automatically interleave and cascade
the optimizations, depending on the dimension of a specific
CNN layer and available resources. Post layout Synthesis
results reveal that MOCHA provides up to 63% higher energy
efficiency, up to 42% higher throughput, and up to 30% less
storage, compared to the next best accelerator, at the cost of
26-35% additional area.

",16.800685031470465,16.38489159376189,241
IPDPS_17_029.txt,15.143216249990786,13.527605191841396,IPDPS,5070,"
Stencil computations expose a large and complex
space of equivalent implementations. These computations often
rely on autotuning techniques, based on iterative compilation or
machine learning (ML), to achieve high performance. Iterative
compilation autotuning is a challenging and time-consuming
task that may be unaffordable in many scenarios. Meanwhile,
traditional ML autotuning approaches exploiting classification algorithms (such as neural networks and support vector machines)
face difficulties in capturing all features of large search spaces.
This paper proposes a new way of automatically tuning stencil
computations based on structural learning. By organizing the
training data in a set of partially-sorted samples (i.e., rankings),
the problem is formulated as a ranking prediction model, which
translates to an ordinal regression problem. Our approach can
be coupled with an iterative compilation method or used as a
standalone autotuner. We demonstrate its potential by comparing
it with state-of-the-art iterative compilation methods on a set of
nine stencil codes and by analyzing the quality of the obtained
ranking in terms of Kendall rank correlation coefficients.

",16.827784334635936,16.12058139534884,174
IPDPS_17_030.txt,14.18680127710926,12.214887065637065,IPDPS,6405,"
Increasingly complex memory systems and onchip interconnects are developed to mitigate the data movement
bottlenecks in manycore processors. One example of such a
complex system is the Xeon Phi KNL CPU with three different
types of memory, fifteen memory configuration options, and
a complex on-chip mesh network connecting up to 72 cores.
Users require a detailed understanding of the performance
characteristics of the different options to utilize the system efficiently. Unfortunately, peak performance is rarely achievable
and achievable performance is hardly documented. We address
this with capability models of the memory subsystem, derived
by systematic measurements, to guide users to navigate the
complex optimization space. As a case study, we provide an
extensive model of all memory configuration options for Xeon
Phi KNL. We demonstrate how our capability model can be
used to automatically derive new close-to-optimal algorithms
for various communication functions yielding improvements 5x
and 24x over Intel’s tuned OpenMP and MPI implementations,
respectively. Furthermore, we demonstrate how to use the
models to assess how efficiently a bitonic sort application
utilizes the memory resources. Interestingly, our capability
models predict and explain that the high bandwidth MCDRAM
does not improve the bitonic sort performance over DRAM.

",17.879347455551382,16.751926298157453,201
IPDPS_17_031.txt,16.427470456799686,15.155787731943786,IPDPS,4392,"
Increasing architectural diversity makes
performance portability extremely important for parallel
simulation codes. Emerging on-node parallelization frameworks
such as Kokkos and RAJA decouple the work done in kernels
from the parallelization mechanism, allowing for a single source
kernel to be tuned for different architectures at compile time.
However, computational demands in production applications
change at runtime, and performance depends both on the
architecture and the input problem, and tuning a kernel for one
set of inputs may not improve its performance on another. The
statically optimized versions need to be chosen dynamically to
obtain the best performance. Existing auto-tuning approaches
can handle slowly evolving applications effectively, but are too
slow to tune highly input-dependent kernels. We developed
Apollo, an auto-tuning extension for RAJA that uses pre-trained,
reusable models to tune input-dependent code at runtime.
Apollo is designed for highly dynamic applications; it generates
sufficiently low-overhead code to tune parameters each time
a kernel runs, making fast decisions. We apply Apollo to two
hydrodynamics benchmarks and to a production multi-physics
code, and show that it can achieve speedups from 1.2x to 4.8x.

",17.833180683606166,16.421808510638296,191
IPDPS_17_032.txt,14.427137822399235,12.285359672256227,IPDPS,7742,"
Many applications have irregular behavior — e.g.,
input-dependent solvers, irregular memory accesses, or unbiased
branches — that cannot be captured using today’s automated
performance modeling techniques. We describe new hierarchical
critical path analyses for the Palm model generation tool. To
obtain a good tradeoff between model accuracy, generality,
and generation cost, we combine static and dynamic analysis.
To create a model’s outer structure, we capture tasks along
representative MPI critical paths. We create a histogram of
critical tasks with parameterized task arguments and instance
counts. To model each task, we identify hot instruction-level
paths and model each path based on data flow, data locality, and
microarchitectural constraints. We describe application models
that generate accurate predictions for strong scaling when
varying CPU speed, cache and memory speed, microarchitecture,
and (with supervision) input data class. Our models’ errors are
usually below 8%; and always below 13%.

Index Terms—analytical performance models, model generation, critical path analysis, Palm

",15.903189008614273,15.640215053763445,159
IPDPS_17_033.txt,14.353327130037357,12.659202059202059,IPDPS,7184,"
In the reliable message transmission problem
(RMTP) processors communicate by exchanging messages, but
the channel that connects two processors is subject to message
loss, duplication, and reordering. Previous work focused on
proposing protocols in asynchronous systems, where message
size is finite and sequence numbers are bounded. However,
if the channel can duplicate messages—but not lose them-—and
arbitrarily reorder the messages, the problem is unsolvable.
We consider a strengthening of the asynchronous model in
which reordering of messages is bounded. In this model, we
develop an efficient protocol to solve the RMTP when messages
may be duplicated but not lost. This result is in contrast
to the impossibility of such an algorithm when reordering
is unbounded. Our protocol has the pleasing property that
no messages need to be sent from the receiver to the sender
and it works when message loss is allowed with some minimal
modifications.

",16.785175570968402,14.262471042471045,149
IPDPS_17_034.txt,12.182945617745748,9.179359022611951,IPDPS,6240,"
Dynamic behavior is an essential part of wireless
networking, due mobility, environmental changes or failures.
We analyze a natural exponential backoff procedure to manage
contention in a fading channel, in the presence of both node churn
and link changes. We show that it attains a fast convergence,
stabilizing contention from any state in logarithmic time. We
use it to obtain optimal algorithm for Local Broadcast that even
improves known results for the static case. The results illustrate
the utility of carrier sensing, a stock feature of wireless nodes.

",14.554592549557764,12.460363636363638,89
IPDPS_17_035.txt,12.60126797321778,11.564421031694764,IPDPS,7030,"
We consider the problem of scheduling packets
of different lengths via k directed parallel communication
links. The links are prone to simultaneous errors — if an
error occurs, all links are affected. Dynamic packet arrivals
and errors are modelled by a worst-case adversary. The goal
is to optimize competitive throughput of online scheduling
algorithms. Two types of failures are considered: jamming,
when currently scheduled packets are simply not delivered,
and crashes, when additionally the channel scheduler crashes
losing its current state. For the former, milder type of failures,
we prove an upper bound on competitive throughput of
3/4—1/(4k) for odd values of k, and 3/4—1/(4k+-4) for even
values of k. On constructive side, we design an online algorithm
that, for packets of two different lengths, matches the upper
bound on competitive throughput. To compare, scheduling on
independent channels, that is, when adversary could cause
errors on each channel independently, reaches throughput of
1/2. This shows that scheduling under simultaneous jamming
is provably more efficient than scheduling under channelindependent jamming. In the setting with crash failures we
prove a general upper bound for competitive throughput of
(./5—1)/2 and design an algorithm achieving it for packets of
two different lengths. This result has two interesting implications. First, simultaneous crashes are significantly stronger than
simultaneous jamming. Second, due to the above mentioned
upper bound of 1/2 on throughput under channel-independent
errors, scheduling under simultaneous crashes is significantly
stronger than channel-independent crashes, similarly as in the
case of jamming errors.

",14.554592549557764,13.586153846153849,260
IPDPS_17_036.txt,14.888602108104255,12.627045446469818,IPDPS,3722,"
Large-scale parallel programming environments
and algorithms require efficient group-communication on computing systems with failing nodes. Existing reliable broadcast
algorithms either cannot guarantee that all nodes are reached
or are very expensive in terms of the number of messages
and latency. This paper proposes Corrected-Gossip, a method
that combines Monte Carlo style gossiping with a deterministic
correction phase, to construct a Las Vegas style reliable
broadcast that guarantees reaching all the nodes at low cost.
We analyze the performance of this method both analytically
and by simulations and show how it reduces the latency and
network load compared to existing algorithms. Our method
improves the latency by 20% and the network load by 53%
compared to the fastest known algorithm on 4,096 nodes. We
believe that the principle of corrected-gossip opens an avenue
for many other reliable group communication operations.

Keywords-gossip algorithms; reliable broadcast

",16.785175570968402,15.139498069498071,150
IPDPS_17_037.txt,14.456711732358023,12.585225276443335,IPDPS,6882,"
Non-Uniform Memory Access (NUMA) architectures are widely used in mainstream multi-socket computer
systems to scale memory bandwidth. Without a NUMA-aware
design, programs can suffer from significant performance degradation due to inter-socket bandwidth contention. However, identifying bandwidth contention is challenging. Existing methods
measure bandwidth consumption. However, consumption alone
is insufficient to quantify bandwidth contention. Furthermore,
existing methods diagnose bandwidth for the entire program execution, but lack the ability to associate bandwidth performance
to the source code and data structures involved. To address these
challenges, we propose DR-BW, a new tool based on machine
learning to identify bandwidth contention in NUMA architectures
and provide optimization guidance. DR-BW first trains a set
of micro benchmarks and extracts useful features to identify
bandwidth contention via a supervised machine learning model.
Our experiments show that DR-BW achieves more than 96%
accuracy. Second, DR-BW associates memory accesses that incur
bandwidth contention with data objects, which provides intuitive
guidance for optimization. Third, we apply DR-BW to a number
of real benchmarks. Our optimization based on the insights
obtained from DR-BW yields up to a 6.5x speedup in modern
NUMA architectures.

",14.314028922438442,13.548821243523317,195
IPDPS_17_038.txt,14.281648190598283,12.685262921146151,IPDPS,7514,"
Chapel is an emerging PGAS (Partitioned Global
Address Space) language whose design goal is to make parallel
programming more productive and generally accessible. To
date, the implementation effort has focused primarily on
correctness over performance. We present a performance
Measurement technique for Chapel and the idea is also
applicable to other PGAS models. The unique feature of our
tool is that it associates the performance statistics not to the
code regions (functions), but to the variables (including the
heap allocated, static, and local variables) in the source code.
Unlike code-centric methods, this data-centric analysis
capability exposes new optimization opportunities that are
useful in resolving data locality problems. This paper
introduces our idea and implementations of the approach with
three benchmarks. We also include a case study optimizing
benchmarks based on the information from our tool. The
optimized versions improved the performance by a factor of
1.4x for LULESH, 2.3x for MiniMD, and 2.1x for CLOMP
with simple modifications to the source code.

",15.414825405933506,14.565975609756098,168
IPDPS_17_039.txt,15.460397297172346,13.210556567666874,IPDPS,6515,"
Detecting data races in multithreaded programs is
critical to ensure the correctness of the programs. To discover
data races precisely without false alarms, dynamic detection
approaches are often applied. However, the overhead of the
existing dynamic detection approaches, even with recent innovations, is still substantially high. In this paper, we present a
simple but efficient approach to parallelize data race detection
in multicore SMP (Symmetric Multiprocessing) machines. In
our approach, data access information needed for dynamic
detection is collected at application threads and passed to detection threads. The access information is distributed in a way
that the operation performed by each detection thread is independent of that of other detection threads. As a consequence,
the overhead caused by locking operations in data race detection can be alleviated and multiple cores can be fully utilized to
speed up and scale up the detection. Furthermore, each detection thread deals with only its own assigned memory access
region rather than the whole address space. The executions of
detection threads can exploit the spatial locality of accesses
leading to an improved cache performance. We have applied
our parallel approach on the FastTrack algorithm and demonstrated the validity of our approach on an Intel Xeon machine.
Our experimental results show that the parallel FastTrack
detector, on average, runs 2.2 times faster than the original
FastTrack detector on the 8 core machine.

",16.581925556534415,14.717417538214004,228
IPDPS_17_040.txt,14.691740436519236,12.885359449386517,IPDPS,8281,"
Nested fork-join programs scheduled using work
stealing can automatically balance load and adapt to changes
in the execution environment. In this paper, we design an
approach to efficiently recover from faults encountered by
these programs. Specifically, we focus on localized recovery
of the task space in the presence of fail-stop failures. We
present an approach to efficiently track, under work stealing,
the relationships between the work executed by various threads.
This information is used to identify and schedule the tasks to
be re-executed without interfering with normal task execution.
The algorithm precisely computes the work lost, incurs minimal
re-execution overhead, and can recover from an arbitrary
number of failures. Experimental evaluation demonstrates low
overheads in the absence of failures, recovery overheads on the
same order as the lost work, and much lower recovery costs
than alternative strategies.

",15.719379583869454,14.191955420466062,142
IPDPS_17_041.txt,15.994391174681812,14.839107163307563,IPDPS,7252,"
Emerging applications for data analytics and knowledge discovery typically have irregular or unpredictable communication patterns that do not scale well on parallel systems
designed for traditional bulk-synchronous HPC applications. New
network architectures that focus on minimizing (short) message
latencies, rather than maximizing (large) transfer bandwidths,
are emerging as possible alternatives to better support those
applications with irregular communication patterns. We explore
a system based upon one such novel network architecture,
the Data Vortex interconnection network, and examine how
this system performs by running benchmark code written for
the Data Vortex network, as well as a reference MPI-overInfiniband implementation, on the same cluster. Simple communication primitives (ping-pong and barrier synchronization), a
few common communication kernels (distributed 1D Fast Fourier
Transform, breadth-first search, Giga-Updates Per Second) and
three prototype applications (a proxy application for simulating
neutron transport-’SNAP”, a finite difference simulation for
computing incompressible fluid flow, and an implementation of
the heat equation) were all implemented for both network models.
The results were compared and analyzed to determine what
characteristics make an application a good candidate for porting
to a Data Vortex system, and to what extent applications could
potentially benefit from this new architecture.

",22.076135915942103,24.243557788944724,200
IPDPS_17_042.txt,13.9264237050091,12.14183569432722,IPDPS,7460,"
Multi-path TCP has recently shown great potential to take advantage of the rich path diversity in data center
networks (DCN) to increase transmission throughput. However,
the small flows, which take a large fraction of data center
traffic, will easily get a timeout when split onto multiple paths.
Moreover, the dynamic congestions and node failures in DCN
will exacerbate the reorder problem of parallel multi-path
transmissions for large flows. In this paper, we propose DC?MTCP (Data Center Coded Multi-path TCP), which employs
a fast and light-weight coding method to address the above
challenges while maintaining the benefit of parallel multi-path
transmissions. To meet the high flow performance in DCN, we
insert a very low ratio of coded packets with a careful selection
of the packets to be coded. We further present a progressive
decoding algorithm to decode the packets online with a low
time complexity. Extensive ns2-based simulations show that
with two orders of magnitude lower coding delay, DC?-MTCP
can reduce on average 40% flow completion time for small flows
and increase 30% flow throughput for large flows compared
to the peer schemes in varying network conditions.

",14.90622815163357,13.148943298969073,195
IPDPS_17_043.txt,14.516836091271937,13.281593765810367,IPDPS,7487,"
High-radix routers with low latency and high
bandwidth play an increasingly important role in the design of
large-scale interconnection networks such as those used in supercomputers and datacenters. The tile-based crossbar approach
partitions a single large crossbar into many small tiles and can
considerably reduce the complexity of arbitration while providing
throughput higher than the conventional switch implementation.
However, it is not scalable due to power consumption, placement,
and routing problems. In this paper, we propose a truly scalable
router microarchitecture called Multiport Binding Tile-based
Router (MBTR). By aggregating multiple physical ports into a
single tile a high-radix router can be flexibly organized into a
different array of tiles, thus the number of tiles and hardware
overhead can be considerably reduced. Compared with a
hierarchical crossbar, MBTR achieves up to 50%~75% reduction
in memory consumption as well as wire area. Simulation results
demonstrate MBTR is indistinguishable from the YARC router in
terms of throughput and delay, and can even outperform it by
reducing potential contention for output ports. We have fabricated
an ASIC MBTR chip with 28nm technology. Internally, it runs at
700MHz and 30ns latency without any speedup. We also discuss
how the microarchitecture parameters of MBTR can be adjusted
based on the power, area, and design complexity constraints of the
arbitration logic.

",15.774802946060372,14.653434389140273,221
IPDPS_17_044.txt,15.364411876063702,14.216243515531094,IPDPS,6619,"
On most supercomputers, except some torus network based systems, resource managers allocate nodes to jobs
without considering the sharing of network resources by different
jobs. Such network-oblivious resource allocations result in link
sharing among multiple jobs that can cause significant performance variability and performance degradation for individual
jobs. In this paper, we explore low-diameter networks and
corresponding node allocation policies that can eliminate interjob interference. We propose a variation to n-dimensional mesh
networks called express mesh. An express mesh is denser than the
corresponding mesh network, has a low diameter independent of
the number of routers, and is easily partitionable. We compare
structural properties and performance of express mesh with
other popular low-diameter networks. We present practical node
allocation policies for express mesh and fat-tree networks that not
only eliminate inter-job interference and performance variability,
but also improve overall performance.

",17.28802050969988,16.629217221135033,147
IPDPS_17_045.txt,14.814054262403335,13.577287559887427,IPDPS,7743,"
Apache Spark is a framework for distributed
computing that supports the map-reduce programming model.
The SQL module of Spark contains Datasets, i.e., distributed
collections of records stored in a serialized low-level format
in a manually managed chunk of memory. However, the
functions users provide to the map-reduce computations expect
Java objects. Datasets perform an additional deserialization
step beforehand to support the user-provided function, which
increases the overhead. We tackled this problem by replacing
map functions with their counterparts that accepted the serialized data. This allowed us to skip the unnecessary part of
deserialization and achieve faster data processing speeds.

",15.247664890283005,14.246747572815536,105
IPDPS_17_046.txt,14.182724711521459,12.462038024270072,IPDPS,5912,"
Data have been generated and collected at an
accelerating pace. Hadoop has made analyzing large scale
data much simpler to developers/analysts using commodity
hardware. Interestingly, it has been shown that most Hadoop
jobs have small input size and do not run for long time. For
example, higher level query languages, such as Hive and Pig,
would handle a complex query by breaking it into smaller adhoc ones. Although Hadoop is designed for handling complex
queries with large data sets, we found that it is highly inefficient
to operate at small scale data, despite a new Uber mode was
introduced specifically to handle jobs with small input size.

In this paper, we propose an optimized Hadoop extension
called MRapid, which significantly speeds up the execution of
short jobs. It is completely backward compatible to Hadoop,
and imposes negligible overhead. Our experiments on Microsoft
Azure public cloud show that MRapid can improve performance by up to 88% compared to the original Hadoop.

",13.81666964889586,12.633905279503104,162
IPDPS_17_047.txt,18.12921449400748,17.379158058345293,IPDPS,4794,"
Hybrid parallel program models that combine
message passing and multithreading (MP+MT) are becoming
more popular, extending the basic message passing (MP)
model that uses single-threaded processes for both inter- and
intra-node parallelism. A consequence is that coupled parallel applications increasingly comprise MP libraries together
with MP+MT libraries with differing preferred degrees of
threading, resulting in thread-level heterogeneity. Retroactively
matching threading levels between independently developed
and maintained libraries is difficult; the challenge is exacerbated because contemporary parallel job launchers provide
only static resource binding policies over entire application
executions. A standard approach for accommodating threadlevel heterogeneity is to under-subscribe compute resources
such that the library with the highest degree of threading per
process has one processing element per thread. This results in
libraries with fewer threads per process utilizing only a fraction
of the available compute resources.

We present and evaluate a novel approach for accommodating thread-level heterogeneity. Our approach enables full
utilization of all available compute resources throughout an
application’s execution by providing programmable facilities
to dynamically reconfigure runtime environments for compute
phases with differing threading factors and memory affinities.
We show that our approach can improve overall application
performance by up to 5.8x in real-world production codes.
Furthermore, the practicality and utility of our approach has
been demonstrated by continuous production use for over one
year, and by more recent incorporation into a number of
production codes.

",19.174585242480724,19.711568627450983,239
IPDPS_17_048.txt,15.240181984419614,13.492563512921596,IPDPS,7501,"
 We present a_ single-node, multi-GPU programmable graph processing library that allows programmers
to easily extend single-GPU graph algorithms to achieve
scalable performance on large graphs with billions of edges.
Directly using the single-GPU implementations, our design only
requires programmers to specify a few algorithm-dependent
concerns, hiding most multi-GPU related implementation details. We analyze the theoretical and practical limits to scalability in the context of varying graph primitives and datasets.
We describe several optimizations, such as direction optimizing
traversal, and a just-enough memory allocation scheme, for better performance and smaller memory consumption. Compared
to previous work, we achieve best-of-class performance across
operations and datasets, including excellent strong and weak
scalability on most primitives as we increase the number of
GPUs in the system.

",19.686211704642208,18.498229007633586,132
IPDPS_17_049.txt,14.355040129920255,12.313727790220845,IPDPS,7910,"
The LOOK-COMPUTE-MOVE model for a set of
autonomous robots has been thoroughly studied for over two
decades. Each robot repeatedly LOOKS at its surroundings and
obtains a snapshot containing the positions of all robots; based
on this information, the robot COMPUTES a destination and
then MOVES to it. Previous work assumed all robots are present
at the beginning of the computation. What would be the effect
of robots appearing asynchronously? This paper studies this
question, for problems of bringing the robots close together, and
exposes an intimate connection with combinatorial topology.

A central problem in the mobile robots area is the gathering
problem. In its discrete version, the robots start at vertices in
some graph G known to them, move towards the same vertex
and stop. The paper shows that if robots are asynchronous
and may crash, then gathering is impossible for any graph G
with at least two vertices, even if robots can have unique IDs,
remember the past, know the same names for the vertices of
G and use an arbitrary number of lights to communicate with
each other.

Next, the paper studies two weaker variants of gathering:
edge gathering and 1-gathering. For both problems we present
possibility and impossibility results. The solvability of edge
gathering is fully characterized: it is solvable for three or more
robots on a given graph if and only if the graph is acyclic.

Finally, general robot tasks in a graph are considered. A
combinatorial topology characterization for the solvable tasks
is presented, by a reduction of the asynchronous fault-tolerant
LOOK-COMPUTE-MOVE model to a wait-free read/write
shared-memory computing model, bringing together two areas
that have been independently studied for a long time into a
common theoretical foundation.

Keywords: Mobile Robots, Multiple and Distributed Robots,
Convergence, Gathering, Rendezvous, Fault Tolerance.

",15.438520173184436,13.488723653395784,306
IPDPS_17_050.txt,13.847162063638184,11.273530340681369,IPDPS,7662,"
The classic vehicle routing problem (VRP) is generally concerned with the optimal design of routes by a fleet
of vehicles to service a set of customers by minimizing the
overall cost, usually the travel distance for the whole set of
routes. Although the problem has been extensively studied in the
context of operations research and optimization, there is little
research on solving the VRP, where distributed vehicles need to
compute their respective routes in a decentralized fashion. Our
first contribution is a synchronous distributed approximation
algorithm that solves the VRP. Using the duality theorem of
linear programming, we show that the approximation ratio of our
algorithm is O(n - (p)'/”-log(n+m)), where p is the maximum
cost of travel or service in the input VRP instance, 7 is the size
of the graph, and m is the number of vehicles. We report results
of simulations and discuss implementation of our algorithm on
a real fleet of unmanned aerial systems (UASs) that carry out a
set of tasks.
",17.58133193835471,17.799411764705884,170
IPDPS_17_051.txt,10.900700238785653,9.41018283750282,IPDPS,8131,"
We consider the distributed setting of N autonomous mobile robots that operate in Look-Compute-Move
(LCM) cycles and communicate with other robots using colored
lights (the robots with lights model). We study the fundamental
problem of repositioning NV autonomous robots on a plane
so that each robot is visible to all others (the COMPLETE
VISIBILITY problem) on this model; a robot cannot see another
robot if a third robot is positioned between them on the straight
line connecting them. There exists an O(1) time, O(1) color
algorithm for this problem in the semi-synchronous setting.
In this paper, we provide the first O(log NV) time, O(1) color
algorithm for this problem in the asynchronous setting. This
is a significant improvement over an O(V)-time translation of
the semi-synchronous algorithm to the asynchronous setting.
The proposed algorithm is collision-free — robots do not share
positions and their paths do not cross.

",15.021129683784007,14.457038216560509,158
IPDPS_17_052.txt,15.955577836827619,14.785797670671787,IPDPS,5110,"
Similarity search is a critical primitive for a wide
variety of applications including natural language processing, content-based search, machine learning, computer vision,
databases, robotics, and recommendation systems. At its core,
similarity search is implemented using the k-nearest neighbors
(KNN) algorithm, where computation consists of highly parallel
distance calculations and a global top-k sort. In contemporary von-Neumann architectures, KNN is bottlenecked by data
movement which limits throughput and latency. In this paper,
we present and evaluate a novel automata-based algorithm for
KNN on the Micron Automata Processor (AP), which is a nonvon Neumann near-data processing architecture. By employing
near-data processing, the AP minimizes the data movement
bottleneck and is able to achieve better performance. Unlike
prior work in the automata processing space, our work combines
temporal encodings with automata design to augment the space
of applications for the AP. We evaluate our design’s performance
on the AP and compare to state-of-the-art CPU, GPU, and
FPGA implementations; we show that the current generation
of AP hardware can achieve over 50x speedup over CPUs
while maintaining competitive energy efficiency gains. We also
propose several automata optimization techniques and simple
architectural extensions that highlight the potential of the AP
hardware.
",18.10804710084791,17.995218446601942,208
IPDPS_17_053.txt,15.944663432700139,14.707887738443905,IPDPS,5980,"
Stencil computation arises from a broad set of
scientific and engineering applications and often plays a critical
role in the performance of extreme-scale simulations. Due to
the memory bound nature, it is a challenging task to optimize stencil computation kernels on modern supercomputers
with relatively high computing throughput whilst relatively
low data-moving capability. This work serves as a demonstration on the details of the algorithms, implementations
and optimizations of a real-world stencil computation in 3D
nonhydrostatic atmospheric modeling on the newly announced
Sunway TaihuLight supercomputer. At the algorithm level, we
present a computation-communication overlapping technique
to reduce the inter-process communication overhead, a localityaware blocking method to fully exploit on-chip parallelism
with enhanced data locality, and a collaborative data accessing
scheme for sharing data among different threads. In addition,
a variety of effective hardware specific implementation and
optimization strategies on both the process- and thread-level,
from the fine-grained data management to the data layout
transformation, are developed to further improve the performance. Our experiments demonstrate that a single-process
many-core speedup of as high as 170x can be achieved by using
the proposed algorithm and optimization strategies. The code
scales well to millions of cores in terms of strong scalability.
And for the weak-scaling tests, the code can scale in a nearly
ideal way to the full system scale of more than 10 million cores,
sustaining 25.96 PFLOPS in double precision, which is 20%
of the peak performance.

",18.643177196211187,18.952464574898787,249
IPDPS_17_054.txt,15.406084227021157,13.838170293666781,IPDPS,3454,"
Realizing the next generation of radio telescopes
such as the Square Kilometre Array (SKA) requires both more
efficient hardware and algorithms than today’s technology provides. The recently introduced image-domain gridding (DG)
algorithm is a novel approach towards solving the most computeintensive parts of creating sky images: gridding and degridding. It
avoids the performance bottlenecks of traditional AW-projection
gridding by applying instrumental and environmental corrections
in the image domain instead of in the Fourier domain. In this
paper, we present the first implementations of this new algorithm
for CPUs and Graphics Processing Units (GPUs). A thorough performance analysis, in which we apply a modified roofline analysis,
shows that our parallelization approaches and optimization leads
to nearly optimal performance on all architectures. The analysis
also indicates that, by leveraging dedicated hardware to evaluate
trigonometric functions, GPUs are both much faster and more
energy-efficient than regular CPUs. This makes IDG on GPUs
a candidate for meeting the computational and energy-efficiency
constraints of future telescopes.

",18.848423458724294,17.631948051948054,167
IPDPS_17_055.txt,15.420972879626152,13.964618252869446,IPDPS,5348,"
Aces4 is a parallel programming platform comprising a DSL for Computational Chemistry and its runtime
system. It offers a convenient way to express parallelism
together with extensive support for extremely large, possibly
sparse distributed arrays. It aids scientists in the creation and
use of performant, scalable, massively parallel programs that
can effectively take advantage of leadership class computing
systems to address important scientific questions. Aces4 has
enabled the development and implementation of new methods
in electronic structure theory which are breaking new ground
in their ability to perform highly accurate calculations on ever
larger molecular systems. In this paper, the design of Aces4 is
described along with scaling results from a Molecular Cluster
Perturbation Theory calculation, one of the new methods
enabled by Aces4.

",18.669449996058646,17.586838709677426,125
IPDPS_17_056.txt,16.02608073581565,14.52765292483408,IPDPS,6407,"
The Secure Sockets Layer (SSL) is the main protocol
used to secure Internet traffic and cloud computing. It relies on
the computation-intensive RSA cryptography, which primarily
limits the throughput of the handshake process. In this paper, we
design and implement an OpenSSL library, termed PhiOpenSSL,
which targets the Intel Xeon Phi (KNC) coprocessor, and utilizes
Intel Phi’s SIMD and multi-threading capability to reduce the
SSL computation latency. In particular, PhiOpenSSL vectorizes
all big integer multiplications and Montgomery operations involved in RSA calculations and employs the Chinese Remainder
Theorem and fixed-window exponentiation in its customized
library. In an experiment involving the computation of Montgomery exponentiation, PhiOpenSSL was as much as 15.3 times
faster than the two other reference libcrypto libraries, one
from the Intel Many-core Platform Software Stack (MPSS) and
the other from the default OpenSSL. Our RSA private key
cryptography routines in PhiOpenSSL are 1.6-5.7 times faster
than those in these two reference systems.
",16.92669308720184,16.41544025157233,164
IPDPS_17_057.txt,14.754563847133884,13.320186516626372,IPDPS,6656,"
The community needs simpler mechanisms to
access the performance available in accelerators, such as GPUs,
FPGAs, and APUs, due to their increasing use in stateof-the-art supercomputers. Programming models like CUDA,
OpenMP, OpenACC and OpenCL can efficiently offload
compute-intensive workloads to these devices. By default these
models naively offload computation without overlapping it with
communication (copying data to or from the device). Achieving
performance can require extensive refactoring and hand-tuning
to apply optimizations such as pipelining. Further, users must
manually partition the dataset whenever its size is larger than
device memory, which can be especially difficult when the
device memory size is not exposed to the user.

We propose a directive-based partitioning and pipelining
extension for accelerators appropriate for either OpenMP or
OpenACC., Its interface supports overlap of data transfers and
kernel computation without explicit user splitting of data. It
can map data to a pre-allocated device buffer and automate
memory-constrained array indexing and sub-task scheduling.
We evaluate a prototype implementation with four different
applications. The experimental results show that our approach
can reduce memory usage by 52% to 97% while delivering a
1.41x to 1.65x speedup over the naive offload model.

",17.5058628484301,16.455443886097154,202
IPDPS_17_058.txt,14.517045643716955,13.117672279416073,IPDPS,7611,"
Today’s rapid development of supercomputers has
caused I/O performance to become a major performance bottleneck for many scientific applications. Trace analysis tools have
thus become vital for diagnosing root causes of I/O problems. This
work contributes an I/O tracing framework with (a) techniques
to gather a set of lossless, elastic I/O trace files for small number
of nodes, (b) a mathematical model to analyze trace data and
extrapolate it to larger number of nodes, and (c) a replay engine
for the extrapolated trace file to verify its accuracy. The traces
can in principle be extrapolated even beyond the scale of presentday systems and provide a test if applications scale in terms
of I/O. We conducted our experiments on three platforms: a
commodity Linux cluster, an IBM BG/Q system, and a discrete
event simulation of an IBM BG/P system. We investigate a
combination of synthetic benchmarks on all platforms as well
as a production scientific application on the BG/Q system. The
extrapolated I/O trace replays closely resemble the I/O behavior
of equivalent applications in all cases.

",17.5058628484301,17.356081081081083,187
IPDPS_17_059.txt,13.8540836414107,12.13480042719975,IPDPS,7558,"
 Today, there is a steep rise in the amount of data
being collected from diverse applications. Consequently, data
analytic workloads are gaining popularity to gain insight that
can benefit the application, e.g., financial trading, social media
analysis. To study the architectural behavior of the workloads,
architectural simulation is one of the most common approaches.
However, because of the long-running nature of the workloads,
it is not trivial to identify which parts of the analysis to simulate.

In the current work, we introduce SimProf, a sampling
framework for data analytic workloads. Using this tool, we are
able to select representative simulation points based on the phase
behavior of the analysis at a method level granularity. This
provides a better understanding of the simulation point and
also reduces the simulation time for different input sets. We
present the framework for Apache Hadoop and Apache Spark
frameworks, which can be easily extended to other data analytic
workloads.


",15.414825405933506,14.348185483870967,157
IPDPS_17_060.txt,15.751754997737965,14.487117634932762,IPDPS,6464,"
Today, big data applications can generate largescale data sets at an unprecedented rate; and scientists have
turned to parallel and distributed systems for data analysis.
Although many big data processing systems provide advanced
mechanisms to partition data and tackle the computational
skew, it is difficult to efficiently implement skew-resistant
mechanisms, because the runtime of different partitions not
only depends on input data size but also algorithms that
will be applied on data. As a result, many research efforts
have been undertaken to explore user-defined partitioning
methods for different types of applications and algorithms.
However, manually writing application-specific partitioning
methods requires significant coding effort, and finding the
optimal data partitioning strategy is particularly challenging
even for developers that have mastered sufficient application
knowledge.

In this paper, we propose PaPar, a Parallel data Partitioning
framework for big data applications, to simplify the implementations of data partitioning algorithms. PaPar provides a
set of computational operators and distribution strategies for
programmers to describe desired data partitioning methods.
Taking an input data configuration file and a workflow configuration file as the input, PaPar can automatically generate
the parallel partitioning codes by formalizing the user-defined
workflow as a sequence of key-value operations and matrixvector multiplications, and efficiently mapping to the parallel
implementations with MPI and MapReduce. We apply our
approach on two applications: muBLAST, a MPI implementation of BLAST algorithms for biological sequence search; and
PowerLyra, a computation and partitioning method for skewed
graphs. The experimental results show that compared to the
partitioning methods of applications, the codes generated by
PaPar can produce the same data partitions with comparable
or less partitioning time.

",20.477631830292346,21.315049019607844,273
IPDPS_17_061.txt,17.386738586064926,16.467168369667387,IPDPS,3302,"
To explore the potential of training complex deep
neural networks (DNNs) on other commercial chips rather than
GPUs, we report our work on swDNN, which is a highlyefficient library for accelerating deep learning applications on
the newly announced world-leading supercomputer, Sunway
TaihuLight. Targeting SW26010 processor, we derive a performance model that guides us in the process of identifying the
most suitable approach for mapping the convolutional neural
networks (CNNs) onto the 260 cores within the chip. By performing a systematic optimization that explores major factors,
such as organization of convolution loops, blocking techniques,
register data communication schemes, as well as reordering
strategies for the two pipelines of instructions, we manage
to achieve a double-precision performance over 1.6 Tflops for
the convolution kernel, achieving 54% of the theoretical peak.
Compared with Tesla K40m with cuDNNv5, swDNN results in
1.91-9.75x performance speedup in an evaluation with over 100
parameter configurations.


",19.53771442962202,21.32587748344371,155
IPDPS_17_062.txt,14.802740772920167,12.92218507717578,IPDPS,7922,"
We present and evaluate a new GPU algorithm
based on the Louvain method for community detection. Our
algorithm is the first for this problem that parallelizes the access
to individual edges. In this way we can fine tune the load balance
when processing networks with nodes of highly varying degrees.
This is achieved by scaling the number of threads assigned to
each node according to its degree. Extensive experiments show
that we obtain speedups up to a factor of 270 compared to
the sequential algorithm. The algorithm consistently outperforms
other recent shared memory implementations and is only one
order of magnitude slower than the current fastest parallel
Louvain method running on a Blue Gene/Q supercomputer using
more than 500K threads.

",14.06817628641468,12.949380165289263,122
IPDPS_17_063.txt,15.879942239561348,14.529954076850984,IPDPS,2415,"
Interest has recently grown in efficiently analyzing
unstructured data such as social network graphs and protein
structures. A fundamental graph algorithm for doing such task
is the Breadth-First Search (BFS) algorithm, the foundation for
many other important graph algorithms such as calculating the
shortest path or finding the maximum flow in graphs.

In this paper, we share our experience of designing and
implementing the BFS algorithm on Sunway TaihuLight, a
newly released machine with 40,960 nodes and 10.6 million
accelerator cores. It tops the Top500 list of June 2016 with a
93.01 petaflops Linpack performance [1].

Designed for extremely large-scale computation and power
efficiency, processors on Sunway TaihuLight employ a unique
heterogeneous many-core architecture and memory hierarchy.
With its extremely large size, the machine provides both opportunities and challenges for implementing high-performance
irregular algorithms, such as BFS.

We propose several techniques, including pipelined module
mapping, contention-free data shuffling, and group-based message batching, to address the challenges of efficiently utilizing
the features of this large scale heterogeneous machine. We
ultimately achieved 23755.7 giga-traversed edges per second
(GTEPS), which is the best among heterogeneous machines
and the second overall in the Graph500s June 2016 list [2].

Index Terms—Breadth-First Search; heterogeneous; parallel
algorithm; micro-architecture;

",16.860833078287435,15.755229067930493,216
IPDPS_17_064.txt,15.344048936566857,13.603360953723818,IPDPS,7281,"
We introduce XTRAPULP, a new distributedmemory graph partitioner designed to process trillion-edge
graphs. XTRAPULP is based on the scalable label propagation
community detection technique, which has been demonstrated
as a viable means to produce high quality partitions with
minimal computation time. On a collection of large sparse graphs,
we show that XTRAPULP partitioning quality is comparable
to state-of-the-art partitioning methods. We also demonstrate
that XTRAPULP can produce partitions of real-world graphs
with billion+ vertices in minutes. Further, we show that using
XTRAPULP partitions for distributed-memory graph analytics
leads to significant end-to-end execution time reduction.

",16.647925096878797,14.440116504854377,103
IPDPS_17_065.txt,15.851990049041532,14.063154275741713,IPDPS,4441,"
Matrix multiplication (GEMM) is a core operation to numerous scientific applications. Traditional implementations of Strassen-like fast matrix multiplication (FMM)
algorithms often do not perform well except for very large
matrix sizes, due to the increased cost of memory movement, which is particularly noticeable for non-square matrices.
Such implementations also require considerable workspace and
modifications to the standard BLAS interface. We propose
a code generator framework to automatically implement a
large family of FMM algorithms suitable for multiplications
of arbitrary matrix sizes and shapes. By representing FMM
with a triple of matrices [U,V,W] that capture the linear
combinations of submatrices that are formed, we can use the
Kronecker product to define a multi-level representation of
Strassen-like algorithms. Incorporating the matrix additions
that must be performed for Strassen-like algorithms into the
inherent packing and micro-kernel operations inside GEMM
avoids extra workspace and reduces the cost of memory movement. Adopting the same loop structures as high-performance
GEMM implementations allows parallelization of all FMM
algorithms with simple but efficient data parallelism without
the overhead of task parallelism. We present a simple performance model for general FMM algorithms and compare actual
performance of 20+ FMM < algorithms to modeled predictions.
Our implementations demonstrate a performance benefit over
conventional GEMM on single core and multi-core systems.
This study shows that Strassen-like fast matrix multiplication
can be incorporated into libraries for practical use.

",18.1352568364455,17.78779831932773,237
IPDPS_17_066.txt,14.835877540118876,13.143375477976381,IPDPS,7048,"
We study tiled algorithms for going from a “full”
matrix to a condensed “band bidiagonal” form using orthogonal transformations: (i) the tiled bidiagonalization algorithm
BIDIAG, which is a tiled version of the standard scalar bidiagonalization algorithm; and (ii) the R-bidiagonalization algorithm
R-BIDIAG, which is a tiled version of the algorithm which
consists in first performing the QR factorization of the initial
matrix, then performing the band-bidiagonalization of the Rfactor. For both BIDIAG and R-BIDIAG, we use four main types
of reduction trees, namely FLATTS, FLATTT, GREEDY, and a
newly introduced auto-adaptive tree, AUTO. We provide a study
of critical path lengths for these tiled algorithms, which shows
that (i) R-BIDIAG has a shorter critical path length than BIDIAG
for tall and skinny matrices, and (ii) GREEDY based schemes are
much better than earlier proposed algorithms with unbounded
resources. We provide experiments on a single multicore node,
and on a few multicore nodes of a parallel distributed sharedMemory system, to show the superiority of the new algorithms
on a variety of matrix sizes, matrix shapes and core counts.


",22.918633597333717,29.478688524590172,184
IPDPS_17_067.txt,14.38142377106892,12.061701156151361,IPDPS,5213,"
We present a new parallel algorithm for solving
triangular systems with multiple right hand sides (TRSM).
TRSM is used extensively in numerical linear algebra computations, both to solve triangular linear systems of equations
as well as to compute factorizations with triangular matrices,
such as Cholesky, LU, and QR. Our algorithm achieves
better theoretical scalability than known alternatives, while
maintaining numerical stability, via selective use of triangular
matrix inversion. We leverage the fact that triangular inversion
and matrix multiplication are more parallelizable than the
standard TRSM algorithm. By only inverting triangular blocks
along the diagonal of the initial matrix, we generalize the
usual way of TRSM computation and the full matrix inversion
approach. This flexibility leads to an efficient algorithm for
any ratio of the number of right hand sides to the triangular
matrix dimension. We provide a detailed communication cost
analysis for our algorithm as well as for the recursive triangular
matrix inversion. This cost analysis makes it possible to
determine optimal block sizes and processor grids a priori.
Relative to the best known algorithms for TRSM, our approach
can require asymptotically fewer messages, while performing
optimal amounts of computation and communication in terms
of words sent.

",18.71604785175511,17.26544839255499,198
IPDPS_17_068.txt,14.30186477645529,12.569798709560164,IPDPS,5808,"
We design and develop a work-efficient multithreaded algorithm for sparse matrix-sparse vector multiplication
(SpMSpV) where the matrix, the input vector, and the output
vector are all sparse. SpMSpV is an important primitive in
the emerging GraphBLAS standard and is the workhorse of
many graph algorithms including breadth-first search, bipartite
graph matching, and maximal independent set. As thread counts
increase, existing multithreaded SpMSpV algorithms can spend
more time accessing the sparse matrix data structure than doing
arithmetic. Our shared-memory parallel SpMSpV algorithm is
work efficient in the sense that its total work is proportional to
the number of arithmetic operations required. The key insight is
to avoid each thread individually scan the list of matrix columns.

Our algorithm is simple to implement and operates on existing
column-based sparse matrix formats. It performs well on diverse
matrices and vectors with heterogeneous sparsity patterns. A
high-performance implementation of the algorithm attains up to
15x speedup on a 24-core Intel Ivy Bridge processor and up to 49x
speedup on a 64-core Intel KNL manycore processor. In contrast
to implementations of existing algorithms, the performance of our
algorithm is sustained on a variety of different input types include
matrices representing scale-free and high-diameter graphs.

",16.594172100314452,15.203508771929823,210
IPDPS_17_069.txt,13.610848587687904,11.554210069229054,IPDPS,3971,"
The power consumed by memory system in GPUs
is a significant fraction of the total chip power. As thread level
parallelism increases, GPUs are likely to stress cache and memory
bandwidth even more, thereby exacerbating power consumption.
We observe that neighboring concurrent thread arrays (CTAs)
within GPU applications share considerable amount of data.
However, the default GPU scheduling policy spreads these CTAs
to different streaming multiprocessor cores (SM) in a roundrobin fashion. Since each SM has a private L1 cache, the shared
data among CTAs are replicated across L1 caches of different
SMs. Data replication reduces the effective L1 cache size which
in turn increases the data movement and power consumption.

The goal of this paper is to reduce data movement and
increase effective cache space in GPUs. We propose a sharingaware CTA scheduler that attempts to assign CTAs with data
sharing to the same SM to reduce redundant storage of data in
private L1 caches across SMs. We further enhance the scheduler
with a sharing-aware cache allocation and replacement policy.
The sharing-aware cache management approach dynamically
classifies private and shared data. Private blocks are given higher
priority to stay longer in L1 cache, and shared blocks are
given higher priority to stay longer in L2 cache. Essentially,
this approach increases the lifetime of shared blocks and private
blocks in different cache levels. The experimental results show
that the proposed scheme reduces the off-chip traffic by 19%
which translates to an average DRAM power reduction of 10%
and performance improvement of 7%.

",15.062637766997835,13.904242424242423,256
IPDPS_17_070.txt,13.678759746609401,12.730555555555558,IPDPS,4756,"
Scalable Networks-on-chip (NoCs) have become the
de facto interconnection mechanism in large scale Chip Multiprocessors. Not only are NoCs devouring a large fraction of
the on-chip power budget but static NoC power consumption is
becoming the dominant component as technology scales down.
Hence reducing static NoC power consumption is critical for
energy-efficient computing. Previous research has proposed to
power-gate routers attached to inactive cores so as to save static
power, but they either required centralized decision making and
global network knowledge or a non-scalable escape ring network.
In this paper, we propose Fly-Over (FLOV), a light-weight distributed mechanism for power gating routers, which encompasses
FLOV router microarchitecture and a partition-based dynamic
routing algorithm to maintain network functionality. With simple
modifications to the baseline router microarchitecture, FLOV can
facilitate fly-over links over power-gated routers. The proposed
routing algorithm provides best-effort minimal path routing
without the necessity for global network information. We evaluate
our scheme using synthetic workloads as well as real workloads
from PARSEC 2.1 benchmark suite. The results show that FLOV
can achieve 19.2% latency reduction and 16.9% total power
savings.

",15.322241378113624,14.967115716753025,197
IPDPS_17_071.txt,13.294322274375904,11.914548008191304,IPDPS,8041,"
Designing a cost-effective network for data centers
that can deliver sufficient bandwidth and provide high availability
has drawn tremendous attentions recently. In this paper, we
propose a novel server-centric network structure called RCube,
which is energy efficient and can deploy a redundancy scheme
to improve the availability of data centers. Moreover, RCube
shares many good properties with BCube, a well known servercentric network structure, yet its network size can be adjusted
more conveniently. We also present a routing algorithm to find
paths in RCube and an algorithm to build multiple parallel paths
between any pair of source and destination servers. In addition,
we theoretically analyze the power efficiency of the network and
availability of RCube under server failure. Our comprehensive
simulations demonstrate that RCube provides higher availability
and flexibility to make trade-off among many factors, such as
power consumption and aggregate throughput, than BCube, while
delivering similar performance to BCube in many critical metrics,
such as average path length, path distribution and graceful
degradation, which makes RCube a very promising empirical
structure for an enterprise data center network product.

",18.599290044081553,19.38615384615385,183
IPDPS_17_072.txt,15.031726618963713,13.377750824625672,IPDPS,6944,"
Limited power budget is becoming one of the
most crucial challenges in developing supercomputer systems.
Hardware overprovisioning which installs a larger number of
nodes beyond the limitations of the power constraint is an
attractive way to design next generation supercomputers. In air
cooled HPC centers, about half of the total power is consumed
by cooling facilities. Reducing cooling power and effectively
utilizing power resource for computing nodes are important
challenges. It is known that the cooling power depends on
the hotspot temperature of the node inlets. Therefore, if we
minimize the hotspot temperature, performance efficiency of
the HPC system will be increased.

One of the ways to reduce the hotspot temperature is to
allocate power-hungry jobs to compute nodes whose effect
on the hotspot temperature is small. It can be accomplished
by optimizing job-to-node mapping in the job scheduler. In
this paper, we propose a cooling and node location-aware
job scheduling strategy which tries to optimize job-to-node
mapping while improving the total system throughput under
the constraint of total system (compute nodes and cooling
facilities) power consumption. Experimental results with the
job scheduling simulation show that our scheduling scheme
achieves 1.49X higher total system throughput than the conventional scheme.

",14.554592549557764,13.472317073170732,207
IPDPS_17_073.txt,15.730724735604987,14.309714732135955,IPDPS,7026,"
We propose a model for scheduling jobs in a
parallel machine setting that takes into account the cost of
migrations by assuming that the processing time of a job
may depend on the specific set of machines among which
the job is migrated. For the makespan minimization objective,
the model generalizes classical scheduling problems such as
unrelated parallel machine scheduling, as well as novel ones
such as semi-partitioned and clustered scheduling. In the case
of a hierarchical family of machines, we derive a compact
integer linear programming formulation of the problem and
leverage its fractional relaxation to obtain a polynomial-time 2approximation algorithm. Extensions that incorporate memory
capacity constraints are also discussed.

Keywords-processor affinities; makespan minimization; unrelated machines; laminar family; wrap-around rule; clustered
scheduling

",19.686211704642208,18.19474015748032,128
IPDPS_17_074.txt,14.901728771086692,13.252904794281829,IPDPS,4501,"
Many services used in large scale web applications
should be able to tolerate faults without impacting their performance. State machine replication is a well-known approach to
implementing fault-tolerant services, providing high availability
and strong consistency. To boost the performance of state
machine replication, recent proposals have introduced parallel
execution of commands. In parallel state machine replication,
incoming commands may or may not depend on other commands
that are waiting for execution. Although dependent commands
must be processed in the same relative order at every replica to
avoid inconsistencies, independent commands can be executed
in parallel and benefit from multi-core architectures. Since
many application workloads are mostly composed of independent commands, these parallel models promise high throughput
without sacrificing strong consistency. The efficient execution
of commands in such environments, however, requires effective
scheduling strategies. Existing approaches rely on dependency
tracking based on pairwise comparison between commands,
which introduces scheduling contention. In this paper, we propose
a new and highly efficient scheduler for parallel state machine
replication. Our scheduler considers batches of commands, instead of commands individually. Moreover, each batch of commands is augmented with a compact data structure that encodes
commands information needed to the dependency analysis. We
show, by means of experimental evaluation, that our technique
outperforms schedulers for parallel state machine replication by
a fairly large margin.

Keywords-parallel state machine replication; fault tolerance;
high throughput; deterministic scheduling

",17.03242331605538,15.987241379310348,233
IPDPS_17_075.txt,16.163531991956578,13.821791182935204,IPDPS,6477,"
Factorizing sparse matrices using direct multifrontal methods generates directed tree-shaped task graphs,
where edges represent data dependency between tasks. This
paper revisits the execution of tree-shaped task graphs using
multiple processors that share a bounded memory. A task can
only be executed if all its input and output data can fit into the
memory. The key difficulty is to manage the order of the task
executions so that we can achieve high parallelism while staying
below the memory bound. In particular, because input data of
unprocessed tasks must be kept in memory, a bad scheduling
strategy might compromise the termination of the algorithm.
In the single processor case, solutions that are guaranteed to be
below a memory bound are known. The multi-processor case
(when one tries to minimize the total completion time) has been
shown to be NP-complete. We present in this paper a novel
heuristic solution that has a low complexity and is guaranteed
to complete the tree within a given memory bound. We compare
our algorithm to state of the art strategies, and observe that
on both actual execution trees and synthetic trees, we always
perform better than these solutions, with average speedups
between 1.25 and 1.45 on actual assembly trees. Moreover, we
show that the overhead of our algorithm is negligible even on
deep trees (10°), and would allow its runtime execution.

",16.280829582073984,13.799130434782615,232
IPDPS_17_076.txt,15.078166124597352,12.95855410571686,IPDPS,7039,"
In High Performance Computing, heterogeneity
is now the norm with specialized accelerators like GPUs
providing efficient computational power. The added complexity
has led to the development of task-based runtime systems,
which allow complex computations to be expressed as task
graphs, and rely on scheduling algorithms to perform load balancing between all resources of the platforms. Developing good
scheduling algorithms, even on a single node, and analyzing
them can thus have a very high impact on the performance of
current HPC systems. The special case of two types of resources
(mamely CPUs and GPUs) is of practical interest. HeteroPrio
is such an algorithm which has been proposed in the context
of fast multipole computations, and then extended to general
task graphs with very interesting results. In this paper, we
provide a theoretical insight on the performance of HeteroPrio,
by proving approximation bounds compared to the optimal
schedule in the case where all tasks are independent and for
different platform sizes. Interestingly, this shows that spoliation
allows to prove approximation ratios for a list scheduling
algorithm on two unrelated resources, which is not possible
otherwise. We also establish that almost all our bounds are
tight. Additionally, we provide an experimental evaluation of
HeteroPrio on real task graphs from dense linear algebra
computation, which highlights the reasons explaining its good
practical performance.

",17.879347455551382,16.256060606060604,221
IPDPS_17_077.txt,17.72354007197579,17.021333939878506,IPDPS,6019,"
Loop collapsing is a well-known loop transformation which combines some loops that are perfectly nested into
one single loop. It allows to take advantage of the whole amount
of parallelism exhibited by the collapsed loops, and provides a
perfect load balancing of iterations among the parallel threads.

However, in the current implementations of this loop optimization, as the ones of the OpenMP language, automatic loop
collapsing is limited to loops with constant loop bounds that
define rectangular iteration spaces, although load imbalance
is a particularly crucial issue with non-rectangular loops. The
OpenMP language addresses load balance mostly through dynamic runtime scheduling of the parallel threads. Nevertheless,
this runtime schedule introduces some unavoidable executiontime overhead, while preventing to exploit the entire parallelism
of all the parallel loops.

In this paper, we propose a technique to automatically
collapse any perfectly nested loops defining non-rectangular
iteration spaces, whose bounds are linear functions of the
loop iterators. Such spaces may be triangular, tetrahedral,
trapezoidal, rhomboidal or parallelepiped. Our solution is
based on original mathematical results addressing the inversion
of a multi-variate polynomial that defines a ranking of the
integer points contained in a convex polyhedron.

We show on a set of non-rectangular loop nests that our
technique allows to generate parallel OpenMP codes that
outperform the original parallel loop nests, parallelized either
by using options “static” or “dynamic” of the OpenMPschedule clause.


",18.481644305966572,17.75216022889843,234
IPDPS_17_078.txt,17.757627806260842,16.43696554749479,IPDPS,6828,"
Heterogeneous computing systems, e.g., those with
accelerators than the host CPUs, offer the accelerated performance for a variety of workloads. However, most parallel programming models require platform dependent, time-consuming
hand-tuning efforts for collectively using all the resources in a
system to achieve efficient results. In this work, we explore the
use of OpenMP parallel language extensions to empower users
with the ability to design applications that automatically and
simultaneously leverage CPUs and accelerators to further optimize use of available resources. We believe such automation will
be key to ensuring codes adapt to increases in the number and
diversity of accelerator resources for future computing systems.
The proposed system combines language extensions to OpenMP,
load-balancing algorithms and heuristics, and a runtime system
for loop distribution across heterogeneous processing elements.
We demonstrate the effectiveness of our automated approach to
program on systems with multiple CPUs, GPUs, and MICs.

Keywords-accelerator architecture; OpenMP; parallel loops;
performance model; runtime system; data and computation
distribution; alignment; load balance
",19.142268018852484,18.395920826161795,168
IPDPS_17_079.txt,16.881320263430666,15.513469712277097,IPDPS,4142,"
 The overwhelming wealth of parallelism exposed
by Extreme-scale computing is rekindling the interest for finegrain multithreading, particularly at the intranode level. Indeed, popular parallel programming models, such as OpenMP,
are integrating fine-grain tasking in their newest standards.
Yet, classical coarse-grain constructs are still largely preferred,
as they are considered simpler to express parallelism. In
this paper, we present a Multigrain Parallel Programming
environment that allows programmers to use these well-known
coarse-grain constructs to generate a fine-grain multithreaded
application to be run on top of a fine-grain event-driven program execution model. Experimental results with four scientific
benchmarks (Graph500, NAS Data Cube, NWChem-SCF, and
ExMatEx’s CoMD) show that fine-grain applications generated
by and run on our environment are competitive and even
outperform their OpenMP counterparts, especially for dataintensive workloads with irregular and dynamic parallelism,
reaching speedups as high as 2.6x for Graph500 and 50x for
NAS Data Cube.

",19.287186520377343,18.740820512820516,159
IPDPS_17_080.txt,13.48628933671301,11.905750305757579,IPDPS,4566,"
The tasking model of OpenMP 4.0 supports both
nesting and the definition of dependences between sibling tasks. A
natural way to parallelize many codes with tasks is to first taskify
the high-level functions and then to further refine these tasks with
additional subtasks. However, this top-down approach has some
drawbacks since combining nesting with dependencies usually
requires additional measures to enforce the correct coordination
of dependencies across nesting levels. For instance, most non-leaf
tasks need to include a taskwait at the end of their code. While
these measures enforce the correct order of execution, as a side
effect, they also limit the discovery of parallelism.

In this paper we extend the OpenMP tasking model to improve
the integration of nesting and dependencies. Our proposal builds
on both formulas, nesting and dependencies, and benefits from
their individual strengths. On one hand, it encourages a top-down
approach to parallelizing codes that also enables the parallel
instantiation of tasks. On the other hand, it allows the runtime
to control dependencies at a fine grain that until now was only
possible using a single domain of dependencies.

Our proposal is realized through additions to the OpenMP task
directive that ensure backward compatibility with current codes.
We have implemented a new runtime with these extensions and
used it to evaluate the impact on several benchmarks. Our initial
findings show that our extensions improve performance in three
areas. First, they expose more parallelism. Second, they uncover
dependencies across nesting levels, which allows the runtime
to make better scheduling decisions. And third, they allow the
parallel instantiation of tasks with dependencies between them.

Index Terms—computer languages; runtime library; OpenMP;
task nesting; task dependencies; weak dependencies; weakwait;
taskwait; single dependency domain; top-down programming;
task decomposition;

",14.643508229474929,13.460407423208192,295
IPDPS_17_081.txt,15.211635949122272,13.656595454739303,IPDPS,5803,"
Reducing communication is an important objective, as it can save energy or improve the performance
of a communication-bound application. The graph algorithm
PageRank computes the importance of vertices in a graph,
and it serves as an important benchmark for graph algorithm
performance. If the input graph to PageRank has poor locality,
the execution will need to read many cache lines from memory,
some of which may not be fully utilized. We present propagation
blocking, an optimization to improve spatial locality, and we
demonstrate its application to PageRank. In contrast to cache
blocking which partitions the graph, we partition the data
transfers between vertices (propagations).

If the input graph has poor locality, our approach will
reduce communication. Our approach reduces communication
more than conventional cache blocking if the input graph
is sufficiently sparse or if number of vertices is sufficiently
large relative to the cache size. To evaluate our approach, we
use both simple analytic models to gain insights and precise
hardware performance counter measurements to compare implementations on a suite of 8 real-world and synthetic graphs.
We demonstrate our parallel implementations substantially
outperform prior work in execution time and communication
volume. Although we present results for PageRank, propagation blocking could be generalized to SpMV (sparse matrix
multiplying dense vector) or other graph programming models.

",16.76809479433877,15.232148148148152,217
IPDPS_17_082.txt,14.37256726801375,12.402863284109696,IPDPS,5889,"
Large datasets in astronomy and geoscience often
require clustering and visualizations of phenomena at different
densities and scales in order to generate scientific insight. We
examine the problem of maximizing clustering throughput for
concurrent dataset clustering in spatial dimensions. We introduce a novel hybrid approach that uses GPUs in conjunction
with multicore CPUs for algorithmic throughput optimizations.
The key idea is to exploit the fast memory on the GPU for index
searches and optimize I/O transfers in such a way that the lowbandwidth host-GPU bottleneck does not have a significant
negative performance impact. To achieve this, we derive two
distinct GPU kernels that exploit grid-based indexing schemes
to improve clustering performance. To obviate limited GPU
memory and enable large dataset clustering, our method is
complemented by an efficient batching scheme for transfers
between the host and GPU accelerator. This scheme is robust
with respect to both sparse and dense data distributions
and intelligently avoids buffer overflows that would otherwise
degrade performance, all while minimizing the number of
data transfers between the host and GPU. We evaluate our
approaches on ionospheric total electron content datasets as
well as intermediate-redshift galaxies from the Sloan Digital
Sky Survey. Our hybrid approach yields a speedup of up to 50x
over the sequential implementation on one of the experimental
scenarios, which is respectable for I/O intensive clustering.

",18.36309006607702,16.70305433186491,228
IPDPS_17_083.txt,14.167391547442637,12.399393318084886,IPDPS,8957,"
Deadlock avoidance mechanisms for lossless lowdistance networks typically increase the order of virtual channel
(VC) index with each hop. This restricts the number of buffer
resources depending on the routing mechanism and limits performance due to an inefficient use. Dynamic buffer organizations
increase implementation complexity and only provide small gains
in this context because a significant amount of buffering needs
to be allocated statically to avoid congestion.

We introduce Flex VC, a simple buffer management mechanism
which permits a more flexible use of VCs. It combines statically
partitioned buffers, opportunistic routing and a relaxed distancebased deadlock avoidance policy. FlexVC mitigates Head-of-Line
blocking and reduces up to 50% the memory requirements.
Simulation results in a Dragonfly network show congestion reduction and up to 37.8% throughput improvement, outperforming
more complex dynamic approaches. FlexVC merges different
flows of traffic in the same buffers, which in some cases makes
more difficult to identify the traffic pattern in order to support
nonminimal adaptive routing. An alternative denoted FlexVCminCred improves congestion sensing for adaptive routing by
tracking separately packets routed minimally and nonminimally,
rising throughput up to 20.4% with 25% savings in buffer area.
",17.251386760058843,16.18122807017544,193
IPDPS_17_084.txt,14.31968215955575,12.529805304335344,IPDPS,5127,"
Accelerators, such as GPUs, have proven to be
highly successful in reducing execution time and power consumption of compute-intensive applications. Even though they
are already used pervasively, they are typically supervised
by general-purpose CPUs, which results in frequent control
flow switches and data transfers as CPUs are handling all
communication tasks. However, we observe that accelerators
are recently being augmented with peer-to-peer communication
capabilities that allow for autonomous traffic sourcing and
sinking. While appropriate hardware support is becoming
available, it seems that the right communication semantics
are yet to be identified. Maintaining the semantics of existing
communication models, such as the Message Passing Interface
(MPD), seems problematic as they have been designed for the
CPU’s execution model, which inherently differs from such
specialized processors.

In this paper, we analyze the compatibility of traditional
message passing with massively parallel Single Instruction Multiple Thread (SIMT) architectures, as represented by GPUs,
and focus on the message matching problem. We begin with
a fully MPI-compliant set of guarantees, including tag and
source wildcards and message ordering. Based on an analysis of
exascale proxy applications, we start relaxing these guarantees
to adapt message passing to the GPU’s execution model. We
present suitable algorithms for message matching on GPUs
that can yield matching rates of 60M and 500M matches/s,
depending on the constraints that are being relaxed. We discuss
our experiments and create an understanding of the mismatch
of current message passing protocols and the architecture and
execution model of SIMT processors.

",17.58133193835471,16.71419047619047,255
IPDPS_17_085.txt,15.870074010563872,14.766189744544992,IPDPS,8137,"
The massive parallelism and high memory bandwidth of GPU’s are particularly well matched with the exigencies of Big Data analytics applications, for which many
independent computations and high data throughput are prevalent. These applications often produce (intermediary or final)
results in the form of key-value (KV) pairs, and hash tables are
particularly well-suited for storing these KV pairs in memory.
How such hash tables are implemented on GPUs, however,
has a large impact on performance. Unfortunately, all hash
table solutions designed for GPUs to date have limitations that
prevent acceleration for Big Data analytics applications.

In this paper, we present the design and implementation of
a GPU-based hash table for efficiently storing the KV pairs of
Big Data analytics applications. The hash table is able to grow
beyond the size of available GPU memory without excessive
performance penalties. Central to our hash table design is the
SEPO model of computation, where the processing of individual
tasks is selectively postponed when processing is expected to be
inefficient. A performance evaluation on seven GPU-based Big
Data analytics applications, each processing several Gigabytes
of input data, shows that our hash table allows the applications
to achieve, on average, a speedup of 3.5 over their CPU-based
multi-threaded implementations. This gain is realized despite
having hash tables that grow up to four times larger than the
size of available GPU memory.

",16.860833078287435,16.384885057471266,235
IPDPS_17_086.txt,15.134931603849854,14.110895184280189,IPDPS,6484,"
Elastic distributed storage systems have been increasingly studied in recent years because power consumption
has become a major problem in data centers. Much progress
has been made in improving the agility of resizing small- and
large-scale distributed storage systems. However, most of these
studies focus on metadata based distributed storage systems. On
the other hand, emerging consistent hashing based distributed
storage systems are considered to allow better scalability and are
highly attractive. We identify challenges in achieving elasticity
in consistent hashing based distributed storage. These challenges
cannot be easily solved by techniques used in current studies.
In this paper, we propose an elastic consistent hashing based
distributed storage to solve two problems. First, in order to
allow a distributed storage to resize quickly, we modify the
data placement algorithm using a primary server design and
achieve an egual-work data layout. Second, we propose a selective
data re-integration technique to reduce the performance impact
when resizing a cluster. Our experimental and trace analysis
results confirm that our proposed elastic consistent hashing works
effectively and allows significantly better elasticity.

",15.112257680678326,14.833333333333336,181
IPDPS_17_087.txt,12.599953988862385,10.071589103830267,IPDPS,7609,"
Kernel matrices appear in machine learning and
non-parametric statistics. Given N points in d dimensions and
a kernel function that requires O(d) work to evaluate, we
present an O(dN log N)-work algorithm for the approximate
factorization of a regularized kernel matrix, a common computational bottleneck in the training phase of a learning task.
With this factorization, solving a linear system with a kernel
matrix can be done with O(N log N) work. Our algorithm only
requires kernel evaluations and does not require that the kernel
matrix admits an efficient global low rank approximation.
Instead, our factorization only assumes low-rank properties
for the off-diagonal blocks under an appropriate row and
column ordering. We also present a hybrid method that, when
the factorization is prohibitively expensive, combines a partial
factorization with iterative methods. As a highlight, we are able
to approximately factorize a dense 11M x 11M kernel matrix
in 2 minutes on 3,072 x86 “Haswell” cores and a 4.5M x 4.5M
matrix in 1 minute using 4,352 “Knights Landing” cores.

",15.343465313023842,15.536168831168833,181
IPDPS_17_088.txt,14.653450566921538,12.738377877928382,IPDPS,7035,"
We present the design and implementation of a
parallel and fully algebraic preconditioner based on an approximate sparse factorization using low-rank matrix compression.
The sparse factorization uses a multifrontal algorithm with fill-in
occurring in dense frontal matrices. These frontal matrices are
approximated as hierarchically semi-separable matrices, which
are constructed using a randomized sampling technique. The
resulting preconditioner has (close to) optimal complexity in
terms of flops and memory usage for many discretized partial
differential equations. We illustrate the robustness and performance of this new preconditioner for a number of unstructured
grid problems. Initial results show that the rank-structured
preconditioner could be a viable alternative to algebraic multigrid
and incomplete LU, for instance. Our implementation uses MPI
and OpenMP and supports real and complex arithmetic and 32
and 64 bit integers. We present a detailed performance analysis.
The code is released as the STRUMPACK library with a BSD
license, and a PETSc interface is available to allow for easy
integration in existing applications.

",16.594172100314452,15.274417670682734,167
IPDPS_17_089.txt,14.536770837649094,12.363761451235987,IPDPS,8788,"
With the spreading of multi-core architectures, operating systems and applications are becoming increasingly more
concurrent and their scalability is often limited by the primitives
used to synchronize the different hardware threads. In this paper,
we address the problem of how to optimize the throughput
of a system with multiple producer and consumer threads.
Such applications typically synchronize their threads via multiproducer/multi-consumer FIFO queues, but existing solutions
have poor scalability, as we could observe when designing a
secure application framework that requires high-throughput
communication between many concurrent threads. In our target
system, however, the items enqueued by different producers do
not necessarily need to be FIFO ordered. Hence, we propose a
fast FIFO queue, FFQ, that aims at maximizing throughput by
specializing the algorithm for single-producer/multiple-consumer
settings: each producer has its own queue from which multiple
consumers can concurrently dequeue. Furthermore, while we provide a wait-free interface for producers, we limit ourselves to lockfree consumers to eliminate the need for helping. We also propose
a multi-producer variant to show which synchronization operations we were able to remove by focusing on a single producer
variant. Our evaluation analyses the performance using microbenchmarks and compares our results with other state-of-the-art
solutions: FFQ exhibits excellent performance and scalability.

",19.287186520377343,18.048333333333336,217
IPDPS_17_090.txt,15.347134943664852,14.08338562003501,IPDPS,8419,"
Dynamic vectors are among the most commonly used
data structures in programming. They provide constant time
random access and resizable data storage. Additionally, they
provide constant time insertion (pushback) and deletion (popback)
at the end of the sequence. However, in a multithreaded system,
concurrent pushback and popback operations attempt to update
the same shared object, creating a synchronization bottleneck.

In this paper, we present a lock-free vector design that efficiently
addresses the synchronization bottlenecks by utilizing a combining
technique on pushback operations. Typical combining techniques
come with the price of blocking. Our design introduces combining
without sacrificing lock-freedom. We evaluate the performance
of our design on a dual socket NUMA Intel server. The results
show that our design performs comparably at low loads, and
out-performs prior concurrent blocking and non-blocking vector
implementations at high contention, by as much as 2.7x.

",13.901157680251561,13.102777777777778,146
IPDPS_17_091.txt,14.406010250871095,12.115436075857247,IPDPS,6536,"
Current monitor based systems have many disadvantages for multi object operations. They require the
programmers to (1) manually determine the order of locking
operations, (2) manually determine the points of execution
where threads should signal other threads, (3) use global locks
or perform busy waiting for operations that depend upon a
condition that spans multiple objects. Transactional memory
systems eliminate the need for explicit locks, but do not support
conditional synchronization. They also require the ability to
rollback transactions. In this paper, we propose new monitor
based methods that provide automatic signaling for global
conditions that span multiple objects.

Our system provides automatic notification for global conditions. Assuming that the global condition is a Boolean
expression of local predicates, our method allows efficient
monitoring of the conditions without any need for global locks.
Furthermore, our system solves the compositionally problem
of monitor systems without requiring global locks. We have
implemented our constructs on top of Java and have evaluated
their overhead. Our results show that on most of the multiobject problems, not only our code is simpler but also faster
than Java’s reentrant-lock as well as the Deuce transactional
memory system.

",15.903189008614273,15.006333333333334,194
IPDPS_17_092.txt,11.883814911924091,8.831267923975577,IPDPS,6929,"
We give efficient algorithms to solve fundamental data movement problems on mesh-connected computers
augmented with limited global bandwidth. Adding a small
amount of global bandwidth makes a practical design that
combines aspects of mesh and fully connected models to achieve
the benefits of each. We give algorithms for sorting, finding
the median, finding a spanning tree, and determining various
graph properties to show that the small amount of global
communication can significantly reduce the time, and that
concurrent read helps even more. Most of these algorithms
are optimal. We also extend our results to mesh-connected
computers with row and column buses.

Keywords-mesh-connected computer; limited bandwidth;
",14.790194502661404,13.038119266055048,110
IPDPS_17_093.txt,15.684545417680873,13.517729853742473,IPDPS,3466,"
Energy efficiency in high performance computing
(HPC) will be critical to limit operating costs and carbon
footprints in future supercomputing centers. Energy efficiency of
a computation can be improved by reducing time to completion
without a substantial increase in power drawn or by reducing
power with a little increase in time to completion. We present an
Adaptive Core-specific Runtime (ACR) that dynamically adapts
core frequencies to workload characteristics, and show examples
of both reductions in power and improvement in the average
performance. This improvement in energy efficiency is obtained
without changes to the application.

The adaptation policy embedded in the runtime uses existing
core-specific power controls like software-controlled clock modulation and per-core Dynamic Voltage Frequency Scaling (DVFS)
introduced in Intel Haswell. Experiments on six standard MPI
benchmarks and a real world application show an overall 20%
improvement in energy efficiency with less than 1% increase in
execution time on 32 nodes (1024 cores) using per-core DVFS.

An improvement in energy efficiency of up to 42% is obtained
with the real world application ParaDis through a combination
of speedup and power reduction. For one configuration, ParaDis
achieves an average speedup of 11%, while the power is lowered
by about 31%. The average improvement in the performance
seen is a direct result of the reduction in run-to-run variation
and running at turbo frequencies.

",18.243605946275583,16.078596491228073,229
IPDPS_17_094.txt,15.16571441152799,13.787772775157112,IPDPS,6272,"
Limited power budgets will be one of the biggest
challenges for deploying future exascale supercomputers. One
of the promising ways to deal with this challenge is hardware
overprovisioning, that is, installing more hardware resources than
can be fully powered under a given power limit coupled with
software mechanisms to steer the limited power to where it is
needed most. Prior research has demonstrated the viability of
this approach, but could only rely on small-scale simulations of
the software stack. While such research is useful to understand
the boundaries of performance benefits that can be achieved, it
does not cover any deployment or operational concerns of using
overprovisioning on production systems.

This paper is the first to present an extensible power-aware
resource management framework for production-sized
overprovisioned systems based on the widely established
SLURM resource manager. Our framework provides flexible
plugin interfaces and APIs for power management that can
be easily extended to implement site-specific strategies and for
comparison of different power management techniques. We
demonstrate our framework on a 965-node HA8000 production
system at Kyushu University. Our results indicate that it is
indeed possible to safely overprovision hardware in production.
We also find that the power consumption of idle nodes, which
depends on the degree of overprovisioning, can become a
bottleneck. Using real-world data, we then draw conclusions
about the impact of the total number of nodes provided in an
overprovisioned environment.

",16.280829582073984,15.804605042016806,239
IPDPS_17_095.txt,14.579548150930286,12.195245760511497,IPDPS,6928,"
This paper presents the first systematic study
on co-scheduling independent jobs on integrated CPU-GPU
systems with power caps considered. It reveals the performance
degradations caused by the co-run contentions at the levels of
both memory and power. It then examines the problem of
using job co-scheduling to alleviate the degradations in this
less understood scenario. It offers several algorithms and a
lightweight co-run performance and power predictive model
for computing the performance bounds of the optimal coschedules and finding appropriate schedules. Results show that
the method can efficiently find co-schedules that significantly
improve the system throughput (9-46% on average over the
default schedules).

",16.647925096878797,14.873636363636368,111
IPDPS_17_096.txt,15.752998547812183,14.492560061066811,IPDPS,5010,"
Plans for exascale computing have identified power
and energy as looming problems for simulations running at that
scale. In particular, writing to disk all the data generated by these
simulations is becoming prohibitively expensive due to the energy
consumption of the supercomputer while it idles waiting for data
to be written to permanent storage. In addition, the power cost
of data movement is also steadily increasing. A solution to this
problem is to write only a small fraction of the data generated
while still maintaining the cognitive fidelity of the visualization.
With domain scientists increasingly amenable towards adopting
an in-situ framework that can identify and extract valuable
data from extremely large simulation results and write them to
permanent storage as compact images, a large-scale simulation
will commit to disk a reduced dataset of data extracts that will
be much smaller than the raw results, resulting in a savings in
both power and energy.

The goal of this paper is two-fold: (i) to understand the role
of in-situ techniques in combating power and energy issues of
extreme-scale visualization and (ii) to create a model for performance, power, energy, and storage to facilitate what-if analysis.
Our experiments on a specially instrumented, dedicated 150-node
cluster show that while it is difficult to achieve power savings
in practice using in-situ techniques, applications can achieve
significant energy savings due to shorter write times for in-situ
visualization. We present a characterization of power and energy for in-situ visualization; an application-aware, architecturespecific methodology for modeling and analysis of such in-situ
workflows; and results that uncover indirect power savings in
visualization workflows for high-performance computing (HPC).


",19.78447435784618,20.44896352313167,282
IPDPS_17_097.txt,15.492682223549028,13.695895445009445,IPDPS,7581,"
We present methods for the effective application level reordering of non-blocking RDMA operations. We supplement
out-of-order hardware delivery mechanisms with heuristics
to account for the CPU side overhead of communication and
for differences in network latency: a runtime scheduler takes
into account message sizes, destination and concurrency
and reorders operations to improve overall communication
throughput. Results are validated on InfiniBand and Cray
Aries networks, for SPMD and hybrid (SPMD+OpenMP)
programming models. We show up to 5x potential speedup,
with 30-50% more typical, for synthetic message patterns in
microbenchmarks. We also obtain up to 33% improvement in
the communication stages in application settings. While the
design space is complex, the resulting scheduler is simple,
both internally and at the application level interfaces. It also
provides performance portability across networks and programming models. We believe these techniques can be easily
retrofitted within any application or runtime framework that
uses one-sided communication, e.g. using GASNet, MPI 3.0
RMA or low level APIs such as IBVerbs.

",16.373557378465907,15.8298150887574,171
IPDPS_17_098.txt,14.96099261221066,13.309150642314037,IPDPS,6699,"
We discuss early results with Toucan, a sourceto-source translator that automatically restructures C/C++
MPI applications to overlap communication with computation.
We co-designed the translator and runtime system to enable
dynamic, dependence-driven execution of MPI applications,
and require only a modest amount of programmer annotation.
Co-design was essential to realizing overlap through dynamic
code block reordering and avoiding the limitations of static
code relocation and inlining. We demonstrate that Toucan hides
significant communication in four representative applications
running on up to 24K cores of NERSC’s Edison platform.
Using Toucan, we have hidden from 33% to 85% of the communication overhead, with performance meeting or exceeding
that of painstakingly hand-written overlap variants.

",19.287186520377343,17.614,118
IPDPS_17_099.txt,15.0262677396227,13.63621831025036,IPDPS,12549,"
MPI allows applications to treat processes as a logical collection of integer ranks for each MPI communicator, while
internally translating these logical ranks into actual network
addresses. In current MPI implementations the management and
lookup of such network addresses use memory sizes that are
proportional to the number of processes in each communicator. In this paper, we propose a new mechanism, called AVRankmap, for managing such logical addressing. AV-Rankmap
takes advantage of logical patterns in rank mapping that most
applications naturally tend to have, and it exploits the fact
some aspects of the network address translation are naturally
more performance critical than others. It uses this information
to compress the network address management structures. We
demonstrate that AV-Rankmap can achieve similar or better
performance compared with that of other MPI implementations
while using significantly less memory.

",17.693802365651003,16.039420289855077,139
IPDPS_17_100.txt,14.16429085746315,12.216256984255924,IPDPS,7529,"
The constantly increasing gap between communication and computation performance emphasizes the importance of communication-avoidance techniques. Caching is
a well-known concept used to reduce accesses to slow local
memories. In this work, we extend the caching idea to MPI3 Remote Memory Access (RMA) operations. Here, caching
can avoid inter-node communications and achieve similar
benefits for irregular applications as communication-avoiding
algorithms for structured applications. We propose CLaMPI,
a caching library layered on top of MPI-3 RMA, to automatically optimize code with minimum user intervention. We
demonstrate how cached RMA improves the performance of
a Barnes Hut simulation and a Local Clustering Coefficient
computation up to a factor of 1.8x and 5x, respectively. Due
to the low overheads in the cache miss case and the potential
benefits, we expect that our ideas around transparent RMA
caching will soon be an integral part of many MPI libraries.
",17.122413403193683,15.139498069498071,150
IPDPS_17_101.txt,13.866775735342749,12.954172674837782,IPDPS,6876,"
Neural networks have been traditionally considered robust in the sense that their precision degrades gracefully
with the failure of neurons and can be compensated by
additional learning phases. Nevertheless, critical applications
for which neural networks are now appealing solutions, cannot
afford any additional learning at run-time.

In this paper, we view a multilayer neural network as a
distributed system of which neurons can fail independently,
and we evaluate its robustness in the absence of any (recovery)
learning phase. We give tight bounds on the number of neurons
that can fail without harming the result of a computation.
To determine our bounds, we leverage the fact that neural
activation functions are Lipschitz-continuous. Our bound is on
a quantity, we call the Forward Error Propagation, capturing
how much error is propagated by a neural network when a
given number of components is failing, computing this quantity
only requires looking at the topology of the network, while
experimentally assessing the robustness of a network requires
the costly experiment of looking at all the possible inputs and
testing all the possible configurations of the network corresponding to different failure situations, facing a discouraging
combinatorial explosion.

We distinguish the case of neurons that can fail and stop
their activity (crashed neurons) from the case of neurons that
can fail by transmitting arbitrary values (Byzantine neurons).
In the crash case, our bound involves the number of neurons
per layer, the Lipschitz constant of the neural activation
function, the number of failing neurons, the synaptic weights
and the depth of the layer where the failure occurred. In the
case of Byzantine failures, our bound involves, in addition, the
synaptic transmission capacity. Interestingly, as we show in
the paper, our bound can easily be extended to the case where
synapses can fail.

We present three applications of our results. The first is a
quantification of the effect of memory cost reduction on the
accuracy of a neural network. The second is a quantification of
the amount of information any neuron needs from its preceding
layer, enabling thereby a boosting scheme that prevents neurons
from waiting for unnecessary signals. Our third application
is a quantification of the trade-off between neural networks
robustness and learning cost.

",16.351538315227643,15.954868370112276,370
IPDPS_17_102.txt,13.561895871917262,11.329167243374645,IPDPS,7172,"
The Tucker decomposition expresses a given tensor as the product of a small core tensor and a set of factor
matrices. Our objective is to develop an efficient distributed
implementation for the case of dense tensors. The implementation is based on the HOOI (Higher Order Orthogonal Iterator)
procedure, wherein the tensor-times-matrix product forms the
core routine. Prior work have proposed heuristics for reducing
the computational load and communication volume incurred
by the routine. We study the two metrics in a formal and
systematic manner, and design strategies that are optimal under
the two fundamental metrics. Our experimental evaluation on
a large benchmark of tensors shows that the optimal strategies
provide significant reduction in load and volume compared to
prior heuristics, and provide up to 7x speed-up in the overall
running time.
",15.470042427545799,14.78268656716418,135
IPDPS_17_103.txt,12.753466805930369,11.00766360754162,IPDPS,6691,"
Given an input tensor, its CANDECOMP/PARAFAC
decomposition (or CPD) is a low-rank representation. CPDs
are of particular interest in data analysis and mining, especially when the data tensor is sparse and of higher order
(dimension). This paper focuses on the central bottleneck of a
CPD algorithm, which is evaluating a sequence of matricized
tensor times Khatri-Rao products (MTTKRPs). To speed up
the MTTKRP sequence, we propose a novel, adaptive tensor
memoization algorithm, ADATM. Besides removing redundant
computations within the MTTKRP sequence, which potentially
reduces its overall asymptotic complexity, our technique also
allows a user to make a space-time tradeoff by automatically
tuning algorithmic and machine parameters using a model-driven
framework. Our method improves as the tensor order grows,
making its performance more scalable for higher-order data
problems. We show speedups of up to 8x and 820 on real
sparse data tensors with orders as high as 85 over the SPLATT
package and Tensor Toolbox library respectively; and on a full
CPD algorithm (CP-ALS), ADATM can be up to 8x faster than
state-of-the-art method implemented in SPLATT.

",15.719379583869454,15.9621044546851,187
IPDPS_17_104.txt,13.58803209441363,11.673119316705638,IPDPS,3804,"
HPC systems are increasingly used for data intensive computations which exhibit irregular memory accesses,
non-uniform work distributions, large memory footprints, and
high memory bandwidth demands. To address these challenging demands, HPC systems are turning to many-core architectures that feature a large number of energy-efficient cores
backed by high-bandwidth memory. These features are exemplified in Intel’s recent Knights Landing many-core processor
(KNL), which typically has 68 cores and 16GB of on-package
multi-channel DRAM (MCDRAM). This work investigates how
the novel architectural features offered by KNL can be used
in the context of decomposing sparse, unstructured tensors
using the canonical polyadic decomposition (CPD). The CPD
is used extensively to analyze large multi-way datasets arising
in various areas including precision healthcare, cybersecurity,
and e-commerce. Towards this end, we (i) develop problem
decompositions for the CPD which are amenable to hundreds
of concurrent threads while maintaining load balance and
low synchronization costs; and (ii) explore the utilization of
architectural features such as MCDRAM. Using one KNL
processor, our algorithm achieves up to 1.8x speedup over
a dual socket Intel Xeon system with 44 cores.

",17.613555460941566,16.854285714285712,192
IPDPS_17_105.txt,13.309695003013555,10.710915229417509,IPDPS,6229,"
We consider load balancing in a network of caching
servers delivering contents to end users. Randomized load
balancing via the so-called power of two choices is a wellknown approach in parallel and distributed systems that reduces
network imbalance. In this paper, we propose a randomized
load balancing scheme which simultaneously considers cache size
limitation and proximity in the server redirection process.

Since the memory limitation and the proximity constraint
cause correlation in the server selection process, we may not
benefit from the power of two choices in general. However, we
prove that in certain regimes, in terms of memory limitation
and proximity constraint, our scheme results in the maximum
load of order O(log log 7) (here n is the number of servers and
requests), and at the same time, leads to a low communication
cost. This is an exponential improvement in the maximum load
compared to the scheme which assigns each request to the nearest
available replica. Finally, we investigate our scheme performance
by extensive simulations.

",16.613394197324528,14.841428571428573,169
IPDPS_17_106.txt,14.401269016497796,12.659112848081865,IPDPS,7983,"
MapReduce applications, which require access to
a large number of computing nodes, are commonly deployed
in heterogeneous environments. The performance discrepancy
between individual nodes in a heterogeneous cluster present
significant challenges to attain good performance in MapReduce
jobs. MapReduce implementations designed and optimized for
homogeneous environments perform poorly on heterogeneous
clusters.

We attribute suboptimal performance in heterogeneous clusters to significant load imbalance between map tasks. We identify
two MapReduce designs that hinder load balancing: (1) static
binding between mappers and their data makes it difficult to
exploit data redundancy for load balancing; (2) uniform map
sizes is not optimal for nodes with heterogeneous performance.
To address these issues, we propose FlexMap, a user-transparent
approach that dynamically provisions map tasks to match distinct
machine capacity in heterogeneous environments. We implemented FlexMap in Hadoop-2.6.0. Experimental results show that
it reduces job completion time by as much as 40% compared to
stock Hadoop and 30% to SkewTune.

",17.122413403193683,16.67397435897436,159
IPDPS_17_107.txt,15.730778662332874,14.030845086839957,IPDPS,5919,"
Large-scale data analysis applications are becoming more and more prevalent in a wide variety of areas.
These applications are composed of many currently available
programs called analysis components. Thousands of analysis component processes are orchestrated on many compute
nodes. This paper proposes a novel self-tuning framework
for optimizing an application’s throughput in large-scale data
analysis. One challenge is developing efficient orchestration
that takes into account the diversity of analysis components
and the varying performances of compute nodes. In our
previous work, we achieved such an orchestration to a certain
degree by introducing our own middleware, which wraps
each analysis component as a remote procedure call (RPC)
service. The middleware also pools the processes to reduce
startup overhead, which is a serious obstacle to achieving
high throughput. This work tackles the remaining task of
tuning the size of the analysis components’ process pools to
maximize the application’s throughput. This is challenging
because analysis components differ drastically in turnaround
times and memory footprints. The size of the process pool
for each type of analysis component should be set by giving
consideration to these properties as well as the constraints on
both the memory capacity and the processor core counts. In this
work, we formulate this task as a linear programming problem
and obtain the optimal pool sizes by solving it. Compared to our
previous work, we significantly improved the scalability of our
framework by reformulating the performance model to work
on hundreds of heterogeneous nodes. We also extended the
service allocation mechanism to manage the computational load
on each compute node and reduce communication overhead.
The experimental results show that our approach is scalable
to thousands of analysis component processes running on 200
compute nodes across three clusters. Moreover, our approach
significantly reduces memory footprint.

",16.567214971974305,14.988432432432436,299
IPDPS_17_108.txt,14.492645553974423,12.808842981553443,IPDPS,7905,"
In this paper we present Mimir, a new implementation of MapReduce over MPI. Mimir inherits the core principles
of existing MapReduce frameworks, such as MR-MPI, while
redesigning the execution model to incorporate a number of
sophisticated optimization techniques that achieve similar or
better performance with significant reduction in the amount of
memory used. Consequently, Mimir allows significantly larger
problems to be executed in memory, achieving large performance
gains. We evaluate Mimir with three benchmarks on two highend platforms to demonstrate its superiority compared with that
of other frameworks.

",17.410965686947208,17.350421348314608,90
IPDPS_17_109.txt,15.819969301839162,14.169944940180176,IPDPS,6820,"
Data compression has become a commodity feature
for space efficiency and reliability in flash-based storage systems
by reducing write traffic and space capacity demand. However,
it introduces noticeable processing overheads on the critical //O
path, which degrades the system performance significantly. Existing data compression schemes for flash-based storage systems
use fixed compression algorithms for all the incoming write
data, failing to recognize and exploit the significant diversity
in compressibility and access patterns of data and missing
an opportunity to improve the system performance, the space
efficiency or both. To achieve a reasonable trade-off between
these two important design objectives, in this paper we introduce
an Elastic Data Compression scheme, called EDC, which exploits
the data compressibility and access intensity characteristics by
judiciously matching data of different compressibility with different compression algorithms while leveraging the access idleness.
Specifically, for compressible data blocks EDC exploits the
compression diversity of the workload, and employs algorithms of
higher compression rate in periods of lower system utilization and
algorithms of lower compression rate in periods of higher system
utilization. For non-compressible (or very lowly compressible)
data blocks, it will write them through to the flash storage
directly without any compression. The experiments conducted
on our lightweight prototype implementation of the EDC system
show that EDC saves storage space by up to 38.7%, with an
average of 33.7%. In addition, it significantly outperforms the
fixed compression schemes in the I/O performance measure by
up to 61.4%, with an average of 36.7%.

",20.027631371098494,19.7255,255
IPDPS_17_110.txt,13.567625721047584,10.462897446953594,IPDPS,7418,"
Modern Graphics Processing Units (GPUs) provide
much higher off-chip memory bandwidth than CPUs, but many
GPU applications are still limited by memory bandwidth. Unfortunately, off-chip memory bandwidth is growing slower than the
number of cores and has become a performance bottleneck. Thus,
optimizations of effective memory bandwidth play a significant
role for scaling the performance of GPUs.

Memory compression is a promising approach for improving
memory bandwidth which can translate into higher performance
and energy efficiency. However, compression is not free and
its challenges need to be addressed, otherwise the benefits of
compression may be offset by its overhead. We propose an
entropy encoding based memory compression (E7MC) technique
for GPUs, which is based on the well-known Huffman encoding.
We study the feasibility of entropy encoding for GPUs and show
that it achieves higher compression ratios than state-of-the-art
GPU compression techniques. Furthermore, we address the key
challenges of probability estimation, choosing an appropriate
symbol length for encoding, and decompression with low latency.
The average compression ratio of E7MC is 53% higher than
the state of the art. This translates into an average speedup
of 20% compared to no compression and 8% higher compared
to the state of the art. Energy consumption and energy-delayproduct are reduced by 13% and 27%, respectively. Moreover,
the compression ratio achieved by E?MC is close to the optimal
compression ratio given by Shannon’s source coding theorem.

",15.80454591841917,12.72058823529412,240
IPDPS_17_111.txt,15.1172212658354,13.282262964254262,IPDPS,8203,"
Today’s HPC applications are producing extremely large amounts of data, such that data storage and
analysis are becoming more challenging for scientific research.
In this work, we design a new error-controlled lossy compression algorithm for large-scale scientific data. Our key
contribution is significantly improving the prediction hitting
rate (or prediction accuracy) for each data point based on
its nearby data values along multiple dimensions. We derive
a series of multilayer prediction formulas and their unified
formula in the context of data compression. One serious
challenge is that the data prediction has to be performed based
on the preceding decompressed values during the compression
in order to guarantee the error bounds, which may degrade
the prediction accuracy in turn. We explore the best layer
for the prediction by considering the impact of compression
errors on the prediction accuracy. Moreover, we propose an
adaptive error-controlled quantization encoder, which can further improve the prediction hitting rate considerably. The data
size can be reduced significantly after performing the variablelength encoding because of the uneven distribution produced by
our quantization encoder. We evaluate the new compressor on
production scientific data sets and compare it with many other
state-of-the-art compressors: GZIP, FPZIP, ZFP, SZ-1.1, and
ISABELA. Experiments show that our compressor is the best
in class, especially with regard to compression factors (or bitrates) and compression errors (including RMSE, NRMSE, and
PSNR). Our solution is better than the second-best solution
by more than a 2x increase in the compression factor and
3.8x reduction in the normalized root mean squared error on
average, with reasonable error bounds and user-desired bitrates.

",17.845785984406827,16.048489304812836,276
IPDPS_17_112.txt,16.021309288088247,14.268208150662733,IPDPS,3772,"
Redundant computations appear during the execution of real programs. Multiple factors contribute to these
unnecessary computations, such as repetitive inputs and patterns, calling functions with the same parameters or bad
programming habits. Compilers minimize non useful code
with static analysis. However, redundant execution might be
dynamic and there are no current approaches to reduce these
inefficiencies. Additionally, many algorithms can be computed
with different levels of accuracy. Approximate computing
exploits this fact to reduce execution time at the cost of slightly
less accurate results. In this case, expert developers determine
the desired tradeoff between performance and accuracy for
each application.

In this paper, we present Approximate Task Memoization
(ATM), a novel approach in the runtime system that transparently exploits both dynamic redundancy and approximation
at the task granularity of a parallel application. Memoization
of previous task executions allows predicting the results of
future tasks without having to execute them and without losing
accuracy. To further increase performance improvements, the
runtime system can memoize similar tasks, which leads to
task approximate computing. By defining how to measure task
similarity and correctness, we present an adaptive algorithm
in the runtime system that automatically decides if task
approximation is beneficial or not. When evaluated on a real
8-core processor with applications from different domains
(financial analysis, stencil-computation, machine-learning and
linear-algebra), ATM achieves a 1.4x average speedup when
only applying memoization techniques. When adding task
approximation, ATM achieves a 2.5x average speedup with an
average 0.7% accuracy loss (maximum of 3.2%).

",17.822506213463445,16.66828685258964,256
IPDPS_17_113.txt,15.545436438861614,13.607902183120313,IPDPS,7790,"
A surprising development in recently announced
HPC platforms is the addition of, sometimes massive amounts
of, persistent (nonvolatile) memory (NVM) in order to increase
memory capacity and compensate for plateauing I/O capabilities. However, there are no portable and scalable programming
interfaces using aggregate NVM effectively. This paper introduces Papyrus: a new software system built to exploit emerging
capability of NVM in HPC architectures. Papyrus (or Parallel
Aggregate Persistent -YRU- Storage) is a novel programming
system that provides features for scalable, aggregate, persistent
memory in an extreme-scale system for typical HPC usage
scenarios. Papyrus mainly consists of Papyrus Virtual File
System (VFS) and Papyrus Template Container Library (TCL).
Papyrus VFS provides a uniform aggregate NVM storage
image across diverse NVM architectures. It enables Papyrus
TCL to provide a portable and scalable high-level container
programming interface whose data elements are distributed
across multiple NVM nodes without requiring the user to
handle complex communication, synchronization, replication,
and consistency model. We evaluate Papyrus on two HPC
systems, including UTK Beacon and NERSC Cori, using real
NVM storage devices.
",19.287186520377343,17.77208333333333,178
IPDPS_17_114.txt,15.068303945450626,13.401375922206483,IPDPS,7468,"
Substantial advances in nonvolatile memory
(NVM) technologies have motivated wide-spread integration of
NVM into mobile, enterprise, and HPC systems. Recently, considerable research has focused on architectural integration of
NVM and respective programming systems, exploiting NVM’s
trait of persistence correctly and efficiently. In this regard, we
design several novel language-based optimization techniques
for programming NVM and demonstrate them as an extension
of our NVL-C system. Specifically, we focus on optimizing the
performance of atomic updates to complex data structures
residing in NVM. We build on two variants of automatic
undo logging: canonical undo logging, and shadow updates.
We show these techniques can be implemented transparently
and efficiently, using dynamic selection and other logging
optimizations. Our empirical results on several applications
gathered on an NVM testbed illustrate that our cost-modelbased dynamic selection technique can accurately choose the
best logging variant across different NVM modes and input
sizes. In comparison to statically choosing canonical undo
logging, this improvement reduces execution time to as little
as 53% for block-addressable NVM and 73% for emulated
byte-addressable NVM on a Fusion-io ioScale device.

",18.377959752453624,17.428913043478264,186
IPDPS_17_115.txt,14.253242630345675,12.2021095890411,IPDPS,7264,"
Distributed burst buffers are a promising storage
architecture for handling I/O workloads for exascale computing.
Their aggregate storage bandwidth grows linearly with system
node count. However, although scientific applications can achieve
scalable write bandwidth by having each process write to its
node-local burst buffer, metadata challenges remain formidable,
especially for files shared across many processes. This is due to the
need to track and organize file segments across the distributed
burst buffers in a global index. Because this global index can
be accessed concurrently by thousands or more processes in a
scientific application, the scalability of metadata management is
a severe performance-limiting factor.

In this paper, we propose MetaKV: a key-value store that
provides fast and scalable metadata management for HPC
metadata workloads on distributed burst buffers. MetaKV complements the functionality of an existing key-value store with
specialized metadata services that efficiently handle bursty and
concurrent metadata workloads: compressed storage management, supervised block clustering, and log-ring based collective
message reduction. Our experiments demonstrate that MetaKV
outperforms the state-of-the-art key-value stores by a significant
margin. It improves put and get metadata operations by as much
as 2.66 and 6.29x, respectively, and the benefits of MetaKV
increase with increasing metadata workload demand.

",17.37919286519448,16.210952380952385,213
IPDPS_17_116.txt,14.282054348829313,12.153675645342314,IPDPS,5820,"
In this paper, we propose PGIS, a parallelism and
garbage collection aware I/O Scheduler, which identifies the hot
data based on trace characteristics to exploit the channel level
internal parallelism of flash-based storage systems. PGIS not
only fully exploits abundant channel resource in the SSD, but
also it introduces a hot data identification mechanism to reduce
the garbage collection overhead. By dispatching hot read data
to different channel, the channel level internal parallelism is
fully exploited. By dispatching hot write data to the same
physical block, the garbage collection overhead has been
alleviated. The experiment results show that compared with
existing I/O schedulers, PGIS improves the response time and
garbage collection performance significantly. Consequently,
PGIS reduces the garbage collection overhead up to 30.9%,
while exploiting channel level internal parallelism.

",17.122413403193683,16.23242424242424,134
ISC_17_001.txt,14.938483651090749,12.626304845102887,ISC,6350,"
. The formation and dynamics of dunes is an important phenomenon that occurs in many environmental systems, such as riverbeds.
The physical interactions are complex and thus evaluating and quantifying the factors of influence is challenging. Simulation models can be
used to conduct large scale parameter studies and allow a more detailed
analysis of the system than laboratory experiments. Here, we present new
coupled numerical models for sediment transport that are based on first
principles. The lattice Boltzmann method is used in combination with
a non-smooth granular dynamics model to simulate the fluid flow and
the sediment particles. Numerical predictions of dune formation require
a fully resolved modeling of the particulate flow which is only achieved
by massively parallel simulations. For that purpose, the method employs
advanced parallel grid refinement techniques and carefully designed compute kernels. The weak- and strong-scaling behavior is evaluated in detail
and shows overall excellent parallel performance and efficiency.

",16.678067442207542,14.751266233766234,155
ISC_17_002.txt,16.103560831198116,14.281956091517571,ISC,5878,"
. Covariance matrices are ubiquitous in computational science
and engineering. In particular, large covariance matrices arise from multivariate spatial data sets, for instance, in climate/weather modeling applications to improve prediction using statistical methods and spatial data.
One of the most time-consuming computational steps consists in calculating the Cholesky factorization of the symmetric, positive-definite covariance matrix problem. The structure of such covariance matrices is also
often data-sparse, in other words, effectively of low rank, though formally
dense. While not typically globally of low rank, covariance matrices in
which correlation decays with distance are nearly always hierarchically
of low rank. While symmetry and positive definiteness should be, and
nearly always are, exploited for performance purposes, exploiting low
rank character in this context is very recent, and will be a key to solving these challenging problems at large-scale dimensions. The authors
design a new and flexible tile row rank Cholesky factorization and propose a high performance implementation using OpenMP task-based programming model on various leading-edge manycore architectures. Performance comparisons and memory footprint saving on up to 200K x 200i
covariance matrix size show a gain of more than an order of magnitude for both metrics, against state-of-the-art open-source and vendor
optimized numerical libraries, while preserving the numerical accuracy
fidelity of the original model. This research represents an important milestone in enabling large-scale simulations for covariance-based scientific
applications.

",19.174585242480724,18.18666666666667,237
ISC_17_003.txt,14.508564009361375,12.607906416754425,ISC,6757,"
. This article introduces EDGE, a solver package for fused
seismic simulations. Fused seismic simulations are a novel technique
addressing one of the grand challenges of computational seismology: large
ensemble runs of geometrically similar forward simulations. Application
fields include, but are not limited to: uncertainty quantification in the
context of seismic hazard analysis or the accurate derivation of velocity
models through tomographic inversion. For efficient and accurate handling of complex model geometries (topography, fault geometries, material heterogeneities), EDGE utilizes the Discontinuous Galerkin (DG)
method for spatial and Arbitrary high order DERivatives (ADER) for
time discretization, implemented for unstructured tetrahedral meshes.
EDGE’s ADER-DG scheme requires sparse and dense matrix-matrix
multiplications at the kernel level. By choosing a sufficient memory layout and relying on runtime code generation and specialization, both,
sparse and dense operations, can be efficiently vectorized on wide-SIMD
machines. We present a convergence study of single and fused seismic
simulations, code verification in an established benchmark, as well as a
detailed performance assessment for different discretization orders. As
target architecture we select the recently released Intel Xeon Phi processor, which powers the Theta and Cori-II supercomputers. For a single
sixth order seismic forward simulation we achieved 10.4 PFLOPS of hardware performance and 5.0 PFLOPS for fused simulations in fourth order,
both occupying 9,000 nodes of Cori-II. From a throughput perspective,
fused seismic simulations can outperform a single forward simulation by
1.8x to 4.6x, depending on the chosen order of the method.

",18.243605946275583,17.853979591836737,252
ISC_17_004.txt,14.510487974497106,13.135719746402746,ISC,6464,"
. Molecular Dynamics is an important tool for computational
biologists, chemists, and materials scientists, consuming a sizable amount
of supercomputing resources. Many of the investigated systems contain
charged particles, which can only be simulated accurately using a longrange solver, such as PPPM. We extend the popular LAMMPS molecular
dynamics code with an implementation of PPPM particularly suitable
for the second generation Intel Xeon Phi. Our main target is the optimization of computational kernels by means of vectorization, and we
observe speedups in these kernels of up to 12x. These improvements
carry over to LAMMPS users, with overall speedups ranging between
2-3x, without requiring users to retune input parameters. Furthermore,
our optimizations make it easier for users to determine optimal input
parameters for attaining top performance.

",17.5058628484301,16.984600000000004,126
ISC_17_005.txt,15.767762561578838,14.01119521711377,ISC,5778,"
. Reduction of communication and efficient partitioning are
key issues for achieving scalability in hierarchical N-Body algorithms
like Fast Multipole Method (FMM). In the present work, we propose three
independent strategies to improve partitioning and reduce communication. First, we show that the conventional wisdom of using space-filling
curve partitioning may not work well for boundary integral problems,
which constitute a significant portion of FMM’s application user base. We
propose an alternative method that modifies orthogonal recursive bisection to relieve the cell-partition misalignment that has kept it from scaling previously. Secondly, we optimize the granularity of communication
to find the optimal balance between a bulk-synchronous collective communication of the local essential tree and an RDMA per task per cell.
Finally, we take the dynamic sparse data exchange proposed by Hoefler
et al. [1] and extend it to a hierarchical sparse data exchange, which is
demonstrated at scale to be faster than the MPI library’s MPI_Alltoallv
that is commonly used.

",18.7741,17.302329192546583,164
ISC_17_006.txt,16.40845378920313,15.044930393697722,ISC,8588,"
. With the prevalence of the World Wide Web and social networks, there has been a growing interest in high performance analytics
for constantly-evolving dynamic graphs. Modern GPUs provide massive
amount of parallelism for efficient graph processing, but the challenges
remain due to their lack of support for the near real-time streaming
nature of dynamic graphs. Specifically, due to the current high volume and velocity of graph data combined with the complexity of user
queries, traditional processing methods by first storing the updates and
then repeatedly running static graph analytics on a sequence of versions or snapshots are deemed undesirable and computational infeasible
on GPU. We present EvoGraph, a highly efficient and scalable GPUbased dynamic graph analytics framework that incrementally processes
graphs on-the-fly using fixed-sized batches of updates. The runtime realizes this vision with a user friendly programming model, along with a
vertex property-based optimization to choose between static and incremental execution; and efficient utilization of all hardware resources using
GPU streams, including its computational and data movement engines.
Extensive experimental evaluations for a wide variety of graph inputs
and algorithms demonstrate that EvoGraph achieves up to 429 million
updates/sec and over 2321 speedup compared to the competing frameworks such as STINGER.

",19.620377997778096,20.225192307692314,209
ISC_17_007.txt,15.720761233210663,14.140839448343012,ISC,6074,"
. Support vector machines (SVMs) are conventionally batch
trained. Such implementations can be very inefficient for online streaming applications demanding real-time guarantees, as the inclusion of each
new data point requires retraining of the model from scratch. This paper
focuses on the high-performance implementation of an accurate incremental SVM algorithm on Intel® Xeon Phi™ processors that efficiently
updates the trained SVM model with streaming data. We propose a
novel cycle break heuristic to fix an inherent drawback of the algorithm
that leads to a deadlock scenario which is not acceptable in real-world
applications. We further employ intelligent caching of dynamically changing data as well as other programming optimization ideas to speed up
the incremental SVM algorithm. Experiments on a number of real-world
datasets show that our implementation achieves high performance on
Intel® Xeon Phi™ processors (1.1 — 2.1x faster than Intel® Xeon®
processors) and is up to 2.1x faster than existing high-performance incremental algorithms while achieving comparable accuracy.

",18.422482065455632,17.31050898203593,165
ISC_17_008.txt,15.10570464680952,13.196356904085139,ISC,6353,"
. In this work we present AWP-ODC-OS§, an end-to-end optimization of AWP-ODC targeting homogeneous, manycore supercomputers. AWP-ODC is an established community software package simulating seismic wave propagation using a staggered finite difference scheme
which is fourth order accurate in space and second order in time. Recent
production simulations, e.g. using the software for the computation of
seismic hazard maps, largely relied on GPU accelerated supercomputers.
In contrast, our work gives a comprehensive overview of the required
steps to achieve near-optimal performance on the Intel® Xeon Phi™
x200 processor (code-named Knights Landing), and compares our competitive performance results to the most recent GPU architectures.

At the level of a single vector operation, we apply the vector folding technique to AWP-ODC-OS, yielding a 1.6x performance increase
over traditional vectorization. Further, we present a novel strategy utilizing both DDR4 RAM and High Bandwidth Memory, increasing the
maximum problem size by 26% while still operating at maximum performance. The presented shared and distributed parallelization carefully
schedules work to the cores and ensures overlapping communication and
computation. We conclude with a detailed study of AWP-ODC-O$’s fullapplication performance on the Intel Xeon Phi x200 processor, achieving
up to 98.5% of the most recent P100 GPU generation’s performance.
Additionally, our weak scaling study on up to 9,000 nodes of the supercomputer Cori Phase II achieves a parallel efficiency of greater than
91%, equivalent to the performance of over twenty thousand NVIDIA
Tesla K20X GPUs.

",17.37919286519448,17.103809523809527,255
ISC_17_009.txt,14.749596750195813,12.769109414273832,ISC,7093,"
. Many important applications — from big data analytics to
information retrieval, gene expression analysis, and numerical weather
prediction — require the solution of large dense singular value decompositions (SVD). In many cases the problems are too large to fit into
the computer’s main memory, and thus require specialized out-of-core
algorithms that use disk storage. In this paper, we analyze the SVD
communications, as related to hierarchical memories, and design a class
of algorithms that minimizes them. This class includes out-of-core SVDs
but can also be applied between other consecutive levels of the memory
hierarchy, e.g., GPU SVD using the CPU memory for large problems.
We call these out-of-memory (OOM) algorithms. To design OOM SVDs,
we first study the communications for both classical one-stage blocked
SVD and two-stage tiled SVD. We present the theoretical analysis and
strategies to design, as well as implement, these communication avoiding OOM SVD algorithms. We show performance results for multicore
architecture that illustrate our theoretical findings and match our performance models.

",16.975882523387877,14.897127192982456,174
ISC_17_010.txt,16.706635304547603,15.721040677875209,ISC,7160,"
. The increasing complexity and heterogeneity of extreme
scale systems makes the optimization of large scale scientific applications
particularly challenging. Efficiently leveraging these complex systems
requires a great deal of technical expertise and a considerable amount
of man-hours. The computational neuroscience community relies on an
handful of those frameworks to model the electrical activity of brain tissue at different scales. As the members of the Blue Brain Project actively
contribute to a large part of those frameworks, it becomes mandatory to
implement a strategy to reduce the overall development cost. Therefore,
we present Neuromapp, a computational neuroscience mini-application
framework. Neuromapp consists of a number of mini-apps (small standalone applications) that represent a single functionality in one of the
large scientific frameworks. The collection of several mini-apps forms a
skeleton which is able to reproduce the original workflow of the scientific
application. Thus, it becomes easy to investigate both single component and workflow optimizations, new software and hardware systems
or future system design. New solutions can then be integrated into the
large scientific applications if proved to be successful, reducing the overall
development and optimization effort.

",17.37919286519448,16.596028368794332,189
ISC_17_011.txt,14.96589469088726,13.749831870318832,ISC,5793,"
. Fast Fourier Transforms (FFTs) are exploited in a wide variety of fields ranging from computer science to natural sciences and engineering. With the rising data production bandwidths of modern FFT
applications, judging best which algorithmic tool to apply, can be vital
to any scientific endeavor. As tailored FFT implementations exist for an
ever increasing variety of high performance computer hardware, choosing the best performing FFT implementation has strong implications for
future hardware purchase decisions, for resources FFTs consume and for
possibly decisive financial and time savings ahead of the competition.
This paper therefor presents gearshifft, which is an open-source and
vendor agnostic benchmark suite to process a wide variety of problem
sizes and types with state-of-the-art FFT implementations (fftw, cLFFT
and cuFFT). gearshifft provides a reproducible, unbiased and fair comparison on a wide variety of hardware to explore which FFT variant is
best for a given problem size.

",20.027631371098494,20.889210526315797,153
ISC_17_012.txt,14.369801511973503,12.422515140505695,ISC,7073,"
. The scale of applications and computing systems is tremendously increasing and needs to increase even more to realize exascale
systems. As the number of nodes keeps growing, communication has
become key to high performance.

The Message Passing Interface (MPI) has evolved to the de facto standard for inter-node data transfers. Consequently, MPI is well suited to
serve as proxy for an analysis of communication characteristics of exascale proxy applications.

This work presents characteristics like time spent in certain operations, point-to-point versus collective communication, and message sizes
and rates, gathered from a comprehensive trace analysis. We provide an
understanding of how applications use MPI to exploit node-level parallelism, always with respect to scalability, and also locate parts which
require more optimization. We emphasize on the analysis of the message
matching and report queue lengths and associated matching rates.

It is shown that most data is transferred via point-to-point operations,
but the most time is spent in collectives. Message matching rates significantly depend on the length of message queues, which tend to increase
with the number of processes. As messages are also become smaller, the
matching is important to high message rates in large-scale applications.

",14.117786864225405,13.332000000000004,201
ISC_17_013.txt,16.242267659607098,15.078745262503883,ISC,6970,"
. This study compares the performance of high-order discontinuous Galerkin finite elements on modern hardware. The main computational kernel is the matrix-free evaluation of differential operators
by sum factorization, exemplified on the symmetric interior penalty discretization of the Laplacian as a metric for a complex application code in
fluid dynamics. State-of-the-art implementations of these kernels stress
both arithmetics and memory transfer. The implementations of SIMD
vectorization and shared-memory parallelization are detailed. Computational results are presented for dual-socket Intel Haswell CPUs at 28
cores, a 64-core Intel Knights Landing, and a 16-core IBM Power8 processor. Up to polynomial degree six, Knights Landing is approximately twice
as fast as Haswell. Power8 performs similarly to Haswell, trading a higher
frequency for narrower SIMD units. The performance comparison shows
that simple ways to express parallelism through for loops perform better
on medium and high core counts than a more elaborate task-based parallelization with dynamic scheduling according to dependency graphs,
despite less memory transfer in the latter algorithm.

",16.678067442207542,16.051976744186046,173
ISC_17_014.txt,15.508494479129538,13.262578360151178,ISC,6773,"
. The Parallel Research Kernels (PRK) are a tool to study
parallel architectures and runtime systems from an application perspective. They provide paper and pencil specifications and reference implementations of elementary operations covering a broad range of parallel
application patterns. Most of the current PRK are trivially statically
load-balanced. In a prior study we described a novel PRK that requires
dynamic load balancing, and demonstrated its effectiveness to assess
automatic dynamic load balancing capabilities of runtimes. While useful,
it did not fully represent the problem of greatest interest to researchers
of extreme scale computing systems, namely the occurrence of localized, discrete, transient disturbances (system noise). For that purpose we
introduce a new PRK, inspired by Adaptive Mesh Refinement (AMR)
applications, which provides a proxy for the most detrimental property
of noise, namely abrupt and discrete change of local system load. We
give a detailed specification of the new PRK, highlighting the challenges
and corresponding design choices that make it compact, arbitrarily scalable and self-verifying. We also present an implementation of the AMR
PRK in MPI, with application-specific load balancing, as well as one in
Adaptive MPI that leverages the MPI version, but adds runtime orchestrated dynamic load balancing, along with a set of performance results.
These show that for applications that can be load balanced statically,
but experience occasional local changes in computational load, automatic
dynamic load balancing typically does not offer an advantage.

",18.83193753551143,17.43666666666667,237
ISC_17_015.txt,15.558509097121114,14.033873603576442,ISC,5740,"
. Numerical simulations present challenges because they generate petabyte-scale data that must be extracted and reduced during the
simulation. We demonstrate a seamless integration of feature extraction
for a simulation of turbulent fluid dynamics. The simulation produces on
the order of 6 TB per timestep. In order to analyze and store this data, we
extract velocity data from a dilated volume of the strong vortical regions
and also store a lossy compressed representation of the data. Both reduce
data by one or more orders of magnitude. We extract data from user checkpoints in transit while they reside on temporary burst buffer SSD stores.
In this way, analysis and compression algorithms are designed to meet specific time constraints so they do not interfere with simulation computations. Our results demonstrate that we can perform feature extraction on
a world-class direct numerical simulation of turbulence while it is running
and gather meaningful scientific data for archival and post analysis.

",15.742502247213078,14.160565286624209,158
ISC_17_016.txt,14.877939114729873,12.884985241049264,ISC,7361,"
. This paper presents a survey of architectural features among
four generations of Intel server processors (Sandy Bridge, Ivy Bridge,
Haswell, and Broadwell) with a focus on performance with floating point
workloads. Starting at the core level and going down the memory hierarchy we cover instruction throughput for floating-point instructions, L1
cache, address generation capabilities, core clock speed and its limitations, L2 and L3 cache bandwidth and latency, the impact of Cluster on
Die (CoD) and cache snoop modes, and the Uncore clock speed. Using
microbenchmarks we study the influence of these factors on code performance. We show that the energy efficiency of the LINPACK and HPCG
benchmarks can be improved significantly by tuning the Uncore clock
speed without sacrificing performance, and that the Graph500 benchmark performance may benefit from a suitable choice of cache snoop
mode settings.

",17.410965686947208,17.959202898550725,139
ISC_17_017.txt,14.0352088945376,12.326775358885019,ISC,6120,"
. Nowadays parallel file systems have been widely used in
many supercomputers. Lustre is one of the most used parallel file systems, and its enhanced file system named FEFS (Fujitsu Exabyte File
System) has been used at K computer. The K computer has adopted twolayered file system consisting of a local file system and a shared global
file system with data staging scheme in order to guarantee sufficient I/O
throughput on the local file system during computation. However, huge
data staging on the shared file system sometimes has led to big I/O
interference in light-weight file accesses which have taken place at the
same time. Alleviation of such I/O interference on shared file systems
is an important issue in managing a big scale of parallel file systems in
shared use. In this paper, we focus on I/O interference alleviation by
using workload-aware striping and load-balancing. Appropriate striping
configuration with effective load-balancing in service thread allocation
for incoming I/O requests has improved performance of light-weight file
accesses against huge data accesses without excessive sacrifice to data
staging performance at the K computer. It is expected that the proposed
optimization can be used as a system-wide I/O interference mitigation
approach.

",16.218646115125612,14.754730861244024,210
ISC_17_018.txt,16.604630763502715,15.415857296266953,ISC,7553,"
. Broadly, there exist two protocols for point-to-point data
transfer in the Message Passing Interface (MPI) programming model Eager and Rendezvous. State-of-the-art MPI libraries decide the switch
point between these protocols based on the trade-off between memory
footprint of the MPI library and communication performance without
considering the overlap potential of these communication protocols. This
results in sub-par overlap of communication and computation at the
application level. While application developers can manually tune this
threshold to achieve better overlap, it involves significant effort. Further,
the communication pattern may change based on the size of the job and
the input requiring constant re-tuning making such a solution impractical. In this paper, we take up this challenge and propose designs for pointto-point data transfer in MPI which accounts for overlap in addition to
performance and memory footprint. The proposed designs dynamically
adapt to the communication characteristic of each communicating pair of
processes at runtime. Our proposed full in-band design is able to transition from one eager-threshold to another without impacting the communication throughput of the application. The proposed enhancements to
limit the memory footprint by dynamically freeing unused internal communication buffer is able to significantly cut down on memory footprint
of the MPI library without affecting the communication performance.
Experimental evaluations show that the proposed dynamic and adaptive design is able to deliver performance on-par with what exhaustive
manual tuning provides while limiting the memory consumed to the
absolute minimum necessary to deliver the desired benefits. For instance,
with the Amber molecular dynamics application at 1,024 processes, the
proposed design is able to perform on-par with the best manually tuned
versions while reducing the memory footprint of the MPI library by 25%.
With the 3D-Stencil benchmark at 8,192 processes, the proposed design
is able to deliver much better overlap of computation and communication as well as improved overall time compared to the default version.

",18.243605946275583,17.410653846153853,328
ISC_17_019.txt,16.941846955676066,15.800582406063537,ISC,6171,"
. With the growing complexity and scale of high performance computing (HPC) systems, application performance variation has
become a significant challenge in efficient and resilient system management. Application performance variation can be caused by resource contention as well as software- and firmware-related problems, and can lead
to premature job termination, reduced performance, and wasted compute
platform resources. To effectively alleviate this problem, system administrators must detect and identify the anomalies that are responsible for
performance variation and take preventive actions. However, diagnosing
anomalies is often a difficult task given the vast amount of noisy and
high-dimensional data being collected via a variety of system monitoring
infrastructures.

In this paper, we present a novel framework that uses machine learning
to automatically diagnose previously encountered performance anomalies
in HPC systems. Our framework leverages resource usage and performance counter data collected during application runs. We first convert
the collected time series data into statistical features that retain application characteristics to significantly reduce the computational overhead of
our technique. We then use machine learning algorithms to learn anomaly
characteristics from this historical data and to identify the types of anomalies observed while running applications. We evaluate our framework
both on an HPC cluster and on a public cloud, and demonstrate that
our approach outperforms current state-of-the-art techniques in detecting anomalies, reaching an F-score over 0.97.

",18.71604785175511,18.638731563421832,228
ISC_17_020.txt,15.30157151856887,13.384963057686846,ISC,6898,"
. The reduction of the CPU frequency and voltage is a wellknown approach to improve energy consumption of memory-bound applications. This is based on the conception that the performance of the main
memory sees little or no degradation at reduced processor clock speeds
while power consumption decreases significantly improving the overall
energy efficiency. We study this effect on the Haswell generation of Intel
Xeon processors as well as the ARMv7 generation of the 32-bit ARM
big.LITTLE architecture. The goal is to analyse and compare computational performance, energy consumption and energy efficiency on a series
of tasks, each focusing on different parts of the system and provide an
analysis and generalisation to other similar architectures.

The benchmark suit consists of compute and memory intensive benchmarks as well as both single and multi-threaded scientific applications.
The results show that frequency and voltage scaling can significantly
improve algorithms’ energy efficiency. Up to 2.5x on ARM and 1.5x
on Intel compared to the maximum frequency. ARM is up to 2x more
efficient than Intel.

",17.267425705330172,14.954302325581398,176
ISC_17_021.txt,16.622928788720802,15.178857307747801,ISC,7121,"
. The power scaling challenge associated with Exascale systems is a well-known issue. In this work, we introduce the Global Extensible Open Power Manager (GEOPM): a tree-hierarchical, open source
runtime framework we are contributing to the HPC community to foster increased collaboration and accelerated progress toward softwarehardware co-designed energy management solutions that address Exascale power challenges and improve performance and energy efficiency
in current systems. Through its plugin extensible architecture, GEOPM
enables rapid prototyping of new energy management strategies. Different plugins can be tailored to the specific performance or energy efficiency priorities of each HPC center. To demonstrate the potential of
the framework, this work develops an example plugin for GEOPM. This
power rebalancing plugin targets power-capped systems and improves
efficiency by minimizing job time-to-solution within a power budget. Our
results demonstrate up to 30% improvements in the time-to-solution of
CORAL system procurement benchmarks on a Xeon Phi cluster.

",17.613555460941566,16.701428571428576,157
ISC_17_022.txt,14.713138746732533,12.734572871024326,ISC,5877,"
. Energy consumption is rapidly becoming a limiting factor
in scientific computing. As a result, hardware manufacturers increasingly prioritise energy efficiency in their processor designs. Performance
engineers are also beginning to explore software optimisation and hardware/software co-design as a means to reduce energy consumption.
Energy efficiency metrics developed by the hardware community are
often re-purposed to guide these software optimisation efforts.

In this paper we argue that established metrics, and in particular those
in the Energy Delay Product (£t”) family, are unsuitable for energyaware software optimisation. A good metric should provide meaningful
values for a single experiment, allow fair comparison between experiments, and drive optimisation in a sensible direction. We show that Et”
metrics are unable to fulfil these basic requirements and present suitable
alternatives for guiding energy-aware software optimisation. We finish
with a practical demonstration of the utility of our proposed metrics.

",17.122413403193683,16.460603741496602,147
ISCA_17_001.txt,14.159871740235818,12.350914601163353,ISCA,8935,"

Many architects believe that major improvements in cost-energyperformance must now come from domain-specific hardware.
This paper evaluates a custom ASIC —called a Tensor Processing
Unit (TPU)— deployed in datacenters since 2015 that accelerates
the inference phase of neural networks (NN). The heart of the
TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak
throughput of 92 TeraOps/second (TOPS) and a large (28 MiB)
software-managed on-chip memory. The TPU’s deterministic
execution model is a better match to the 99th-percentile responsetime requirement of our NN applications than are the time-varying
optimizations of CPUs and GPUs that help average throughput
more than guaranteed latency. The lack of such features helps
explain why, despite having myriad MACs and a big memory, the
TPU is relatively small and low power. We compare the TPU to a
server-class Intel Haswell CPU and an Nvidia K80 GPU, which
are contemporaries deployed in the same datacenters. Our
workload, written in the high-level TensorFlow framework, uses
production NN applications (MLPs, CNNs, and LSTMs) that
represent 95% of our datacenters’ NN inference demand. Despite
low utilization for some applications, the TPU is on average about
15X - 30X faster than its contemporary GPU or CPU, with
TOPS/Watt about 30X — 80X higher. Moreover, using the GPU’s
GDDR5 memory in the TPU would triple achieved TOPS and
raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.
",15.470042427545799,14.634246196403879,245
ISCA_17_002.txt,14.493712623949019,12.939924929965454,ISCA,10943,"

Deep Neural Networks (DNNs) have demonstrated. state-of-the-art
performance on a broad range of tasks involving natural language,
speech, image, and video processing, and are deployed in many
real world applications. However, DNNs impose significant computational challenges owing to the complexity of the networks and.
the amount of data they process, both of which are projected to
grow in the future. To improve the efficiency of DNNs, we propose
SCALEDEEP, a dense, scalable server architecture, whose processing, memory and interconnect subsystems are specialized to leverage
the compute and communication characteristics of DNNs. While
several DNN accelerator designs have been proposed in recent years,
the key difference is that SCALEDEEP primarily targets DNN training, as opposed to only inference or evaluation. The key architectural
features from which SCALEDEEP derives its efficiency are: (i) heterogeneous processing tiles and chips to match the wide diversity in
computational characteristics (FLOPs and Bytes/FLOP ratio) that
manifest at different levels of granularity in DNNs, (ii) a memory
hierarchy and 3-tiered interconnect topology that is suited to the
memory access and communication patterns in DNNs, (iii) a lowoverhead synchronization mechanism based on hardware data-flow
trackers, and (iv) methods to map DNNs to the proposed architecture
that minimize data movement and improve core utilization through
nested pipelining. We have developed a compiler to allow any DNN
topology to be programmed onto SCALEDEEP, and a detailed architectural simulator to estimate performance and energy. The simulator
incorporates timing and power models of SCALEDEEP’s components
based on synthesis to Intel’s 14nm technology. We evaluate an embodiment of SCALEDEEP with 7032 processing tiles that operates
at 600 MHz and has a peak performance of 680 TFLOPs (single
precision) and 1.35 PFLOPs (half-precision) at 1.4KW. Across 11
state-of-the-art DNNs containing 0.65M-14.9M neurons and 6.8M145.9M weights, including winners from 5 years of the ImageNet
competition, SCALEDEEP demonstrates 6-28 speedup at isopower over the state-of-the-art performance on GPUs.

",20.992536772730304,20.664403669724773,336
ISCA_17_003.txt,16.07784608179916,14.76641874337836,ISCA,8581,"

Convolutional Neural Networks (CNNs) have emerged as a fundamental technology for machine learning. High performance and
extreme energy efficiency are critical for deployments of CNNs, especially in mobile platforms such as autonomous vehicles, cameras,
and electronic personal assistants. This paper introduces the Sparse
CNN (SCNN) accelerator architecture, which improves performance
and energy efficiency by exploiting the zero-valued weights that stem
from network pruning during training and zero-valued activations
that arise from the common ReLU operator. Specifically, SCNN employs a novel dataflow that enables maintaining the sparse weights
and activations in a compressed encoding, which eliminates unnecessary data transfers and reduces storage requirements. Furthermore,
the SCNN dataflow facilitates efficient delivery of those weights and
activations to a multiplier array, where they are extensively reused;
product accumulation is performed in a novel accumulator array.
On contemporary neural networks, SCNN can improve both performance and energy by a factor of 2.7 and 2.3x, respectively, over a
comparably provisioned dense CNN accelerator.

",20.10790988173199,19.7375,163
ISCA_17_004.txt,17.03406897189544,16.09104674796748,ISCA,9764,"

A large number of emerging applications such as implantables, wearables, printed electronics, and IoT have ultra-low area and power
constraints. These applications rely on ultra-low-power general purpose microcontrollers and microprocessors, making them the most
abundant type of processor produced and used today. While general purpose processors have several advantages, such as amortized
development cost across many applications, they are significantly
over-provisioned for many area- and power-constrained systems,
which tend to run only one or a small number of applications over
their lifetime. In this paper, we make a case for bespoke processor
design, an automated approach that tailors a general purpose processor IP to a target application by removing all gates from the design
that can never be used by the application. Since removed gates are
never used by an application, bespoke processors can achieve significantly lower area and power than their general purpose counterparts
without any performance degradation. Also, gate removal can expose additional timing slack that can be exploited to increase area
and power savings or performance of a bespoke design. Bespoke
processor design reduces area and power by 62% and 50%, on average, while exploiting exposed timing slack improves average power
savings to 65%.

",18.243605946275583,17.97913719943423,203
ISCA_17_005.txt,15.452703546587514,13.944344573635913,ISCA,7547,"

This paper investigates the feasibility of a unified processor architecture to enable error coding flexibility and secure communication
in low power Internet of Things (IoT) wireless networks. Error coding flexibility for wireless communication allows IoT applications
to exploit the large tradeoff space in data rate, link distance and
energy-efficiency. As a solution, we present a light-weight Galois
Field (GF) processor to enable energy-efficient block coding and
symmetric/asymmetric cryptography kernel processing for a wide
range of GF sizes (2”, m = 2,3, ...,233) and arbitrary irreducible
polynomials. Program directed connections among primitive GF
arithmetic units enable dynamically configured parallelism to efficiently perform either four-way SIMD 5- to 8-bit GF operations,
including multiplicative inverse, or a wide bit-width (e.g., 32-bit)
GF product in a single cycle. To illustrate our ideas, we synthesized.
our GF processor in a 28nm technology. Compared to a baseline
software implementation optimized for a general purpose ARM M0+
processor, our processor exhibits a 5— 20x speedup for a range
of error correction codes and symmetric/asymmetric cryptography
applications. Additionally, our proposed GF processor consumes
431uW at 0.9V and 100MHz, and achieves 35.5pJ/b energy efficiency while executing AES operations at 12.2Mbps. We achieve
this within an area of 0.01mm?.

",18.903936251131103,18.135119047619046,215
ISCA_17_006.txt,16.078406949036232,14.498645912337953,ISCA,8665,"

Wearable computing systems have spurred many opportunities to
continuously monitor human bodies with sensors worn on or implanted in the body. These emerging platforms have started to revolutionize many fields, including healthcare and wellness applications,
particularly when integrated with intelligent analytic capabilities.
However, a significant challenge that computer architects are facing is how to embed sophisticated analytic capabilities in wearable
computers in an energy-efficient way while not compromising system performance. In this paper, we present XPro, a novel cross-end.
analytic engine architecture for wearable computing systems. The
proposed cross-end architecture is able to realize a generic classification design across wearable sensors and a data aggregator with
high energy-efficiency. To facilitate the practical use of XPro, we
also develop an Automatic XPro Generator that formally generates
XPro instances according to specific design constraints. As a proof
of concept, we study the design and implementation of XPro with six
different health applications. Evaluation results show that, compared.
with state-of-the-art methods, XPro can increase the battery life of
the sensor node by 1.6-2.4X while at the same time reducing system
delay by 15.6-60.8% for wearable computing systems.

",18.511140095513987,17.357610103626943,198
ISCA_17_007.txt,15.238088780616632,13.665478342934055,ISCA,4578,"

Intel’s SGX secure execution technology allows running computations on secret data using untrusted servers. While recent work
showed how to port applications and large-scale computations to run
under SGX, the performance implications of using the technology
remains an open question. We present the first comprehensive quantitative study to evaluate the performance of SGX. We show that
straightforward use of SGX library primitives for calling functions
add between 8,200 - 17,000 cycles overhead, compared to 150 cycles
of a typical system call. We quantify the performance impact of these
library calls and show that in applications with high system calls frequency, such as memcached, openVPN, and lighttpd, which all have
high bandwidth network requirements, the performance degradation
may be as high as 79%. We investigate the sources of this performance degradation by leveraging a new set of microbenchmarks for
SGX-specific operations such as enclave entry-calls and out-calls,
and encrypted memory I/O accesses. We leverage the insights we
gain from these analyses to design a new SGX interface framework
HotCalls. HotCalls are based on a synchronization spin-lock mechanism and provide a 13-27x speedup over the default interface. It can
easily be integrated into existing code, making it a practical solution.
Compared to a baseline SGX implementation of memcached, openVPN, and lighttpd - we show that using the new interface boosts the
throughput by 2.6-3.7x, and reduces application latency by 62-74%.

",17.122413403193683,15.43163120567376,241
ISCA_17_008.txt,15.061419001967131,13.148223486156606,ISCA,5610,"

A practically feasible low-overhead hardware design that provides
strong defenses against memory bus side channel remains elusive.
This paper observes that smart memory, memory with compute capability and a packetized interface, can dramatically simplify this
problem. InvisiMem expands the trust base to include the logic layer
in the smart memory to implement cryptographic primitives, which
aid in addressing several memory bus side channel vulnerabilities
efficiently. This allows the secure host processor to send encrypted
addresses over the untrusted memory bus, and thereby eliminates the
need for expensive address obfuscation techniques based on Oblivious RAM (ORAM). In addition, smart memory enables efficient
solutions for ensuring freshness without using expensive Merkle
trees, and mitigates memory bus timing channel using constant
heart-beat packets. We demonstrate that InvisiMem designs have
one to two orders of magnitude of lower overheads for performance,
space, energy, and memory bandwidth, compared to prior solutions.

",19.454632303725965,17.86918918918919,149
ISCA_17_009.txt,16.113011202999985,14.235841409311938,ISCA,6517,"
Trustworthy software requires strong privacy and security guarantees
from a secure trust base in hardware. While chipmakers provide
hardware support for basic security and privacy primitives such as
enclaves and memory encryption. these primitives do not address
hiding of the memory access pattern, information about which may
enable attacks on the system or reveal characteristics of sensitive user
data. State-of-the-art approaches to protecting the access pattern are
largely based on Oblivious RAM (ORAM). Unfortunately, current
ORAM implementations suffer from very significant practicality
and overhead concerns, including roughly an order of magnitude
slowdown, more than 100% memory capacity overheads, and the
potential for system deadlock.

Memory technology trends are moving towards 3D and 2.5D
integration, enabling significant logic capabilities and sophisticated.
memory interfaces. Leveraging the trends, we propose a new approach to access pattern obfuscation, called ObfusMem. ObfusMem
adds the memory to the trusted computing base and incorporates
cryptographic engines within the memory. ObfusMem encrypts commands and addresses on the memory bus, hence the access pattern is
cryptographically obfuscated from external observers. Our evaluation shows that ObfusMem incurs an overhead of 10.9% on average,
which is about an order of magnitude faster than ORAM implementations. Furthermore, ObfusMem does not incur capacity overheads
and does not amplify writes. We analyze and compare the security
protections provided by ObfusMem and ORAM, and highlight their
differences.

",17.845785984406827,16.878828282828284,228
ISCA_17_010.txt,16.05609793868655,14.77459311837475,ISCA,8138,"

Tailoring the operating voltage to fine-grain temporal changes in
the power and performance needs of the workload can effectively
enhance power efficiency. Therefore, power-limited computing platforms of today widely deploy integrated (i.e., on-chip) voltage regulation which enables fast fine-grain voltage control. Voltage regulators
convert and distribute power from an external energy source to the
processor. Unfortunately, power conversion loss is inevitable and
projected integrated regulator designs are unlikely to eliminate this
loss even asymptotically. Reconfigurable power delivery by selective
shut-down, i.e., gating, of distributed on-chip regulators in response
to spatio-temporal changes in power demand can sustain operation
at the minimum conversion loss. However, even the minimum conversion loss is sizable, and as conversion loss gets dissipated as heat,
on-chip regulators can easily cause thermal emergencies due to their
small footprint.

Although reconfigurable distributed on-chip power delivery is
emerging as a new design paradigm to enforce sustained operation at
minimum possible power conversion loss, thermal implications have
been overlooked at the architectural level. This paper hence provides
a thermal characterization. We introduce ThermoGater, an architectural governor for a collection of practical, thermally-aware regulator
gating policies to mitigate (if not prevent) regulator-induced thermal
emergencies, which also consider potential implications for voltage
noise. Practical ThermoGater policies can not only sustain minimum power conversion loss throughout execution effectively, but
also keep the maximum temperature (thermal gradient) across chip
within 0.6°C (0.3°C) on average in comparison to thermally-optimal
oracular regulator gating, while the maximum voltage noise stays
within 1.0% of the best case voltage noise profile.
",20.075844112070616,19.527868913857677,271
ISCA_17_011.txt,18.116355778648998,17.212309344739385,ISCA,5843,"

Modern user facing applications consist of multiple processing stages
with a number of service instances in each stage. The latency profile
of these multi-stage applications is intrinsically variable, making it
challenging to provide satisfactory responsiveness. Given a limited
power budget, improving the end-to-end latency requires intelligently
boosting the bottleneck service across stages using multiple boosting
techniques. However, prior work fail to acknowledge the multi-stage
nature of user-facing applications and perform poorly in improving
responsiveness on power constrained CMP, as they are unable to accurately identify bottleneck service and apply the boosting techniques
adaptively.

In this paper, we present PowerChief, a runtime framework that
1) provides joint design of service and query to monitor the latency
statistics across service stages and accurately identifies the bottleneck
service during runtime; 2) adaptively chooses the boosting technique
to accelerate the bottleneck service with improved responsiveness; 3)
dynamically reallocates the constrained power budget across service
stages to accommodate the chosen boosting technique. Evaluated
with real world multi-stage applications, PowerChief improves the
average latency by 20.3 and 32.4« (99% tail latency by 13.3x
and 19.4x) for Sirius and Natural Language Processing applications
respectively compared to stage-agnostic power allocation. In addition,
for the given QoS target, PowerChief reduces the power consumption
of Sirius and Web Search applications by 23% and 33% respectively
over prior work.

",20.40282108145781,20.807936507936507,230
ISCA_17_012.txt,17.11538688334157,16.150759042100187,ISCA,4012,"
High-performance architectures are over-provisioned with resources
to extract the maximum achievable performance out of applications.
Two sources of avoidable power dissipation are the leakage power
from underutilized resources, along with clock power from the clock
hierarchy that feeds these resources. Most reconfiguration mechanisms either focus solely on power gating execution resources alone
or in addition, simply turn off the immediate clock tree segment
which supplied the clock to those resources. These proposals neither
attempt to gate further up the clock hierarchy nor do they involve
the clock hierarchy in influencing the reconfiguration decisions. The
primary contribution of CHARSTAR is optimizing reconfiguration
mechanisms to become clock hierarchy aware. Resource gating decisions are cognizant of the power consumed by each node in the
clock hierarchy and additionally, entire branches of the clock tree
are greedily shut down whenever possible.
The CHARSTAR design is further optimized for balanced spatiotemporal reconfiguration and also enables efficient joint control of
resource and frequency scaling. The proposal is implemented by
leveraging the inherent advantages of spatial architectures, utilizing
a control mechanism driven by a lightweight offline trained neural
predictor. CHARSTAR, when deployed on the CRIB tiled microarchitecture, improves processor energy efficiency by 20-25%, with
efficiency improvements of roughly 2x in comparison to a naive
power gating mechanism. Alternatively, it improves performance by
10-20% under varying power and energy constraints.
",18.1352568364455,17.659398230088502,227
ISCA_17_013.txt,16.779184750074283,15.207809492867973,ISCA,9544,"

An unambiguous and easy-to-understand memory consistency model
is crucial for ensuring correct synchronization and guiding future
design of heterogeneous systems. In a widely adopted approach,
the memory model guarantees sequential consistency (SC) as long
as programmers obey certain rules. The popular data-race-free-0
(DRFO) model exemplifies this SC-centric approach by requiring
programmers to avoid data races. Recent industry models, however,
have extended such SC-centric models to incorporate relaxed atomics. These extensions can improve performance, but are difficult to
specify formally and use correctly. This work addresses the impact of
relaxed atomics on consistency models for heterogeneous systems in
two ways. First, we introduce a new model, Data-Race-Free-Relaxed
(DRFrix), that extends DRFO to provide SC-centric semantics for
the common use cases of relaxed atomics. Second, we evaluate the
performance of relaxed atomics in CPU-GPU systems for these use
cases. We find mixed results — for most cases, relaxed atomics provide only a small benefit in execution time, but for some cases, they
help significantly (e.g., up to 51% for DRFrlx over DRFO).

",15.616094167265928,14.093112338858198,183
ISCA_17_014.txt,14.27815852040758,12.293631417898826,ISCA,7642,"

Byte-addressable non-volatile memory technology is emerging as
an alternative for DRAM for main memory. This new Non- Volatile
Main Memory (NVMM) allows programmers to store important data
in data structures in memory instead of serializing it to the file system,
thereby providing a substantial performance boost. However, modern systems reorder memory operations and utilize volatile caches
for better performance, making it difficult to ensure a consistent state
in NVMM. Intel recently announced a new set of persistence instructions, clflushopt, clwb, and pcommit. These new instructions make it
possible to implement fail-safe code on NVMM, but few workloads
have been written or characterized using these new instructions.

In this work, we describe how these instructions work and how
they can be used to implement write-ahead logging based transactions. We implement several common data structures and kernels
and evaluate the performance overhead incurred over traditional nonpersistent implementations. In particular, we find that persistence
instructions occur in clusters along with expensive fence operations,
they have long latency, and they add a significant execution time
overhead, on average by 20.3% over code with logging but without
fence instructions to order persists.

To deal with this overhead and alleviate the performance bottleneck, we propose to speculate past long latency persistency operations using checkpoint-based processing. Our speculative persistence
architecture reduces the execution time overheads to only 3.6%.

",18.243605946275583,15.62311504424779,229
ISCA_17_015.txt,15.107717745800223,13.225105079988776,ISCA,7090,"

In Total Store Order memory consistency (TSO), loads can be speculatively reordered to improve performance. If a load-load reordering
is seen by other cores, speculative loads must be squashed and
re-executed. In architectures with an unordered interconnection network and directory coherence, this has been the established view
for decades. We show, for the first time, that it is not necessary to
squash and re-execute speculatively reordered loads in TSO when
their reordering is seen. Instead, the reordering can be hidden form
other cores by the coherence protocol. The implication is that we
can irrevocably bind speculative loads. This allows us to commit
reordered loads out-of-order without having to wait (for the loads to
become non-speculative) or without having to checkpoint committed
state (and rollback if needed), just to ensure correctness in the rare
case of some core seeing the reordering. We show that by exposing
a reordering to the coherence layer and by appropriately modifying a typical directory protocol we can successfully hide load-load
reordering without perceptible performance cost and without deadlock. Our solution is cost-effective and increases the performance
of out-of-order commit by a sizable margin, compared to the base
case where memory operations are not allowed to commit if the
consistency model could be violated.

",16.728156217252725,14.911950844854072,218
ISCA_17_016.txt,15.776823510576914,14.061443615353827,ISCA,9245,"

This work presents a minimally-intrusive, high-performance, postsilicon validation framework for validating memory consistency

in multi-core systems. Our framework generates constrained-random

tests that are instrumented with observability-enhancing code for
memory consistency verification. For each test, we generate a set
of compact signatures reflecting the memory-ordering patterns
observed over many executions of the test, with each of the signatures corresponding to a unique memory-ordering pattern. We
then leverage an efficient and novel analysis to quickly determine
if the observed execution patterns represented by each unique signature abide by the memory consistency model. Our analysis derives its efficiency by exploiting the structural similarities among
the patterns observed.

We evaluated our framework, MTraceCheck, on two platforms:
an x86-based desktop and an ARM-based SoC platform, both
running multi-threaded test programs in a bare-metal environment. We show that MTraceCheck reduces the perturbation introduced by the memory-ordering monitoring activity by 93% on
average, compared to a baseline register flushing approach that
saves the register’s state after each load operation. We also reduce the computation requirements of our consistency checking
analysis by 81% on average, compared to a conventional topological sorting solution. We finally demonstrate the effectiveness
of MTraceCheck on buggy designs, by evaluating multiple case
studies where it successfully exposes subtle bugs in a full-system
simulation environment.

",19.287186520377343,18.4670553064275,225
ISCA_17_017.txt,14.423453498780443,12.011387579478946,ISCA,9084,"

Memory hardware errors may result from transient particle-induced
faults as well as device defects due to aging. These errors are an
important threat to computer system reliability as VLSI technologies
continue to scale. Managing memory hardware errors is a critical
component in developing an overall system dependability strategy.
Memory error detection and correction are supported in a range
of available hardware mechanisms. However, memory protections
(particularly the more advanced ones) come at substantial costs in
performance and energy usage. Moreover, the protection mechanisms are often a fixed, system-wide choice and can not easily adapt
to different protection demand of different applications or memory
regions.

In this paper, we present a new RAIM (redundant array of independent memory) design that compared to the state-of-the-art
implementation can easily provide high protection capability and the
ability to selectively protect a subset of the memory. A straightforward implementation of the design can incur a substantial memory
traffic overhead. We propose a few practical optimizations to mitigate this overhead. With these optimizations the proposed RAIM
design offers significant advantages over existing RAIM design at
lower or comparable costs.

",17.238542476582836,15.530297872340427,189
ISCA_17_018.txt,15.770727411890157,14.21335777028877,ISCA,10555,"
The processors that drive embedded systems are getting smaller;
meanwhile, the batteries used to provide power to those systems
have stagnated. If we are to realize the dream of ubiquitous computing promised by the Internet of Things, processors must shed
large, heavy, expensive, and high maintenance batteries and, instead, harvest energy from their environment. One challenge with
this transition is that harvested energy is insufficient for continuous
operation. Unfortunately, existing programs fail miserably when
executed intermittently.
This paper presents Clank: lightweight architectural support for
correct and efficient execution of long-running applications on harvested energy—without programmer intervention or extreme hardware modifications. Clank is a set of hardware buffers and memoryaccess monitors that dynamically maintain idempotency. Essentially,
Clank dynamically decomposes program execution into a stream of
restartable sub-executions connected via lightweight checkpoints.
To validate Clank’s ability to correctly stretch program execution
across frequent, random power cycles, and to explore the associated
hardware and software overheads, we implement Clank in Verilog,
formally verify it, and then add it to an ARM Cortex M0+ processor
which we use to run a set of 23 embedded systems benchmarks.
Experiments show run-time overheads as low as 2.5%, with run-time
overheads of 6% for a version of Clank that adds 1.7% hardware.
Clank minimizes checkpoints so much that re-execution time becomes the dominate contributor to run-time overhead.
",17.693802365651003,16.073929824561404,231
ISCA_17_019.txt,16.320617633758133,15.291522356983393,ISCA,7111,"

Early reliability assessment of hardware structures using
microarchitecture level simulators can effectively guide
major error protection decisions in microprocessor design.
Statistical fault injection on microarchitectural structures
modeled in performance simulators is an accurate method
to measure their Architectural Vulnerability Factor (AVF)
but requires excessively long campaigns to obtain high statistical significance.

We propose MeRLiN’, a methodology to boost microarchitecture level injection-based reliability assessment by
several orders of magnitude and keep the accuracy of the
assessment unaffected even for large injection campaigns
with very high statistical significance. The core of MeRLiN
is the grouping of faults of an initial list in equivalent classes. All faults in the same group target equivalent vulnerable intervals of program execution ending up to the same
static instruction that reads the faulty entries. Faults in the
same group occur in different times and entries of a structure and it is extremely likely that they all have the same
effect in program execution; thus, fault injection is performed only on a few representatives from each group.

We evaluate MeRLIN for different sizes of the physical
register file, the store queue and the first level data cache of
a contemporary microarchitecture running MiBench and
SPEC CPU2006 benchmarks. For all our experiments,
MeRLIN is from 2 to 3 orders of magnitude faster than an
extremely high statistical significant injection campaign,
reporting the same reliability measurements with negligible
loss of accuracy. Finally, we theoretically analyze MeRLiN’s statistical behavior to further justify its accuracy.

",18.946978176291534,18.97021857923497,246
ISCA_17_020.txt,18.116055092385395,17.041031101240993,ISCA,11244,"

Modern DRAM-based systems suffer from significant energy and
latency penalties due to conservative DRAM refresh standards.
Volatile DRAM cells can retain information across a wide distribution of times ranging from milliseconds to many minutes, but each
cell is currently refreshed every 64ms to account for the extreme
tail end of the retention time distribution, leading to a high refresh
overhead. Due to poor DRAM technology scaling, this problem
is expected to get worse in future device generations. Hence, the
current approach of refreshing all cells with the worst-case refresh
rate must be replaced with a more intelligent design.

Many prior works propose reducing the refresh overhead by extending the default refresh interval to a higher value, which we refer
to as the target refresh interval, across parts or all of a DRAM chip.
These proposals handle the small set of failing cells that cannot
retain data throughout the entire extended refresh interval via retention failure mitigation mechanisms (e.g., error correcting codes or
bit-repair mechanisms). This set of failing cells is discovered via
retention failure profiling, which is currently a brute-force process
that writes a set of known data to DRAM, disables refresh and waits
for the duration of the target refresh interval, and then checks for
retention failures across the DRAM chip. We show that this bruteforce approach is too slow and is detrimental to system execution,
especially with frequent online profiling.

This paper presents reach profiling, a new methodology for retention failure profiling based on the key observation that an overwhelming majority of failing DRAM cells at a target refresh interval
fail more reliably at both longer refresh intervals and higher temperatures. Using 368 state-of-the-art LPDDR4 DRAM chips from
three major vendors, we conduct a thorough experimental characterization of the complex set of tradeoffs inherent in the profiling
process. We identify three key metrics to guide design choices for
retention failure profiling and mitigation mechanisms: coverage,
false positive rate, and runtime. We propose reach profiling, a new
retention failure profiling mechanism whose key idea is to profile
failing cells at a longer refresh interval and/or higher temperature
relative to the target conditions in order to maximize failure coverage while minimizing the false positive rate and profiling runtime.
We thoroughly explore the tradeoffs associated with reach profiling
and show that there is significant room for improvement in DRAM
retention failure profiling beyond the brute-force approach. We show
with experimental data that on average, by profiling at 250ms above
the target refresh interval, our first implementation of reach profiling
(called REAPER) can attain greater than 99% coverage of failing
DRAM cells with less than a 50% false positive rate while running
2.5x faster than the brute-force approach. In addition, our end-to-end.
evaluations show that REAPER enables significant system performance improvement and DRAM power reduction, outperforming the
brute-force approach and enabling high-performance operation at
longer refresh intervals that were previously unreasonable to employ
due to the high associated profiling overhead.

",19.01560269883211,18.59387250996016,505
ISCA_17_021.txt,13.887190929111185,11.863586372749094,ISCA,8108,"

GPUs have been widely adopted in data centers to provide acceleration services to many applications. Sharing a GPU is increasingly
important for better processing throughput and energy efficiency.
However, quality of service (QoS) among concurrent applications is
minimally supported. Previous efforts are too coarse-grained and not
scalable with increasing QoS requirements. We propose QoS mechanisms for a fine-grained form of GPU sharing. Our QoS support can
provide control over the progress of kernels on a per cycle basis and
the amount of thread-level parallelism of each kernel. Due to accurate resource management, our QoS support has significantly better
scalability compared with previous best efforts. Evaluations show
that, when the GPU is shared by three kernels, two of which have
QoS goals, the proposed techniques achieve QoS goals 43.8% more
often than previous techniques and have 20.5% higher throughput.

",13.81666964889586,12.373111702127662,144
ISCA_17_022.txt,15.628082996889056,14.179566992014198,ISCA,5708,"

Snapshot Isolation (SI) is an established model in the database community, which permits write-read conflicts to pass and aborts transactions only on write-write conflicts. With the Write Skew anomaly
correctly eliminated, SI can reduce the occurrence of aborts, save
the work done by transactions, and greatly benefit long transactions
involving complex data structures.

GPUs are evolving towards a general-purpose computing device
with growing support for irregular workloads, including transactional
memory. The usage of snapshot isolation on transactional memory
has proven to be greatly beneficial for performance. In this paper, we
propose a multi-versioned memory subsystem for hardware-based
transactional memory on the GPU, with a method for eliminating
the Write Skew anomaly on the fly, and finally incorporate Snapshot
Isolation with this system.

The results show that snapshot isolation can effectively boost the
performance of dynamically sized data structures such as linked
lists, binary trees and red-black trees, sometimes by as much as
4.5x, which results in improved overall performance of benchmarks
utilizing these data structures.

",18.422482065455632,17.399853801169595,173
ISCA_17_023.txt,15.33485633538671,13.274720223804596,ISCA,8397,"

This paper introduces a method of decoupling affine computations—

a class of expressions that produces extremely regular values
across SIMT threads—from the main execution stream, so
that the affine computations can be performed with greater
efficiency and with greater independence from the main execution stream. This decoupling has two benefits: (1) For
compute-bound programs, it significantly reduces the dynamic warp instruction count; (2) for memory-bound workloads, it significantly reduces memory latency, since it acts as a
non-speculative prefetcher for the data specified by the many
memory address calculations that are affine computations.

We evaluate our solution, known as Decoupled Affine Computation (DAC), using GPGPU-sim and a set of 29 GPGPU programs. We find that on average, DAC improves performance
by 40% and reduces energy consumption by 20%. For the 11
compute-bound benchmarks, DAC improves performance by
34%, compared with 11% for the previous state-of-the-art. For
the 18 memory-bound programs, DAC improves performance
by an average of 44%, compared with 16% for state-of-the-art
GPU prefetcher.

",18.599290044081553,16.515000000000004,178
ISCA_17_024.txt,15.409227839203915,13.388871923464418,ISCA,8285,"

Long latency of memory operation is a prominent performance bottleneck in graphics processing units (GPUs). The small data cache
that must be shared across dozens of warps (a collection of threads)
creates significant cache contention and premature data eviction.
Prior works have recognized this problem and proposed warp throttling which reduces the number of active warps contending for cache
space. In this paper we discover that individual load instructions
in a warp exhibit four different types of data locality behavior: (1)
data brought by a warp load instruction is used only once, which
is classified as streaming data (2) data brought by a warp load is
reused multiple times within the same warp, called intra-warp locality (3) data brought by a warp is reused multiple times but across
different warps, called inter-warp locality (4) and some data exhibit
both a mix of intra- and inter-warp locality. Furthermore, each load.
instruction exhibits consistently the same locality type across all
watps within a GPU kernel. Based on this discovery we argue that
cache management must be done using per-load locality type information, rather than applying warp-wide cache management policies.
We propose Access Pattern-aware Cache Management (APCM),
which dynamically detects the locality type of each load instruction
by monitoring the accesses from one exemplary warp. APCM then
uses the detected locality type to selectively apply cache bypassing
and cache pinning of data based on load locality characterization.
Using an extensive set of simulations we show that APCM improves
performance of GPUs by 34% for cache sensitive applications while
saving 27% of energy consumption over baseline GPU.

",18.36309006607702,16.727633209417593,270
ISCA_17_025.txt,16.458231348711767,15.170325920277183,ISCA,5366,"

Historically, improvements in GPU-based high performance computing have been tightly coupled to transistor scaling. As Moore’s law
slows down, and the number of transistors per die no longer grows
at historical rates, the performance curve of single monolithic GPUs
will ultimately plateau. However, the need for higher performing
GPUs continues to exist in many domains. To address this need, in
this paper we demonstrate that package-level integration of multiple
GPU modules to build larger logical GPUs can enable continuous
performance scaling beyond Moore’s law. Specifically, we propose
partitioning GPUs into easily manufacturable basic GPU Modules
(GPMs), and integrating them on package using high bandwidth and
power efficient signaling technologies. We lay out the details and
evaluate the feasibility of a basic Multi-Chip-Module GPU (MCMGPU) design. We then propose three architectural optimizations that
significantly improve GPM data locality and minimize the sensitivity
on inter-GPM bandwidth. Our evaluation shows that the optimized
MCM-GPU achieves 22.8% speedup and 5x inter-GPM bandwidth
reduction when compared to the basic MCM-GPU architecture. Most
importantly, the optimized MCM-GPU design is 45.5% faster than
the largest implementable monolithic GPU, and performs within
10% of a hypothetical (and unbuildable) monolithic GPU. Lastly we
show that our optimized MCM-GPU is 26.8% faster than an equally
equipped Multi-GPU system with the same total number of SMs and.
DRAM bandwidth.

",15.308715981407026,14.531710996427154,235
ISCA_17_026.txt,17.246662797136892,16.687855433198305,ISCA,8839,"

This paper describes EM-Based Detection of Deviations in Program Execution (EDDIE), a new method for detecting anomalies
in program execution, such as malware and other code injections,
without introducing any overheads, adding any hardware support,
changing any software, or using any resources on the monitored
system itself. Monitoring with EDDIE involves receiving electromagnetic (EM) emanations that are emitted as a side effect of execution on the monitored system, and it relies on spikes in the EM
spectrum that are produced as a result of periodic (e.g. loop) activity
in the monitored execution. During training, EDDIE characterizes
normal execution behavior in terms of peaks in the EM spectrum
that are observed at various points in the program execution, but it
does not need any characterization of the malware or other code that
might later be injected. During monitoring, EDDIE identifies peaks
in the observed EM spectrum, and compares these peaks to those
learned during training. Since EDDIE requires no resources on the
monitored machine and no changes to the monitored software, it
is especially well suited for security monitoring of embedded and
IoT devices. We evaluate EDDIE on a real IoT system and in a
cycle-accurate simulator, and find that even relatively brief injected
bursts of activity (a few milliseconds) are detected by EDDIE with
high accuracy, and that it also accurately detects when even a few
instructions are injected into an existing loop within the application.

",20.267338824336647,21.744705882352942,240
ISCA_17_027.txt,13.215087703304992,10.8018937572913,ISCA,7141,"

In cache-based side channel attacks, a spy that shares a cache with a
victim probes cache locations to extract information on the victim’s
access patterns. For example, in evict+reload, the spy repeatedly
evicts and then reloads a probe address, checking if the victim has
accessed the address in between the two operations. While there are
many proposals to combat these cache attacks, they all have limitations: they either hurt performance, require programmer intervention,
or can only defend against some types of attacks.

This paper makes the following observation for an environment
with an inclusive cache hierarchy: when the spy evicts the probe
address from the shared cache, the address will also be evicted from
the private cache of the victim process, creating an inclusion victim.
Consequently, to disable cache attacks, this paper proposes to alter
the line replacement algorithm of the shared cache, to prevent a process from creating inclusion victims in the caches of cores running
other processes. By enforcing this rule, the spy cannot evict the probe
address from the shared cache and, hence, cannot glimpse any information on the victim’s access patterns. We call our proposal SHARP
(Secure Hierarchy-Aware cache Replacement Policy). SHARP efficiently defends against all existing cross-core shared-cache attacks,
needs only minimal hardware modifications, and requires no code
modifications. We implement SHARP in a cycle-level full-system
simulator. We show that it protects against real-world attacks, and
that it introduces negligible average performance degradation.

",15.381575749822971,14.346643724696357,249
ISCA_17_028.txt,16.273183056032714,14.783458880730791,ISCA,4801,"

Most architectures are designed to mitigate the usually undesirable
phenomenon of device wearout. We take a contrarian view and harness this phenomenon to create hardware security mechanisms that
resist attacks by statistically enforcing an upper bound on hardware
uses, and consequently attacks. For example, let us assume that a
user may log into a smartphone a maximum of 50 times a day for
5 years, resulting in approximately 91,250 legitimate uses. If we
assume at least 8-character passwords and we require login (and
retrieval of the storage decryption key) to traverse hardware that
wears out in 91,250 uses, then an adversary has a negligible chance
of successful brute-force attack before the hardware wears out, even
assuming real-world password cracking by professionals. M-way
replication of our hardware and periodic re-encryption of storage
can increase the daily usage bound by a factor of M.

The key challenge is to achieve practical statistical bounds on
both minimum and maximum uses for an architecture, given that
individual devices can vary widely in wearout characteristics. We
introduce techniques for architecturally controlling these bounds and
perform a design space exploration for three use cases: a limiteduse connection, a limited-use targeting system and one-time pads.
These techniques include decision trees, parallel structures, Shamir’s
secret-sharing mechanism, Reed-Solomon codes, and module replication. We explore the cost in area, energy and latency of using these
techniques to achieve system-level usage targets given device-level
wearout distributions. With redundant encoding, for example, we
can improve exponential sensitivity to device lifetime variation to
linear sensitivity, reducing the total number of NEMS devices by 4
orders of magnitude to about 0.8 million for limited-use connections
(compared with 4 billion if without redundant encoding).

",18.243605946275583,18.06140549828179,296
ISCA_17_029.txt,16.15745710342762,14.79937168917552,ISCA,7833,"
With the end of Dennard scaling, architects have increasingly turned
to special-purpose hardware accelerators to improve the performance
and energy efficiency for some applications. Unfortunately, accelerators don’t always live up to their expectations and may underperform in some situations. Understanding the factors which effect
the performance of an accelerator is crucial for both architects and.
programmers early in the design stage. Detailed models can be
highly accurate, but often require low-level details which are not
available until late in the design cycle. In contrast, simple analytical
models can provide useful insights by abstracting away low-level
system details.

In this paper, we propose LogCA—a high-level performance
model for hardware accelerators. LogCA helps both programmers
and architects identify performance bounds and design bottlenecks
early in the design cycle, and provide insight into which optimizations may alleviate these bottlenecks. We validate our model across
a variety of kernels, ranging from sub-linear to super-linear complexities on both on-chip and off-chip accelerators. We also describe
the utility of LogCA using two retrospective case studies. First, we
discuss the evolution of interface design in SUN/Oracle’s encryption
accelerators. Second, we discuss the evolution of memory interface
design in three different GPU architectures. In both cases, we show
that the adopted design optimizations for these machines are similar
to LogCA’s suggested optimizations. We argue that architects and
programmers can use insights from these retrospective studies for
improving future designs.

",15.299343439825492,14.603485477178427,245
ISCA_17_030.txt,15.388821448711628,13.86250690939531,ISCA,8293,"

Reconfigurable architectures have gained popularity in recent years
as they allow the design of energy-efficient accelerators. Fine-grain
fabrics (e.g. FPGAs) have traditionally suffered from performance
and power inefficiencies due to bit-level reconfigurable abstractions.
Both fine-grain and coarse-grain architectures (e.g. CGRAs) traditionally require low level programming and suffer from long compilation times. We address both challenges with Plasticine, a new
spatially reconfigurable architecture designed to efficiently execute
applications composed of parallel patterns. Parallel patterns have
emerged from recent research on parallel programming as powerful,
high-level abstractions that can elegantly capture data locality, memory access patterns, and parallelism across a wide range of dense
and sparse applications.

We motivate Plasticine by first observing key application characteristics captured by parallel patterns that are amenable to hardware
acceleration, such as hierarchical parallelism, data locality, memory access patterns, and control flow. Based on these observations,
we architect Plasticine as a collection of Pattern Compute Units
and Pattern Memory Units. Pattern Compute Units are multi-stage
pipelines of reconfigurable SIMD functional units that can efficiently
execute nested patterns. Data locality is exploited in Pattern Memory
Units using banked scratchpad memories and configurable address
decoders. Multiple on-chip address generators and scatter-gather
engines make efficient use of DRAM bandwidth by supporting a
large number of outstanding memory requests, memory coalescing,
and burst mode for dense accesses. Plasticine has an area footprint
of 113 mm? in a 28nm process, and consumes a maximum power
of 49 W at a 1 GHz clock. Using a cycle-accurate simulator, we
demonstrate that Plasticine provides an improvement of up to 76.9
in performance-per-Watt over a conventional FPGA over a wide
range of dense and sparse applications.

",16.567214971974305,15.394000000000002,288
ISCA_17_031.txt,14.189550479893747,12.412397140418268,ISCA,6686,"

The fast and energy-efficient simulation of dynamical systems defined by coupled ordinary/partial differential equations has emerged
as an important problem. The accelerated simulation of coupled
ODE/PDE is critical for analysis of physical systems as well as
computing with dynamical systems. This paper presents a fast and
programmable accelerator for simulating dynamical systems. The
computing model of the proposed platform is based on multilayer cellular nonlinear network (CeNN) augmented with nonlinear function
evaluation engines. The platform can be programmed to accelerate
wide classes of ODEs/PDEs by modulating the connectivity within
the multilayer CeNN engine. An innovative hardware architecture
including data reuse, memory hierarchy, and near-memory processing is designed to accelerate the augmented multilayer CeNN. A
dataflow model is presented which is supported by optimized memory hierarchy for efficient function evaluation. The proposed solver
is designed and synthesized in 15nm technology for the hardware
analysis. The performance is evaluated and compared to GPU nodes
when solving wide classes of differential equations and the power
consumption is analyzed to show orders of magnitude improvement
in energy efficiency.

",18.001758247042904,17.08510242085661,180
ISCA_17_032.txt,15.837134672739644,14.26934018844424,ISCA,7291,"

Demand for low-power data processing hardware continues to rise
inexorably. Existing programmable and “general purpose” solutions
(eg. SIMD, GPGPUs) are insufficient, as evidenced by the orderof-magnitude improvements and industry adoption of application
and domain-specific accelerators in important areas like machine
learning, computer vision and big data. The stark tradeoffs between
efficiency and generality at these two extremes poses a difficult question: how could domain-specific hardware efficiency be achieved
without domain-specific hardware solutions?

In this work, we rely on the insight that “acceleratable” algorithms have broad common properties: high computational intensity
with long phases, simple control patterns and dependences, and
simple streaming memory access and reuse patterns. We define a
general architecture (a hardware-software interface) which can more
efficiently expresses programs with these properties called streamdataflow. The dataflow component of this architecture enables high
concurrency, and the stream component enables communication
and coordination at very-low power and area overhead. This paper explores the hardware and software implications, describes its
detailed microarchitecture, and evaluates an implementation. Compared to a state-of-the-art domain specific accelerator (DianNao),
and fixed-function accelerators for MachSuite, Softbrain can match
their performance with only 2x power overhead on average.

",18.243605946275583,17.679666666666666,201
ISCA_17_033.txt,14.449550271202202,12.24265630225354,ISCA,10826,"

To improve system performance, operating systems (OSes) often
undertake activities that require modification of virtual-to-physical
address translations. For example, the OS may migrate data between
physical pages to manage heterogeneous memory devices. We refer
to such activities as page remappings. Unfortunately, page remappings are expensive. We show that a big part of this cost arises from
address translation coherence, particularly on systems employing
virtualization. In response, we propose hardware translation invalidation and coherence or HATRIC, a readily implementable
hardware mechanism to piggyback translation coherence atop existing cache coherence protocols. We perform detailed studies using
KVM-based virtualization, showing that HATRIC achieves up to 30%
performance and 10% energy benefits, for per-CPU area overheads
of 0.2%. We also quantify HATRIC’s benefits on systems running
Xen and find up to 33% performance improvements.

",17.122413403193683,15.687276119402984,137
ISCA_17_034.txt,15.260801549847695,13.559703769404944,ISCA,9550,"

To mitigate excessive TLB misses in large memory applications,
techniques such as large pages, variable length segments, and HW
coalescing, increase the coverage of limited hardware translation
entries by exploiting the contiguous memory allocation. However,
recent studies show that in non-uniform memory systems, using
large pages often leads to performance degradation, or allocating
large chunks of memory becomes more difficult due to memory
fragmentation. Although each of the prior techniques favors its own
best chunk size, diverse contiguity of memory allocation in real
systems cannot always provide the optimal chunk of each technique.

Under such fragmented and diverse memory allocations, this paper proposes a novel HW-SW hybrid translation architecture, which
can adapt to different memory mappings efficiently. In the proposed.
hybrid coalescing technique, the operating system encodes memory
contiguity information in a subset of page table entries, called anchor entries. During address translation through TLBs, an anchor
entry provides translation for contiguous pages following the anchor entry. As a smaller number of anchor entries can cover a large
portion of virtual address space, the efficiency of TLB can be significantly improved. The most important benefit of hybrid coalescing is
its ability to change the coverage of the anchor entry dynamically,
reflecting the current allocation contiguity status. By using the contiguity information directly set by the operating system, the technique
can provide scalable translation coverage improvements with minor
hardware changes, while allowing the flexibility of memory allocation. Our experimental results show that across diverse allocation
scenarios with different distributions of contiguous memory chunks,
the proposed scheme can effectively reap the potential translation
coverage improvement from the existing contiguity.

",19.686211704642208,19.02739405204461,270
ISCA_17_035.txt,15.891272878214217,13.574235650389497,ISCA,9054,"

In this paper, we introduce the Do-It-Yourself virtual memory translation (DVMT) architecture as a flexible complement for current
hardware-fixed translation flows. DVMT decouples the virtual-tophysical mapping process from the access permissions, giving applications freedom in choosing mapping schemes, while maintaining
security within the operating system. Furthermore, DVMT is designed to support virtualized environments, as a means to collapse
the costly, hardware-assisted two-dimensional translations. We describe the architecture in detail and demonstrate its effectiveness
by evaluating several different DVMT schemes on a range of virtualized applications with a model based on measurements from a
commercial system. We show that different DVMT configurations
preserve the native performance, while achieving speedups of 1.2 x
to 2.0x in virtualized environments.

",18.878054631974784,17.235836065573775,125
ISCA_17_036.txt,14.570628336466125,12.67702222023367,ISCA,7956,"

With increasing deployment of virtual machines for cloud services
and server applications, memory address translation overheads in
virtualized environments have received great attention. In the radix-4
type of page tables used in x86 architectures, a TLB-miss necessitates up to 24 memory references for one guest to host translation.
While dedicated page walk caches and such recent enhancements
eliminate many of these memory references, our measurements on
the Intel Skylake processors indicate that many programs in virtualized mode of execution still spend hundreds of cycles for translations
that do not hit in the TLBs.

This paper presents an innovative scheme to reduce the cost of
address translations by using a very large Translation Lookaside
Buffer that is part of memory, the POM-TLB. In the POM-TLB,
only one access is required instead of up to 24 accesses required in
commonly used 2D walks with radix-4 type of page tables. Even
if many of the 24 accesses may hit in the page walk caches, the
aggregated cost of the many hits plus the overhead of occasional
misses from page walk caches still exceeds the cost of one access
to the POM-TLB. Since the POM-TLB is part of the memory space,
TLB entries (as opposed to multiple page table entries) can be cached
in large L2 and L3 data caches, yielding significant benefits. Through
detailed evaluation running SPEC, PARSEC and graph workloads,
we demonstrate that the proposed POM-TLB improves performance
by approximately 10% on average. The improvement is more than
16% for 5 of the benchmarks. It is further seen that a POM-TLB of
16MB size can eliminate nearly all TLB misses in 8-core systems.

",16.15616582465906,14.643733096085413,282
ISCA_17_037.txt,16.265263827074595,15.018601862206044,ISCA,7065,"

The commercial release of byte-addressable persistent memories,
such as Intel/Micron 3D XPoint memory, is imminent. Ongoing
research has sought mechanisms to allow programmers to implement
recoverable data structures in these new main memories. Ensuring
recoverability requires programmer control of the order of persistent
stores; recent work proposes persistency models as an extension
to memory consistency to specify such ordering. Prior work has
considered persistency models at the abstraction of the instruction
set architecture. Instead, we argue for extending the language-level
memory model to provide guarantees on the order of persistent
writes.

We explore a taxonomy of guarantees a language-level persistency model might provide, considering both atomicity and ordering
constraints on groups of persistent stores. Then, we propose and
evaluate Acquire-Release Persistency (ARP), a language-level persistency model for C++11. We describe how to compile code written
for ARP to a state-of-the-art ISA-level persistency model. We then
consider enhancements to the ISA-level persistency model that can
distinguish memory consistency constraints required for proper synchronization but unnecessary for correct recovery. With these optimizations, we show that ARP increases performance by up to 33.2%
(19.8% avg.) over coding directly to the baseline ISA-level persistency model for a suite of persistent-write-intensive workloads.

",17.353723509956247,15.859738317757014,215
ISCA_17_038.txt,13.848311022430533,11.202208142999009,ISCA,9472,"

The same flexibility that makes dynamic scripting languages appealing to programmers is also the primary cause of their low performance. To access objects of potentially different types, the compiler
creates a dispatcher with a series of if statements, each performing a
comparison to a type and a jump to a handler. This induces major
overhead in instructions executed and branches mispredicted.

This paper proposes architectural support to significantly improve
the efficiency of accesses to objects. The idea is to modify the instruction that calls the dispatcher so that, under most conditions,
it skips most of the branches and instructions needed to reach the
correct handler, and sometimes even the execution of the handler
itself. Our novel architecture, called ShortCut, performs two levels
of optimization. Its Plain design transforms the call to the dispatcher
into a call to the correct handler — bypassing the whole dispatcher
execution. Its Aggressive design transforms the call to the dispatcher
into a simple load or store — bypassing the execution of both dispatcher and handler. We implement the ShortCut software in the
state-of-the-art Google V8 JIT compiler, and the ShortCut hardware
in a simulator. We evaluate ShortCut with the Octane and SunSpider
JavaScript application suites. Plain ShortCut reduces the average execution time of the applications by 30% running under the baseline
compiler, and by 11% running under the maximum level of compiler
optimization. Aggressive ShortCut performs only slightly better.

",15.579741850924794,13.638989361702127,236
ISCA_17_039.txt,15.686049864025119,14.187545810580634,ISCA,9657,"

PHP is the dominant server-side scripting language used to implement dynamic web content. Just-in-time compilation, as implemented in Facebook’s state-of-the-art HipHopVM, helps mitigate
the poor performance of PHP, but substantial overheads remain,
especially for realistic, large-scale PHP applications. This paper analyzes such applications and shows that there is little opportunity for
conventional microarchitectural enhancements. Furthermore, prior
approaches for function-level hardware acceleration present many
challenges due to the extremely flat distribution of execution time
across a large number of functions in these complex applications.
In-depth analysis reveals a more promising alternative: targeted acceleration of four fine-grained PHP activities: hash table accesses,
heap management, string manipulation, and regular expression handling. We highlight a set of guiding principles and then propose and.
evaluate inexpensive hardware accelerators for these activities that
accrue substantial performance and energy gains across dozens of
functions. Our results reflect an average 17.93% improvement in
performance and 21.01% reduction in energy while executing these
complex PHP workloads on a state-of-the-art software and hardware
platform.

",18.699421769314853,17.47142857142858,181
ISCA_17_040.txt,15.744639206524848,14.237053905033111,ISCA,9162,"

Heterogeneous memory management combined with server virtualization in datacenters is expected to increase the software and OS
management complexity. State-of-the-art solutions rely exclusively
on the hypervisor (VMM) for expensive page hotness tracking and
migrations, limiting the benefits from heterogeneity. To address this,
we design HeteroOS, a novel application-transparent OS-level solution for managing memory heterogeneity in virtualized system.
The HeteroOS design first makes the guest-OSes heterogeneityaware and then extracts rich OS-level information about applications’ memory usage to place data in the ‘right’ memory avoiding
page migrations. When such pro-active placements are not possible, HeteroOS combines the power of the guest-OSes’ information about applications with the VMM’s hardware control to track
for hotness and migrate only performance-critical pages. Finally,
HeteroOS also designs an efficient heterogeneous memory sharing
across multiple guest-VMs. Evaluation of HeteroOS with memory,
storage, and network-intensive datacenter applications shows up
to 2x performance improvement compared to the state-of-the-art
VMM-exclusive approach.

",20.10790988173199,19.653012048192775,168
ISCA_17_041.txt,14.733542369034616,13.489453724446587,ISCA,6618,"
Convolutional neural networks (CNNs) are revolutionizing machine
learning, but they present significant computational challenges. Recently, many FPGA-based accelerators have been proposed to improve the performance and efficiency of CNNs. Current approaches
construct a single processor that computes the CNN layers one at
a time; the processor is optimized to maximize the throughput at
which the collection of layers is computed. However, this approach
leads to inefficient designs because the same processor structure is
used to compute CNN layers of radically varying dimensions.

We present a new CNN accelerator paradigm and an accompanying automated design methodology that partitions the available
FPGA resources into multiple processors, each of which is tailored.
for a different subset of the CNN convolutional layers. Using the
same FPGA resources as a single large processor, multiple smaller
specialized processors increase computational efficiency and lead to
a higher overall throughput. Our design methodology achieves 3.8x
higher throughput than the state-of-the-art approach on evaluating
the popular AlexNet CNN on a Xilinx Virtex-7 FPGA. For the more
recent SqueezeNet and GoogLeNet, the speedups are 2.2x and 2.0x.

",17.833180683606166,16.687994505494505,186
ISCA_17_042.txt,13.587045489132102,12.069270168855535,ISCA,3694,"

As the size of Deep Neural Networks (DNNs) continues to grow to
increase accuracy and solve more complex problems, their energy
footprint also scales. Weight pruning reduces DNN model size and.
the computation by removing redundant weights. However, we implemented weight pruning for several popular networks on a variety
of hardware platforms and observed surprising results. For many networks, the network sparsity caused by weight pruning will actually
hurt the overall performance despite large reductions in the model
size and required multiply-accumulate operations. Also, encoding
the sparse format of pruned networks incurs additional storage space
overhead. To overcome these challenges, we propose Scalpel that
customizes DNN pruning to the underlying hardware by matching
the pruned network structure to the data-parallel hardware organization. Scalpel consists of two techniques: SIMD-aware weight
pruning and node pruning. For low-parallelism hardware (e.g., microcontroller), SIMD-aware weight pruning maintains weights in
aligned fixed-size groups to fully utilize the SIMD units. For highparallelism hardware (e.g., GPU), node pruning removes redundant
nodes, not redundant weights, thereby reducing computation without
sacrificing the dense matrix format. For hardware with moderate
parallelism (e.g., desktop CPU), SIMD-aware weight pruning and
node pruning are synergistically applied together. Across the microcontroller, CPU and GPU, Scalpel achieves mean speedups of
3.54x, 2.61x, and 1.25x while reducing the model sizes by 88%,
82%, and 53%. In comparison, traditional weight pruning achieves
mean speedups of 1.90x, 1.06x, 0.41x across the three platforms.

",14.790194502661404,13.883993775933611,251
ISCA_17_043.txt,15.858735171052206,14.428604099147858,ISCA,9879,"

Stochastic gradient descent (SGD) is one of the most popular numerical algorithms used in machine learning and other domains. Since
this is likely to continue for the foreseeable future, it is important to
study techniques that can make it run fast on parallel hardware. In
this paper, we provide the first analysis of a technique called BuckWILD! that uses both asynchronous execution and low-precision
computation. We introduce the DMGC model, the first conceptualization of the parameter space that exists when implementing
low-precision SGD, and show that it provides a way to both classify
these algorithms and model their performance. We leverage this
insight to propose and analyze techniques to improve the speed of
low-precision SGD. First, we propose software optimizations that
can increase throughput on existing CPUs by up to 11x. Second, we
propose architectural changes, including a new cache technique we
call an obstinate cache, that increase throughput beyond the limits
of current-generation hardware. We also implement and analyze
low-precision SGD on the FPGA, which is a promising alternative
to the CPU for future SGD systems.

",14.712192995108573,12.71974358974359,183
ISCA_17_044.txt,15.648279474203452,14.096181718303669,ISCA,8291,"

CPU-FPGA heterogeneous platforms offer a promising solution for
high-performance and energy-efficient computing systems by providing specialized accelerators with post-silicon reconfigurability. To
unleash the power of FPGA, however, the programmability gap has
to be filled so that applications specified in high-level programming
languages can be efficiently mapped and scheduled on FPGA. The
above problem is even more challenging for irregular applications, in
which the execution dependency can only be determined at run time.
Thus over-serialized accelerators are generated from existing works
that rely on compile time analysis to schedule the computation.

In this work, we propose a comprehensive software-hardware
co-design framework, which captures parallelism in irregular applications and aggressively schedules pipelined execution on reconfigurable platform. Based on an inherently parallel abstraction
packaging parallelism for runtime schedule, our framework significantly differs from existing works that tend to schedule executions at
compile time. An irregular application is formulated as a set of tasks
with their dependencies specified as rules describing the conditions
under which a subset of tasks can be executed concurrently. Then
datapaths on FPGA will be generated by transforming applications
in the formulation into task pipelines orchestrated by evaluating rules
at runtime, which could exploit fine-grained pipeline parallelism as
handcrafted accelerators do.

An evaluation shows that this framework is able to produce datapath with its quality close to handcrafted designs. Experiments show
that generated accelerators are dramatically more efficient than those
created by current high-level synthesis tools. Meanwhile, accelerators generated for a set of irregular applications attain 0.5x~1.9x
performance compared to equivalent software implementations we
selected on a server-grade 10-core processor, with the memory subsystem remaining as the bottleneck.

",19.739881575394175,18.92874919406834,284
ISCA_17_045.txt,14.11409899154496,12.456742905807992,ISCA,8488,"

Most systems that support speculative parallelization, like hardware
transactional memory (HTM), do not support nested parallelism.
This sacrifices substantial parallelism and precludes composing parallel algorithms. And the few HTMs that do support nested parallelism focus on parallelizing at the coarsest (shallowest) levels,
incurring large overheads that squander most of their potential.

We present FRACTAL, a new execution model that supports unordered and timestamp-ordered nested parallelism. FRACTAL lets
programmers seamlessly compose speculative parallel algorithms,
and lets the architecture exploit parallelism at all levels. FRACTAL
can parallelize a broader range of applications than prior speculative execution models. We design a FRACTAL implementation that
extends the Swarm architecture and focuses on parallelizing at the
finest (deepest) levels. Our approach sidesteps the issues of nested
parallel HTMs and uncovers abundant fine-grain parallelism. As a
result, FRACTAL outperforms prior speculative architectures by up
to 88x at 256 cores.

",15.616094167265928,15.83954022988506,146
ISCA_17_046.txt,14.694117936509866,12.97572426575421,ISCA,9297,"

Finite State Machines (FSM) are widely used computation models for many application domains. These embarrassingly sequential
applications with irregular memory access patterns perform poorly
on conventional von-Neumann architectures. The Micron Automata
Processor (AP) is an in-situ memory-based computational architecture that accelerates non-deterministic finite automata (NFA) processing in hardware. However, each FSM on the AP is processed
sequentially, limiting potential speedups.

In this paper, we explore the FSM parallelization problem in the
context of the AP. Extending classical parallelization techniques to
NFAs executing on AP is non-trivial because of high state-transition
tracking overheads and exponential computation complexity. We
present the associated challenges and propose solutions that leverage both the unique properties of the NFAs (connected components, input symbol ranges, convergence, common parent states)
and unique features in the AP (support for simultaneous transitions,
low-overhead flow switching, state vector cache) to realize parallel
NFA execution on the AP.

We evaluate our techniques against several important benchmarks
including NFAs used for network intrusion detection, malware detection, text processing, protein motif searching, DNA sequencing, and
data analytics. Our proposed parallelization scheme demonstrates
significant speedup (25.5 x on average) compared to sequential execution on AP. Prior work has already shown that sequential execution
on AP is at least an order of magnitude better than GPUs, multi-core
processors and Xeon Phi accelerator.

",18.243605946275583,17.85225,226
ISCA_17_047.txt,15.786063136287208,14.66962100945247,ISCA,6377,"

Non-Volatile Memories (NVMs) can significantly improve the performance of data-intensive applications. A popular form of NVM
is Battery-backed DRAM, which is available and in use today with
DRAMs latency and without the endurance problems of emerging
NVM technologies. Modern servers can be provisioned with up-to
4 TB of DRAM, and provisioning battery backup to write out such
large memories is hard because of the large battery sizes and the
added hardware and cooling costs. We present Viyojit, a system
that exploits the skew in write working sets of applications to provision substantially smaller batteries while still ensuring durability
for the entire DRAM capacity. Viyojit achieves this by bounding the
number of dirty pages in DRAM based on the provisioned battery
capacity and proactively writing out infrequently written pages to
an SSD. Even for write-heavy workloads with less skew than we
observe in analysis of real data center traces, Viyojit reduces the
required battery capacity to 11% of the original size, with a performance overhead of 7-25%. Thus, Viyojit frees battery-backed
DRAM from stunted growth of battery capacities and enables servers
with terabytes of battery-backed DRAM.

",17.613555460941566,16.133841598815692,194
ISCA_17_048.txt,13.799841470217949,11.310510477570851,ISCA,8835,"

This paper investigates compression for DRAM caches. As the capacity of DRAM cache is typically large, prior techniques on cache
compression, which solely focus on improving cache capacity, provide only a marginal benefit. We show that more performance benefit
can be obtained if the compression of the DRAM cache is tailored
to provide higher bandwidth. If a DRAM cache can provide two
compressed lines in a single access, and both lines are useful, the effective bandwidth of the DRAM cache would double. Unfortunately,
it is not straight-forward to compress DRAM caches for bandwidth.
The typically used Traditional Set Indexing (TSI) maps consecutive
lines to consecutive sets, so the multiple compressed lines obtained.
from the set are from spatially distant locations and unlikely to be
used within a short period of each other. We can change the indexing
of the cache to place consecutive lines in the same set to improve
bandwidth; however, when the data is incompressible, such spatial
indexing reduces effective capacity and causes significant slowdown.

Ideally, we would like to have spatial indexing when the data is
compressible and TSI otherwise. To this end, we propose DynamicIndexing Cache comprEssion (DICE), a dynamic design that can
adapt between spatial indexing and TSI, depending on the compressibility of the data. We also propose low-cost Cache Index Predictors
(CIP) that can accurately predict the cache indexing scheme on access in order to avoid probing both indices for retrieving a given
cache line. Our studies with a 1GB DRAM cache, on a wide range of
workloads (including SPEC and Graph), show that DICE improves
performance by 19.0% and reduces energy-delay-product by 36% on
average. DICE is within 3% of a design that has double the capacity
and double the bandwidth. DICE incurs a storage overhead of less
than 1KB and does not rely on any OS support.

",14.985893962826673,13.155728155339805,311
ISCA_17_049.txt,17.274795174532883,15.930518868190404,ISCA,6468,"

The increasing demand for extracting value out of ever-growing
data poses an ongoing challenge to system designers, a task only
made trickier by the end of Dennard scaling. As the performance
density of traditional CPU-centric architectures stagnates, advancing
compute capabilities necessitates novel architectural approaches.
Neat-memory processing (NMP) architectures are reemerging as
promising candidates to improve computing efficiency through tight
coupling of logic and memory. NMP architectures are especially
fitting for data analytics, as they provide immense bandwidth to
memory-resident data and dramatically reduce data movement, the
main source of energy consumption.

Modern data analytics operators are optimized for CPU execution and hence rely on large caches and employ random memory
accesses. In the context of NMP, such random accesses result in
wasteful DRAM row buffer activations that account for a significant
fraction of the total memory access energy. In addition, utilizing
NMP’s ample bandwidth with fine-grained random accesses requires
complex hardware that cannot be accommodated under NMP’s tight
area and power constraints. Our thesis is that efficient NMP calls for
an algorithm-hardware co-design that favors algorithms with sequential accesses to enable simple hardware that accesses memory in
streams. We introduce an instance of such a co-designed NMP architecture for data analytics, the Mondrian Data Engine. Compared to a
CPU-centric and a baseline NMP system, the Mondrian Data Engine
improves the performance of basic data analytics operators by up to
49x and 5x, and efficiency by up to 28x and 5x, respectively.

",18.243605946275583,17.288,253
ISCA_17_050.txt,14.563714700751074,13.12596745366626,ISCA,6840,"
Caches are traditionally organized as a rigid hierarchy, with multiple levels of progressively larger and slower memories. Hierarchy
allows a simple, fixed design to benefit a wide range of applications, since working sets settle at the smallest (i.e., fastest and most
energy-efficient) level they fit in. However, rigid hierarchies also add
overheads, because each level adds latency and energy even when
it does not fit the working set. These overheads are expensive on
emerging systems with heterogeneous memories, where the differences in latency and energy across levels are small. Significant gains
are possible by specializing the hierarchy to applications.
We propose Jenga, a reconfigurable cache hierarchy that dynamically and transparently specializes itself to applications. Jenga builds
virtual cache hierarchies out of heterogeneous, distributed cache
banks using simple hardware mechanisms and an OS runtime. In
contrast to prior techniques that trade energy and bandwidth for performance (e.g., dynamic bypassing or prefetching), Jenga eliminates
accesses to unwanted cache levels. Jenga thus improves both performance and energy efficiency. On a 36-core chip with a 1 GB DRAM
cache, Jenga improves energy-delay product over a combination of
state-of-the-art techniques by 23% on average and by up to 85%.
",16.52667757954773,14.748000000000001,203
ISCA_17_051.txt,16.380063047648303,15.423586942831083,ISCA,6882,"

The trend of unsustainable power consumption and large memory
bandwidth demands in massively parallel multicore systems, with
the advent of the big data era, has brought upon the onset of alternate computation paradigms utilizing heterogeneity, specialization,
processor-in-memory and approximation. Approximate Computing
is being touted as a viable solution for high performance computation by relaxing the accuracy constraints of applications. This trend.
has been accentuated by emerging data intensive applications in
domains like image/video processing, machine learning and big data
analytics that allow inaccurate outputs within an acceptable variance.
Leveraging relaxed accuracy for high throughput in Networks-onChip (NoCs), which have rapidly become the accepted method for
connecting a large number of on-chip components, has not yet been
explored. We propose APPROX-NoC, a hardware data approximation framework with an online data error control mechanism for high
performance NoCs. APPROX-NoC facilitates approximate matching
of data patterns, within a controllable value range, to compress them
thereby reducing the volume of data movement across the chip.

Our evaluation shows that APPROX-NoC achieves on average
up to 9% latency reduction and 60% throughput improvement compared with state-of-the-art NoC data compression mechanisms, while
maintaining low application error. Additionally, with a data intensive
graph processing application we achieve a 36.7% latency reduction
compared to state-of-the-art compression mechanisms.

",19.287186520377343,19.039994394618834,225
ISCA_17_052.txt,15.921511880583271,14.104851084721854,ISCA,10051,"

High-performance computing, enterprise, and datacenter servers are
driving demands for higher total memory capacity as well as memory performance. Memory “cubes” with high per-package capacity
(from 3D integration) along with high-speed point-to-point interconnects provide a scalable memory system architecture with the
potential to deliver both capacity and performance. Multiple such
cubes connected together can form a “Memory Network” (MN),
but the design space for such MNs is quite vast, including multiple
topology types and multiple memory technologies per memory cube.

In this work, we first analyze several MN topologies with different
mixes of memory package technologies to understand the key tradeoffs and bottlenecks for such systems. We find that most of a MN’s
performance challenges arise from the interconnection network that
binds the memory cubes together. In particular, arbitration schemes
used to route through MNs, ratio of NVM to DRAM, and specific
topologies used have dramatic impact on performance and energy
results. Our initial analysis indicates that introducing non-volatile
memory to the MN presents a unique tradeoff between memory array
latency and network latency. We observe that placing NVM cubes in
a specific order in the MN improves performance by reducing the
network size/diameter up to a certain NVM to DRAM ratio. Novel
MN topologies and arbitration schemes also provide performance
and energy deltas by reducing the hop count of requests and response
in the MN. Based on our analyses, we introduce three techniques to
address MN latency issues: (1) Distance-based arbitration scheme
to improve queuing latencies throughout the network, (2) skip-list
topology, derived from the classic data structure, to improve network
latency and link usage, and (3) the MetaCube, a denser memory cube
that leverages advanced packaging technologies to improve latency
by reducing MN size.

",19.686211704642208,17.834999999999997,297
ISCA_17_053.txt,16.331121118590914,15.653869093871112,ISCA,6310,"

Routing algorithms can improve network performance by maximizing routing adaptiveness but can be problematic in the presence of endpoint congestion. Tree-saturation is a well-known behavior caused by endpoint congestion. Adaptive routing can, however,
spread the congestion and result in thick branches of the congestion tree — creating Head-of-Line (HoL) blocking and degrading
performance. In this work, we identify how ignoring virtual channels (VCs) and their occupancy during adaptive routing results in
congestion trees with thick branches as congestion is spread to all
VCs. To address this limitation, we propose Footprint routing algorithm — a new adaptive routing algorithm that minimizes the size
of the congestion tree, both in terms of the number of nodes in the
congestion tree as well as branch thickness. Footprint achieves this
by regulating adaptiveness by requiring packets to follow the path
of prior packets to the same destination if the network is congested
instead of forking a new path or VC. Thus, the congestion tree is
dynamically kept as slim as possible and reduces HoL blocking or
congestion spreading while maintaining high adaptivity and maximizing VC buffer utilization. We evaluate the proposed Footprint
routing algorithm against other adaptive routing algorithms and our
simulation results show that the network saturation throughput can
be improved by up to 43% (58%) compared with the fully adaptive
routing (partially adaptive routing) algorithms.

",17.971250198000288,16.98586111111111,226
ISCA_17_054.txt,14.273229775279713,12.41386012489684,ISCA,9013,"

Freedom from deadlock is one of the most important issues when
designing routing algorithms in on-chip/off-chip networks. Many
works have been developed upon Dally’s theory proving that a network is deadlock-free if there is no cyclic dependency on the channel
dependency graph. However, finding such acyclic graph has been
very challenging, which limits Dally’s theory to networks with a low
number of channels. In this paper, we introduce three theorems that
directly lead to routing algorithms with an acyclic channel dependency graph. We also propose the partitioning methodology, enabling
a design to reach the maximum adaptiveness for the n-dimensional
mesh and k-ary n-cube topologies with any given number of channels. In addition, deadlock-free routing algorithms can be derived
ranging from maximally fully adaptive routing down to deterministic routing. The proposed theorems can drastically remove the
difficulties of designing deadlock-free routing algorithms.

",14.955608360458719,14.40047619047619,153
ISPASS_17_001.txt,15.800800649594795,14.631129203582919,ISPASS,5504,"
Managed applications, written in programming languages such as Java, C# and others, represent a significant
share of workloads in the mobile, desktop, and server domains.
Microarchitectural timing simulation of such workloads is useful
for characterization and performance analysis, of both hardware
and software, as well as for research and development of novel
hardware extensions.

This paper introduces MaxSim, a simulation platform based
on the Maxine VM, the ZSim simulator, and the McPAT modeling
framework. MaxSim is able to simulate fast and accurately
managed workloads running on top of Maxine VM and its
capabilities are showcased with novel simulation techniques for:
1) low-intrusive microarchitectural profiling via pointer tagging
on the x86-64 platforms, 2) modeling of hardware extensions
related, but not limited to, tagged pointers, and 3) modeling of
complex software changes via address-space morphing.

Low-intrusive microarchitectural profiling is achieved by utilizing tagged pointers to collect type- and allocation-site- related
hardware events. Furthermore, MaxSim allows, through a novel
technique called address space morphing, the easy modeling of
complex object layout transformations. Finally, through the codesigned capabilities of MaxSim, novel hardware extensions can
be implemented and evaluated.

We showcase MaxSim’s capabilities by simulating the whole
set of the DaCapo-9.12-bach benchmarks in less than a day
while performing an up-to-date microarchitectural power and
performance characterization. Furthermore, we demonstrate a
hardware/software co-designed optimization that performs dynamic load elimination for array length retrieval achieving up to
14% L1 data cache loads reduction and up to 4% dynamic energy
reduction. MaxSim is available at https://github.com/arodchen/
MaxSim released as free software.

",17.467979349516824,17.724939393939398,270
ISPASS_17_002.txt,15.559935249491481,13.814820539375088,ISPASS,6487,"

When analyzing a distributed computer system, we often
observe that the complex interplay among processor, node,
and network sub-systems can profoundly affect the performance and power efficiency of the distributed computer
system. Therefore, to effectively cross-optimize hardware
and software components of a distributed computer system,
we need a full-system simulation infrastructure that can
precisely capture the complex interplay. Responding to the
aforementioned need, we present dist-gem5, a flexible, detailed, and open-source full-system simulation infrastructure that can model and simulate a distributed computer
system using multiple simulation hosts. Then we validate
dist-gem5 against a physical cluster and show that the latency and bandwidth of the simulated network sub-system
are within 18% of the physical one. Compared with the single threaded and parallel versions of gem5, dist-gem5
speeds up the simulation of a 63-node computer cluster by
83.1x and 12.8, respectively.

",18.878054631974784,17.889718120805373,152
ISPASS_17_003.txt,15.586001076381379,13.805411392036472,ISPASS,7039,"
Cloud computing is gaining popularity due to its
ability to provide infrastructure, platform and software services
to clients on a global scale. Using cloud services, clients reduce
the cost and complexity of buying and managing the underlying
hardware and software layers. Popular services like web search,
data analytics and data mining typically work with big data sets
that do not fit into top level caches. Thus performance efficiency
of last-level caches and the off-chip memory becomes a crucial
determinant of cloud application performance. In this paper we
use CloudSuite as an example and we study how prefetching
schemes affect cloud workloads. We conduct detailed analysis on
address patterns to explore the correlation between prefetching
performance and intrinsic workload characteristics. Our work
focuses particularly on the behavior of memory accesses at the
last-level cache and beyond. We observe that cloud workloads in
general do not have dominant strides. State-of-the-art prefetching
schemes are only able to improve performance for some cloud
applications such as web search. Our analysis shows that cloud
workloads with long temporal reuse patterns often get negatively
impacted by prefetching, especially if their working set is larger
than the cache size.

",15.112257680678326,13.288565656565655,199
ISPASS_17_004.txt,14.774505165583044,13.167751640802312,ISPASS,5373,"
The advent of high speed input sensor and
display technologies and the drive for faster interactive
response suggests that human-computer interaction
(HCI) task processing deadlines of a few milliseconds
or less may be required in future handheld devices.
At the same time, users will expect the same, if not
better, battery life than today’s devices under these
more stringent response requirements.

In this paper, we present a toolbox for exploring
the design space of HCI event processors. We first
describe the simulation platform for interactive environments that runs mobile user interface code with
inputs recorded from human users. We validate it
against a hardware platform from prior work. Given
system-level constraints on latency, we demonstrate
how this toolbox can be used to design a custom
heterogeneous event processor that maximizes battery
life. We show that our toolbox can pick design points
that are 1.5-2.5x more energy-efficient than generalpurpose big.LITTLE architectures.

",14.955608360458719,13.90818181818182,159
ISPASS_17_005.txt,16.475418532613542,14.56311240608915,ISPASS,7770,"
Improving single thread performance is a key challenge in modern microprocessors especially because the traditional approach of increasing clock frequency and deep pipelining
cannot be pushed further due to power constraints. Therefore,
researchers have been looking at unconventional architectures
to boost single thread performance without running into the
power wall. HW/SW co-designed processors like Nvidia Denver,
are emerging as a promising alternative.

However, HW/SW co-designed processors need to address
some key challenges such as startup delay, providing high performance with simple hardware, translation/optimization overhead, etc. before they can become mainstream. A fundamental
requirement for evaluating different design choices and trade-offs
to meet these challenges is to have a simulation infrastructure.
Unfortunately, there is no such infrastructure available today.
Building the aforementioned infrastructure itself poses significant
challenges as it encompasses the complexities of not only an
architectural framework but also of a compilation one.

This paper identifies the key challenges that HW/SW codesigned processors face and the basic requirements for a simulation infrastructure targeting these architectures. Furthermore,
the paper presents DARCO, a simulation infrastructure to enable
research in this domain.

",17.37919286519448,16.38720430107527,187
ISPASS_17_006.txt,14.089484465918641,12.383626566302873,ISPASS,7561,"
The chip industry faces two key challenges today the impending end of Moore’s Law and the rising costs of chip
design and verification (millions of dollars today). Heterogeneous
IPs - cores and domain-specific accelerators - are a promising
answer to the first challenge, enabling performance and energy
benefits no longer provided by technology scaling. IP-reuse
with plug-and-play designs can help with the second challenge,
amortizing NRE costs tremendously. A key requirement in a
heterogeneous IP-based plug-and-play SoC environment is an
interconnection fabric to connect these IPs together. This fabric
needs to be scalable - low latency, low energy and low area and yet be flexible/parametrizable for use across designs. The
key scalability challenge in any Network-on-Chip (NoC) today is
that the latency increases proportional to the number of hops.

In this work, we present a NoC generator called OpenSMART,
which generates low-latency NoCs based on SMART'. SMART
is a recently proposed NoC microarchitecture that enables multihop on-chip traversals within a single cycle, removing the
dependence of latency on hops. SMART leverages wire delay of
the underlying repeated wires, and augments each router with
the ability to request and setup bypass paths. OpenSMART takes
SMART from a NoC optimization to a design methodology for
SoCs, enabling users to generate verified RTL for a class of userspecified network configurations, such as network size, topology,
routing algorithm, number of VCs/buffers, router pipeline stages,
and so on. OpenSMART also provides the ability to generate
any heterogeneous topology with low and high-radix routers
and optimized single-stage pipelines, leveraging fast logic delays
in technology nodes today. OpenSMART v1.0 comes with both
Bluespec System Verilog and Chisel implementations, and this
paper also presents a case study of our experiences with both
languages. OpenSMART is available for download’ and is going
to be a key addition to the emerging open-source hardware
movement, providing a glue for interconnecting existing and
emerging IPs .

",17.122413403193683,16.197361963190186,329
ISPASS_17_007.txt,14.750873036626185,12.950534891481137,ISPASS,8401,"
Computer architects use a variety of workloads to
measure system performance. For many workloads, the workload
configuration determines the stress applied to the system and
the corresponding performance behavior. Therefore, architects
must make great efforts to explore and find the correct workload
configuration before performing detailed analysis.

However, such explorations become impossible for indevelopment systems which exist only as a software model. The
existing system modeling platforms are either accurate but too
slow, or fast but inaccurate to get workload-reported performance
metrics (e.g., latency and throughput) which are necessary for
configuring workloads.

In this paper, we propose SéressRight, a method to quickly
model the first-order performance of full-system workloads and
reconstruct the workload-reported performance metrics. StressRight allows to explore how the workload configurations affect
the stress and performance. The key idea is to execute workloads
on a fast but timing-agnostic platform (e.g. emulators), and efficiently reconstruct the timing/performance details by analyzing
only the unique code blocks. Our evaluation using memcached
and PARSEC shows that StressRight achieves 8~45x speedup
compared to a cycle-level simulator while maintaining good
accuracy.

",15.903189008614273,15.245268817204302,188
ISPASS_17_008.txt,15.587210422911102,13.852672987031447,ISPASS,6342,"
Full-system simulators are increasingly finding their
way into the consumer space for the purposes of backwards
compatibility and hardware emulation (e.g. for games consoles).
For such compute-intensive applications simulation performance
is paramount. In this paper we argue that existing benchmark
suites such as SPEC CPU2006, originally designed for architecture and compiler performance evaluation, are not well suited
for the identification of performance bottlenecks in full-system
simulators. While their large, complex workloads provide an
indication as to the performance of the simulator on ‘real-world’
workloads, this does not give any indication of why a particular
simulator might run an application faster or slower than another.
In this paper we present SimBench, an extensive suite of
targeted micro-benchmarks designed to run bare-metal on a fullsystem simulator. SimBench exercises dynamic binary translation
(BT) performance, interrupt and exception handling, memory
access performance, I/O and other performance-sensitive areas.
SimBench is cross-platform benchmarking framework and can
be retargeted to new architectures with minimal effort. For
several simulators, including QEMU, Gem5 and SimIt-ARM, and
targeting ARM and Intel x86 architectures, we demonstrate that
SimBench is capable of accurately pinpointing and explaining
real-world performance anomalies, which are largely obfuscated
by existing application-oriented benchmarks.

",19.412932280823824,18.044245169082128,209
ISPASS_17_009.txt,15.416339203093692,13.467413906199472,ISPASS,6855,"
An interesting class of irregular algorithms is tree
traversal algorithms, which repeatedly traverse various trees to
perform efficient computations. Tree traversal algorithms form
the algorithmic kernels in an important set of applications in
scientific computing, computer graphics, bioinformatics, and data
mining, etc. There has been increasing interest in understanding
tree traversal algorithms, optimizing them, and applying them
in a wide variety of settings. Crucially, while there are many
possible optimizations for tree traversal algorithms, which optimizations apply to which algorithms is dependent on algorithmic
characteristics.

In this work, we present a suite of tree traversal kernels,
drawn from diverse domains, called Treelogy, to explore the
connection between tree traversal algorithms and state-of-the-art
optimizations. We characterize these algorithms by developing
an ontology based on their structural properties. The attributes
extracted through our ontology, for a given traversal kernel,
can aid in quick analysis of the suitability of platform- and
application- specific as well as independent optimizations. We
provide reference implementations of these kernels for three platforms: shared memory multicores, distributed memory systems,
and GPUs, and evaluate their scalability.

",19.661554785965695,18.489444444444448,181
ISPASS_17_010.txt,15.436092040174685,13.34002698940311,ISPASS,7773,"
GPwUs are often limited by off-chip memory bandwidth. With the advent of general-purpose computing on GPUs,
a cache hierarchy has been introduced to filter the bandwidth
demand to the off-chip memory. However, the cache hierarchy
presents its own bandwidth limitations in sustaining such high
levels of memory traffic.

In this paper, we characterize the bandwidth bottlenecks
present across the memory hierarchy in GPUs for generalpurpose applications. We quantify the stalls throughout the
memory hierarchy and identify the architectural parameters that
play a pivotal role in leading to a congested memory system. We
explore the architectural design space to mitigate the bandwidth
bottlenecks and show that performance improvement achieved
by mitigating the bandwidth bottleneck in the cache hierarchy
can exceed the speedup obtained by a memory system with a
baseline cache hierarchy and High Bandwidth Memory (HBM)
DRAM. We also show that addressing the bandwidth bottleneck
in isolation at specific levels can be sub-optimal and can even be
counter-productive. Therefore, we show that it is imperative to
resolve the bandwidth bottlenecks synergistically across different
levels of the memory hierarchy. With the insights developed in
this paper, we perform a cost-benefit analysis and identify costeffective configurations of the memory hierarchy that effectively
mitigate the bandwidth bottlenecks. We show that our final
configuration achieves a performance improvement of 29% on
average with a minimal area overhead of 1.6%.

",18.026119701940384,16.303848484848487,233
ISPASS_17_011.txt,16.2860484429368,14.654036436928472,ISPASS,7146,"
As GPUs become more pervasive in both scalable
high-performance computing systems and safety-critical embedded systems, evaluating and analyzing their resilience to
soft errors caused by high-energy particle strikes will grow
increasingly important. GPU designers must develop tools
and techniques to understand the effect of these soft errors
on applications. This paper presents an error injection-based
methodology and tool called SASSIFI to study the soft error
resilience of massively parallel applications running on stateof-the-art NVIDIA GPUs. Our approach uses a low-level
assembly-language instrumentation tool called SASSI to profile
and inject errors. SASSI provides efficiency by allowing instrumentation code to execute entirely on the GPU and provides
the ability to inject into different architecture-visible state.
For example, SASSIFI can inject errors in general-purpose
registers, GPU memory, condition code registers, and predicate
registers. SASSIFI can also inject errors into addresses and
register indices. In this paper, we describe the SASSIFI tool,
its capabilities, and present experiments to illustrate some of
the analyses SASSIFI can be used to perform.

",17.971250198000288,16.271535714285715,176
ISPASS_17_012.txt,18.207972483863486,17.004795160263217,ISPASS,5398,"
Optimization, portability and development of
GPGPU applications are not trivial tasks, since the capabilities
and organization of GPU processing elements and memory
subsystem greatly differ from the traditional CPU concepts, as
well as among different GPU architectures. This work goes a
step further in aiding this process by delivering a set of visual
models that can be used by GPU programmers to analyze
and improve application performance and energy-efficiency
across a range of different GPU devices. For the first time in
this paper, the state-of-the-art Cache-aware Roofline Modeling
principles are applied for insightful modeling of GPU upperbounds for performance, power consumption and energyefficiency. The proposed models are developed by relying on
extensive GPU micro-benchmarking aimed at fully exercising
the capabilities of GPU functional units and memory hierarchy
levels. The models are experimentally validated across 8 GPU
devices from 3 different NVIDIA generations, and their benefits
are explored when characterizing the behavior of 23 real-world
applications from 5 different benchmark suites. Furthermore,
the DVFS effects on GPU performance upper-bounds are also
analyzed by scaling both core and memory frequencies.

",20.890749979661237,19.40215053763441,187
ISPASS_17_013.txt,14.879552855430394,13.139524999999999,ISPASS,6253,"
Presilicon simulation is one of the key toolsets for
computer architects to evaluate and optimize their future designs.
As Graphics Processing Units (GPUs) have become the platform
of choice in many computing communities due to their impressive
processing capabilities, computer architecture researchers need a
simulation framework that allows them to quantitatively consider
design tradeoffs.

In this paper, we present the Multi2Sim Kepler simulator
framework, a new detailed GPU microarchitecture performance
simulator that supports NVIDIA’s Kepler shader assembly
(SASS) code execution. The toolset provides a disassembler, a
functional simulator and a detailed cycle-based simulator. We
provide insight into the architecture of the NVIDIA Kepler GPU,
describing the details of the streaming multiprocessor, front-end
and instruction pipelines. We compare the performance of this
new simulator against an NVIDIA K20X, a high-end Kepler
device. We also evaluate the performance of NVIDIA’s CUDA
benchmark suite on our GPU performance simulator.

",16.613394197324528,16.073844678811124,152
ISPASS_17_014.txt,14.349340545954927,12.228184167770639,ISPASS,7248,"
High performance computing (HPC) applications
have parallel code sections that must scale to large numbers of
cores, which makes them sensitive to serial regions. Current supercomputing systems with heterogeneous or asymmetric CMPs
(ACMP) combine few high-performance big cores for serial
regions, together with many low-power lean cores for throughput
computing. The low requirements of HPC applications in the
core front-end lead some designs, such as SMT and GPU cores,
to share front-end structures including the instruction cache
(I-cache). However, little work exists to analyze the benefit of
sharing the I-cache among full cores, which seems compelling as
a solution to reduce silicon area and power.

This paper analyzes the performance, power and area impact
of such a design on an ACMP with one high-performance core
and multiple low-power cores. Having identified that multiple
cores run the same code during parallel regions, the lean cores
share the I-cache with the intent of benefiting from mutual
prefetching, without increasing the average access latency. Our
exploration of the multiple parameters finds the sweet spot on a
wide interconnect to access the shared I-cache and the inclusion
of a few line buffers to provide the required bandwidth and
latency to sustain performance. The projections with McPAT and
a rich set of HPC benchmarks show 11% area savings with a
5% energy reduction at no performance cost.

",17.553077303434723,15.251034482758623,233
ISPASS_17_015.txt,15.029659940733254,13.376555919780884,ISPASS,5492,"
Web developers generally prefer coding in dynamically compiled or scripting languages, such as Java and
JavaScript, because those languages allow them to deploy
applications quickly to deliver new features and bug fixes in
a timely manner. In this paper, we compared the Go language,
a popular statically compiled language, with two dynamically
compiled languages, JavaScript and Java. We evaluated the
Acme Air benchmark for three implementations - one in each
language. Our experimental results have shown that the Go implementation achieved a 3.8x and 2.4x higher throughput than
the JavaScript and Java implementations respectively, after a
simple tuning in the server configuration. Our detailed analysis
indicated that this is primarily because Go suffers less from
polymorphism due to static typing than JavaScript, and because
the Web framework for Go causes less overhead to process
RESTful Web service requests than that for Java. We argue that
statically compiled languages will play more significant roles for
Web applications because of their performance advantages and
also because of emerging continuous integration and deployment
methodologies which eliminate some of the shortcomings in
statically compiled languages over dynamically compiled and
scripting languages.

",18.599290044081553,19.592473118279568,189
ISPASS_17_016.txt,16.357986712741045,15.213952439591072,ISPASS,7001,"
Understanding the reasons why multi-threaded applications do not achieve perfect scaling on modern multicore
hardware is challenging. Furthermore, more and more modern
programs are written in managed languages, which have extra
service threads (e.g., to perform memory management), which
may retard scalability and complicate performance analysis. In
this paper, we extend speedup stacks, a previously-presented
visualization tool to analyze multi-threaded program scalability,
to managed applications. Speedup stacks are comprehensive bar
graphs that break down an application’s execution to explain the
main causes of sublinear speedup, i.e., when some threads are
not allowing the application to progress, and thus increasing the
execution time.
We not only expand speedup stacks to analyze how the
managed language’s service threads affect overall scalability, but
also implement speedup stacks while running on native hardware.
We monitor the application and service threads’ scheduling
behavior using light-weight OS kernel modules, incurring under
1% overhead running unmodified Java benchmarks. We add two
performance delimiters targeting managed applications: garbage
collection and main initialization activities. We analyze the scalability limitations of these benchmarks and the impact of using
both a stop-the-world and a concurrent garbage collector with
speedup stacks. Our visualization tool facilitates the identification
of scalability bottlenecks both between application threads and
of service threads, pointing developers to whether optimization
should be focused on the language runtime or the application.
Speedup stacks provide better program understanding for both
program and system designers, which can help optimize multicore
processor performance.
",17.80541091248751,17.556032520325207,251
ISPASS_17_017.txt,14.244971886685487,12.585337007650619,ISPASS,7425,"
The advent of Persistent Memory (PM), which is
anticipated to have byte-addressable access latency in par with
DRAM and yet nonvolatile, has stepped up interest in using PM
as storage. Hence, PM storage targeted file systems are being
developed under the premise that legacy file systems are suboptimal on memory bus attached PM-based storage. However,
many years of time and effort are ingrained in legacy file
systems that are now time-tested and mature. Simply scrapping
them altogether may be unwarranted. In this paper, we look
into how we can leverage the maturity ingrained in legacy file
systems to the fullest, while, at the same time, reaping the high
performance offered by PM. To this end, we first go through
a thorough analysis of legacy Ext4 file systems, and compare it
with NOVA, PMES, and Ext4 with DAX extension, which are new
PM file systems available in Linux. Based on these analyses, we
then propose the Persistent Memory Adaptation Layer (PMAL)
module that is lightweight (roughly 180 LoC) and can easily
be integrated into legacy file systems to take advantage of PM
storage. Using Ext4, we show that the performance of PMAL
integrated Ext4 is in par with PM file systems for the Filebench
and key-value store benchmarks.

",15.078166124597352,14.157861374407585,212
ISPASS_17_018.txt,15.516284411400576,14.053544454133299,ISPASS,7332,"
Heterogeneous system architectures are evolving towards tighter integration among devices, with emerging features
such as shared virtual memory, memory coherence, and systemwide atomics. Languages, device architectures, system specifications, and applications are rapidly adapting to the challenges
and opportunities of tightly integrated heterogeneous platforms.
Programming languages such as OpenCL 2.0, CUDA 8.0, and
C++ AMP allow programmers to exploit these architectures for
productive collaboration between CPU and GPU threads. To
evaluate these new architectures and programming languages,
and to empower researchers to experiment with new ideas, a suite
of benchmarks targeting these architectures with close CPU-GPU
collaboration is needed.

In this paper, we classify applications that target heterogeneous
architectures into generic collaboration patterns including data
partitioning, fine-grain task partitioning, and coarse-grain task
partitioning. We present Chai, a new suite of 14 benchmarks that
cover these patterns and exercise different features of heterogeneous architectures with varying intensity. Each benchmark
in Chai has seven different implementations in different programming models such as OpenCL, C++ AMP, and CUDA, and
with and without the use of the latest heterogeneous architecture
features. We characterize the behavior of each benchmark with
respect to varying input sizes and collaboration combinations,
and evaluate the impact of using the emerging features of
heterogeneous architectures on application performance.

",20.503739492662863,19.765864485981314,213
ISPASS_17_019.txt,13.58594689125821,11.79249125071988,ISPASS,6477,"
Thanks to modern deep learning frameworks that
exploit GPUs, convolutional neural networks (CNNs) have been
greatly successful in visual recognition tasks. In this paper, we
analyze the GPU performance characteristics of five popular
deep learning frameworks: Caffe, CNTK, TensorFlow, Theano,
and Torch in the perspective of a representative CNN model,
AlexNet. Based on the characteristics obtained, we suggest possible
optimization methods to increase the efficiency of CNN models
built by the frameworks. We also show the GPU performance
characteristics of different convolution algorithms each of which
uses one of GEMM, direct convolution, FFT, and the Winograd
method. We also suggest criteria to choose convolution algorithms
for GPUs and methods to build efficient CNN models on GPUs.
Since scaling DNNs in a multi-GPU context becomes increasingly
important, we also analyze the scalability of the CNN models built
by the deep learning frameworks in the multi-GPU context and
their overhead. The result indicates that we can increase the speed
of training the AlexNet model up to 2X by just changing options
provided by the frameworks.

",16.613394197324528,15.737142857142857,176
ISPASS_17_020.txt,14.956228182180823,13.212515723270442,ISPASS,6346,"
 Graphics-as-a-service (GaaS) is gaining popularity
in cloud computing community. There is an emerging trend of
running GaaS workload using virtualized GPU in current data
center deployment. This paper provides a _ detailed
characterization of GaaS workload under virtualized GPU
NUMA environment, and found that: (1) GaaS workloads exhibit
different behavior with GPGPU workloads by having more
frequent real-time data exchange between CPU and GPU; (2)
GaaS workloads have no NUMA overhead, whether considering
the influence of remote memory access or the resource contention
of CPU uncore. We also test the performance and power tradeoff
among the frequency scaling of CPU clock, GPU core clock, and
GPU memory clock. Characterization results show that (1)
ondemand CPU frequency scaling achieves the best balance
between performance and power consumption; (2) GaaS
workloads are GPU-computation intensive. GPU memory
frequency can be set lower to save energy with little performance
sacrifice.

",17.315433740611066,16.186666666666667,152
ISPASS_17_021.txt,15.903189008614273,14.559121903769324,ISPASS,7070,"
Accurate IPC estimates are critical for generating
performance projections of key workloads on future designs.
However, the need to respond to projections requests in a
timely manner in the face of rapidly evolving applications and
software stacks and tight schedule constraints, often preclude
design teams from executing detailed workload analysis, sampling and simulation flows for such purposes.

We address this problem by taking advantage of the large
amount of data that performance modeling teams commonly
generate as part of architectural studies across thousands of
workload scenarios. We propose two methods for exploiting
these datasets: one that builds proxy suites, and another that
builds decision-tree based classifiers. Both methods can generate IPC estimates for a target workload without collecting new
workload samples, or running a single additional simulation.
We discuss our experience using these techniques to estimate
the IPC of numerous commercial workloads on four industrial
x86 processor designs. The resulting IPC estimates were on
average, within 2% of those obtained via measurements or
detailed cycle-accurate simulations Importantly, using these
methods, we were able to generate IPC estimates for a target
workload in a matter of hours to 1-2 days, compared to several
weeks using conventional approaches.

",18.3970566412798,17.43684134960517,200
ISPASS_17_022.txt,16.177453693304447,14.799345245006801,ISPASS,7616,"
With increasing memory footprints and working set
sizes of emerging workloads, system designers need to evaluate
new memory hierarchies with large last level caches (LLCs),
DRAM caches, large DRAMs, etc. to optimize performance
gains. This requires a deep understanding of the memory access
behavior of the target workloads. It is important to have accurate
mechanisms to generate address streams to study memory access
behavior at and beyond LLCs. Prior memory trace generation
proposals such as WEST and STM utilize LRU stack distance to
capture temporal locality in the data access streams. In addition,
STM also captures spatial locality information by modeling
stride-based access patterns. However, a key drawback of prior
models is that the metadata that they store to capture locality is
significantly high.

In this paper, we propose an efficient, light-weight methodology to generate accurate traces for modeling address Streams
for LLC And Beyond (SLAB). SLAB leverages the key insight
that memory access patterns can be efficiently characterized
by combining locality and reuse statistics captured from both
instruction and address streams. Compared to prior studies,
which capture patterns solely based on data addresses, using the
additional instruction stream localized information significantly
reduces the space complexity. For programs where dominant
instruction-localized patterns do not exist, SLAB exploits multigranularity data reuse distances. We evaluate SLAB using SPEC
CPU2006 and Cloudsuite benchmarks. With meta-data sizes of
less than 7% of the original LLC traces, SLAB demonstrates over
91% accuracy in replicating original application behavior across
~9000 different cache, prefetcher and memory configurations.

",17.5058628484301,15.973578431372548,255
ISPASS_17_023.txt,14.825692589349238,12.705032727232183,ISPASS,6489,"
Computer system designers need a deep understanding of end users’ workload in order to arrive at an
optimum design. However, current design practices suffer from
two problems: fime mismatch where designers rely on workloads
available today to design systems that will be produced years
into the future to run future workloads, and sparse behavior
where many performance behavior is not represented by the
limited set of applications available today. We propose clone
morphing, a systematic method for producing new synthetic
workloads (morphs) with performance behavior that does not
currently exist. The morphs are generated automatically without
knowing or changing the original application’s source code. There
are three different aspects a morph can differ from the original
benchmark it is built on: temporal locality, spatial locality, and
memory footprint. We showed how each of these aspects can be
varied largely independently of other aspects. Furthermore, we
also presented a method for merging two different applications
into one that has an average behavior of both applications. We
evaluated the morphs by running them on simulators and collect
statistics that capture their behavior, and validated that morphs
can be used for projecting future workloads and for generating
new behavior that fills up the behavior map densely.

",17.122413403193683,16.046151477832513,205
ISPASS_17_024.txt,16.542589197482524,15.290670087014508,ISPASS,7293,"
Exascale computing will get mankind closer to
solving important social, scientific and engineering problems.
Due to high prototyping costs, High Performance Computing
(HPC) system architects make use of simulation models for design
space exploration and hardware-software co-design. However, as
HPC systems reach exascale proportions, the cost of simulation
increases, since simulators themselves are largely single-threaded.
Tools for selecting representative parts of parallel applications to
reduce running costs are widespread, e.g., BarrierPoint achieves
this by analysing, in simulation, abstract characteristics such as
basic blocks and reuse distances. However, architectures new to
HPC have a limited set of tools available.

In this work, we provide an independent cross-architectural
evaluation on real hardware—across Intel and ARM—of the
BarrierPoint methodology, when applied to parallel HPC proxy
applications. We present both cases: when the methodology can
be applied and when it cannot. In the former case, results show
that we can predict the performance of full application execution
by running shorter representative sections. In the latter case,
we dive into the underlying issues and suggest improvements.
We demonstrate a total simulation time reduction of up to
178x, whilst keeping the error below 2.3% for both cycles and
instructions.

",15.6451,14.525773869346736,202
KDD_17_001.txt,14.327235833311914,12.802866450846214,KDD,8204,"

Local graph clustering methods aim to find a cluster of nodes by
exploring a small region of the graph. These methods are attractive
because they enable targeted clustering around a given seed node
and are faster than traditional global graph clustering methods
because their runtime does not depend on the size of the input
graph. However, current local graph partitioning methods are not
designed to account for the higher-order structures crucial to the
network, nor can they effectively handle directed networks. Here
we introduce a new class of local graph clustering methods that
address these issues by incorporating higher-order network information captured by small subgraphs, also called network motifs.
We develop the Motif-based Approximate Personalized PageRank
(MAPPR) algorithm that finds clusters containing a seed node with
minimal motif conductance, a generalization of the conductance
metric for network motifs. We generalize existing theory to prove
the fast running time (independent of the size of the graph) and obtain theoretical guarantees on the cluster quality (in terms of motif
conductance). We also develop a theory of node neighborhoods
for finding sets that have small motif conductance, and apply these
results to the case of finding good seed nodes to use as input to the
MAPPR algorithm. Experimental validation on community detection tasks in both synthetic and real-world networks, shows that
our new framework MAPPR outperforms the current edge-based
personalized PageRank methodology.

",16.975882523387877,16.84570512820513,235
KDD_17_002.txt,15.018007186767129,13.4935730325525,KDD,6487,"

Nonlinear kernel machines often yield superior predictive performance on various tasks; however, they suffer from severe computational challenges. In this paper, we show how to overcome the
important challenge of speeding up kernel machines using multiple
computers. In particular, we develop a parallel block minimization
framework, and demonstrate its good scalability in solving nonlinear kernel SVM and logistic regression. Our framework proceeds by
dividing the problem into smaller subproblems by forming a blockdiagonal approximation of the Hessian matrix. The subproblems
are then solved approximately in parallel. After that, a communication efficient line search procedure is developed to ensure sufficient
reduction of the objective function value by exploiting the problem
structure of kernel machines. We prove global linear convergence
rate of the proposed method with a wide class of subproblem solvers,
and our analysis covers strongly convex and some non-strongly
convex functions. We apply our algorithm to solve large-scale kernel SVM problems on distributed systems, and show a significant
improvement over existing parallel solvers. As an example, on the
covtype dataset with half-a-million samples, our algorithm can obtain an approximate solution with 96% accuracy in 20 seconds using
32 machines, while all the other parallel kernel SVM solvers require
more than 2000 seconds to achieve a solution with 95% accuracy.
Moreover, our algorithm is the first distributed kernel SVM solver
that can scale to massive data sets. On the kddb dataset (20 million
samples and 30 million features), our parallel solver can compute
the kernel SVM solution within half an hour using 32 machines
with 640 cores in total, while existing solvers can not scale to this
dataset.

",17.015998829145012,16.137739013753777,272
KDD_17_003.txt,14.460243349426975,13.09289172932331,KDD,4903,"

Matrix sketching is aimed at finding compact representations of
a matrix while simultaneously preserving most of its properties,
which is a fundamental building block in modern scientific computing. Randomized algorithms represent state-of-the-art and have
attracted huge interest from the fields of machine learning, data
mining, and theoretic computer science. However, it still requires
the use of the entire input matrix in producing desired factorizations, which can be a major computational and memory bottleneck
in truly large problems. In this paper, we uncover an interesting
theoretic connection between matrix low-rank decomposition and
lossy signal compression, based on which a cascaded compression
sampling framework is devised to approximate an m Xx n matrix
in only O(m + n) time and space. Indeed, the proposed method
accesses only a small number of matrix rows and columns, which
significantly improves the memory footprint. Meanwhile, by sequentially teaming two rounds of approximation procedures and
upgrading the sampling strategy from a uniform probability to
more sophisticated, encoding-orientated sampling, significant algorithmic boosting is achieved to uncover more granular structures
in the data. Empirical results on a wide spectrum of real-world,
large-scale matrices show that by taking only linear time and space,
the accuracy of our method rivals those state-of-the-art randomized
algorithms consuming a quadratic, O(mn), amount of resources.

",19.5731925562951,19.2701601537476,223
KDD_17_004.txt,13.974334437300097,12.392770183714099,KDD,5309,"

Current deep learning architectures are growing larger in order
to learn from complex datasets. These architectures require giant
matrix multiplication operations to train millions of parameters.
Conversely, there is another growing trend to bring deep learning
to low-power, embedded devices. The matrix operations, associated
with the training and testing of deep networks, are very expensive
from a computational and energy standpoint. We present a novel
hashing-based technique to drastically reduce the amount of computation needed to train and test neural networks. Our approach
combines two recent ideas, Adaptive Dropout and Randomized
Hashing for Maximum Inner Product Search (MIPS), to select the
nodes with the highest activations efficiently. Our new algorithm
for deep learning reduces the overall computational cost of the forward and backward propagation steps by operating on significantly
fewer nodes. As a consequence, our algorithm uses only 5% of the
total multiplications, while keeping within 1% of the accuracy of
the original model on average. A unique property of the proposed
hashing-based back-propagation is that the updates are always
sparse. Due to the sparse gradient updates, our algorithm is ideally
suited for asynchronous, parallel training, leading to near-linear
speedup, as the number of cores increases. We demonstrate the
scalability and sustainability (energy efficiency) of our proposed algorithm via rigorous experimental evaluations on several datasets.

",16.133371291317395,15.702727272727277,221
KDD_17_005.txt,15.46286203336863,14.04866678717045,KDD,6845,"

Detecting local events (e.g., protest, disaster) at their onsets is an
important task for a wide spectrum of applications, ranging from
disaster control to crime monitoring and place recommendation.
Recent years have witnessed growing interest in leveraging geotagged tweet streams for online local event detection. Nevertheless,
the accuracies of existing methods still remain unsatisfactory for
building reliable local event detection systems. We propose TrIOVECEVENT, a method that leverages multimodal embeddings to
achieve accurate online local event detection. The effectiveness of
TRIOVECEVENT is underpinned by its two-step detection scheme.
First, it ensures a high coverage of the underlying local events by
dividing the tweets in the query window into coherent geo-topic
clusters. To generate quality geo-topic clusters, we capture shorttext semantics by learning multimodal embeddings of the location,
time, and text, and then perform online clustering with a novel
Bayesian mixture model. Second, TRIOVECEVENT considers the
geo-topic clusters as candidate events and extracts a set of features for classifying the candidates. Leveraging the multimodal
embeddings as background knowledge, we introduce discriminative features that can well characterize local events, which enable
pinpointing true local events from the candidate pool with a small
amount of training data. We have used crowdsourcing to evaluate
TRIOVECEVENT, and found that it improves the performance of the
state-of-the-art method by a large margin.

",16.647925096878797,15.648704035874438,225
KDD_17_006.txt,15.970480548889924,14.662330383977508,KDD,6576,"

Mining a large number of datasets recording human activities for
making sense of individual data is the key enabler of a new wave
of personalized knowledge-based services. In this paper we focus
on the problem of clustering individual transactional data for a
large mass of users. Transactional data is a very pervasive kind of
information that is collected by several services, often involving
huge pools of users. We propose txmeans, a parameter-free clustering algorithm able to efficiently partitioning transactional data
in acompletely automatic way. Txmeans is designed for the case
where clustering must be applied on a massive number of different
datasets, for instance when a large set of users need to be analyzed
individually and each of them has generated a long history of transactions. A deep experimentation on both real and synthetic datasets
shows the practical effectiveness of txmeans for the mass clustering
of different personal datasets, and suggests that txmeans outperforms existing methods in terms of quality and efficiency. Finally,
we present a personal cart assistant application based on txmeans.

",18.3970566412798,16.411428571428576,176
KDD_17_007.txt,14.28968737253555,12.531946514915877,KDD,6783,"

We present the design and implementation of a custom discrete
optimization technique for building rule lists over a categorical
feature space. Our algorithm provides the optimal solution, with a
certificate of optimality. By leveraging algorithmic bounds, efficient
data structures, and computational reuse, we achieve several orders
of magnitude speedup in time and a massive reduction of memory
consumption. We demonstrate that our approach produces optimal
rule lists on practical problems in seconds. This framework is a
novel alternative to CART and other decision tree methods.

",15.381575749822971,14.778823529411767,86
KDD_17_008.txt,13.213802875792783,11.79473950514624,KDD,4114,"

Heterogeneous Information Network (HIN) is a natural and general
representation of data in modern large commercial recommender
systems which involve heterogeneous types of data. HIN based
recommenders face two problems: how to represent the high-level
semantics of recommendations and how to fuse the heterogeneous
information to make recommendations. In this paper, we solve
the two problems by first introducing the concept of meta-graph
to HIN-based recommendation, and then solving the information
fusion problem with a “matrix factorization (MF) + factorization
machine (FM)” approach. For the similarities generated by each
meta-graph, we perform standard MF to generate latent features for
both users and items. With different meta-graph based features, we
propose a group lasso regularized FM to automatically learn from
the observed ratings to effectively select useful meta-graph based
features. Experimental results on two real-world datasets, Amazon
and Yelp, show the effectiveness of our approach compared to stateof-the-art FM and other HIN-based recommendation algorithms.

",16.52667757954773,17.519110429447853,163
KDD_17_009.txt,16.631731649145806,14.97152338645822,KDD,4352,"

Given a database and a target attribute of interest, how can we
tell whether there exists a functional, or approximately functional
dependence of the target on any set of other attributes in the data?
How can we reliably, without bias to sample size or dimensionality,
measure the strength of such a dependence? And, how can we efficiently discover the optimal or a-approximate top-k dependencies?
These are exactly the questions we answer in this paper.

As we want to be agnostic on the form of the dependence, we
adopt an information-theoretic approach, and construct a reliable,
bias correcting score that can be efficiently computed. Moreover, we
give an effective optimistic estimator of this score, by which for the
first time we can mine the approximate functional dependencies
from data with guarantees of optimality. Empirical evaluation
shows that the derived score achieves a good bias for variance
trade-off, can be used within an efficient discovery algorithm, and
indeed discovers meaningful dependencies. Most important, it
remains reliable in the face of data sparsity.

",16.827784334635936,14.458017241379313,175
KDD_17_010.txt,15.918033974702595,14.944361136818227,KDD,6067,"

The mobile in-App service analysis, aiming at classifying mobile internet traffic into different types of service usages, has
become a challenging and emergent task for mobile service
providers due to the increasing adoption of secure protocols
for in-App services. While some efforts have been made for
the classification of mobile internet traffic, existing methods
rely on complex feature construction and large storage cache,
which lead to low processing speed, and thus not practical
for online real-time scenarios. To this end, we develop an
iterative analyzer for classifying encrypted mobile traffic in
a real-time way. Specifically, we first select an optimal set
of most discriminative features from raw features extracted
from traffic packet sequences by a novel Maximizing Inner
activity similarity and Minimizing Different activity similarity (MIMD) measurement. To develop the online analyzer,
we first represent a traffic flow with a series of time windows,
which are described by the optimal feature vector and are
updated iteratively at the packet level. Instead of extracting
feature elements from a series of raw traffic packets, our
feature elements are updated when a new traffic packet is
observed and the storage of raw traffic packets is not required.
The time windows generated from the same service usage activity are grouped by our proposed method, namely, recursive
time continuity constrained KMeans clustering (rCKC). The
feature vectors of cluster centers are then fed into a random
forest classifier to identify corresponding service usages. Finally, we provide extensive experiments on real-world traffic
data from Wechat, Whatsapp, and Facebook to demonstrate
the effectiveness and efficiency of our approach. The results
show that the proposed analyzer provides high accuracy in
real-world scenarios, and has low storage cache requirement
as well as fast processing speed.


",18.026119701940384,17.69645704467354,292
KDD_17_011.txt,14.780137498200869,12.511427669682792,KDD,4804,"

Functional annotation of human genes is fundamentally important for understanding the molecular basis of various genetic diseases. A major challenge in determining the functions of human
genes lies in the functional diversity of proteins, that is, a gene
can perform different functions as it may consist of multiple protein coding isoforms (PCIs). Therefore, differentiating functions of
PCls can significantly deepen our understanding of the functions
of genes. However, due to the lack of isoform-level gold-standards
(ground-truth annotation), many existing functional annotation
approaches are developed at gene-level. In this paper, we propose a
novel approach to differentiate the functions of PCIs by integrating
sparse simplex projection—that is, a nonconvex sparsity-inducing

regularizer—with the framework of multi-instance learning (MIL).
Specifically, we label the genes that are annotated to the function
under consideration as positive bags and the genes without the
function as negative bags. Then, by sparse projections onto simplex,
we learn a mapping that embeds the original bag space to a discriminative feature space. Our framework is flexible to incorporate
various smooth and non-smooth loss functions such as logistic loss
and hinge loss. To solve the resulting highly nontrivial non-convex
and non-smooth optimization problem, we further develop an efficient block coordinate descent algorithm. Extensive experiments
on human genome data demonstrate that the proposed approaches
significantly outperform the state-of-the-art methods in terms of
functional annotation accuracy of human PCIs and efficiency.

",17.353723509956247,16.228991735537196,243
KDD_17_012.txt,13.613851032357722,11.89034748089583,KDD,6085,"

Given a graph G, a source node s and a target node ¢, the personalized PageRank (PPR) of t with respect to s is the probability that
a random walk starting from s terminates at ¢. A single-source PPR
(SSPPR) query enumerates all nodes in G, and returns the top-k
nodes with the highest PPR values with respect to a given source
node s. SSPPR has important applications in web search and social
networks, e.g., in Twitter’s Who-To-Follow recommendation service. However, SSPPR computation is immensely expensive, and
at the same time resistant to indexing and materialization. So far,
existing solutions either use heuristics, which do not guarantee result quality, or rely on the strong computing power of modern data
centers, which is costly.

Motivated by this, we propose FORA, a simple and effective
index-based solution for approximate SSPPR processing, with rigorous guarantees on result quality. The basic idea of FORA is to
combine two existing methods Forward Push (which is fast but
does not guarantee quality) and Monte Carlo Random Walk (accurate but slow) in a simple and yet non-trivial way, leading to an
algorithm that is both fast and accurate. Further, FORA includes
a simple and effective indexing scheme, as well as a module for
top-k selection with high pruning power. Extensive experiments
demonstrate that FORA is orders of magnitude more efficient than
its main competitors. Notably, on a billion-edge Twitter dataset,
FORA answers a top-500 approximate SSPPR query within 5 seconds, using a single commodity server.

",15.903189008614273,14.359848249027241,258
KDD_17_013.txt,14.61776489232167,12.176472642100354,KDD,5102,"

In recent years there has been a growing interest in developing “streaming algorithms” for efficient processing and querying of continuous data streams. These algorithms seek to provide accurate results while minimizing the required storage
and the processing time, at the price of a small inaccuracy
in their output. A fundamental query of interest is the intersection size of two big data streams. This problem arises
in many different application areas, such as network monitoring, database systems, data integration and information
retrieval. In this paper we develop a new algorithm for this
problem, based on the Maximum Likelihood (ML) method.
We show that this algorithm outperforms all known schemes
in terms of the estimation’s quality (lower variance) and that
it asymptotically achieves the optimal variance.
",17.122413403193683,14.928064516129037,126
KDD_17_014.txt,15.416649702536617,14.113487303448771,KDD,5929,"

Autoencoders have been successful in learning meaningful representations from image datasets. However, their performance on text
datasets has not been widely studied. Traditional autoencoders tend
to learn possibly trivial representations of text documents due to
their confounding properties such as high-dimensionality, sparsity
and power-law word distributions. In this paper, we propose a novel
k-competitive autoencoder, called KATE, for text documents. Due
to the competition between the neurons in the hidden layer, each
neuron becomes specialized in recognizing specific data patterns,
and overall the model can learn meaningful representations of textual data. A comprehensive set of experiments show that KATE can
learn better representations than traditional autoencoders including
denoising, contractive, variational, and k-sparse autoencoders. Our
model also outperforms deep generative models, probabilistic topic
models, and even word representation models (e.g., Word2Vec) in
terms of several downstream tasks such as document classification,
regression, and retrieval.

",18.243605946275583,17.24353741496599,149
KDD_17_015.txt,15.366394043469295,13.871039644962249,KDD,7172,"

We study the online constrained ranking problem motivated by
an application to web-traffic shaping: an online stream of sessions
arrive in which, within each session, we are asked to rank items.
The challenge involves optimizing the ranking in each session so
that local vs. global objectives are controlled: within each session
one wishes to maximize a reward (local) while satisfying certain
constraints over the entire set of sessions (global). A typical application of this setup is that of page optimization in a web portal. We
wish to rank items so that not only is user engagement maximized
in each session, but also other business constraints (such as the
number of views/clicks delivered to various publishing partners)
are satisfied.

We describe an online algorithm for performing this optimization.
A novel element of our approach is the use of linear programming
duality and connections to the celebrated Hungarian algorithm.
This framework enables us to determine a set of shadow prices for
each traffic-shaping constraint that can then be used directly in the
final ranking function to assign near-optimal rankings. The (dual)
linear program can be solved off-line periodically to determine
the prices. At serving time these prices are used as weights to
compute weighted rank-scores for the items, and the simplicity of
the approach facilitates scalability to web applications. We provide
rigorous theoretical guarantees for the performance of our online
algorithm and validate our approach using numerical experiments
on real web-traffic data from a prominent internet portal.

",15.6451,15.403079365079368,253
KDD_17_016.txt,15.448219346937869,14.269421770129846,KDD,6452,"

Precisely recommending relevant items from massive candidates
to a large number of users is an indispensable yet computationally
expensive task in many online platforms (e.g., Amazon.com and
Netflix.com). A promising way is to project users and items into a
Hamming space and then recommend items via Hamming distance.
However, previous studies didn’t address the cold-start challenges
and couldn’t make the best use of preference data like implicit feedback. To fill this gap, we propose a Discrete Content-aware Matrix
Factorization (DCMF) model, 1) to derive compact yet informative
binary codes at the presence of user/item content information; 2)
to support the classification task based on a local upper bound of
logit loss; 3) to introduce an interaction regularization for dealing
with the sparsity issue. We further develop an efficient discrete
optimization algorithm for parameter learning. Based on extensive
experiments on three real-world datasets, we show that DCFM outperforms the state-of-the-arts on both regression and classification
tasks.

",17.122413403193683,16.86469135802469,168
KDD_17_017.txt,13.355510775450142,11.382033193557955,KDD,6435,"

Kernel regression is an essential and ubiquitous tool for non-parametric

data analysis, particularly popular among time series and spatial
data. However, the central operation which is performed many
times, evaluating a kernel on the data set, takes linear time. This is
impractical for modern large data sets.

In this paper we describe coresets for kernel regression: compressed data sets which can be used as proxy for the original data
and have provably bounded worst case error. The size of the coresets are independent of the raw number of data points; rather they
only depend on the error guarantee, and in some cases the size of
domain and amount of smoothing. We evaluate our methods on
very large time series and spatial data, and demonstrate that they
incur negligible error, can be constructed extremely efficiently, and
allow for great computational gains.

",15.021129683784007,14.496985815602837,142
KDD_17_018.txt,15.442889606028618,13.36591743119266,KDD,4155,"

Classification problems with a large number of classes inevitably
involve overlapping or similar classes. In such cases it seems reasonable to allow the learning algorithm to make mistakes on similar
classes, as long as the true class is still among the top-k (say) predictions. Likewise, in applications such as search engine or ad
display, we are allowed to present k predictions at a time and the
customer would be satisfied as long as her interested prediction is
included. Inspired by the recent work of [15], we propose a very
generic, robust multiclass SVM formulation that directly aims at
minimizing a weighted and truncated combination of the ordered
prediction scores. Our method includes many previous works as
special cases. Computationally, using the Jordan decomposition
Lemma we show how to rewrite our objective as the difference of
two convex functions, based on which we develop an efficient algorithm that allows incorporating many popular regularizers (such
as the Ig and I; norms). We conduct extensive experiments on four
real large-scale visual category recognition datasets, and obtain very
promising performances.

",17.28802050969988,16.203547351524886,179
KDD_17_019.txt,15.49868798635002,13.78532116956372,KDD,5685,"

The key challenge on estimating treatment effect in the wild
observational studies is to handle confounding bias induced
by imbalance of the confounder distributions between treated
and control units. Traditional methods remove confounding
bias by re-weighting units with supposedly accurate propensity score estimation under the unconfoundedness assumption.
Controlling high-dimensional variables may make the unconfoundedness assumption more plausible, but poses new
challenge on accurate propensity score estimation. One strand of recent literature seeks to directly optimize weights to
balance confounder distributions, bypassing propensity score
estimation. But existing balancing methods fail to do selection and differentiation among the pool of a large number of
potential confounders, leading to possible underperformance
in many high dimensional settings. In this paper, we propose
a data-driven Differentiated Confounder Balancing (DCB)
algorithm to jointly select confounders, differentiate weights
of confounders and balance confounder distributions for treatment effect estimation in the wild high dimensional settings.
The synergistic learning algorithm we proposed is more capable of reducing the confounding bias in many observational
studies. To validate the effectiveness of our DCB algorithm,
we conduct extensive experiments on both synthetic and real
datasets. The experimental results clearly demonstrate that
our DCB algorithm outperforms the state-of-the-art methods
on treatment effect estimation.
",18.946978176291534,18.540550161812302,207
KDD_17_020.txt,16.62280492943922,15.438188390071886,KDD,5432,"

Evaluating whether machines improve on human performance is
one of the central questions of machine learning. However, there
are many domains where the data is selectively labeled in the sense
that the observed outcomes are themselves a consequence of the
existing choices of the human decision-makers. For instance, in
the context of judicial bail decisions, we observe the outcome of
whether a defendant fails to return for their court appearance only
if the human judge decides to release the defendant on bail. This
selective labeling makes it harder to evaluate predictive models
as the instances for which outcomes are observed do not represent a random sample of the population. Here we propose a novel
framework for evaluating the performance of predictive models on
selectively labeled data. We develop an approach called contraction
which allows us to compare the performance of predictive models
and human decision-makers without resorting to counterfactual
inference. Our methodology harnesses the heterogeneity of human
decision-makers and facilitates effective evaluation of predictive
models even in the presence of unmeasured confounders (unobservables) which influence both human decisions and the resulting
outcomes. Experimental results on real world datasets spanning
diverse domains such as health care, insurance, and criminal justice demonstrate the utility of our evaluation metric in comparing
human decisions and machine predictions.

",18.10804710084791,17.66592592592593,217
KDD_17_021.txt,16.437942528957237,15.40826490144891,KDD,5903,"
We study the problem of representation learning in heterogeneous
networks. Its unique challenges come from the existence of multiple types of nodes and links, which limit the feasibility of the
conventional network embedding techniques. We develop two
scalable representation learning models, namely metapath2vec and
metapath2vec++. The metapath2vec model formalizes meta-pathbased random walks to construct the heterogeneous neighborhood
of a node and then leverages a heterogeneous skip-gram model
to perform node embeddings. The metapath2vec++ model further
enables the simultaneous modeling of structural and semantic correlations in heterogeneous networks. Extensive experiments show
that metapath2vec and metapath2vec++ are able to not only outperform state-of-the-art embedding models in various heterogeneous
network mining tasks, such as node classification, clustering, and
similarity search, but also discern the structural and semantic correlations between diverse network objects.

",18.422482065455632,18.037142857142857,135
KDD_17_022.txt,14.397838132529571,12.504407252356899,KDD,4481,"

The method of “random Fourier features (RFF)” has become a popular tool for approximating the “radial basis function (RBF)” kernel.
The variance of RFF is actually large. Interestingly, the variance
can be substantially reduced by a simple normalization step as we
theoretically demonstrate. We name the improved scheme as the
“normalized RFF (NRFF)”, and we provide a technical proof of the
asymptotic variance of NRFF, as validated by simulations.

We also propose the “generalized min-max (GMM)” kernel as a
measure of data similarity, where data vectors can have both positive and negative entries. GMM is positive definite as there is an
associate hashing method named “generalized consistent weighted
sampling (GCWS)” which linearizes this (nonlinear) kernel. We
provide an extensive empirical evaluation of the RBF and GMM
kernels on more than 50 datasets. For a majority of the datasets, the
(tuning-free) GMM kernel outperforms the best-tuned RBF kernel.

We then conduct extensive classification experiments for comparing the linearized RBF kernel using NRFF with the linearized GMM
kernel using GCWS. We observe that, in order to reach a similar accuracy, GCWS typically requires substantially fewer samples than
NRFF, even on datasets where the original RBF kernel outperforms
the original GMM kernel. As the training, storage, and processing costs are directly proportional to the sample size, our experiments can help demonstrate that GCWS would be a more practical
scheme for large-scale machine learning applications.

The empirical success of GCWS (compared to NRFF) can also be
explained theoretically, from at least two aspects. Firstly, the relative variance (normalized by the squared expectation) of GCWS is
substantially smaller than that of NRFF, except for the very high
similarity region (where the variances of both methods approach
zero). Secondly, if we are allowed to make a general model assumption on the data, then we can show analytically that GCWS exhibits
much smaller variance than NRFF for estimating the same object
(eg., the RBF kernel), except for the very high similarity region.

",16.351538315227643,15.0588905775076,330
KDD_17_023.txt,14.420197573723947,12.40270541581247,KDD,6275,"

How do people make friends dynamically in social networks?
What are the temporal patterns for an individual increasing
its social connectivity? What are the basic mechanisms
governing the formation of these temporal patterns? No
matter cyber or physical social systems, their structure and
dynamics are mainly driven by the connectivity dynamics of
each individual. However, due to the lack of empirical data,
little is known about the empirical dynamic patterns of social
connectivity at microscopic level, let alone the regularities or
models governing these microscopic dynamics.

We examine the detailed growth process of “WeChat”,
the largest online social network in China, with 300 million
users and 4.75 billion links spanning two years. We uncover
a wide range of long-term power law growth and short-term
bursty growth for the social connectivity of different users. We propose three key ingredients, namely average-effect,
multiscale-effect and correlation-effect, which govern the observed growth patterns at microscopic level. As a result, we
propose the long short memory process incorporating these
ingredients, demonstrating that it successfully reproduces
the complex growth patterns observed in the empirical data.
By analyzing modeling parameters, we discover statistical
regularities underlying the empirical growth dynamics. Our
model and discoveries provide a foundation for the microscopic mechanisms of network growth dynamics, potentially
leading to implications for prediction, clustering and outlier
detection on human dynamics.

",16.581925556534415,15.987272727272728,227
KDD_17_024.txt,14.45860203053261,12.356900639010572,KDD,5268,"
 observe medium-sized communities (around 100 nodes) with clear

We propose ego-splitting, a new framework for detecting clusters
in complex networks which leverage the local structures known as
ego-nets (i.e. the subgraph induced by the neighborhood of each
node) to de-couple overlapping clusters. Ego-splitting is a highly
scalable and flexible framework, with provable theoretical guarantees, that reduces the complex overlapping clustering problem
to a simpler and more amenable non-overlapping (partitioning)
problem. We can scale community detection to graphs with tens
of billions of edges and outperform previous solutions based on
ego-nets analysis.

More precisely, our framework works in two steps: a local egonet analysis phase, and a global graph partitioning phase. In the
local step, we first partition the nodes’ ego-nets using a partitioning
algorithm. We then use the computed clusters to split each node
into its persona nodes that represent the instantiations of the node
in its communities. Finally, in the global step, we partition the
newly created graph to obtain an overlapping clustering of the
original graph.

",17.122413403193683,15.469123376623376,178
KDD_17_025.txt,15.736889600559937,14.531440991351335,KDD,7615,"
Feature engineering has found increasing interest in recent years because of its ability to improve the effectiveness of various machine
learning models. Although tailored feature engineering methods
have been designed for various domains, there are few that simulate the consistent effectiveness of kernel methods. At the core,
the success of kernel methods is achieved by using similarity functions that emphasize local variations in similarity. Unfortunately,
this ability comes at the price of the high level of computational
resources required and the inflexibility of the representation as it
only provides the similarity of two data points instead of vector
representations of each data point; while the vector representations can be readily used as input to facilitate various models for
different tasks. Furthermore, kernel methods are also highly susceptible to overfitting and noise and it cannot capture the variety
of data locality. In this paper, we first analyze the inner working
and weaknesses of kernel method, which serves as guidance for
designing feature engineering. With the guidance, we explore the
use of randomized methods for feature engineering by capturing
multi-granular locality of data. This approach has the merit of being
time and space efficient for feature construction. Furthermore, the
approach is resistant to overfitting and noise because the randomized approach naturally enables fast and robust ensemble methods.
Extensive experiments on a number of real world datasets are conducted to show the effectiveness of the approach for various tasks
such as clustering, classification and outlier detection.

",17.353723509956247,16.70386885245902,245
KDD_17_026.txt,14.046124850309653,12.958513433742745,KDD,6888,"

Time series motif discovery has emerged as perhaps the most
used primitive for time series data mining, and has seen
applications to domains as diverse as robotics, medicine and
climatology. There has been recent significant progress on the
scalability of motif discovery. However, we believe that the
current definitions of motif discovery are limited, and can create
a mismatch between the user’s intent/expectations, and the
motif discovery search outcomes. In this work, we explain the
reasons behind these issues, and introduce a novel and general
framework to address them. Our ideas can be used with current
state-of-the-art algorithms with virtually no time or space
overhead, and are fast enough to allow real-time interaction and
hypotheses testing on massive datasets. We demonstrate the
utility of our ideas on domains as diverse as seismology and
epileptic seizure monitoring.

",15.903189008614273,15.333865248226953,143
KDD_17_027.txt,15.85617727642132,14.925062625847982,KDD,5713,"

The availability of large idea repositories (e.g., the U.S. patent database) could significantly accelerate innovation and discovery by
providing people with inspiration from solutions to analogous problems. However, finding useful analogies in these large, messy, realworld repositories remains a persistent challenge for either human
or automated methods. Previous approaches include costly handcreated databases that have high relational structure (e.g., predicate
calculus representations) but are very sparse. Simpler machinelearning/information-retrieval similarity metrics can scale to large,
natural-language datasets, but struggle to account for structural
similarity, which is central to analogy. In this paper we explore the
viability and value of learning simpler structural representations,
specifically, “problem schemas”, which specify the purpose of a
product and the mechanisms by which it achieves that purpose. Our
approach combines crowdsourcing and recurrent neural networks
to extract purpose and mechanism vector representations from
product descriptions. We demonstrate that these learned vectors
allow us to find analogies with higher precision and recall than traditional information-retrieval methods. In an ideation experiment,
analogies retrieved by our models significantly increased people’s
likelihood of generating creative ideas compared to analogies retrieved by traditional methods. Our results suggest a promising
approach to enabling computational analogy at scale is to learn and
leverage weaker structural representations.
",19.061188166129803,19.54521739130435,212
KDD_17_028.txt,15.52122333115457,14.17436156149402,KDD,6391,"

A text corpus typically contains two types of context information
— global context and local context. Global context carries topical
information which can be utilized by topic models to discover topic
structures from the text corpus, while local context can train word
embeddings to capture semantic regularities reflected in the text
corpus. This encourages us to exploit the useful information in
both the global and the local context information. In this paper, we
propose a unified language model based on matrix factorization
techniques which 1) takes the complementary global and local context information into consideration simultaneously, and 2) models
topics and learns word embeddings collaboratively. We empirically show that by incorporating both global and local context, this
collaborative model can not only significantly improve the performance of topic discovery over the baseline topic models, but also
learn better word embeddings than the baseline word embedding
models. We also provide qualitative analysis that explains how the
cooperation of global and local context information can result in
better topic structures and word embeddings.

",16.728156217252725,18.574117647058824,171
KDD_17_029.txt,14.119493579598625,12.221243796294413,KDD,4979,"

Is there an optimal dimensionality reduction for k-means, revealing
the prominent cluster structure hidden in the data? We propose
SuBKMEANS, which extends the classic k-means algorithm. The
goal of this algorithm is twofold: find a sufficient k-means-style
clustering partition and transform the clusters onto a common
subspace, which is optimal for the cluster structure. Our solution is
able to pursue these two goals simultaneously. The dimensionality
of this subspace is found automatically and therefore the algorithm
comes without the burden of additional parameters. At the same
time this subspace helps to mitigate the curse of dimensionality.
The SuBKMEANS optimization algorithm is intriguingly simple and
efficient. It is easy to implement and can readily be adopted to the
current situation. Furthermore, it is compatible to many existing
extensions and improvements of k-means.
",14.06817628641468,12.51509803921569,137
KDD_17_030.txt,16.03271413599696,14.657168874710521,KDD,3423,"
voxels in a typical fMRI scan can be grouped into 116 anatomical
regions[16]; what is unknown is how those regions are coactive to
form a functional network for different tasks. Our earlier work[4]
addresses this problem by adding strong supervision in the form of
the anatomical regions’ boundaries.
In this paper we remove this requirement that the regions are
given and instead require our method to discover their boundaries
and the connectivity between them without supervision. Figure 1
shows a simplified example involving a single slice of brain activity.
This problem is particularly difficult given that these studies rarely
involve more than 100 patients, rendering methods such as deep
learning inapplicable.

A common problem with spatiotemporal data is how to simplify the
data to discover an underlying network that consists of cohesive
spatial regions (nodes) and relationships between those regions
(edges). This network discovery problem naturally exists in a multitude of domains including climate data (dipoles), astronomical data
(gravitational lensing) and the focus of this paper, fMRI scans of human subjects. Whereas previous work requires strong supervision,
we propose an unsupervised matrix tri-factorization formulation
with complex constraints and spatial regularization. We show that
this formulation works well in controlled experiments with synthetic networks and is able to recover the underlying ground-truth
network. We then show that for real fMRI data our approach can
reproduce well known results in neurology regarding the default
mode network in resting-state healthy and Alzheimer affected individuals.

",16.52667757954773,16.212943089430897,247
KDD_17_031.txt,15.569168055513458,14.233960290747262,KDD,5216,"

In the study of various diseases, heterogeneity among patients
usually leads to different progression patterns and may require
different types of therapeutic intervention. Therefore, it is important to study patient subtyping, which is grouping of patients into
disease characterizing subtypes. Subtyping from complex patient
data is challenging because of the information heterogeneity and
temporal dynamics. Long-Short Term Memory (LSTM) has been
successfully used in many domains for processing sequential data,
and recently applied for analyzing longitudinal patient records. The
LSTM units are designed to handle data with constant elapsed times
between consecutive elements of a sequence. Given that time lapse
between successive elements in patient records can vary from days
to months, the design of traditional LSTM may lead to suboptimal
performance. In this paper, we propose a novel LSTM unit called
Time-Aware LSTM (T-LSTM) to handle irregular time intervals in
longitudinal patient records. We learn a subspace decomposition of
the cell memory which enables time decay to discount the memory
content according to the elapsed time. We propose a patient subtyping model that leverages the proposed T-LSTM in an auto-encoder
to learn a powerful single representation for sequential records
of patients, which are then used to cluster patients into clinical
subtypes. Experiments on synthetic and real world datasets show
that the proposed T-LSTM architecture captures the underlying
structures in the sequences with time irregularities.

",16.76809479433877,15.99735497835498,232
KDD_17_032.txt,14.145093312645933,12.659342701078124,KDD,6175,"

Extreme multi-label classification methods have been widely used
in Web-scale classification tasks such as Web page tagging and
product recommendation. In this paper, we present a novel graph
embedding method called “AnnexML”. At the training step, AnnexML constructs a k-nearest neighbor graph of label vectors and
attempts to reproduce the graph structure in the embedding space.
The prediction is efficiently performed by using an approximate
nearest neighbor search method that efficiently explores the learned
k-nearest neighbor graph in the embedding space. We conducted
evaluations on several large-scale real-world data sets and compared our method with recent state-of-the-art methods. Experimental results show that our AnnexML can significantly improve
prediction accuracy, especially on data sets that have larger a label space. In addition, AnnexML improves the trade-off between
prediction time and accuracy. At the same level of accuracy, the
prediction time of AnnexML was up to 58 times faster than that of
SLEEC, which is a state-of-the-art embedding-based method.

",13.227904075235838,12.619934210526317,172
KDD_17_033.txt,13.309456980147761,10.943808178725046,KDD,3949,"
We consider the edge partitioning problem that partitions the edges
of an input graph into multiple balanced components, while minimizing the total number of vertices replicated (one vertex might
appear in more than one partition). This problem is critical in minimizing communication costs and running time for several largescale distributed graph computation platforms (e.g., PowerGraph,
Spark GraphX). We first prove that this problem is NP-hard, and
then present a new partitioning heuristic with polynomial running
time. We provide a worst-case upper bound of replication factor for
our heuristic on general graphs. To our knowledge, we are the first
to provide such bound for edge partitioning algorithms on general
graphs. Applying this bound to random power-law graphs greatly
improves the previous bounds of expected replication factor. Extensive experiments demonstrated that our partitioning algorithm
consistently produces much smaller replication factors on various
benchmark data sets than the state-of-the-art. When deployed in
the production graph engine, PowerGraph, in average it reduces
replication factor, communication, and running time by 54%, 66%,
and 21%, respectively.

",16.827784334635936,15.115000000000006,178
KDD_17_034.txt,16.505217406265434,15.340095055265543,KDD,6639,"

Due to the sparseness of quality rating data, unsupervised recommender systems are used in many applications in Peer to Peer
(P2P) rental marketplaces such as Airbnb, FlipKey, and HomeAway.
We present an integer programming based recommender systems,
where both accommodation benefits and community risks of lodging places are measured and incorporated into an objective function
as utility measurements. More specifically, we first present an unsupervised fused scoring method for quantifying the accommodation
benefits and community risks of a lodging with crowd-sourced geotagged data. In order to the utility of recommendations, we formulate the unsupervised P2P rental recommendations as a constrained
integer programming problem, where the accommodation benefits
of recommendations are maximized and the community risks of
recommendations are minimized, while maintaining constraints
on personalization. Furthermore, we provide an efficient solution
for the optimization problem by developing a learning-to-integerprogramming method for combining aggregated listwise learning
to rank into branching variable selection. We apply the proposed
approach to the Airbnb data of New York City and provide lodging
recommendations to travelers. In our empirical experiments, we
demonstrate both the efficiency and effectiveness of our method
in terms of striving a trade-off between the user satisfaction, time
on market, and the number of reviews, and achieving a balance
between positive and negative sides.

",20.130776976110326,20.31883845126836,215
KDD_17_035.txt,14.997032334435382,13.726868837372368,KDD,7197,"
Random forests are among the most successful methods used in
data mining because of their extraordinary accuracy and effectiveness. However, their use is primarily limited to multidimensional
data because they sample features from the original data set. In
this paper, we propose a method for extending random forests to
work with any arbitrary set of data objects, as long as similarities
can be computed among the data objects. Furthermore, since it
is understood that similarity computation between all O(n?) pairs
of n objects might be expensive, our method computes only a very
small fraction of the O(n”) pairwise similarities between objects to
construct the forests. Our results show that the proposed similarity
forest approach is very efficient and accurate on a wide variety of
data sets. Therefore, this paper significantly extends the applicability of random forest methods to arbitrary data domains. Furthermore, the approach even outperforms traditional random forests
on multidimensional data. We show that similarity forests are robust to the noisy similarity values that are ubiquitous in real-world
applications. In many practical settings, the similarity values between objects are incompletely specified because of the difficulty
in collecting such values. Similarity forests can be used in such
cases with straightforward modifications.

",14.554592549557764,15.12704099821747,205
KDD_17_036.txt,13.281874207732955,12.105017791741162,KDD,5281,"
Latent factor models have become a prevalent method in recommender systems, to predict users’ preference on items based on the
historical user feedback. Most of the existing methods, explicitly
or implicitly, are built upon the first-order rating distance principle,
which aims to minimize the difference between the estimated and
real ratings. In this paper, we generalize such first-order rating distance principle and propose a new latent factor model (HoORAYs)
for recommender systems. The core idea of the proposed method is
to explore high-order rating distance, which aims to minimize not
only (i) the difference between the estimated and real ratings of the
same (user, item) pair (i.e., the first-order rating distance), but also
(ii) the difference between the estimated and real rating difference
of the same user across different items (ie., the second-order rating
distance). We formulate it as a regularized optimization problem,
and propose an effective and scalable algorithm to solve it. Our
analysis from the geometry and Bayesian perspectives indicate that
by exploring the high-order rating distance, it helps to reduce the
variance of the estimator, which in turns leads to better generalization performance (e.g., smaller prediction error). We evaluate
the proposed method on four real-world data sets, two with explicit user feedback and the other two with implicit user feedback.
Experimental results show that the proposed method consistently
outperforms the state-of-the-art methods in terms of the prediction
accuracy.

",16.827784334635936,17.408542531120336,244
KDD_17_037.txt,14.605789934658311,12.40873702823609,KDD,6545,"

Many modern clustering methods scale well to a large number of
data points, N, but not to a large number of clusters, K. This paper
introduces PERCH, a new non-greedy, incremental algorithm for
hierarchical clustering that scales to both massive N and K—a problem setting we term extreme clustering. Our algorithm efficiently
routes new data points to the leaves of an incrementally-built tree.
Motivated by the desire for both accuracy and speed, our approach
performs tree rotations for the sake of enhancing subtree purity
and encouraging balancedness. We prove that, under a natural separability assumption, our non-greedy algorithm will produce trees
with perfect dendrogram purity regardless of data arrival order.
Our experiments demonstrate that PERcH constructs more accurate
trees than other tree-building clustering algorithms and scales well
with both N and K, achieving a higher quality clustering than the
strongest flat clustering competitor in nearly half the time.


",17.122413403193683,15.71839869281046,154
KDD_17_038.txt,14.714546564775272,12.709850558237985,KDD,5443,"
The PART-WHOLE relationship routinely finds itself in many dis- S vos =Sum
ciplines, ranging from collaborative teams, crowdsourcing, au- wi ei asec
tonomous systems to networked systems. From the algorithmic BD 02 ;
perspective, the existing work has primarily focused on predicting g miLassow! Ractpartinieependency
the outcomes of the whole and parts, by either separate models eB” ~
or linear joint models, which assume the outcome of the parts has F oa
a linear and independent effect on the outcome of the whole. In s ~ .
this paper, we propose a joint predictive method named PAROLE B (0.05 L - ~
to simultaneously and mutually predict the part and whole out- g L

comes. The proposed method offers two distinct advantages over
the existing work. First (Model Generality), we formulate joint
PART-WHOLE outcome prediction as a generic optimization problem, which is able to encode a variety of complex relationships
between the outcome of the whole and parts, beyond the linear
independence assumption. Second (Algorithm Efficacy), we propose an effective and efficient block coordinate descent algorithm,
which is able to find the coordinate-wise optimum with a linear
complexity in both time and space. Extensive empirical evaluations
on real-world datasets demonstrate that the proposed PAROLE (1)
leads to consistent prediction performance improvement by modeling the non-linear part-whole relationship as well as part-part
interdependency, and (2) scales linearly in terms of the size of the
training dataset.
",18.377959752453624,17.39916666666667,238
KDD_17_039.txt,15.037453685543607,13.87181264711208,KDD,3780,"
Tensor completion has become an effective computational tool in
many real-world data-driven applications. Beyond traditional static
setting, with the increasing popularity of high velocity streaming
data, it requires efficient online processing without reconstructing
the whole model from scratch. Existing work on streaming tensor
completion is usually built upon the assumption that tensors only
grow in one mode. Unfortunately, the assumption does not hold in
many real-world situations in which tensors may grow in multiple
modes, i.e., multi-aspect streaming tensors. Efficiently modeling
and completing these incremental tensors without sacrificing its
effectiveness remains a challenging task due to the uncertainty of
tensor mode changes and complex data structure of multi-aspect
streaming tensors. To bridge this gap, we propose a Multi-Aspect
Streaming Tensor completion framework (MAST) based on CANDECOMP/PARAFAC (CP) decomposition to track the subspace of
general incremental tensors for completion. In addition, we investigate a special situation where time is one mode of the tensors,
and leverage its extra structure information to improve the general
framework towards higher effectiveness. Experimental results on
four datasets collected from various real-world applications demonstrate the effectiveness and efficiency of the proposed framework.
",17.410965686947208,16.920077319587623,196
KDD_17_040.txt,15.309223454834108,12.59345275276684,KDD,4908,"
Feature selection is one of the most important data mining research
topics with many applications. In practical problems, features often
have group structure to effect the outcomes. Thus, it is crucial to
automatically identify homogenous groups of features for highdimensional data analysis. Octagonal shrinkage and clustering
algorithm for regression (OSCAR) is an important sparse regression approach with automatic feature grouping and selection by
€; norm and pairwise fo. norm. However, due to over-complex
representation of the penalty (especially the pairwise fo. norm),
so far OSCAR has no solution path algorithm which is mostly useful for tuning the model. To address this challenge, in this paper,
we propose a groups-keeping solution path algorithm to solve the
OSCAR model (OscarGKPath). Given a set of homogenous groups
of features and an accuracy bound ¢, OscarGKPath can fit the solutions in an interval of regularization parameters while keeping
the feature groups. The entire solution path can be obtained by
combining multiple such intervals. We prove that all solutions in
the solution path produced by OscarGKPath can strictly satisfy the
given accuracy bound ¢. The experimental results on benchmark
datasets not only confirm the effectiveness of our OscarGKPath
algorithm, but also show the superiority of our OscarGKPath in
cross validation compared with the existing batch algorithm.
",16.52667757954773,15.098220657277,211
KDD_17_041.txt,13.896975168861026,11.471581512031339,KDD,6507,"

We study the problem of edit similarity joins, where given a set
of strings and a threshold value K, we want to output all pairs of
strings whose edit distances are at most K. Edit similarity join is a
fundamental problem in data cleaning/integration, bioinformatics,
collaborative filtering and natural language processing, and has
been identified as a primitive operator for database systems. This
problem has been studied extensively in the literature. However,
we have observed that all the existing algorithms fall short on long
strings and large distance thresholds.

In this paper we propose an algorithm named EmbedJoin which
scales very well with string length and distance threshold. Our
algorithm is built on the recent advance of metric embeddings
for edit distance, and is very different from all of the previous approaches. We demonstrate via an extensive set of experiments that
EmbedJoin significantly outperforms the previous best algorithms
on long strings and large distance thresholds.

",15.719379583869454,14.129633699633704,157
KDD_17_042.txt,15.364056114623402,14.318810153462266,KDD,4812,"

Modern recommender systems usually employ collaborative filtering with rating information to recommend items to users due to
its successful performance. However, because of the drawbacks of
collaborative-based methods such as sparsity, cold start, etc., more
attention has been drawn to hybrid methods that consider both the
rating and content information. Most of the previous works in this
area cannot learn a good representation from content for recommendation task or consider only text modality of the content, thus
their methods are very limited in current multimedia scenario. This
paper proposes a Bayesian generative model called collaborative
variational autoencoder (CVAE) that considers both rating and content for recommendation in multimedia scenario. The model learns
deep latent representations from content data in an unsupervised
manner and also learns implicit relationships between items and
users from both content and rating. Unlike previous works with
denoising criteria, the proposed CVAE learns a latent distribution
for content in latent space instead of observation space through an
inference network and can be easily extended to other multimedia
modalities other than text. Experiments show that CVAE is able
to significantly outperform the state-of-the-art recommendation
methods with more robust performance.

",18.699421769314853,18.813772893772896,196
KDD_17_043.txt,13.823503862308044,12.10869727915447,KDD,6325,"
Crowdfunding is an emerging Internet fundraising mechanism by
raising monetary contributions from the crowd for projects or ventures. In these platforms, the dynamics, i.e., daily funding amount
on campaigns and perks (backing options with rewards), are the
most concerned issue for creators, backers and platforms. However,
tracking the dynamics in crowdfunding is very challenging and still
under-explored. To that end, in this paper, we present a focused
study on this important problem. A special goal is to forecast the
funding amount for a given campaign and its perks in the future
days. Specifically, we formalize the dynamics in crowdfunding as
a hierarchical time series, i.e., campaign level and perk level. Specific to each level, we develop a special regression by modeling
the decision making process of the crowd (visitors and backing
probability) and exploring various factors that impact the decision;
on this basis, an enhanced switching regression is proposed at each
level to address the heterogeneity of funding sequences. Further,
we employ a revision matrix to combine the two-level base forecasts
for the final forecasting. We conduct extensive experiments on a
real-world crowdfunding data collected from Indiegogo.com. The
experimental results clearly demonstrate the effectiveness of our
approaches on tracking the dynamics in crowdfunding.
",15.247664890283005,14.039145631067964,210
KDD_17_044.txt,16.802320682015466,15.366401722142566,KDD,5072,"

While exploring human mobility can benefit many applications
such as smart transportation, city planning, and urban economics,
there are two key questions that need to be answered: (i) What is the
nature of the spatial diffusion of human mobility across regions with
different urban functions? (ii) How to spot and trace the trip purposes of human mobility trajectories? To answer these questions,
we study large-scale and city-wide taxi trajectories; and furtherly
organize them as arrival sequences according to the chronological
arrival time. We figure out an important property across different
regions from the arrival sequences, namely human mobility synchronization effect, which can be exploited to explain the phenomenon
that two regions have similar arrival patterns in particular time
periods if they share similar urban functions. In addition, the arrival
sequences are mixed by arrival events with distinct trip purposes,
which can be revealed by the regional environment of both the
origins and destinations. To that end, in this paper, we develop a
joint model that integrates Mixture of Hawkes Process (MHP) with
a hierarchical topic model to capture the arrival sequences with
mixed trip purposes. Essentially, the human mobility synchronization effect is encoded as a synchronization rate in the MHP; while
the regional environment is modeled by introducing latent Trip
Purpose and POI Topic to generate the Point of Interests (POIs) in
the regions. Moreover, we provide an effective inference algorithm
for parameter learning. Finally, we conduct intensive experiments
on synthetic data and real-world data, and the experimental results
have demonstrated the effectiveness of the proposed model.

",19.061188166129803,17.91512820512821,261
KDD_17_045.txt,14.345637005531149,12.741498046143139,KDD,6978,"

Subsequence clustering of multivariate time series is a useful tool
for discovering repeated patterns in temporal data. Once these patterns have been discovered, seemingly complicated datasets can be
interpreted as a temporal sequence of only a small number of states,
or clusters. For example, raw sensor data from a fitness-tracking
application can be expressed as a timeline of a select few actions
(ie, walking, sitting, running). However, discovering these patterns
is challenging because it requires simultaneous segmentation and
clustering of the time series. Furthermore, interpreting the resulting
clusters is difficult, especially when the data is high-dimensional.
Here we propose a new method of model-based clustering, which
we call Toeplitz Inverse Covariance-based Clustering (TICC). Each
cluster in the TICC method is defined by a correlation network, or
Markov random field (MRF), characterizing the interdependencies
between different observations in a typical subsequence of that
cluster. Based on this graphical representation, TICC simultaneously segments and clusters the time series data. We solve the TICC
problem through alternating minimization, using a variation of the
expectation maximization (EM) algorithm. We derive closed-form
solutions to efficiently solve the two resulting subproblems in a
scalable way, through dynamic programming and the alternating
direction method of multipliers (ADMM), respectively. We validate
our approach by comparing TICC to several state-of-the-art baselines in a series of synthetic experiments, and we then demonstrate
on an automobile sensor dataset how TICC can be used to learn
interpretable clusters in real-world scenarios.

",17.228024813938504,15.89869501466276,249
KDD_17_046.txt,15.555867156184387,13.971314332939077,KDD,5885,"
In exploratory tensor mining, a common problem is how to analyze
a set of variables across a set of subjects whose observations do
not align naturally. For example, when modeling medical features
across a set of patients, the number and duration of treatments
may vary widely in time, meaning there is no meaningful way to
align their clinical records across time points for analysis purposes.
To handle such data, the state-of-the-art tensor model is the socalled PARAFAC2, which yields interpretable and robust output
and can naturally handle sparse data. However, its main limitation
up to now has been the lack of efficient algorithms that can handle
large-scale datasets.

In this work, we fill this gap by developing a scalable method
to compute the PARAFAC2 decomposition of large and sparse
datasets, called SPARTan. Our method exploits special structure
within PARAFAC2, leading to a novel algorithmic reformulation
that is both faster (in absolute time) and more memory-efficient
than prior work. We evaluate SPARTan on both synthetic and real
datasets, showing 22x performance gains over the best previous
implementation and also handling larger problem instances for
which the baseline fails. Furthermore, we are able to apply SPARTan
to the mining of temporally-evolving phenotypes on data taken
from real and medically complex pediatric patients. The clinical
meaningfulness of the phenotypes identified in this process, as well
as their temporal evolution over time for several patients, have
been endorsed by clinical experts.

",16.99224021665606,16.160576131687247,244
KDD_17_047.txt,13.548559320197594,11.321009534598321,KDD,5190,"

In this paper, we propose a next-generation link prediction method,
Weisfeiler-Lehman Neural Machine (WLNM), which learns topological features in the form of graph patterns that promote the
formation of links. WLNM has unmatched advantages including
higher performance than state-of-the-art methods and universal
applicability over various kinds of networks. WLNM extracts an
enclosing subgraph of each target link and encodes the subgraph
as an adjacency matrix. The key novelty of the encoding comes
from a fast hashing-based Weisfeiler-Lehman (WL) algorithm that
labels the vertices according to their structural roles in the subgraph
while preserving the subgraph’s intrinsic directionality. After that,
a neural network is trained on these adjacency matrices to learn a
predictive model. Compared with traditional link prediction methods, WLNM does not assume a particular link formation mechanism
(such as common neighbors), but learns this mechanism from the
graph itself. We conduct comprehensive experiments to show that
WLM not only outperforms a great number of state-of-the-art
link prediction methods, but also consistently performs well across
networks with different characteristics.
",16.785175570968402,15.741516360734241,181
KDD_17_048.txt,14.917161585662353,13.82124323482979,KDD,4484,"

Nowadays, large-scale graph data is being generated in a variety
of real-world applications, from social networks to co-authorship
networks, from protein-protein interaction networks to road traffic
networks. Many existing works on graph mining focus on the
vertices and edges, with first-order Markov chain as the underlying
model. They fail to explore the high-order network structures,
which are of key importance in many high impact domains. For
example, in bank customer personally identifiable information (PI)
networks, the start structures often correspond to a set of synthetic
identities; in financial transaction networks, the loop structures
indicate the existence of money laundering; in signed networks, the
triangle structures play an essential role in the balance theory for
edge prediction. In this paper, we focus on mining user-specified
high-order network structures, and aim to find a structure-rich subgraph which does not break many such structures by separating
the sub-graph from the rest.

A key challenge associated with finding a structure-rich subgraph is the prohibitive computational cost. To address this problem, inspired by the family of local graph clustering algorithms for
efficiently identifying a low-conductance cut without exploring the
whole entire graph, we propose to generalize the key idea to model
high-order network structures. In particular, we start with a generic
definition of high-order conductance, and define the high-order diffusion core, which is based on a high-order random walk induced by
any user-specified high-order network structures. Then we propose
a novel High-Order Structure-Preserving Local Clustering framework named HOSPLOC. It starts with a seed node, and iteratively
explores its neighborhood until it finds a sub-graph with a small
high-order conductance. Furthermore, we analyze its performance
in terms of both effectiveness and efficiency. The experimental
results on both synthetic graphs and real graphs demonstrate the
effectiveness and efficiency of our proposed HOSPLOC algorithm.

",16.32212239822248,15.747515723270443,319
KDD_17_049.txt,16.509982772340667,15.33094935382832,KDD,3768,"

Deep autoencoders, and other deep neural networks, have demonstrated their effectiveness in discovering non-linear features across
many problem domains. However, in many real-world problems,
large outliers and pervasive noise are commonplace, and one may
not have access to clean training data as required by standard deep
denoising autoencoders. Herein, we demonstrate novel extensions
to deep autoencoders which not only maintain a deep autoencoders’
ability to discover high quality, non-linear features but can also
eliminate outliers and noise without access to any clean training
data. Our model is inspired by Robust Principal Component Analysis, and we split the input data X into two parts, X = Lp + S,
where Lp can be effectively reconstructed by a deep autoencoder
and S contains the outliers and noise in the original data X. Since
such splitting increases the robustness of standard deep autoencoders, we name our model a “Robust Deep Autoencoder (RDA)”.
Further, we present generalizations of our results to grouped sparsity norms which allow one to distinguish random anomalies from
other types of structured corruptions, such as a collection of features being corrupted across many instances or a collection of
instances having more corruptions than their fellows. Such “Group
Robust Deep Autoencoders (GRDA)” give rise to novel anomaly
detection approaches whose superior performance we demonstrate
on a selection of benchmark problems.

",20.267338824336647,20.68594594594595,221
KDD_17_050.txt,14.756829357015494,14.735266575529735,KDD,4631,"

In this paper, we consider the Collaborative Ranking (CR) problem
for recommendation systems. Given a set of pairwise preferences
between items for each user, collaborative ranking can be used to
rank un-rated items for each user, and this ranking can be naturally
used for recommendation. It is observed that collaborative ranking
algorithms usually achieve better performance since they directly
minimize the ranking loss; however, they are rarely used in practice
due to the poor scalability. All the existing CR algorithms have time
complexity at least O(|Q|r) per iteration, where r is the target rank
and |Q| is number of pairs which grows quadratically with number
of ratings per user. For example, the Netflix data contains totally 20
billion rating pairs, and at this scale all the current algorithms have
to work with significant subsampling, resulting in poor prediction
on testing data.

In this paper, we propose a new collaborative ranking algorithm
called Primal-CR that reduces the time complexity to O(|Q|+d dor),
where dj is number of users and dy is the averaged number of items
rated by a user. Note that d,dz is strictly smaller and often much
smaller than |Q].

Furthermore, by exploiting the fact that most data is in the
form of numerical ratings instead of pairwise comparisons, we propose Primal-CR++ with O(d,d2(r + log d2)) time complexity. Both
algorithms have better theoretical time complexity than existing
approaches and also outperform existing approaches in terms of
NDCG and pairwise error on real data sets. To the best of our
knowledge, this is the first collaborative ranking algorithm capable of working on the full Netflix dataset using all the 20 billion
rating pairs, and this leads to a model with much better recommendation compared with previous models trained on subsamples.
Finally, compared with classical matrix factorization algorithm
which also requires O(d,d2r) time, our algorithm has almost the
same efficiency while making much better recommendations since
we consider the ranking loss.
",16.800685031470465,16.94764270613108,334
KDD_17_051.txt,17.617708114542065,16.474395068594742,KDD,4024,"

Correlated topic modeling has been limited to small model and
problem sizes due to their high computational cost and poor scaling.
In this paper, we propose a new model which learns compact topic
embeddings and captures topic correlations through the closeness
between the topic vectors. Our method enables efficient inference
in the low-dimensional embedding space, reducing previous cubic
or quadratic time complexity to linear w.r.t the topic size. We
further speedup variational inference with a fast sampler to exploit
sparsity of topic occurrence. Extensive experiments show that our
approach is capable of handling model and data scales which are
several orders of magnitude larger than existing correlation results,
without sacrificing modeling quality by providing competitive or
superior performance in document classification and retrieval.

",18.669449996058646,17.49167741935484,127
KDD_17_052.txt,15.986680675215386,14.86810906289821,KDD,7795,"

Machine-learned models are often described as “black boxes”. In
many real-world applications however, models may have to sacrifice predictive power in favour of human-interpretability. When
this is the case, feature engineering becomes a crucial task, which
requires significant and time-consuming human effort. Whilst some
features are inherently static, representing properties that cannot be
influenced (e.g., the age of an individual), others capture characteristics that could be adjusted (e.g., the daily amount of carbohydrates
taken). Nonetheless, once a model is learned from the data, each
prediction it makes on new instances is irreversible - assuming every instance to be a static point located in the chosen feature space.
There are many circumstances however where it is important to
understand (i) why a model outputs a certain prediction on a given
instance, (ii) which adjustable features of that instance should be
modified, and finally (iii) how to alter such a prediction when the
mutated instance is input back to the model.

In this paper, we present a technique that exploits the internals
of a tree-based ensemble classifier to offer recommendations for
transforming true negative instances into positively predicted ones.
We demonstrate the validity of our approach using an online advertising application. First, we design a Random Forest classifier
that effectively separates between two types of ads: low (negative)
and high (positive) quality ads (instances). Then, we introduce an
algorithm that provides recommendations that aim to transform
a low quality ad (negative instance) into a high quality one (positive instance). Finally, we evaluate our approach on a subset of the
active inventory of a large ad network, Yahoo Gemini.


",17.015998829145012,15.65976430976431,273
KDD_17_053.txt,14.17069370290358,13.476140261706501,KDD,7855,"

Motifs are a powerful tool for analyzing physiological waveform
data. Standard motif methods, however, ignore important contextual information (e.g., what the patient was doing at the time the
data were collected). We hypothesize that these additional contextual data could increase the utility of motifs. Thus, we propose an
extension to motifs, contextual motifs, that incorporates context.
Recognizing that, oftentimes, context may be unobserved or unavailable, we focus on methods to jointly infer motifs and context.
Applied to both simulated and real physiological data, our proposed
approach improves upon existing motif methods in terms of the
discriminative utility of the discovered motifs. In particular, we
discovered contextual motifs in continuous glucose monitor (CGM)
data collected from patients with type 1 diabetes. Compared to
their contextless counterparts, these contextual motifs led to better
predictions of hypo- and hyperglycemic events. Our results suggest that even when inferred, context is useful in both a long- and
short-term prediction horizon when processing and interpreting
physiological waveform data.

",15.470042427545799,15.580081799591003,165
KDD_17_054.txt,15.51169973563782,13.990961035477167,KDD,6779,"

Mining from neuroimaging data is becoming increasingly popular
in the field of healthcare and bioinformatics, due to its potential to
discover clinically meaningful structure patterns that could facilitate the understanding and diagnosis of neurological and neuropsychiatric disorders. Most recent research concentrates on applying
subgraph mining techniques to discover connected subgraph patterns in the brain network. However, the underlying brain network
structure is complicated. As a shallow linear model, subgraph mining cannot capture the highly non-linear structures, resulting in
sub-optimal patterns. Therefore, how to learn representations that
can capture the highly non-linearity of brain networks and preserve
the underlying structures is a critical problem.

In this paper, we propose a Structural Deep Brain Network mining method, namely SDBN, to learn highly non-linear and structurepreserving representations of brain networks. Specifically, we first
introduce a novel graph reordering approach based on module
identification, which rearranges the order of the nodes to preserve
the modular structure of the graph. Next, we perform structural
augmentation to further enhance the spatial information of the
reordered graph. Then we propose a deep feature learning framework for combining supervised learning and unsupervised learning
in a small-scale setting, by augmenting Convolutional Neural Network (CNN) with decoding pathways for reconstruction. With the
help of the multiple layers of non-linear mapping, the proposed
SDBN approach can capture the highly non-linear structure of
brain networks. Further, it has better generalization capability for
high-dimensional brain networks and works well even for small
sample learning. Benefit from CNN’s task-oriented learning style,
the learned hierarchical representation is meaningful for the clinical

“Corresponding author. This work was done while the author was at the University of

",16.09822454788146,15.407240143369176,281
KDD_17_055.txt,14.978811755746108,13.36206874370215,KDD,6763,"

Structural identity is a concept of symmetry in which network
nodes are identified according to the network structure and their
relationship to other nodes. Structural identity has been studied
in theory and practice over the past decades, but only recently
has it been addressed with representational learning techniques.
This work presents struc2vec, a novel and flexible framework for
learning latent representations for the structural identity of nodes.
struc2vec uses a hierarchy to measure node similarity at different scales, and constructs a multilayer graph to encode structural
similarities and generate structural context for nodes. Numerical
experiments indicate that state-of-the-art techniques for learning
node representations fail in capturing stronger notions of structural
identity, while struc2vec exhibits much superior performance in
this task, as it overcomes limitations of prior approaches. As a consequence, numerical experiments indicate that struc2vec improves
performance on classification tasks that depend more on structural
identity.

",19.882160675589997,19.632,150
KDD_17_056.txt,14.877737038664907,13.102960249216306,KDD,7487,"

Many important problems can be modeled as a system of interconnected entities, where each entity is recording time-dependent
observations or measurements. In order to spot trends, detect anomalies, and interpret the temporal dynamics of such data, it is essential to understand the relationships between the different entities
and how these relationships evolve over time. In this paper, we
introduce the time-varying graphical lasso (TVGL), a method of
inferring time-varying networks from raw time series data. We cast
the problem in terms of estimating a sparse time-varying inverse
covariance matrix, which reveals a dynamic network of interdependencies between the entities. Since dynamic network inference is
a computationally expensive task, we derive a scalable messagepassing algorithm based on the Alternating Direction Method of
Multipliers (ADMM) to solve this problem in an efficient way. We
also discuss several extensions, including a streaming algorithm to
update the model and incorporate new observations in real time.
Finally, we evaluate our TVGL algorithm on both real and synthetic datasets, obtaining interpretable results and outperforming
state-of-the-art baselines in terms of both accuracy and scalability.

",19.287186520377343,17.68425465838509,185
KDD_17_057.txt,15.162321822149405,13.839763102022577,KDD,4459,"

As a powerful representation paradigm for networked and multityped data, the heterogeneous information network (HIN) is ubiquitous. Meanwhile, defining proper relevance measures has always
been a fundamental problem and of great pragmatic importance for
network mining tasks. Inspired by our probabilistic interpretation
of existing path-based relevance measures, we propose to study
HIN relevance from a probabilistic perspective. We also identify,
from real-world data, and propose to model cross-meta-path synergy, which is a characteristic important for defining path-based
HIN relevance and has not been modeled by existing methods. A
generative model is established to derive a novel path-based relevance measure, which is data-driven and tailored for each HIN. We
develop an inference algorithm to find the maximum a posteriori
(MAP) estimate of the model parameters, which entails non-trivial
tricks. Experiments on two real-world datasets demonstrate the
effectiveness of the proposed model and relevance measure.
",17.28802050969988,15.702255639097746,153
KDD_17_058.txt,15.442805708339993,13.853022635954968,KDD,5442,"
Almost all real-world social networks are dynamic and evolving
with time, where new links may form and old links may drop,
largely determined by the homophily of social actors (i.e., nodes
in the network). Meanwhile, (latent) properties of social actors,
such as their opinions, are changing along the time, partially due
to social influence received from the network, which will in turn
affect the network structure. Social network evolution and node
property migration are usually treated as two orthogonal problems, and have been studied separately. In this paper, we propose
a co-evolution model that closes the loop by modeling the two
phenomena together, which contains two major components: (1)
a network generative model when the node property is known;
and (2) a property migration model when the social network structure is known. Simulation shows that our model has several nice
properties: (1) it can model a broad range of phenomena such as
opinion convergence (i.e., herding) and community-based opinion
divergence; and (2) it allows to control the evolution via a set of factors such as social influence scope, opinion leader, and noise level.
Finally, the usefulness of our model is demonstrated by an application of co-sponsorship prediction for legislative bills in Congress,
which outperforms several state-of-the-art baselines.

",18.599290044081553,18.72186915887851,217
KDD_17_059.txt,12.07595433754867,10.160344310195057,KDD,3866,"

Consider a random preferential attachment model G(p) for network
evolution that allows both node and edge arrivals. Starting with
an arbitrary nonempty graph Go, at each time step, there are two
possible events: with probability p > 0 a new node arrives and a
new edge is added between the new node and an existing node, and
with probability 1 — p a new edge is added between two existing
nodes. In both cases, the involved existing nodes are chosen at
random according to preferential attachment, ie., with probability
proportional to their degree. G(p) is known to generate power law
networks, i.e., the fraction of nodes with degree k is proportional to
k-, Here B = (4 - p)/(2 —p) is in the range (2,3].

Denoting the number of nodes of degree k at time t by my, we
significantly improve some long-standing results. In particular, we
show that m, , is concentrated around its mean with a deviation
of O( Vt), which is independent of k. We also tightly bound the
expectation E [mg, r| with an additive error of O(1/k), which is
independent of t. These new bounds allow us to tightly estimate
my, for a considerably larger k values than before. This, in turn,
enables us to estimate other important quantities, e.g., the size of
the k-rich club, namely, the set of all nodes with a degree at least k.

Finally, we introduce a new generalized model, G(p;,71,q2),
which extends G(p) by allowing also time-varying probabilities
for node and edge arrivals, as well as the formation of new components. We show that the extended model can produce power law
networks with any exponent f in the range (1,00). Furthermore,
the concentration bounds established for m,,; in G(p) also apply
in G(pr,rt, qt).
",14.672994598444667,12.847922077922082,310
KDD_17_060.txt,14.700484027753852,12.77116733175555,KDD,6301,"

Developing transparent predictive analytics has attracted significant research attention recently. There have been multiple theories
on how to model learning transparency but none of them aims to
understand the internal and often complicated modeling processes.
In this paper we adopt a contemporary philosophical concept called
“constructivism”, which is a theory regarding how human learns. We
hypothesize that a critical aspect of transparent machine learning
is to “reveal” model construction with two key process: (1) the assimilation process where we enhance our existing learning models
and (2) the accommodation process where we create new learning
models. With this intuition we propose a new learning paradigm,
constructivism learning, using a Bayesian nonparametric model to
dynamically handle the creation of new learning tasks. Our empirical study on both synthetic and real data sets demonstrate that
the new learning algorithm is capable of delivering higher quality
models (as compared to base lines and state-of-the-art) and at the
same time increasing the transparency of the learning process.
",18.062587368997235,17.519242424242425,166
KDD_17_061.txt,13.191161270319945,10.840094832854266,KDD,6969,"

One of the most common statistics computed over data elements
is the number of distinct keys. A thread of research pioneered by
Flajolet and Martin three decades ago culminated in the design
of optimal approximate counting sketches, which have size that
is double logarithmic in the number of distinct keys and provide
estimates with a small relative error. Moreover, the sketches are
composable, and thus suitable for streamed, parallel, or distributed
computation.

We consider here all statistics of the frequency distribution of
keys, where a contribution of a key to the aggregate is concave
and grows (sub)linearly with its frequency. These fundamental
aggregations are very common in text, graphs, and logs analysis and
include logarithms, low frequency moments, and capping statistics.

We design composable sketches of double-logarithmic size for
all concave sublinear statistics. Our design combines theoretical
optimality and practical simplicity. In a nutshell, we specify tailored
mapping functions of data elements to output elements so that our
target statistics on the data elements is approximated by the (max-)
distinct statistics of the output elements, which can be approximated using off-the-shelf sketches. Our key insight is relating these
target statistics to the complement Laplace transform of the input
frequencies.

",16.99224021665606,15.411592775041054,204
KDD_17_062.txt,12.682254492103642,10.147729844684708,KDD,5019,"

k-plexes are a formal yet flexible way of defining communities
in networks. They generalize the notion of cliques and are more
appropriate in most real cases: while a node of a clique C is connected to all other nodes of C, a node of a k-plex may miss up to
k connections. Unfortunately, computing all maximal k-plexes is
a gruesome task and state-of-the-art algorithms can only process
small-size networks. In this paper we propose a new approach
for enumerating large k-plexes in networks that speeds up the
search by several orders of magnitude, leveraging on (i) methods
for strongly reducing the search space and (ii) efficient techniques
for the computation of maximal cliques. Several experiments show
that our strategy is effective and is able to increase the size of the
networks for which the computation of large k-plexes is feasible
from a few hundred to several hundred thousand nodes.

",16.15616582465906,15.330202531645572,159
KDD_17_063.txt,13.000279716730923,10.08817373346427,KDD,9284,"

Existing research on finding social groups mostly focuses on dense
subgraphs in social networks. However, finding socially tenuous
groups also has many important applications. In this paper, we introduce the notion of k-triangles to measure the tenuity of a group.
We then formulate a new research problem, Minimum k-Triangle
Disconnected Group (MkTG), to find a socially tenuous group from
online social networks. We prove that MkTG is NP-Hard and inapproximable within any ratio in arbitrary graphs but polynomialtime tractable in threshold graphs. Two algorithms, namely TERA
and TERA-ADV, are designed to exploit graph-theoretical approaches
for solving MkTG on general graphs effectively and efficiently. Experimental results on seven real datasets manifest that the proposed algorithms outperform existing approaches in both efficiency
and solution quality.
",16.26309291913925,14.588303571428572,129
KDD_17_064.txt,14.844210194744221,13.983462485790074,KDD,5661,"

Extreme Classification comprises multi-class or multi-label prediction where there is a large number of classes, and is increasingly
relevant to many real-world applications such as text and image
tagging. In this setting, standard classification methods, with complexity linear in the number of classes, become intractable, while
enforcing structural constraints among classes (such as low-rank
or tree-structure) to reduce complexity often sacrifices accuracy for
efficiency. The recent PD-Sparse method addresses this via an algorithm that is sub-linear in the number of variables, by exploiting
primal-dual sparsity inherent in a specific loss function, namely
the max-margin loss. In this work, we extend PD-Sparse to be
efficiently parallelized in large-scale distributed settings. By introducing separable loss functions, we can scale out the training, with
network communication and space efficiency comparable to those
in one-versus-all approaches while maintaining an overall complexity sub-linear in the number of classes. On several large-scale
benchmarks our proposed method achieves accuracy competitive
to the state-of-the-art while reducing the training time from days to
tens of minutes compared with existing parallel or sparse methods
on a cluster of 100 cores.

",18.422482065455632,18.615353535353538,199
MASCOTS_17_001.txt,14.91760623682978,12.839493629230727,MASCOTS,8309,"
Graphics processing units (GPUs) are increasingly
applied to accelerate tasks such as graph problems and discreteevent simulation that are characterized by irregularity, i.e., a
strong dependence of the control flow and memory accesses on the
input. The core data structure in many of these irregular tasks are
priority queues that guide the progress of the computations and
which can easily become the bottleneck of an application. To our
knowledge, currently no systematic comparison of priority queue
implementations on GPUs exists in the literature. We close this
gap by a performance evaluation of GPU-based priority queue
implementations for two applications: discrete-event simulation
and parallel A* path searches on grids. We focus on scenarios
requiring large numbers of priority queues holding up to a few
thousand items each. We present performance measurements
covering linear queue designs, implicit binary heaps, splay
trees, and a GPU-specific proposal from the literature. The
measurement results show that up to about 500 items per
queue, circular buffers frequently outperform tree-based queues
for the considered applications, particularly under a simple
parallelization of individual item enqueue operations. We analyze
profiling metrics to explore classical queue designs in light of the
importance of high hardware utilization as well as homogeneous
computations and memory accesses across GPU threads.

",18.7741,17.737008293838866,213
MASCOTS_17_002.txt,17.080252073851305,16.4765315642822,MASCOTS,6843,"
Demand is mounting in the industry for scalable GPU-based deep learning systems. Unfortunately, existing training applications built atop popular deep learning frameworks, including Caffe, Theano, and Torch, etc,
are incapable of conducting distributed GPU training over
large-scale clusters.

To remedy such a situation, this paper presents Nexus,
a platform that allows existing deep learning frameworks
to easily scale out to multiple machines without sacrificing model accuracy. Nexus leverages recently proposed
distributed parameter management architecture to orchestrate distributed training by a large number of learners
spread across the cluster. Through characterizing the runtime behavior of existing single-node based applications,
Nexus is equipped with a suite of optimization schemes,
including hierarchical and hybrid parameter aggregation,
enhanced network and computation layer, and qualityguided communication adjustment, etc, to strengthen the
communication channels and resource utilization. Empirical evaluations with a diverse set of deep learning
applications demonstrate that Nexus is easy to integrate
and can deliver efficient distributed training services to
major deep learning frameworks. In addition, Nexus’s
optimization schemes are highly effective to shorten the
training time with targeted accuracy bounds.

",19.430816780756558,19.743015873015874,182
MASCOTS_17_003.txt,15.465438457071254,13.29603216736928,MASCOTS,7749,"
 GPUs have become part of the mainstream high
performance computing facilities that increasingly require more
computational power to simulate physical phenomena quickly and
accurately. However, GPU nodes also consume significantly more
power than traditional CPU nodes, and high power consumption
introduces new system operation challenges, including increased
temperature, power/cooling cost, and lower system reliability. This
paper explores how power consumption and temperature characteristics affect reliability, provides insights into what are the
implications of such understanding, and how to exploit these
insights toward predicting GPU errors using neural networks.

",20.581828153500815,21.701348314606744,90
MASCOTS_17_004.txt,15.169600223335436,13.641029327245906,MASCOTS,8695,"
Personal Cloud Storage (PCS) is a very popular
Internet service. It allows users to backup data to the cloud
as well as to perform collaborative work while sharing content.
Notably, content sharing is a key feature for PCS users. It
however comes with extra costs for service providers, as shared
files must be synchronized to multiple user devices, generating
more downloads from cloud servers. Despite the increasing
interest in this type of service, a thorough investigation on the
costs and benefits of PCS for service providers and end users
has not been conducted yet. To that end, we propose a model to
analyze cost-benefit tradeoffs for both parties. We develop utility
functions that capture, in an abstract level, the satisfaction of the
service provider and users in various scenarios. Then, we apply
our model to evaluate alternative policies for content sharing
in PCS. We consider two alternative policies for the current
PCS sharing architecture, which count on user collaboration to
reduce providers’ costs. Our results show that such policies are
advantageous for providers and users, leading to 39% utility
improvements for both parties, while requiring low commitment
of resources from participating users.!

",14.975302809339372,13.45824352331606,194
MASCOTS_17_005.txt,14.758528199797155,14.089484924919265,MASCOTS,5664,"
Cloud-based services are increasingly popular for
big data analytics due to the flexibility, scalability, and costeffectiveness of provisioning elastic resources on-demand. However, data analytics-as-a-service suffers from the overheads of
data movement between compute and storage clusters, due to
their decoupled architecture in existing cloud infrastructure. In
this work, we propose a novel approach of in-situ big data
processing on cloud storage by dynamically offloading dataintensive jobs from compute cluster to storage cluster, and
improve job throughput. However, it is challenging to achieve this
goal since introducing additional workload on the storage cluster
can significantly impact interactive web requests that fetch cloud
storage data, with strict SLA (service-level agreement) for tail
latency. In this work, we present MPLEX, a system that augments
data analytics-as-a-service by efficiently multiplexing compute
and storage cluster to improve job throughput without violating
the SLA of cloud storage service in terms of tail response time.
It applies an SLA-aware opportunistic job scheduling technique
supported by a machine learning based prediction model to
exploit the dynamic workload conditions in the compute, and
storage cluster. Performance evaluations on an OpenStack Swift
cluster, and an OpenStack based virtual cluster of Hadoop VMs
built atop NSFCloud’s Chameleon testbed show that MPLEX
improves the Hadoop job throughput by up to 1.7X, while
maintaining the SLA for cloud storage service requests.

",17.77360955136429,18.810492825951346,232
MASCOTS_17_006.txt,17.869241084063006,16.620295887479205,MASCOTS,7444,"
Memory price will continue dropping in the next
few years according to Gartner. Such trend renders it affordable for in-memory key-value stores (IMKVs) to maintain
redundant memory-resident copies of each key-value pair to
provision enhanced reliability and high availability services.
Though contemporary IMKVs have reached unprecedented
performance, delivering single-digit microsecond-scale latency
with up to tens of millions queries per second throughput,
existing replication protocols are unable to keep pace with such
an advancement of IMKVs, either incurring unbearable latency
overhead or demanding intensive resource usage. Consequently,
the adoption of those replication techniques always results in
substantial performance degradation.

In this paper, we propose MacR, a RDMA-based highperformance and lightweight replication protocol for IMKVs.
The design of MacR centers around sharing the remote backup
memory to enable RDMA-based replication protocol, and
synthesizes a collection of optimizations, including memory
allocator cooperative replication and adaptive bulk data synchronization to control the number of network operations and to
enhance the recovery performance. Performance evaluations
with a variety of YCSB workloads demonstrate that MacR can
efficiently outperform alternative replication methods in terms
of the throughput while preserving sufficiently low latency
overhead. It can also efficiently speed up the recovery process.

",19.906493383657665,18.72004310344828,204
MASCOTS_17_007.txt,15.59513899830295,13.859006324942012,MASCOTS,7234,"
Most load balancing techniques implemented in
current data centers tend to rely on a mapping from packets
to server IP addresses through a hash value calculated from
the flow five-tuple. The hash calculation allows extremely fast
packet forwarding and provides flow ‘stickiness’, meaning that
all packets belonging to the same flow get dispatched to the
same server. Unfortunately, such static hashing may not yield
an optimal degree of load balancing, e.g. due to variations in
server processing speeds or traffic patterns. On the other hand,
dynamic schemes, such as the Join-the-Shortest-Queue (JSQ)
scheme, provide a natural way to mitigate load imbalances, but
at the expense of stickiness violation.

In the present paper we examine the fundamental trade-off
between stickiness violation and packet-level latency performance
in large-scale data centers. We establish that stringent flow
stickiness carries a significant performance penalty in terms of
packet-level delay. Moreover, relaxing the stickiness requirement
by a minuscule amount is highly effective in clipping the tail
of the latency distribution. We further propose a bin-based load
balancing scheme that achieves a good balance among scalability,
stickiness violation and packet-level delay performance. Extensive
simulation experiments corroborate the analytical results and
validate the effectiveness of the bin-based load balancing scheme.

",16.860833078287435,15.2456338028169,215
MASCOTS_17_008.txt,14.239066633764114,12.633882028161654,MASCOTS,6811,"
HPC (high-performance computing) applications
usually show bursty I/O behaviors. In order to expedite the
applications, permanent storage systems are usually provisioned
to serve such I/O bursts. Approaching the era of exascale
computing, non-volatile RAM is introduced as burst buffers,
to absorb the bursty bulk data and relax the I/O provisioning
requirement of the permanent storage systems. However, without
judiciously draining the burst buffers, I/O bursts are passed
down to the underlying storage systems, which causes severe
V/O contention issues.

In order to minimize the I/O provisioning requirement and
resolve the issues caused by I/O bursts, we propose a proactive
draining scheme to manage the draining process of distributed
node-local burst buffers. In addition, we develop an I/O provisioning model to predict the minimized I/O provisioning requirement
for permanent storage systems. Evaluation results show that
applying the proactive draining scheme largely relaxes the I/O
provisioning requirement while preserving the I/O performance
of underlying storage systems.

",17.28802050969988,15.543809523809525,169
MASCOTS_17_009.txt,14.438453200153635,13.079313872050555,MASCOTS,7073,"
Cryptocurrency mining can be said to be the modern
alchemy, involving as it does the transmutation of electricity
into digital gold. The goal of mining is to guess the solution
to a cryptographic puzzle, the difficulty of which is determined
by the network, and thence to win the block reward and
transaction fees. Because the return on solo mining has a very
high variance, miners band together to create so-called mining
pools. These aggregate the power of several individual miners,
and, by distributing the accumulated rewards according to some
scheme, ensure a more predictable return for participants.

In this paper we formulate a model of the dynamics of a queuebased reward distribution scheme in a popular Ethereum mining
pool and develop a corresponding simulation. We show that the
underlying mechanism disadvantages miners with above-average
hash rates. We then consider two-miner scenarios and show how
large miners may perform attacks to increase their profits at the
expense of other participants of the mining pool. The outcomes of
our analysis show the queue-based reward scheme is vulnerable
to manipulation in its current implementation.

",16.061879428645646,14.79631756756757,186
MASCOTS_17_010.txt,13.19133616731127,10.924476003605474,MASCOTS,8179,"
The programmability of Software-Defined Networking (SDN) challenges the correctness and reliability of networks.
There may be design flaws as well as implementation bugs in SDN
applications. White-box testing methods with formal models rely
on source codes, which limits the applicability of these methods.
Black-box methods without behavior models cannot systematically cover an application’s functions. Most previous work has
mainly focused on design flaws and has ignored implementation
bugs. In this paper, we propose a new black-box test framework
to detect both design flaws and implementation bugs. Following
this test framework, we propose a new model, Information
Table Extended State Machine (IT-EFSM), combining a group of
parallel state machines and an abstract topology to specify the
SDN applications. We employ a model checking tool to generate
tests against design flaws and propose a test generation based
on partial composition, symmetry simplification on the topology
and topology simulated execution to expose implementation bugs.
The experimental results of the testing process demonstrate the
effectiveness and applicability of our method.

",15.021129683784007,14.798947368421054,173
MASCOTS_17_011.txt,14.088788847159666,13.246834617664497,MASCOTS,6336,"
In this paper we study how to estimate the
back-off rates in an idealized CSMA network consisting
of 7 links to achieve a given throughput vector using free
energy approximations. More specifically, we introduce
the class of region-based free energy approximations with
clique belief and present a closed form expression for the
back-off rates based on the zero gradient points of the
free energy approximation (in terms of the conflict graph,
target throughput vector and counting numbers).

Next we introduce the size kmaz clique free energy
approximation as a special case and derive an explicit
expression for the counting numbers, as well as a
recursion to compute the back-off rates. We subsequently
show that the size k,,., clique approximation coincides
with a Kikuchi free energy approximation and prove that
it is exact on chordal conflict graphs when kmaz = n.
As a by-product these results provide us with an explicit
expression of a fixed point of the inverse generalized
belief propagation algorithm for CSMA networks.

",17.122413403193683,17.10529411764706,170
MASCOTS_17_012.txt,14.535171334514942,12.685853899414827,MASCOTS,6770,"
Software systems with quality of service
(QoS), such as database management systems and web
servers, are ubiquitous. Such systems must meet strict
performance requirements. Instrumentation is a useful
technique for the analysis and debugging of QoS systems. Dynamic binary instrumentation (DBI) extracts
runtime information to comprehend system’s behavior
and detect performance bottlenecks. However, existing
DBI tools are intrusive; adding unacceptable delay to
the program execution. Such delay alters the performance requirements and degrades the overall quality
and the user experience of the system. Moreover, the
delay may change the system behavior, thus, producing
misleading run-time information.

This paper presents QDIME, a QoS-aware dynamic
binary instrumentation technique that respects system’s performance requirements. QDIME takes a userdefined QoS threshold as an input and periodically
gathers QoS feedback from the system under analysis
to decide its instrumentation budget.

We implemented QDIME on top of PIN, a popular
DBI framework. We evaluated QDIME with Gzip,
MySQE server, Apache HTTP server, and Redis. The
experiments show that QDIME respects the userdefined QoS threshold and, thus, improves the performance of the monitored application by manifolds.
QDIME is able to provide up to 100% instrumentation
coverage with an average of 92% when compared to
PIN. Moreover, QDIME reduces the slow-down factor
of the instrumented application by 1.41, 5.67, and
10.26 folds for Sys-trace, Call-trace, and Branch-profile
respectively. A release of QDIME is available for download at https://github.com/pansy-arafa/qdime.

",14.836745963215662,13.746000000000002,247
MASCOTS_17_013.txt,15.125219455890726,13.633577533577537,MASCOTS,7348,"
A datacenter’s power consumption is a major contributor to its operational expenditures (op-ex) and one-time capital expenditures (cap-ex). The recurring electricity cost is often in
large determined by datacenter peak-demand under peak-based
pricing which is employed by major electric utility providers.
There is a growing interest in reducing a datacenter’s electricity
costs by using throttling techniques and/or energy storage devices
(batteries) which are readily available at most datacenters as a
backup energy source. A datacenter’s power-demand uncertainty
makes this a challenging problem, which is largely neglected
in existing work, by assuming perfect predictability of power
demand. We model this inherent uncertainty as a Markov chain
and also evaluate the risk of over/under charging batteries as a
result of the randomness in power demand. We design an online
optimization framework for peak shaving which considers Conditional Value at Risk and allows for navigating cost-risk trade-offs
of datacenters based on their energy infrastructure and workload
characteristics. We show that this framework offers significantly
higher (up to 2x) cost-savings with small risks of over/under
charging batteries, compared to existing stochastic optimization
techniques. This framework leverages Markov Decision Processes
to perform online dynamic peak shaving, considering battery
degradation costs under peak-based pricing.

",18.511140095513987,17.960704976303322,215
MASCOTS_17_014.txt,15.774802946060372,13.99494949129501,MASCOTS,7518,"
Demand response refers to reducing energy consumption of participating systems in response to transient surge
in power demand or other emergency events. Demand response is
particularly important for maintaining power grid transmission
stability, as well as achieving overall energy saving. High Performance Computing (HPC) systems can be considered as ideal
participants for demand-response programs, due to their massive
energy demand. However, the potential loss of performance
must be weighed against the possible gain in power system
stability and energy reduction. In this paper, we explore the
opportunity of demand response on HPC systems by proposing a
new HPC job scheduling and resource provisioning model. More
specifically, the proposed model applies power-bound energyconservation job scheduling during the critical demand-response
events, while maintaining the traditional performance-optimized
job scheduling during the normal period. We expect such a
model can attract willing participation of the HPC systems in
the demand response programs, as it can improve both power
stability and energy saving without significantly compromising
application performance. We implement the proposed method
in a simulator and compare it with the traditional scheduling
approach. Using trace-driven simulation, we demonstrate that
the HPC demand response is a viable approach toward power
stability and energy savings with only marginal increase in the
jobs’ execution time.

",17.5058628484301,17.129201877934275,214
MASCOTS_17_015.txt,14.167689101296347,11.696312378545752,MASCOTS,8407,"
Die-stacked DRAM (a.k.a., on-chip DRAM) provides much higher bandwidth and lower latency than off-chip
DRAM. It is a promising technology to break the “memory
wall’. Die-stacked DRAM can be used either as a cache (i.e.,
DRAM cache) or as a part of memory (PoM). A DRAM cache
design would suffer from more page faults than a PoM design
as the DRAM cache cannot contribute towards capacity of main
memory. At the same time, obtaining high performance requires
PoM systems to swap requested data to the die-stacked DRAM.
Existing PoM designs fall into two categories — line-based
and page-based. The former ensures low off-chip bandwidth
utilization but suffers from a low hit ratio of on-chip memory
due to limited temporal locality. In contrast, page-based designs
achieve a high hit ratio of on-chip memory albeit at the cost
of moving large amounts of data between on-chip and off-chip
memories, leading to increased off-chip bandwidth utilization and
significant system performance degradation.

To achieve a similar high hit ratio of on-chip memory as pagebased designs, and eliminate excessive off-chip traffic involved,
we propose SELF, a high performance and bandwidth efficient
approach. The key idea is to SElectively swap Lines in a requested
page that are likely to be accessed according to page Footprint,
instead of blindly swapping an entire page. In doing so, SELF
allows incoming requests to be serviced from the on-chip memory
as much as possible, while avoiding swapping unused lines to
reduce memory bandwidth consumption. We evaluate a memory
system which consists of 4GB on-chip DRAM and 12GB offchip DRAM. Compared to a baseline system that has the same
total capacity of 16GB off-chip DRAM, SELF improves the
performance in terms of instructions per cycle by 26.9%, and
reduces the energy consumption per memory access by 47.9% on
average. In contrast, state-of-the-art line-based and page-based
PoM designs can only improve the performance by 9.5% and
9.9%, respectively, against the same baseline system.

",14.856640023380862,12.7562292358804,352
MASCOTS_17_016.txt,14.73692575535815,12.825447110669511,MASCOTS,7845,"
Increasing data set sizes motivate for a shift of focus
from computation-centric systems to data-centric systems, where
data movement is treated as a first-class optimization metric. An
example of this emerging paradigm is in-situ computing in largescale computing systems. Observing that data movement costs
are increasing at an exponential rate even at a node level (as a
node itself is fast-becoming a large manycore system), this paper
provides a limit study of near-data computing within a manycore
chip. Specifically, it makes the following two contributions. First,
it quantifies the potential performance benefits of three incarnations of the near-data computing paradigm under the assumption
of zero on-chip network latency and an infinite number of extra
cores for offloading computations close to data they require.
Our detailed experimental evaluation indicates that the most
successful of these incarnations can boost the performance of the
original execution by as much as 75%. The second contribution
of this paper is an investigation of more realistic schemes that can
approximate the potential savings achieved by perfect near-data
computing. Our results demonstrate performance improvements
ranging between 44% and 52%, over the original execution. We
also discuss the pros and cons of each of these realistic schemes,
and point to further research directions.

",17.122413403193683,15.629127725856701,215
MASCOTS_17_017.txt,15.786204461232096,13.450255707234628,MASCOTS,7952,"
Since main memory system contributes to a large
and increasing fraction of server/datacenter energy consumption,
there have been several efforts to reduce its power and energy
consumption. DVFS schemes have been used to reduce the
memory power, but they come with a performance penalty. In
this work, we propose DEMM, an OS-based, high performance
DVFS mechanism that reduces memory power by dynamically
scaling individual memory channel frequencies/voltages. Our
strategy also involves clustering the running applications based
on their sensitivities to memory latency, and assigning memory
channels to the application clusters. We introduce a new metric
called Discrete Misses per Kilo Cycle (DMPKC) to capture the
performance sensitivities of the applications to memory frequency
modulation. DEMM allows us to save power in the memory
system with negligible impact on performance. We demonstrate
around 25% savings in the memory system energy and 10%
savings in the total system energy, with only a 4% loss in
workload performance.

",17.613555460941566,15.468553345388788,159
MASCOTS_17_018.txt,16.337644436467514,14.946128301961917,MASCOTS,7708,"
This paper investigates the problem of deriving a white box performance model of Hardware Transactional Memory (HTM) systems. The proposed model
targets TSX, a popular implementation of HTM integrated
in Intel processors starting with the Haswell family in 2013.

An inherent difficulty with building white-box models of
commercially available HTM systems is that their internals
are either vaguely documented or undisclosed by their
manufacturers. We tackle this challenge by designing a set
of experiments that allow us to shed lights on the internal
mechanisms used in TSX to manage conflicts among
transactions and to track their readsets and writesets.

We exploit the information inferred from this experimental study to build an analytical model of TSX focused on capturing the impact on performance of two
key mechanisms: the concurrency control scheme and the
management of transactional meta-data in the processor’s
caches. We validate the proposed model by means of an
extensive experimental study encompassing a broad range
of workloads executed on a real system.

",17.693802365651003,17.018636363636364,167
MASCOTS_17_019.txt,15.461801396625063,14.052530018131801,MASCOTS,7385,"
One of the key performance challenges in cloud computing is the problem of interference, or resource contention,
among colocated VMs. While prior work has empirically
analyzed interference for specific workloads under specific
settings, there is a need for a generic approach to estimate
application performance under any interference condition.

In this paper, we present an analytical model to estimate
performance as a function of various workload, system, and
interference conditions, including the intensity and length of
interference, for single- and multi-VM systems. Comparisons
with empirical results under various scenarios show that our
model can provide accurate latency estimations (less than
5% error). We employ our model to analyze systems under
interference, and derive useful results to aid practitioners.

",20.267338824336647,20.00913865546219,120
MASCOTS_17_020.txt,15.048004361411355,13.595140526220728,MASCOTS,7550,"
A significant fraction of the operational expenditures
incurred by cloud service providers relates to their networking
(Internet access) and electricity consumption. Both depend on
the peak-demand over the billing interval. In the future, cloud
services providers may in turn recoup these costs from their
long-term customers through peak-based pricing. We explore two
different methods for the cloud provider to recoup this charge:
(i) equal allocation and (ii) proportional to usage allocation.
Furthermore, we consider multiple strategic tenants whose active
demand response to cloud price settings jointly depends on job
responsiveness (modeled as queueing delay of admitted jobs)
and lost/shed workload (due to excessive delay). Under certain
conditions, we prove existence and uniqueness of Nash equilibria
for regimes (i) and (ii). Due to nonconvexity in the utility (or
cost) functions, existence statements require leveraging potentiality arguments while uniqueness statements rely on imposing
further convexity requirements. The resulting Nash equilibrium
is parametrized by the price per unit demand, which may be
strategically set by the cloud to maximize its revenue subject
to tenants reaching a Nash equilibrium. We model the resulting
interactions as a Stackelberg game between the cloud and a set of
tenants. A relatively general existence statement is provided for
the Stackelberg equilibrium under regime (i). For a special case of
regime (ii), the unique Stackelberg equilibrium is characterized.
Finally, we provide a numerical study for such a framework
using real-world peak-based prices from an electric utility and
demands given by Google workload traces.

",16.11434528070225,14.624600000000001,251
MICRO_17_001.txt,13.653078192429305,11.685435758734315,MICRO,10950,"
Placing the DRAM in the same package as a processor enables
several times higher memory bandwidth than conventional offpackage DRAM. Yet, the latency of in-package DRAM is not appreciably lower than that of off-package DRAM. A promising use of
in-package DRAM is as a large cache. Unfortunately, most previous
DRAM cache designs optimize mainly for cache hit latency and do
not consider bandwidth efficiency as a first-class design constraint.
Hence, as we show in this paper, these designs are suboptimal for
use with in-package DRAM.

We propose a new DRAM cache design, Banshee, that optimizes
for both in-package and off-package DRAM bandwidth efficiency
without degrading access latency. Banshee is based on two key
ideas. First, it eliminates the tag lookup overhead by tracking the
contents of the DRAM cache using TLBs and page table entries,
which is efficiently enabled by a new lightweight TLB coherence
protocol we introduce. Second, it reduces unnecessary DRAM cache
replacement traffic with a new bandwidth-aware frequency-based
replacement policy. Our evaluations show that Banshee significantly improves performance (15% on average) and reduces DRAM
traffic (35.8% on average) over the best-previous latency-optimized
DRAM cache design.
",14.975302809339372,12.628286432160806,201
MICRO_17_002.txt,15.682691941819328,13.930808006833,MICRO,7562,"
Big data analytic applications give rise to large-scale extract-transformload (ETL) as a fundamental step to transform new data into a native
representation. ETL workloads pose signi�cant performance challenges on conventional architectures, so we propose the design of
the unstructured data processor (UDP), a software programmable
accelerator that includes multi-way dispatch, variable-size symbol
support, �exible-source dispatch (stream bu�er and scalar registers), and memory addressing to accelerate ETL kernels both for
current and novel future encoding and compression. Speci�cally,
UDP excels at branch-intensive and symbol and pattern-oriented
workloads, and can o�oad them from CPUs.
To evaluate UDP, we use a broad set of data processing workloads
inspired by ETL, but broad enough to also apply to query execution,
stream processing, and intrusion detection/monitoring. A single
UDP accelerates these data processing tasks 20-fold (geometric
mean, largest increase from 0.4 GB/s to 40 GB/s) and performance
per watt by a geomean of 1,900-fold. UDP ASIC implementation
in 28nm CMOS shows UDP logic area of 3.82mm2 (8.69mm2 with
1MB local memory), and logic power of 0.149W (0.864W with 1MB
local memory); both much smaller than a single core.
",17.879347455551382,18.088627450980393,206
MICRO_17_003.txt,17.000553491444865,15.443289924482666,MICRO,9175,"

We demonstrate the use of an FPGA as a memory buffer in a
POWERS8?® system, creating a novel prototyping platform that
enables innovation in the memory subsystem of POWER-based
servers. Our platform, called ConTutto, is pin-compatible with
POWERS buffered memory DIMMs and plugs into a memory slot
of a standard POWERS8 processor system, running at aggregate
memory channel speeds of 35 GB/s per link. ConTutto, which
means “with everything”, is a platform to experiment with
different memory technologies, such as STT-MRAM and NAND
Flash, in an end-to-end system context. Enablement of STTMRAM and NVDIMM using ConTutto shows up to 12.5x lower
latency and 7.5x higher bandwidth compared to the respective
technologies when attached to the PCIe bus. Moreover, due to the
unique attach-point of the FPGA between the processor and
system memory, ConTutto provides a means for in-line
acceleration of certain computations on-route to memory, and
enables sensitivity analysis for memory latency while running real
applications. To the best of our knowledge, ConTutto is the first
ever FPGA platform on the memory bus of a server class
processor.

",17.122413403193683,15.220455927051677,190
MICRO_17_004.txt,15.208572643614293,13.120268774703558,MICRO,9767,"
Accurate, real-time Automatic Speech Recognition (ASR) requires
huge memory storage and computational power. The main bottleneck in state-of-the-art ASR systems is the Viterbi search on a
Weighted Finite State Transducer (WFST). The WFST is a graphbased model created by composing an Acoustic Model (AM) and
a Language Model (LM) o�ine. O�ine composition simpli�es the
implementation of a speech recognizer as only one WFST has to be
searched. However, the size of the composed WFST is huge, typically larger than a Gigabyte, resulting in a large memory footprint
and memory bandwidth requirements.
In this paper, we take a completely di�erent approach and propose a hardware accelerator for speech recognition that composes
the AM and LM graphs on-the-�y. In our ASR system, the fullycomposed WFST is never generated in main memory. On the contrary, only the subset required for decoding each input speech
fragment is dynamically generated from the AM and LM models.
In addition to the direct bene�ts of this on-the-�y composition,
the resulting approach is more amenable to further reduction in
storage requirements through compression techniques.
The resulting accelerator, called UNFOLD, performs the decoding
in real-time using the compressed AM and LM models, and reduces
the size of the datasets from more than one Gigabyte to less than
40 Megabytes, which can be very important in small form factor
mobile and wearable devices.
Besides, UNFOLD improves energy-e�ciency by orders of magnitude with respect to CPUs and GPUs. Compared to a state-of-the-art
Viterbi search accelerators, the proposed ASR system outperforms
by providing 31x reduction in memory footprint and 28% energy
savings on average.
",16.11434528070225,13.850250865051901,282
MICRO_17_005.txt,16.01378924563265,14.915968163661923,MICRO,11681,"

DRAM cells in close proximity can fail depending on the data
content in neighboring cells. These failures are called datadependent failures. Detecting and mitigating these failures online, while the system is running in the field, enables various optimizations that improve reliability, latency, and energy efficiency
of the system. For example, a system can improve performance
and energy efficiency by using a lower refresh rate for most cells
and mitigate the failing cells using higher refresh rates or error
correcting codes. All these system optimizations depend on accurately detecting every possible data-dependent failure that could
occur with any content in DRAM. Unfortunately, detecting all
data-dependent failures requires the knowledge of DRAM internals specific to each DRAM chip. As internal DRAM architecture
is not exposed to the system, detecting data-dependent failures at
the system-level is a major challenge.

In this paper, we decouple the detection and mitigation of
data-dependent failures from physical DRAM organization such
that it is possible to detect failures without knowledge of DRAM
internals. To this end, we propose MEMCON, a memory contentbased detection and mitigation mechanism for data-dependent
failures in DRAM. MEMCON does not detect every possible datadependent failure. Instead, it detects and mitigates failures that
occur only with the current content in memory while the programs are running in the system. Such a mechanism needs to
detect failures whenever there is a write access that changes the
content of memory. As detection of failure with a runtime testing
has a high overhead, MEMCON selectively initiates a test on a
write, only when the time between two consecutive writes to that
page (i.e. write interval) is long enough to provide significant benefit by lowering the refresh rate during that interval. MEMCON
builds upon a simple, practical mechanism that predicts the long
write intervals based on our observation that the write intervals
in real workloads follow a Pareto distribution: the longer a page
remains idle after a write, the longer it is expected to remain idle.

Our evaluation shows that compared to a system that uses an
aggressive refresh rate, MEMCON reduces refresh operations by
65-74%, leading to a 10%/17%/40% (min) to 12%/22%/50% (max)
performance improvement for a single-core and 10%/23%/52%
(min) to 17%/29%/65% (max) performance improvement for a
4-core system using 8/16/32 Gb DRAM chips.

",16.647925096878797,15.624181818181818,398
MICRO_17_006.txt,14.99525418056141,13.258711684408471,MICRO,8368,"

Computational imaging pipelines (CIPs) convert the raw output
of imaging sensors into the high-quality images that are used for
further processing. This work studies how Block-Matching and
3D filtering (BM3D), a state-of-the-art denoising algorithm can be
implemented to meet the demands of user-interactive (UI) applications. Denoising is the most computationally demanding stage
of a CIP taking more than 95% of time on a highly-optimized software implementation [29]. We analyze the performance and energy
consumption of optimized software implementations on three commodity platforms and find that their performance is inadequate.

Accordingly, we consider two alternatives: a dedicated accelerator, and running recently proposed Neural Network (NN) based
approximations of BM3D [9, 27] on an NN accelerator. We develop Image DEnoising AcceLerator(IDEAL), a hardware BM3D accelerator which incorporates the following techniques: 1) a novel
software-hardware optimization, Matches Reuse (MR), that exploits
typical image content to reduce the computations needed by BM3D,
2) prefetching and judicious use of on-chip buffering to minimize
execution stalls and off-chip bandwidth consumption, 3) a careful
arrangement of specialized computing blocks, and 4) data type precision tuning. Over a dataset of images with resolutions ranging
from 8 megapixel (MP) and up to 42MP, IDEAL is 11, 352x and 591x
faster than high-end general-purpose (CPU) and graphics processor
(GPU) software implementations with orders of magnitude better
energy efficiency. Even when the NN approximations of BM3D are
run on the DaDianNao [14] high-end hardware NN accelerator,
IDEAL is 5.4x faster and 3.95x more energy efficient.

",19.906493383657665,19.489825581395355,261
MICRO_17_007.txt,15.485166137175945,13.597667475275241,MICRO,10808,"

Future GPUs and other high-performance throughput processors will
require multiple TB/s of bandwidth to DRAM. Satisfying this bandwidth demand within an acceptable energy budget is a challenge
in these extreme bandwidth memory systems. We propose a new
high-bandwidth DRAM architecture, Fine-Grained DRAM (FGDRAM),
which improves bandwidth by 4x and improves the energy efficiency
of DRAM by 2x relative to the highest-bandwidth, most energyefficient contemporary DRAM, High Bandwidth Memory (HBM2).
These benefits are in large measure achieved by partitioning the DRAM
die into many independent units, called grains, each of which has a
local, adjacent I/O. This approach unlocks the bandwidth of all the
banks in the DRAM to be used simultaneously, eliminating shared
buses interconnecting various banks. Furthermore, the on-DRAM data
movement energy is significantly reduced due to the much shorter
wiring distance between the cell array and the local I/O. This FGDRAM
architecture readily lends itself to leveraging existing techniques to
reducing the effective DRAM row size in an area efficient manner,
reducing wasteful row activate energy in applications with low locality. In addition, when FGDRAM is paired with a memory controller
optimized to exploit the additional concurrency provided by the independent grains, it improves GPU system performance by 19% over
an iso-bandwidth and iso-capacity future HBM baseline. Thus, this
energy-efficient, high-bandwidth FGDRAM architecture addresses the
needs of future extreme-bandwidth memory systems.

",18.99602597827317,19.05857142857143,237
MICRO_17_008.txt,16.334880376639507,14.96133958689525,MICRO,7596,"

Programmable spatial architectures composed of ensembles
of autonomous fixed-ISA processing elements offer a compelling design point between the flexibility of an FPGA and
the compute density of a GPU or shared-memory many-core.
The design regularity of spatial architectures demands examination of the processing element microarchitecture early in
the design process to optimize overall efficiency.

This paper considers the microarchitectural issues surrounding pipelining a spatial processing element with triggeredinstruction control. We propose two new techniques to mitigate pipeline hazards particular to spatial accelerators and
non-program-counter architectures, evaluating them using invivo performance counters from an FPGA prototype coupled
with a rigorous VLSI power and timing estimation methodology. We consider the effect of modern, post-Dennard-scaling
CMOS technology on the energy-delay tradeoffs and identify
a set of microarchitectures optimal for both high-performance
and low-power application settings. Our analysis reveals the
effectiveness of our hazard mitigation techniques as well as the
range of microarchitectures designers might consider when
selecting a processing element for triggered spatial accelerators.

",20.425298281703412,21.126169590643276,172
MICRO_17_009.txt,15.14339684840866,13.314725857902022,MICRO,10293,"

Operating systems have long relied on the exception handling
mechanism to implement numerous virtual memory features
and optimizations. However, today’s GPUs have a limited
support for exceptions, which prevents implementation of
such techniques. The existing solution forwards GPU memory
faults to the CPU while the faulting instruction is stalled
in the GPU pipeline. This approach prevents preemption of
the faulting threads, and results in underutilized hardware
resources while the page fault is being resolved by the CPU.

In this paper, we present three schemes for supporting
GPU exceptions that allow the system software to preempt
and restart the execution of the faulting code. There is a
trade-off between the performance overhead introduced by
adding exception support and the additional complexity. Our
solutions range from 90% of the baseline performance with
no area overheads, to 99.2% of the baseline performance with
less than 1% area and 2% power overheads. Experimental
results also show 10% performance improvement on some
benchmarks when using this support to context switch the
GPU during page migrations, to hide their latency. We further
observe up to 1.75x average speedup when implementing lazy
memory allocation on the GPU, also possible thanks to our
exception handling support.

",16.32212239822248,14.261474036850924,203
MICRO_17_010.txt,13.994314553358013,11.972582680663667,MICRO,7398,"

Emerging non-volatile memory (NVM) technologies, such as phasechange memory, spin-transfer torque magnetic memory, memristor,
and 3D Xpoint, are encouraging the development of new architectures that support the challenges of persistent programming. An
important remaining challenge is dealing with the high logging overheads introduced by durable transactions.

In this paper, we propose a new logging approach, Proteus for
durable transactions that achieves the favorable characteristics of
both prior software and hardware approaches. Like software, it has
no hardware constraint limiting the number of transactions or logs
available to it, and like hardware, it has very low overhead. Our
approach introduces two new instructions: log-load creates a log
entry by loading the original data, and log-flush writes the log entry
into the log. We add hardware support, primarily within the core,
to manage the execution of these instructions and critical ordering
requirements between logging operations and updates to data. We
also propose a novel optimization at the memory controller that is
enabled by a persistent write pending queue in the memory controller.
We drop log updates that have not yet written back to NVMM by the
time a transaction is considered durable.

We implemented our design on a cycle accurate simulator, MarssX86,

and compared it against state-of-the-art hardware logging, ATOM [19],
and a software only approach. Our experiments show that Proteus
improves performance by 1.44-1.47x depending on configuration,
on average, compared to a system without hardware logging and
9-11% faster than ATOM. A significant advantage of our approach
is dropping writes to the log when they are not needed. On average,
ATOM makes 3.4 more writes to memory than our design.

",16.11434528070225,14.455791366906475,282
MICRO_17_011.txt,18.06589819652245,17.307306414757722,MICRO,7857,"

GPUs achieve high throughput and power efficiency by employing many small single instruction multiple thread (SIMT) cores. To
minimize scheduling logic and performance variance they utilize
a uniform memory system and leverage strong data parallelism exposed via the programming model. With Moore’s law slowing, for
GPUs to continue scaling performance (which largely depends on
SIMT core count) they are likely to embrace multi-socket designs
where transistors are more readily available. However when moving
to such designs, maintaining the illusion of a uniform memory system is increasingly difficult. In this work we investigate multi-socket
non-uniform memory access (NUMA) GPU designs and show that
significant changes are needed to both the GPU interconnect and
cache architectures to achieve performance scalability. We show that
application phase effects can be exploited allowing GPU sockets to
dynamically optimize their individual interconnect and cache policies, minimizing the impact of NUMA effects. Our NUMA-aware
GPU outperforms a single GPU by 1.5x, 2.3, and 3.2x while
achieving 89%, 84%, and 76% of theoretical application scalability
in 2, 4, and 8 sockets designs respectively. Implementable today,
NUMA-aware multi-socket GPUs may be a promising candidate for
scaling GPU performance beyond a single socket.

",17.833180683606166,16.703209798994973,204
MICRO_17_012.txt,13.715561677109545,11.716641012917215,MICRO,8762,"

This paper explores solutions for enabling efficient supports of
position independence of pointer-based data structures on byteaddressable None-Volatile Memory (NVM). When a dynamic data
structure (e.g., a linked list) gets loaded from persistent storage into
main memory in different executions, the locations of the elements
contained in the data structure could differ in the address spaces
from one run to another. As a result, some special support must
be provided to ensure that the pointers contained in the data structures always point to the correct locations, which is called position
independence.

This paper shows the insufficiency of traditional methods in supporting position independence on NVM. It proposes a concept called
implicit self-contained representations of pointers, and develops
two such representations named off-holder and Region ID in Value
(RIV) to materialize the concept. Experiments show that the enabled
representations provide much more efficient and flexible support
of position independence for dynamic data structures, alleviating a
major issue for effective data reuses on NVM.

",18.062587368997235,17.520481927710843,168
MICRO_17_013.txt,15.86434714292076,14.095858031008827,MICRO,13006,"
Contemporary discrete GPUs support rich memory management
features such as virtual memory and demand paging. These features
simplify GPU programming by providing a virtual address space
abstraction similar to CPUs and eliminating manual memory management, but they introduce high performance overheads during
(1) address translation and (2) page faults. A GPU relies on high
degrees of thread-level parallelism (TLP) to hide memory latency.
Address translation can undermine TLP, as a single miss in the
translation lookaside buffer (TLB) invokes an expensive serialized
page table walk that often stalls multiple threads. Demand paging
can also undermine TLP, as multiple threads often stall while they
wait for an expensive data transfer over the system J/O (e.g., PCle)
bus when the GPU demands a page.

In modern GPUs, we face a trade-off on how the page size used
for memory management affects address translation and demand
paging. The address translation overhead is lower when we employ
a larger page size (e.g., 2MB large pages, compared with conventional 4KB base pages), which increases TLB coverage and thus
reduces TLB misses. Conversely, the demand paging overhead is
lower when we employ a smaller page size, which decreases the
system I/O bus transfer latency. Support for multiple page sizes can
help relax the page size trade-off so that address translation and demand paging optimizations work together synergistically. However,
existing page coalescing (i.e., merging base pages into a large page)
and splintering (i.e., splitting a large page into base pages) policies
require costly base page migrations that undermine the benefits
multiple page sizes provide. In this paper, we observe that GPGPU
applications present an opportunity to support multiple page sizes
without costly data migration, as the applications perform most of
their memory allocation en masse (i.e., they allocate a large number
of base pages at once). We show that this en masse allocation allows
us to create intelligent memory allocation policies which ensure
that base pages that are contiguous in virtual memory are allocated
to contiguous physical memory pages. As a result, coalescing and
splintering operations no longer need to migrate base pages.

the TLB to use large pages, reducing address translation overhead.
During data transfer, this mechanism enables the GPU to transfer
only the base pages that are needed by the application over the
system I/O bus, keeping demand paging overhead low. Our evaluations show that Mosaic reduces address translation overheads while
efficiently achieving the benefits of demand paging, compared to
a contemporary GPU that uses only a 4KB page size. Relative to a
state-of-the-art GPU memory manager, Mosaic improves the performance of homogeneous and heterogeneous multi-application
workloads by 55.5% and 29.7% on average, respectively, coming
within 6.8% and 15.4% of the performance of an ideal TLB where
all TLB requests are hits.
",18.10804710084791,17.043232066381155,477
MICRO_17_014.txt,17.128749520069515,16.00646775728298,MICRO,8881,"

Batteryless loI! devices powered through energy harvesting
face a fundamental imbalance between the potential volume of
collected data and the amount of energy available for processing that data locally. However, many such devices perform similar operations across each new input record, which provides
opportunities for mining the potential information in buffered
historical data, at potentially lower effort, while processing
new data rather than abandoning old inputs due to limited
computational energy. We call this approach incidental computing, and highlight synergies between this approach and
approximation techniques when deployed on a non-volatile
processor platform (NVP). In addition to incidental computations, the backup and restore operations in an incidental NVP
provide approximation opportunities and optimizations that
are unique to NVPs.

We propose a variety of incidental approximation approaches suited to NVPs, with a focus on approximate backup and
restore, and approximate recomputation in the face of power
interruptions. We perform RTL level evaluation for many frequently used workloads. We show that these incidental techniques provide an average of 4.2X more forward progress than
precise NVP execution.

",17.833180683606166,17.552678571428576,177
MICRO_17_015.txt,15.231547627013029,13.373927799677084,MICRO,9999,"

The register file is one of the largest and most power-hungry structures in a Graphics Processing Unit (GPU), because massive multithreading requires all the register state for every active thread
to be available. Previous approaches to making register accesses
more efficient have optimized how registers are stored, but they
must keep all values for active threads in a large, high-bandwidth
structure. If operand storage is to be reduced further, there will not
be enough capacity for every live value to be stored at the same
time. Our insight is that computation graphs can be sliced into
regions and operand storage can be allocated to these regions as
they are encountered at run time, allowing a small operand staging
unit to replace the register file. Most operand values have a short
lifetime that is contained in one region, so their value does not need
to persist in the staging unit past the end of that region. The small
number of longer-lived operands can be stored in lower-bandwidth
global memory, but the hardware must anticipate their use to fetch
them early enough to avoid stalls. In RegLess, hardware uses compiler annotations to anticipate warps’ operand usage at run time,
allowing the register file to be replaced with an operand staging
unit 25% of the size, saving 75% of register file energy and 11% of
total GPU energy with no average performance loss.

",16.785175570968402,16.508681318681322,235
MICRO_17_016.txt,16.360868509248977,14.629423384168486,MICRO,8034,"

Modern data center solid state drives (SSDs) integrate multiple
general-purpose embedded cores to manage flash translation layer,
garbage collection, wear-leveling, and etc., to improve the performance and the reliability of SSDs. As the performance of these cores
steadily improves there are opportunities to repurpose these cores
to perform application driven computations on stored data, with
the aim of reducing the communication between the host processor
and the SSD. Reducing host-SSD bandwidth demand cuts down the
I/O time which is a bottleneck for many applications operating on
large data sets. However, the embedded core performance is still
significantly lower than the host processor, as generally wimpy
embedded cores are used within SSD for cost effective reasons. So
there is a trade-off between the computation overhead associated
with near SSD processing and the reduction in communication
overhead to the host system.

In this work, we design a set of application programming interfaces (APIs) that can be used by the host application to offload a
data intensive task to the SSD processor. We describe how these
APIs can be implemented by simple modifications to the existing
Non-Volatile Memory Express (NVMe) command interface between
the host and the SSD processor. We then quantify the computation versus communication tradeoffs for near storage computing
using applications from two important domains, namely data analytics and data integration. Using a fully functional SSD evaluation
platform we perform design space exploration of our proposed
approach by varying the bandwidth and computation capabilities
of the SSD processor. We evaluate static and dynamic approaches
for dividing the work between the host and SSD processor, and

show that our design may improve the performance by up to 20%
when compared to processing at the host processor only, and 6x
when compared to processing at the SSD processor only.

",18.878054631974784,17.834357615894046,303
MICRO_17_017.txt,14.687121335851565,12.927567473366192,MICRO,6364,"

Applying advanced signal processing and artificial intelligence algorithms is often constrained by power and energy consumption
limitations, in high performance and embedded, cyber-physical
and super-computing devices and systems. Although Graphics Processing Units (GPUs) helped to mitigate the throughput-per-Watt
performance problem in many compute-intensive applications, dealing more efficiently with the autonomy requirements of intelligent
systems demands power-oriented customized architectures that
are specially tuned for each application, preferably without manual
redesign of the entire hardware and capable of supporting legacy
code. Hence, this work proposes a new SCRATCH framework that
aims at automatically identifying the specific requirements of each
application kernel, regarding instruction set and computing unit
demands, allowing for the generation of application-specific and
FPGA-implementable trimmed-down GPU-inspired architectures.
The work is based on an improved version of the original MIAOW
system (here named MIAOW2.0), which is herein extended to support a set of 156 instructions and enhanced to provide a fast prefetch
memory system and a dual-clock domain. Experimental results with
17 highly relevant benchmarks, using integer and floating-point
arithmetic, demonstrate that we have been able to achieve an average of 140x speedup and 115x higher energy-efficiency levels
(instructions-per-Joule) when compared to the original MIAOW
system, and a 2.4x speedup and 2.1x energy-efficiency gains compared against our optimized version without pruning.

",24.041184974961247,25.27251327433628,230
MICRO_17_018.txt,15.611098655754631,13.996796765462168,MICRO,8228,"

Non-volatile memories (NVMs) have attracted significant interest
recently due to their high-density, low static power, and persistence.
There are, however, several challenges associated with building
practical systems from NVMs, including limited write endurance
and long latencies. Researchers have proposed a variety of architectural techniques which can achieve different tradeoffs between
lifetime, performance and energy efficiency; however, no individual
technique can satisfy requirements for all applications and different
objectives. Hence, we propose Memory Cocktail Therapy (MCT), a
general, learning-based framework that adaptively chooses the best
techniques for the current application and objectives.

Specifically, MCT performs four procedures to adapt the techniques to various scenarios. First, MCT formulates a high-dimensional
configuration space from all different combinations of techniques.
Second, MCT selects primary features from the configuration space
with lasso regularization. Third, MCT estimates lifetime, performance and energy consumption using lightweight online predictors
(eg. quadratic regression and gradient boosting) and a small set of
configurations guided by the selected features. Finally, given the
estimation of all configurations, MCT selects the optimal configuration based on the user-defined objectives. As a proof of concept,
we test MCT’s ability to guarantee different lifetime targets and
achieve 95% of maximum performance, while minimizing energy
consumption. We find that MCT improves performance by 9.24%
and reduces energy by 7.95% compared to the best static configuration. Moreover, the performance of MCT is 94.49% of the ideal
configuration with only 5.3% more energy consumption.

",18.062587368997235,16.17247907949791,245
MICRO_17_019.txt,15.6700784995441,14.672061774284149,MICRO,8861,"

For many years, the highest energy cost in processing has been
data movement rather than computation, and energy is the limiting
factor in processor design [21]. As the data needed for a single
application grows to exabytes [56], there is clearly an opportunity
to design a bandwidth-optimized architecture for big data computation by specializing hardware for data movement. We present the
Data Processing Unit or DPU, a shared memory many-core that
is specifically designed for high bandwidth analytics workloads.
The DPU contains a unique Data Movement System (DMS), which
provides hardware acceleration for data movement and partitioning operations at the memory controller that is sufficient to keep
up with DDR bandwidth. The DPU also provides acceleration for
core to core communication via a unique hardware RPC mechanism called the Atomic Transaction Engine. Comparison of a DPU
chip fabricated in 40nm with a Xeon processor on a variety of data
processing applications shows a 3x - 15x performance per watt
advantage.

",17.122413403193683,16.715993788819876,162
MICRO_17_020.txt,16.675702669095507,15.06850017491536,MICRO,4351,"

Hardware Malware Detectors (HMDs) have recently been proposed
as a defense against the proliferation of malware. These detectors
use low-level features, that can be collected by the hardware performance monitoring units on modern CPUs to detect malware as
a computational anomaly. Several aspects of the detector construction have been explored, leading to detectors with high accuracy. In
this paper, we explore the question of how well evasive malware
can avoid detection by HMDs. We show that existing HMDs can
be effectively reverse-engineered and subsequently evaded, allowing malware to hide from detection without substantially slowing
it down (which is important for certain types of malware). This
result demonstrates that the current generation of HMDs can be
easily defeated by evasive malware. Next, we explore how well a
detector can evolve if it is exposed to this evasive malware during
training. We show that simple detectors, such as logistic regression,
cannot detect the evasive malware even with retraining. More sophisticated detectors can be retrained to detect evasive malware, but
the retrained detectors can be reverse-engineered and evaded again.
To address these limitations, we propose a new type of Resilient
HMDs (RHMDs) that stochastically switch between different detectors. These detectors can be shown to be provably more difficult to
reverse engineer based on resent results in probably approximately
correct (PAC) learnability theory. We show that indeed such detectors are resilient to both reverse engineering and evasion, and that the
resilience increases with the number and diversity of the individual
detectors. Our results demonstrate that these HMDs offer effective
defense against evasive malware at low additional complexity.

",16.290370192383524,14.348646616541355,267
MICRO_17_021.txt,14.523867541473532,12.52356923534073,MICRO,9985,"

Finite State Automata are widely used to accelerate pattern matching
in many emerging application domains like DNA sequencing and
XML parsing. Conventional CPUs and compute-centric accelerators
are bottlenecked by memory bandwidth and irregular memory access
patterns in automata processing.

We present Cache Automaton, which repurposes last-level cache
for automata processing, and a compiler that automates the process
of mapping large real world Non-Deterministic Finite Automata
(NFAs) to the proposed architecture. Cache Automaton extends a
conventional last-level cache architecture with components to accelerate two phases in NFA processing: state-match and state-transition.
State-matching is made efficient using a sense-amplifier cycling
technique that exploits spatial locality in symbol matches. Statetransition is made efficient using a new compact switch architecture.
By overlapping these two phases for adjacent symbols we realize an
efficient pipelined design.

We evaluate two designs, one optimized for performance and the
other optimized for space, across a set of 20 diverse benchmarks.
The performance optimized design provides a speedup of 15 over
DRAM-based Micron’s Automata Processor and 3840 speedup
over processing in a conventional x86 CPU. The proposed design
utilizes on an average 1.2MB of cache space across benchmarks,
while consuming 2.3nJ of energy per input symbol. Our space optimized design can reduce the cache utilization to 0.72MB, while still
providing a speedup of 9x over AP.

",16.581925556534415,15.515272727272727,230
MICRO_17_022.txt,18.523158144621906,17.89902598062739,MICRO,11166,"

The growing movement to connect literally everything to the internet (internet of things or IoT) through ultra-low-power embedded
microprocessors poses a critical challenge for information security.
Gate-level tracking of information flows has been proposed to guarantee information flow security in computer systems. However, such
solutions rely on non-commodity, secure-by-design processors. In
this work, we observe that the need for secure-by-design processors
arises because previous works on gate-level information flow tracking assume no knowledge of the application running in a system.
Since IoT systems typically run a single application over and over
for the lifetime of the system, we see a unique opportunity to provide application-specific gate-level information flow security for IoT
systems. We develop a gate-level symbolic analysis framework that
uses knowledge of the application running in a system to efficiently
identify all possible information flow security vulnerabilities for the
system. We leverage this information to provide security guarantees
on commodity processors. We also show that security vulnerabilities
identified by our analysis framework can be eliminated through software modifications at 15% energy overhead, on average, obviating the
need for secure-by-design hardware. Our framework also allows us
to identify and eliminate only the vulnerabilities that an application
is prone to, reducing the cost of information flow security by 3.3x
compared to a software-based approach that assumes no application
knowledge.

",18.36309006607702,18.258597997138775,235
MICRO_17_023.txt,15.198174574344176,12.965397652266535,MICRO,10338,"

Many important applications trigger bulk bitwise operations, ie., bitwise operations on large bit vectors. In fact, recent works design techniques that exploit fast bulk bitwise operations to accelerate databases (bitmap indices, BitWeaving)
and web search (BitFunnel). Unfortunately, in existing architectures, the throughput of bulk bitwise operations is limited
by the memory bandwidth available to the processing unit
(e.g., CPU, GPU, FPGA, processing-in-memory).

To overcome this bottleneck, we propose Ambit, an
Accelerator-in-Memory for bulk bitwise operations. Unlike
prior works, Ambit exploits the analog operation of DRAM
technology to perform bitwise operations completely inside
DRAM, thereby exploiting the full internal DRAM bandwidth.
Ambit consists of two components. First, simultaneous activation of three DRAM rows that share the same set of sense
amplifiers enables the system to perform bitwise AND and OR
operations. Second, with modest changes to the sense amplifier, the system can use the inverters present inside the sense
amplifier to perform bitwise NOT operations. With these
two components, Ambit can perform any bulk bitwise operation efficiently inside DRAM. Ambit largely exploits existing
DRAM structure, and hence incurs low cost on top of commodity DRAM designs (1% of DRAM chip area). Importantly,
Ambit uses the modern DRAM interface without any changes,
and therefore it can be directly plugged onto the memory bus.

Our extensive circuit simulations show that Ambit works
as expected even in the presence of significant process variation, Averaged across seven bulk bitwise operations, Ambit improves performance by 32X and reduces energy consumption by 35X compared to state-of-the-art systems. When
integrated with Hybrid Memory Cube (HMC), a 3D-stacked
DRAM with a logic layer, Ambit improves performance of

",16.941846955676066,15.392173913043475,278
MICRO_17_024.txt,15.040332069427777,13.015279894631863,MICRO,7865,"

Security-critical data can leak through very unexpected side channels, making side-channel attacks very dangerous threats to information security. Of these, cache-based side-channel attacks are some
of the most problematic. This is because caches are essential for
the performance of modern computers, but an intrinsic property of
all caches — the different access times for cache hits and misses —
is the property exploited to leak information in time-based cache
side-channel attacks. Recently, different secure cache architectures
have been proposed to defend against these attacks. However, we do
not have a reliable method for evaluating a cache’s resilience against
different classes of cache side-channel attacks, which is the goal of
this paper.

We first propose a novel probabilistic information flow graph
(PIFG) to model the interaction between the victim program, the
attacker program and the cache architecture. From this model, we
derive a new metric, the Probability of Attack Success (PAS), which
gives a quantitative measure for evaluating a cache’s resilience
against a given class of cache side-channel attacks. We show the
generality of our model and metric by applying them to evaluate
nine different cache architectures against all four classes of cache
side-channel attacks. Our new methodology, model and metric can
help verify the security provided by different proposed secure cache
architectures, and compare them in terms of their resilience to cache
side-channel attacks, without the need for simulation or taping out a
chip.

",16.04434344847333,15.863608815427,245
MICRO_17_025.txt,15.045479544224257,13.132944540917016,MICRO,6281,"

Data movement between the processing units and the memory
in traditional von Neumann architecture is creating the “memory
wall” problem. To bridge the gap, two approaches, the memory-rich
processor (more on-chip memory) and the compute-capable memory (processing-in-memory) have been studied. However, the first
one has strong computing capability but limited memory capacity/bandwidth, whereas the second one is the exact the opposite.

To address the challenge, we propose DRISA, a DRAM-based
Reconfigurable In-Situ Accelerator architecture, to provide both
powerful computing capability and large memory capacity/bandwidth.
DRISA is primarily composed of DRAM memory arrays, in which
every memory bitline can perform bitwise Boolean logic operations (such as NOR). DRISA can be reconfigured to compute various functions with the combination of the functionally complete
Boolean logic operations and the proposed hierarchical internal
data movement designs. We further optimize DRISA to achieve high
performance by simultaneously activating multiple rows and subarrays to provide massive parallelism, unblocking the internal data
movement bottlenecks, and optimizing activation latency and energy. We explore four design options and present a comprehensive
case study to demonstrate significant acceleration of convolutional
neural networks. The experimental results show that DRISA can
achieve 8.8x speedup and 1.2x better energy efficiency compared
with ASICs, and 7.7x speedup and 15x better energy efficiency over
GPUs with integer operations.
",19.174585242480724,18.533603603603606,226
MICRO_17_026.txt,15.588771491825558,14.04523456931302,MICRO,9428,"

General Purpose Graphics Processing Units (GPGPUs) are present
in most modern computing platforms. They are also increasingly
integrated as a computational resource on clusters, data centers, and
cloud infrastructure, making them possible targets for attacks. We
present a first study of covert channel attacks on GPGPUs. GPGPU
attacks offer a number of attractive properties relative to CPU covert
channels. These channels also have characteristics different from
their counterparts on CPUs. To enable the attack, we first reverse
engineer the hardware block scheduler as well as the warp to warp
scheduler to characterize how co-location is established. We exploit
this information to manipulate the scheduling algorithms to create
co-residency between the trojan and the spy. We study contention on
different resources including caches, functional units and memory,
and construct operational covert channels on all these resources. We
also investigate approaches to increase the bandwidth of the channel
including: (1) using synchronization to reduce the communication
cycle and increase robustness of the channel; (2) exploiting the available parallelism on the GPU to increase the bandwidth; and (3)
exploiting the scheduling algorithms to create exclusive co-location
to prevent interference from other possible applications. We demonstrate operational versions of all channels on three different Nvidia
GPGPUs, obtaining error-free bandwidth of over 4 Mbps, making it
the fastest known microarchitectural covert channel under realistic
conditions.

",17.00531248756302,16.16653571428572,225
MICRO_17_027.txt,14.313163584235728,12.066727216692001,MICRO,5372,"

To reduce the memory requirements of virtualized environments,
modern hypervisors are equipped with the capability to search the
memory address space and merge identical pages — a process called
page deduplication. This process uses a combination of data hashing
and exhaustive comparison of pages, which consumes processor
cycles and pollutes caches.

In this paper, we present a lightweight hardware mechanism that
augments the memory controller and performs the page merging
process with minimal hypervisor involvement. Our concept, called.
PageForge, is effective. It compares pages in the memory controller,
and repurposes the Error Correction Codes (ECC) engine to generate accurate and inexpensive ECC-based hash keys. We evaluate
PageForge with simulations of a 10-core processor with a virtual
machine (VM) on each core, running a set of applications from the
TailBench suite. When compared with RedHat’s KSM, a state-ofthe-art software implementation of page merging, PageForge attains
identical savings in memory footprint while substantially reducing
the overhead. Compared to a system without same-page merging,
PageForge reduces the memory footprint by an average of 48%, enabling the deployment of twice as many VMs for the same physical
memory. Importantly, it keeps the average latency overhead to 10%,
and the 95 percentile tail latency to 11%. In contrast, in KSM,
these latency overheads are 68% and 136%, respectively.

",16.24694786949722,14.289811478843742,219
MICRO_17_028.txt,15.464241749233537,14.057029400059697,MICRO,9414,"

The growing scale and complexity of Machine Learning (ML) algorithms has resulted in prevalent use of distributed general-purpose
systems. In a rather disjoint effort, the community is focusing mostly
on high performance single-node accelerators for learning. This work
bridges these two paradigms and offers CoSMIC, a full computing
stack constituting language, compiler, system software, template
architecture, and circuit generators, that enable programmable acceleration of learning at scale. CoSMIC enables programmers to exploit
scale-out acceleration using FPGAs and Programmable ASICs (PASICs) from a high-level and mathematical Domain-Specific Language (DSL). Nonetheless, CoSMIC does not require programmers
to delve into the onerous task of system software development or
hardware design. CoSMIC achieves three conflicting objectives of
efficiency, automation, and programmability, by integrating a novel
multi-threaded template accelerator architecture and a cohesive stack
that generates the hardware and software code from its high-level
DSL. CoSMIC can accelerate a wide range of learning algorithms that
are most commonly trained using parallel variants of gradient descent.
The key is to distribute partial gradient calculations of the learning
algorithms across the accelerator-augmented nodes of the scale-out
system. Additionally, CoSMIC leverages the parallelizability of the
algorithms to offer multi-threaded acceleration within each node.
Multi-threading allows CoSMIC to efficiently exploit the numerous
resources that are becoming available on modern FPGAs/P-ASICs
by striking a balance between multi-threaded parallelism and singlethreaded performance. CoSMIC takes advantage of algorithmic properties of ML to offer a specialized system software that optimizes task
allocation, role-assignment, thread management, and internode communication. We evaluate the versatility and efficiency of CoSMIC for
10 different machine learning applications from various domains. On
average, a 16-node CoSMIC with UltraScale+ FPGAs offers 18.8 x

speedup over a 16-node Spark system with Xeon processors while the
programmer only writes 22-55 lines of code. CoSMIC offers higher
scalability compared to the state-of-the-art Spark; scaling from 4 to
16 nodes with CoSMIC yields 2.7 improvements whereas Spark
offers 1.8x. These results confirm that the full-stack approach of
CoSMIC takes an effective and vital step towards enabling scale-out
acceleration for machine learning.

",17.122413403193683,16.873008310249308,364
MICRO_17_029.txt,15.914292086771798,13.894262674650701,MICRO,11490,"

A key problem with implantable brain-machine interfaces is that
they need extreme energy efficiency. One way of lowering energy
consumption is to use the low power modes available on the processors embedded in these devices. We present a technique to predict when neuronal activity of interest is likely to occur so that the
processor can run at nominal operating frequency at those times,
and be placed in low power modes otherwise. To achieve this, we
discover that branch predictors can also predict brain activity. We
perform brain surgeries on awake and anesthetized mice, and evaluate the ability of several branch predictors to predict neuronal
activity in the cerebellum. We find that perceptron branch predictors can predict cerebellar activity with accuracies as high as 85%.
Consequently, we co-opt branch predictors to dictate when to transition between low power and normal operating modes, saving as
much as 59% of processor energy.

",17.122413403193683,14.312923368022705,152
MICRO_17_030.txt,14.957057598667756,13.056596831895856,MICRO,5413,"

Deep Neural Networks expose a high degree of parallelism, making them amenable to highly data parallel architectures. However,
data-parallel architectures often accept inefficiency in individual
computations for the sake of overall efficiency. We show that on
average, activation values of convolutional layers during inference
in modern Deep Convolutional Neural Networks (CNNs) contain
92% zero bits. Processing these zero bits entails ineffectual computations that could be skipped. We propose Pragmatic (PRA), a
massively data-parallel architecture that eliminates most of the
ineffectual computations on-the-fly, improving performance and
energy efficiency compared to state-of-the-art high-performance
accelerators [5]. The idea behind PRA is deceptively simple: use
serial-parallel shift-and-add multiplication while skipping the zero
bits of the serial input. However, a straightforward implementation based on shift-and-add multiplication yields unacceptable area,
power and memory access overheads compared to a conventional
bit-parallel design. PRA incorporates a set of design decisions to
yield a practical, area and energy efficient design.

Measurements demonstrate that for convolutional layers, PRA is
4.31x faster than DaDianNao [5] (DaDN) using a 16-bit fixed-point
representation. While PRA requires 1.68X more area than DaDN,
the performance gains yield a 1.70x increase in energy efficiency
in a 65nm technology. With 8-bit quantized activations, PRA is
2.25x faster and 1.31X more energy efficient than an 8-bit version
of DaDN.

",17.54021597470382,15.922977371973008,235
MICRO_17_031.txt,14.875646874885017,12.659062768299197,MICRO,8764,"

Current flagship processors excel at extracting instructionlevel-parallelism (ILP) by forming large instruction windows.
Even then, extracting ILP is inherently limited by true data
dependencies. Value prediction was proposed to address this
limitation. Many challenges face value prediction, in this
work we focus on two of them. Challenge #1: store instructions change the values in memory, rendering the values in the
value predictor stale, and resulting in value mispredictions
and a retraining penalty. Challenge #2: value mispredictions trigger costly pipeline flushes. To minimize the number
of pipeline flushes, value predictors employ stringent, yet
necessary, high confidence requirements to guarantee high
prediction accuracy. Such requirements can negatively impact training time and coverage.

In this work, we propose Decoupled Load Value Prediction
(DLVP), a technique that targets the value prediction challenges for load instructions. DLVP mitigates the stale state
caused by stores by replacing value prediction with memory
address prediction. Then, it opportunistically probes the data
cache to retrieve the value(s) corresponding to the predicted
address(es) early enough so value prediction can take place.
Since the values captured in the data cache mirror the current
program data (except for in-flight stores), this addresses the
first challenge. Regarding the second challenge, DLVP reduces pipeline flushes by using a new context-based address
prediction scheme that leverages load-path history to deliver
high address prediction accuracy (over 99%) with relaxed
confidence requirements. We call this address prediction
scheme Path-based Address Prediction (PAP). With a modest
8KB prediction table, DLVP improves performance by up
to 71%, and 4.8% on average, without increasing the core
energy consumption.

",15.112257680678326,12.985283018867925,267
MICRO_17_032.txt,16.2463723612218,14.942302679948074,MICRO,9013,"

Large-scale deep neural networks (DNNs) are both compute and
memory intensive. As the size of DNNs continues to grow, it is
critical to improve the energy efficiency and performance while
maintaining accuracy. For DNNs, the model size is an important factor affecting performance, scalability and energy efficiency. Weight
pruning achieves good compression ratios but suffers from three
drawbacks: 1) the irregular network structure after pruning, which
affects performance and throughput; 2) the increased training complexity; and 3) the lack of rigirous guarantee of compression ratio
and inference accuracy.

To overcome these limitations, this paper proposes CIRCNN,
a principled approach to represent weights and process neural
networks using block-circulant matrices. CIRCNN utilizes the Fast
Fourier Transform (FFT)-based fast multiplication, simultaneously
reducing the computational complexity (both in inference and
training) from O(n?) to O(n log n) and the storage complexity from
O(n?) to O(n), with negligible accuracy loss. Compared to other
approaches, C1RCNN is distinct due to its mathematical rigor: the
DNNs based on C1rCNN can converge to the same “effectiveness”
as DNNs without compression. We propose the CIRCNN architecture, a universal DNN inference engine that can be implemented in
various hardware/software platforms with configurable network
architecture (e.g., layer type, size, scales, etc.). In CIRCNN architecture: 1) Due to the recursive property, FFT can be used as the
key computing kernel, which ensures universal and small-footprint
implementations. 2) The compressed but regular network structure
avoids the pitfalls of the network pruning and facilitates high performance and throughput with highly pipelined and parallel design.
To demonstrate the performance and energy efficiency, we test C1rCNN in FPGA, ASIC and embedded processors. Our results show
that CrRCNN architecture achieves very high energy efficiency and

",16.439396014739867,14.475912698412703,290
MICRO_17_033.txt,13.512878535131932,11.451101846306624,MICRO,7972,"

The disparity between last-level cache and memory latencies
motivates the search for efficient cache management policies.
Recent work in predicting reuse of cache blocks enables optimizations that significantly improve cache performance and
efficiency. However, the accuracy of the prediction mechanisms limits the scope of optimization.

This paper introduces multiperspective reuse prediction,
a technique that predicts the future reuse of cache blocks
using several different types of features. The accuracy of the
multiperspective technique is superior to previous work. We
demonstrate the technique using a placement, promotion,
and bypass optimization that outperforms state-of-the-art
policies using a low overhead. On a set of single-thread benchmarks, the technique yields a geometric mean 9.0% speedup
over LRU, compared with 5.1% for Hawkeye and 6.3% for
Perceptron. On multi-programmed workloads, the technique
gives a geometric mean weighted speedup of 8.3% over LRU,
compared with 5.2% for Hawkeye and 5.8% for Perceptron.

",15.414825405933506,13.945,159
MICRO_17_034.txt,15.231624426746677,13.508143541637498,MICRO,6057,"
Computing in virtualized environments has become a common practice for many businesses. Typically, hosting companies aim for lower operational costs by targeting high utilization of host machines maintaining just enough machines to
meet the demand. In this scenario, frequent virtual machine
context switches are common, resulting in increased TLB
miss rates (often, by over 5X when contexts are doubled) and
subsequent expensive page walks. Since each TLB miss in a
virtual environment initiates a 2D page walk, the data caches
get filled with a large fraction of page table entries (often, in
excess of 50%) thereby evicting potentially more useful data
contents.
In this work, we propose CSALT - a Context-Switch Aware
Large TLB, to address the problem of increased TLB miss
rates and their adverse impact on data caches. First, we
demonstrate that the CSALT architecture can effectively cope
with the demands of increased context switches by its capacity to store a very large number of TLB entries. Next, we
show that CSALT mitigates data cache contention caused by
conflicts between data and translation entries by employing
a novel TLB-Aware Cache Partitioning scheme. On 8-core
systems that switch between two virtual machine contexts
executing multi-threaded workloads, CSALT achieves an average performance improvement of 85% over a baseline with
conventional L1-L2 TLBs and 25% over a baseline which
has a large L3 TLB.
",15.078166124597352,15.22955396475771,228
MICRO_17_035.txt,16.92317284397904,16.01856490541422,MICRO,8737,"

In this paper, we present the first automated system-level
analysis of multicore CPUs based on ARMv8 64-bit architecture
(8-core, 28nm X-Gene 2 micro-server by AppliedMicro) when
pushed to operate in scaled voltage conditions. We report detailed
system-level effects including SDCs, corrected/uncorrected errors
and application/system crashes. Our study reveals large voltage
margins (that can be harnessed for energy savings) and also large
Vin Variation among the 8 cores of the CPU chip, among 3
different chips (a nominal rated and two sigma chips), and among
different benchmarks.

Apart from the V,,j;, analysis we propose a new composite
metric (severity) that aggregates the behavior of cores when
undervolted and can support system operation and design
protection decisions. Our undervolting characterization findings
are the first reported analysis for an enterprise class 64-bit
ARMv8 platform and we highlight key differences with previous
studies on x86 platforms. We utilize the results of the system
characterization along with performance counters information to
measure the accuracy of prediction models for the behavior of
benchmarks running in particular cores. Finally, we discuss how
the detailed characterization and the prediction results can be
effectively used to support design and system software decisions
to harness voltage margins for energy efficiency while preserving
operation correctness. Our findings show that, on average, 19.4%
energy saving can be achieved without compromising the
performance, while with 25% performance reduction, the energy
saving raises to 38.8%.

",18.511140095513987,18.387795643153527,244
MICRO_17_036.txt,16.089681924571497,14.158340874811469,MICRO,6046,"

Paramount to the viability of a parallel architecture is the correct
implementation of its memory consistency model (MCM). Although
tools exist for verifying consistency models at several design levels,
a problematic verification gap exists between checking an abstract
microarchitectural specification of a consistency model and verifying that the actual processor RTL implements it correctly.

This paper presents RTLCheck, a methodology and tool for narrowing the microarchitecture/RTL MCM verification gap. Given
a set of microarchitectural axioms about MCM behavior, an RTL
design, and user-provided mappings to assist in connecting the
two, RTLCheck automatically generates the SystemVerilog Assertions (SVA) needed to verify that the implementation satisfies the
microarchitectural specification for a given litmus test program.
When combined with existing automated MCM verification tools,
RTLCheck enables test-based full-stack MCM verification from
high-level languages to RTL. We evaluate RTLCheck on a multicore
version of the RISC-V V-scale processor, and discover a bug in its
memory implementation. Once the bug is fixed, we verify that the
multicore V-scale implementation satisfies sequential consistency
across 56 litmus tests. The JasperGold property verifier finds complete proofs for 89% of our properties, and can find bounded proofs
for the remaining properties.

",19.032712561301913,18.232000000000003,201
MICRO_17_037.txt,15.457707649423618,13.278787161980542,MICRO,6750,"

Video streaming has become the most common application in
handhelds and this trend is expected to grow in future to account for about 75% of all mobile data traffic by 2021. Thus, optimizing the performance and energy consumption of video processing in mobile devices is critical for sustaining the handheld market growth. In this paper, we propose three complementary techniques, race-to-sleep, content caching and display caching, to minimize the energy consumption of the video processing flows. Unlike the state-of-the-art frame-by-frame processing of a video decoder, the first scheme, race-to-sleep, uses two approaches, called
batching of frames and frequency boosting to prolong its sleep state
for saving energy, while avoiding any frame drops. The second
scheme, content caching, exploits the content similarity of smaller
video blocks, called macroblocks, to design a novel cache organization for reducing the memory pressure. The third scheme, in turn,
takes advantage of content similarity at the display controller to
facilitate display caching further improving energy efficiency. We
integrate these three schemes for developing an end-to-end video
processing framework and evaluate our design on a comprehensive mobile system design platform with a variety of video processing workloads. Our evaluations show that the proposed three
techniques complement each other in improving performance by
avoiding frame drops and reducing the energy consumption of
video streaming applications by 21%, on average, compared to the
current baseline design.

",18.511140095513987,17.331827731092435,239
MICRO_17_038.txt,16.355370350756647,14.904182126957064,MICRO,6862,"

Recent work in formal verification theory and verification-aware
design has sought to bridge the divide between the class of protocols architects want to design and the class of protocols that are
verifiable with state of the art tools. Particularly, the recent Neo
work in formal verification theory, for the first time, formalizes how
to compose flat subprotocols with an arbitrary number of nodes
into a hierarchy while maintaining correct behavior. However, it is
unclear if this theory scales to realistic systems. Moreover, there is
a diversity of systems architects would be interested in, to which it
is not clear if the theory applies.

In this paper, we show how the abstract Neo theory can be
leveraged to design a realistic hierarchical coherence protocol. As
such, we present the first realistic hierarchical coherence protocol
verified with fully-automated (push-button) verification tools for all
scales and tree configurations. We explore the practical limitations
posed by both the theory and the verification tools in designing
this verifiable hierarchical protocol. We experimentally evaluate
our protocol, comparing it to more complex protocols that have
optimizations prohibited by the theory and verification tool. Finally,
we discuss how a variety of system configurations and protocols
architects might be interested in can be adapted to the Neo theory,
which we hope opens up the theory to future work in verificationaware protocol design.

",17.5058628484301,16.811297935103244,227
MICRO_17_039.txt,15.556002036948907,13.689141202089179,MICRO,8519,"

Power reduction is one of the primary tasks for designing modern
processors, especially for high-performance throughput processors
such as GPU due to their high power budget. In this paper, we
propose a novel circuit-architecture co-design scheme to harvest
enormous power savings for GPU on-chip SRAM and interconnects. We propose a new 8T SRAM that exhibits asymmetric energy
consumption for bit value 0/1, in terms of read, write and standby.
We name this feature Bit-Value-Favor (BVF). To harvest the power
benefits from BVF on GPUs, we propose three coding methods at
architectural level to maximize the occurrence of bit-1s over bit-0s
in the on-chip data and instruction streams, leading to substantial
chip-level power reduction. Experimental results across a large spectrum of 58 representative GPU applications demonstrate that our
proposed BVF design can bring an average of 21% and 24% chip
power reduction under 28nm and 40nm process technologies, with
negligible design overhead. Further sensitivity studies show that
the effectiveness of our design is robust to DVFS, warp scheduling
policies and different SRAM capacities.

",16.084390811093357,14.981670569867294,184
MICRO_17_040.txt,15.742502247213078,14.113349909266859,MICRO,7428,"

Data race detection is a useful dynamic analysis for multithreaded
programs that is a key building block in record-and-replay, enforcing strong consistency models, and detecting concurrency bugs.
Existing software race detectors are precise but slow, and hardware
support for precise data race detection relies on assumptions like
type safety that many programs violate in practice.

We propose Parsnip, a fully precise hardware-supported data
race detector. Parsnip exploits new insights into the redundancy
of race detection metadata to reduce storage overheads. PARSNIP
also adopts new race detection metadata encodings that accelerate
the common case while preserving soundness and completeness.
When bounded hardware resources are exhausted, PARSNIP falls
back to a software race detector to preserve correctness. PARSNIP
does not assume that target programs are type safe, and is thus
suitable for race detection on arbitrary code.

Our evaluation of Parsnip on several PARSEC benchmarks
shows that performance overheads range from negligible to 2.6x,
with an average overhead of just 1.5x. Moreover, PARSNIP outperforms the state-of-the-art Radish hardware race detector by 4.6x.

",15.470042427545799,13.692348484848484,180
MICRO_17_041.txt,13.038000167959787,10.15965012240449,MICRO,9639,"

In upcoming architectures that stack processor and DRAM dies, temperatures are higher because of the increased transistor density and.
the high inter-layer thermal resistance. However, past research has
underestimated the extent of the thermal bottleneck. Recent experimental work shows that the Die-to-Die (D2D) layers hinder effective
heat transfer, likely leading to the capping of core frequencies.

To address this problem, in this paper, we first show how to create
pillars of high thermal conduction from the processor die to the heat
sink. We do this by aligning and shorting dummy D2D bumps
with thermal TSVs (TTSVs). This lowers processor temperatures
substantially. We then improve application performance by boosting the processor frequency until we consume the available thermal
headroom. Finally, these aligned and shorted dummy pbump-TTSV
sites create die regions of higher vertical thermal conduction. Hence,
we propose to leverage them with three new architectural techniques:
conductivity-aware thread placement, frequency boosting, and thread.
migration. We evaluate our scheme, called Xylem, using simulations
of an 8-core processor at 2.4 GHz and 8 DRAM dies on top. uBumpTTSV alignment and shorting in a generic and in a customized
Xylem design enable an average increase in processor frequency of
400 MHz and 720 MHz, respectively, at an area overhead of 0.63%
and 0.81%, and without exceeding acceptable temperatures. This
improves average application performance by 11% and 18%, respectively. Moreover, applying Xylem’s conductivity-aware techniques
enables further gains.

",15.470042427545799,13.407625523012555,244
MICRO_17_042.txt,14.799475989560271,12.929444039314742,MICRO,9884,"

Computer graphics is generally divided into two branches:
real-time rendering and physically-based rendering. Conventional graphics processing units (GPUs) were designed to accelerate the former which is based on the standard Z-buffer
algorithm. However, many applications in entertainment, science, and industry require high quality visual effects such
as soft-shadows, reflections, and diffuse lighting interactions
which are difficult to achieve with the Z-buffer algorithm, but
are straightforward to implement using physically-based rendering methods. Physically-based rendering can already be
implemented on present programmable GPUs. However, for
physically-based rendering on GPUs, a large portion of the
processing power is wasted due to low utilization of SIMD
units. This is because the core algorithm of physically-based
rendering, ray tracing, suffers from Single Instruction, Multiple Thread (SIMT) control flow divergences. In this paper,
we propose the Dynamic Ray Shuffling (DRS) architecture
for GPUs to address this problem. Our key insight is that
the primary control flow divergences are caused by inconsistent ray traversal states of a warp, and can be eliminated by dynamically shuffling rays. Experimental results show
that, for an estimated 0.11% area cost, DRS significantly improves the SIMD efficiency for the tested benchmarks from
41.06% to 81.04% on average. With this, the performance
of a physically-based rendering method such as path tracing
can be improved by 1.67x—1.92x, and 1.79x on average.

",17.00531248756302,15.194105263157898,235
MICRO_17_043.txt,15.142612165533535,13.399530858332248,MICRO,9828,"

The execution of workloads such as web servers and database servers
typically switches back and forth between different tasks such as
user applications, system call handlers, and interrupt handlers. The
combined size of the instruction footprints of such tasks typically
exceeds that of the i-cache (16-32 KB). This causes a lot of i-cache
misses and thereby reduces the application’s performance. Hence, we
propose SchedTask, a hardware-assisted task scheduler that improves
the performance of such workloads by executing tasks with similar
instruction footprints on the same core. We start by decomposing the
combined execution of the OS and the applications into sequences
of instructions called SuperFunctions. We propose a scheme to
determine the amount of overlap between the instruction footprints
of different SuperFunctions by using Bloom filters. We then use
a hierarchical scheduler to execute SuperFunctions with similar
instruction footprints on the same core. For a suite of 8 popular
OS-intensive workloads, we report an increase in the application’s
performance of up to 29 percentage points (mean: 11.4 percentage
points) over state of the art scheduling techniques.

",16.061879428645646,13.443694751381216,185
MICRO_17_044.txt,15.85783323616566,14.590722005896541,MICRO,8926,"
Graphics Processing Unit (GPU) vendors have been scaling singleGPU architectures to satisfy the ever-increasing user demands for
faster graphics processing. However, as it gets extremely difficult to
further scale single-GPU architectures, the vendors are aiming to
achieve the scaled performance by simultaneously using multiple
GPUs connected with newly developed, fast inter-GPU networks
(e.g., NVIDIA NVLink, AMD XDMA). With fast inter-GPU networks, it is now promising to employ split frame rendering (SFR)
which improves both frame rate and single-frame latency by assigning disjoint regions of a frame to different GPUs. Unfortunately, the
scalability of current SFR implementations is seriously limited as
they suffer from a large amount of redundant computation among
GPUs.

This paper proposes GPUpd, a novel multi-GPU architecture
for fast and scalable SFR. With small hardware extensions, GPUpd
introduces a new graphics pipeline stage called Cooperative Projection & Distribution (C-PD) where all GPUs cooperatively project 3D
objects to 2D screen and efficiently redistribute the objects to their
corresponding GPUs. C-PD not only eliminates the redundant computation among GPUs, but also incurs minimal inter-GPU network
traffic by transferring object IDs instead of mid-pipeline outcomes
between GPUs. To further reduce the redistribution overheads,
GPUpd minimizes inter-GPU synchronizations by implementing
batching and runahead-execution of draw commands. Our detailed
cycle-level simulations with 8 real-world game traces show that
GPUpd achieves a geomean speedup of 4.98x in single-frame latency with 16 GPUs, whereas the current SFR implementations
achieve only 3.07X geomean speedup which saturates on 4 or more
GPUs.

",17.37919286519448,17.45649935649936,263
MICRO_17_045.txt,14.486583061122278,12.201950026924269,MICRO,6400,"

Interactive service providers have strict requirements on highpercentile (tail) latency to meet user expectations. If providers
meet tail latency targets with less energy, they increase profits,
because energy is a significant operating expense. Unfortunately,
optimizing tail latency and energy are typically conflicting goals.
Our work resolves this conflict by exploiting servers with per-core
Dynamic Voltage and Frequency Scaling (DVFS) and Asymmetric
Multicore Processors (AMPs). We introduce the Adaptive Slow-toFast scheduling framework, which matches the heterogeneity of
the workload — a mix of short and long requests — to the heterogeneity of the hardware — cores running at different speeds. The
scheduler prioritizes long requests to faster cores by exploiting the
insight that long requests reveal themselves. We use control theory
to design threshold-based scheduling policies that use individual
request progress, load, competition, and latency targets to optimize
performance and energy. We configure our framework to optimize
Energy Efficiency for a given Tail Latency (EETL) for both DVFS
and AMP. In this framework, each request self-schedules, starting
ona slow core and then migrating itself to faster cores. At high load,
when a desired AMP core speed s is not available for a request but
a faster core is, the longest request on an s core type migrates early
to make room for the other request. Compared to per-core DVFS
systems, EETL for AMPs delivers the same tail latency, reduces
energy by 18% to 50%, and improves capacity (throughput) by 32%
to 82%. We demonstrate that our framework effectively exploits dynamic DVFS and static AMP heterogeneity to reduce provisioning
and operational costs for interactive services.

",16.11434528070225,14.61872641509434,266
MICRO_17_046.txt,14.063605195877702,12.302692746317387,MICRO,10149,"

Pipeline is an important programming pattern, while GPU, designed
mostly for data-level parallel executions, lacks an efficient mechanism to support pipeline programming and executions. This paper
provides a systematic examination of various existing pipeline execution models on GPU, and analyzes their strengths and weaknesses.
To address their shortcomings, this paper then proposes three new
execution models equipped with much improved controllability,
including a hybrid model that is capable of getting the strengths
of all. These insights ultimately lead to the development of a software programming framework named VersaPipe. With VersaPipe,
users only need to write the operations for each pipeline stage.
VersaPipe will then automatically assemble the stages into a hybrid
execution model and configure it to achieve the best performance.
Experiments on a set of pipeline benchmarks and a real-world face
detection application show that VersaPipe produces up to 6.90x
(2.88X on average) speedups over the original manual implementations.

",17.122413403193683,16.01278195488722,155
MICRO_17_047.txt,16.046850045963886,14.540032631456494,MICRO,7628,"

Cache contention in the form of false sharing and true sharing
arises when threads overshare cache lines at high frequency. Such
oversharing can reduce or negate the performance benefits of parallel execution. Prior systems for detecting and repairing cache
contention lack efficiency in detection or repair, contain subtle
memory consistency flaws, or require invasive changes to the program environment.

In this paper, we introduce a new way to combat cache line
oversharing via the Thread Memory Isolation (Tm1) system. TMI
operates completely in userspace, leveraging performance counters
and the Linux ptrace mechanism to tread lightly on monitored
applications, intervening only when necessary. Tm1’s compatibleby-default design allows it to scale to real-world workloads, unlike
previous proposals. T1 introduces a novel code-centric consistency
model to handle cross-language memory consistency issues. TMI
exploits the flexibility of code-centric consistency to efficiently
repair false sharing while preserving strong consistency model
semantics when necessary.

Tai has minimal impact on programs without oversharing, slowing their execution by just 2% on average. We also evaluate TI
on benchmarks with known false sharing, and manually inject a
false sharing bug into the leveldb key-value store from Google.
For these programs, Tm1 provides an average speedup of 5.2x and
achieves 88% of the speedup possible with manual source code fixes.

",16.018793980428352,14.507322999581067,219
MICRO_17_048.txt,14.784240683919219,13.0620712293093,MICRO,8843,"

GPUs lack fundamental support for data-dependent parallelism
and synchronization. While CUDA Dynamic Parallelism signals
progress in this direction, many limitations and challenges still remain. This paper introduces Wireframe, a hardware-software solution
that enables generalized support for data-dependent parallelism and
synchronization. Wireframe enables applications to naturally express
execution dependencies across different thread blocks through a dependency graph abstraction at run-time, which is sent to the GPU
hardware at kernel launch. At run-time, the hardware enforces the dependencies specified in the dependency graph through a dependencyaware thread block scheduler. Overall, Wireframe is able to improve
total execution time up to 65.20% with an average of 45.07%.

",17.122413403193683,16.2858256880734,112
MICRO_17_049.txt,16.559179115112016,14.969899570994823,MICRO,6157,"

Designing a system in an era of rapidly evolving application behaviors and significant technology shifts involves taking on risk
that a design will fail to meet its performance goals. While risk
assessment and management are expected in both business and
investment, these aspects are typically treated as independent to
questions of performance and efficiency in architecture analysis.
As hardware and software characteristics become uncertain (i.e.
samples from a distribution), we demonstrate that the resulting
performance distributions quickly grow beyond our ability to reason about with intuition alone. We further show that knowledge of
the performance distribution can be used to significantly improve
both the average case performance and minimize the risk of underperformance (which we term architectural risk). Our automated
framework can be used to quantify the areas where trade-offs between expected performance and the “tail” of performance are most
acute and provide new insights supporting architectural decision
making (such as core selection) under uncertainty. Importantly it
can do this even without a priori knowledge of an analytic model
governing that uncertainty.

",19.78447435784618,18.641127167630057,175
MICRO_17_050.txt,17.434999133794975,16.033453091564684,MICRO,9246,"

We tackle the important problem class of solving nonlinear
partial differential equations. While nonlinear PDEs are typically solved in high-performance supercomputers, they are
increasingly used in graphics and embedded systems, where
efficiency is important.

We use a hybrid analog-digital computer architecture to
solve nonlinear PDEs that draws on the strengths of each
model of computation and avoids their weaknesses. A weakness of digital methods for solving nonlinear PDEs is they
may not converge unless a good initial guess is used to seed
the solution. A weakness of analog is it cannot produce high
accuracy results. In our hybrid method we seed the digital
solver with a high-quality guess from the analog side.

With a physically prototyped analog accelerator, we use
this hybrid analog-digital method to solve the two-dimensional
viscous Burgers’ equation —an important and representative
PDE. For large grid sizes and nonlinear problem parameters,
the hybrid method reduces the solution time by 5.7x, and reduces energy consumption by 11.6x, compared to a baseline
solver running on a GPU.

",16.678067442207542,14.533923410404626,176
MICRO_17_051.txt,14.432544831112327,11.996388059701491,MICRO,7988,"

Most high-performance microprocessors come equipped with generalpurpose Single Instruction Multiple Data (SIMD) execution engines
to enhance performance. Compilers use auto-vectorization techniques to identify vector parallelism and generate SIMD code so that
applications can enjoy the performance benefits provided by SIMD
units. Superword Level Parallelism (SLP), one such vectorization
technique, forms vector operations by merging isomorphic instructions into a vector operation and linking many such operations into
long isomorphic chains. However, effective grouping of isomorphic
instructions remains a key challenge for SLP algorithms.

In this work, we describe a new hierarchical approach for SLP.
We decouple the selection of isomorphic chains and arrange them
in a hierarchy of choices at the local and global levels. First, we
form small local chains from a set of preferred patterns and rank
them. Next, we form long global chains from the local chains using
a few simple heuristics. Hierarchy allows us to balance the grouping choices of individual instructions more effectively within the
context of larger local and global chains, thereby finding better
opportunities for vectorization.

We implement our algorithm in LLVM, and we compare it against
prior work and the current SLP implementation in LLVM. A set
of applications that benefit from vectorization are taken from the
NAS Parallel Benchmarks and SPEC CPU 2006 suite to compare our
approach and prior techniques. We demonstrate that our new algorithm finds better isomorphic chains. Our new approach achieves
an 8.6% speedup, on average, compared to non-vectorized code and
2.5% speedup, on average, over LLVM-SLP. In the best case, the BT
application has 11% fewer total dynamic instructions and achieves
a 10.9% speedup over LLVM-SLP.

",14.856640023380862,14.04942648592284,278
MICRO_17_052.txt,15.615460100483723,13.736464519529545,MICRO,8904,"
A quantum computer consists of quantum bits (qubits) and a control
processor that acts as an interface between the programmer and the
qubits. As qubits are very sensitive to noise, they rely on continuous
error correction to maintain the correct state. Current proposals rely
on software-managed error correction and require large instruction
bandwidth, which must scale in proportion to the number of qubits.
While such a design may be reasonable for small-scale quantum
computers, we show that instruction bandwidth tends to become a
critical bottleneck for scaling quantum computers.
In this paper, we show that 99.999% of the instructions in the
instruction stream of a typical quantum workload stem from error
correction. Using this observation, we propose QuEST (Quantum
Error-Correction Substrate), an architecture that delegates the task of
quantum error correction to the hardware. QuEST uses a dedicated
programmable micro-coded engine to continuously replay the instruction stream associated with error correction. The instruction
bandwidth requirement of QuEST scales in proportion to the number of active qubits (typically << 0.1%) rather than the total number
of qubits. We analyze the e�ectiveness of QuEST with area and
thermal constraints and propose a scalable microarchitecture using
typical Quantum Error Correction Code (QECC) execution patterns.
Our evaluations show that QuEST reduces instruction bandwidth
demand of several key workloads by �ve orders of magnitude while
ensuring deterministic instruction delivery. Apart from error correction, we also observe a large instruction bandwidth requirement
for fault tolerant quantum instructions (magic state distillation).
We extend QuEST to manage these instructions in hardware and
provide additional reduction in bandwidth. With QuEST, we reduce
the total instruction bandwidth by eight orders of magnitude.
",15.903189008614273,14.429285714285715,279
MICRO_17_053.txt,15.305093252004635,14.071806643028719,MICRO,6308,"

Data access costs dominate the execution times of most parallel
applications and they are expected to be even more important in
the future. To address this, recent research has focused on Near
Data Processing (NDP) as a new paradigm that tries to bring computation to data, instead of bringing data to computation (which
is the norm in conventional computing). This paper explores the
potential of compiler support in exploiting NDP in the context of
emerging manycore systems. To that end, we propose a novel compiler algorithm that partitions the computations in a given loop nest
into subcomputations and schedules the resulting subcomputations
on different cores with the goal of reducing the distance-to-data on
the on-chip network. An important characteristic of our approach
is that it exploits NDP while taking advantage of data locality. Our
experiments with 12 multithreaded applications running on a stateof-the-art commercial manycore system indicate that the proposed
compiler-based approach significantly reduces data movements
on the on-chip network by taking advantage of NDP, and these
benefits lead to an average execution time improvement of 18.4%.

",18.599290044081553,17.853695652173915,186
MICRO_17_054.txt,15.435761965694194,14.16709251618597,MICRO,8447,"

Quantum computing (QC) is at the cusp of a revolution.
Machines with 100 quantum bits (qubits) are anticipated to
be operational by 2020 [30, 73], and several-hundred-qubit
machines are around the corner. Machines of this scale have
the capacity to demonstrate quantum supremacy, the tipping
point where QC is faster than the fastest classical alternative
for a particular problem. Because error correction techniques
will be central to QC and will be the most expensive component of quantum computation, choosing the lowest-overhead
error correction scheme is critical to overall QC success. This
paper evaluates two established quantum error correction
codes—planar and double-defect surface codes—using a set
of compilation, scheduling and network simulation tools. In
considering scalable methods for optimizing both codes, we
do so in the context of a full microarchitectural and compiler analysis. Contrary to previous predictions, we find that
the simpler planar codes are sometimes more favorable for
implementation on superconducting quantum computers, especially under conditions of high communication congestion.

",17.28802050969988,16.183746792130027,168
MICRO_17_055.txt,16.04280068511019,14.424160971402035,MICRO,8570,"

Heterogenous chip multiprocessors (Het-CMPs) offer a combination of large Out-of-Order (O00) cores optimized for high singlethreaded performance and small In-Order (InO) cores optimized for
low-energy and area costs. Due to practical constraints, CMP designers must choose to either optimize for total system throughput
by utilizing many InO cores or maximize single-thread execution
with fewer OoO cores. We propose Mirage Cores, a novel Het-CMP
design where clusters of InO cores are architected around an OoO
in a manner that optimizes for both throughput and single-thread
performance. The insight behind Mirage Cores is that InO cores
can achieve near-OoO performance if they are provided with the
dynamic instruction schedule of an OoO core. To leverage this, Mirage Cores employs an OoO core as an optimal instruction schedule
generator as well as a high-performance alternative for all neighboring InO cores. We also develop intelligent runtime schedulers
which orchestrate the arbitration and migration of applications
between the InO cores and the central OoO. Fast and timely transfer
of dynamic schedules from the OoO to InO allows Mirage Cores to
create the appearance of all OoO cores to the user using underlying
In-Order hardware.

Overall, with an 8 InO per OoO configuration, Mirage Cores can
achieve on average 84% of the performance of a CMP with 8 OoO
cores, a 28% increase relative to current systems, while conserving
55% of energy and 25% of area costs. We find that we can scale the
design to around 12 InOs per OoO before starvation for the OoO
starts to hamper system performance.

",17.410965686947208,17.16246240601504,267
MICRO_17_056.txt,16.53665715454754,14.917973134279382,MICRO,7912,"
Organic thin-film transistors (OTFTs) have attracted increased
attention because of the possibility to produce environmentally
friendly, low-cost, lightweight, flexible, and even biodegradable
devices. With an increasing number of complex applications being
proposed for organic and biodegradable semiconductors, the need
for computation horsepower also rises. However, due to the process characteristic differences, direct adaptation of silicon-based
circuit designs and traditional computer architecture wisdom is not
applicable.
In this paper, we analyze the architectural tradeoffs for processor cores made with an organic semiconductor process. We built
an OTFT simulation framework based on experimental pentacene
OTFTs. This framework includes an organic standard cell library
and can be generalized to other organic semiconductors. Our results
demonstrate that, compared to modern silicon, organic semiconductors favor building deeper pipelines and wider superscalar designs.
To the best of our knowledge, this is the first work to explore the
architectural differences between silicon and organic technology
processes.
",17.267425705330172,17.36078947368421,153
MICRO_17_057.txt,17.141595473066747,15.779469660872213,MICRO,6436,"

Task-based parallel programming frameworks offer compelling productivity and performance benefits for modern chip multi-processors
(CMPs). At the same time, CMPs also provide packed-SIMD units

to exploit fine-grain data parallelism. Two fundamental challenges

make using packed-SIMD units with task-parallel programs particularly difficult: (1) the intra-core parallel abstraction gap; and (2) inefficient execution of irregular tasks. To address these challenges,
we propose augmenting CMPs with intra-core loop-task accelerators

(LTAs). We introduce a lightweight hint in the instruction set to elegantly encode loop-task execution and an LTA microarchitectural

template that can be configured at design time for different amounts

of spatial/temporal decoupling to efficiently execute both regular

and irregular loop tasks. Compared to an in-order CMP baseline,
CMP+LTA results in an average speedup of 4.2x (1.8x area normalized) and similar energy efficiency. Compared to an out-of-order

CMP baseline, CMP+LTA results in an average speedup of 2.3x

(1.5x area normalized) and also improves energy efficiency by 3.2x.
Our work suggests augmenting CMPs with lightweight LTAs can

improve performance and efficiency on both regular and irregular
loop-task parallel programs with minimal software changes.

",17.693802365651003,15.876694162436547,201
MICRO_17_058.txt,15.07693799713385,13.694544285847602,MICRO,6563,"

Processors emit non-trivial amounts of electromagnetic radiation,
creating interference in frequency bands used by wireless communication technologies such as cellular, WiFi and Bluetooth. We
introduce the problem of in-band radio frequency noise as a form
of electromagnetic interference (EMI) to the computer architecture
community as a technical challenge to be addressed.

This paper proposes the new idea of Dynamic EMI Shifting
(DEMIS) where architectural and/or compiler changes allow the
EMI to be shifted at runtime. DEMIS processors dynamically move
the interference from bands used during communication to other
unused frequencies. Unlike previous works that leverage static techniques, DEMIS dynamically targets specific frequency bands; the
type of techniques used here are only possible from an architectural perspective. This paper is also the first to provide insights in
the new area of dynamic EMI shifting by evaluating several platforms and showing the EMI is sensitive to many architectural and
compilation parameters.

Our evaluation over real systems shows a decrease of in-band
EMI ranging from 3 to 15 dB with less than a 10% average performance impact. A 15dB EMI reduction for LTE can represent over
3x bandwidth improvement for EMI bound communication.

",17.833180683606166,17.102551546391755,195
MICRO_17_059.txt,15.86372210993229,15.039311216519824,MICRO,9194,"

Deep neural networks (DNNs) are key computational building
blocks for emerging classes of web services that interact in real time
with users via voice, images and video inputs. Although GPUs have
gained popularity as a key accelerator platform for deep learning
workloads, the increasing demand for DNN computation leaves a
significant gap between the compute capabilities of GPU-enabled
datacenters and the compute needed to service demand.

The state-of-the-art techniques to improve DNN performance
have significant limitations in bridging the gap on real systems.
Current network pruning techniques remove computation, but
the resulting networks map poorly to GPU architectures, yielding
no performance benefit or even slowdowns. Meanwhile, current
bandwidth optimization techniques focus on reducing off-chip bandwidth while overlooking on-chip bandwidth, a key DNN bottleneck.

To address these limitations, this work introduces DeftNN, a
GPU DNN execution framework that targets the key architectural
bottlenecks of DNNs on GPUs to automatically and transparently
improve execution performance. DeftNN is composed of two novel
optimization techniques — (1) synapse vector elimination, a technique that identifies non-contributing synapses in the DNN and
carefully transforms data and removes the computation and data
movement of these synapses while fully utilizing the GPU to improve performance, and (2) near-compute data fission, a mechanism
for scaling down the on-chip data movement requirements within
DNN computations. Our evaluation of DeftNN spans 6 state-of-theart DNNs. By applying both optimizations in concert, DeftNN is
able to achieve an average speedup of 2.1X on real GPU hardware.
We also introduce a small additional hardware unit per GPU core
to facilitate efficient data fission operations, increasing the speedup
achieved by DeftNN to 2.6x.

",16.887214914478655,17.069266187050363,281
MICRO_17_060.txt,14.418635553684343,11.870074583075773,MICRO,7736,"

Emerging non-volatile main memory technologies create a new
opportunity for writing programs with a large, byte-addressable
persistent storage that can be accessed through regular memory
instructions. These new memory-as-storage technologies impose
significant challenges to current programming models. In particular, some emerging persistent programming frameworks, like the
NVM Library (NVML), implement relocatable persistent objects
that can be mapped anywhere in the virtual address space. To make
this work, persistent objects are referenced using object identifiers
(ObjectID), rather than pointers, that need to be translated to an address before the object can be read or written. Frequent translation
from ObjectID to address incurs significant overhead.

We propose treating ObjectIDs as a new persistent memory address space and provide hardware support for efficiently translating
ObjectIDs to virtual addresses. With our design, a program can use
load and store instructions to directly access persistent data using
ObjectIDs, and these new instructions can reduce the programming complexity of this system. We also describe several possible
microarchitectural designs and evaluate them.

We evaluate our design on Sniper modeling both in-order and
out-of-order processors with 6 micro-benchmarks and the TPCC application. The results show our design can give significant
speedup over the baseline system using software translation. We
demonstrate for the Pipelinedimplementation that our design has an
average speedup of 1.96x and 1.58x on an in-order and out-of-order
processor, respectively, over the baseline system on microbenchmarks that place persistent data randomly into persistent pools. For
the same in-order and out-of-order microarchitectures, we measure
a speedup of 1.17x and 1.12x, respectively, on the TPC-C application when B+Trees are put in different pools and rewritten to use
our new hardware.

",17.219254097808864,15.895,292
MICRO_17_061.txt,16.19613613094502,14.689201080875893,MICRO,8171,"

Quantum computers promise to solve certain problems that are
intractable for classical computers, such as factoring large numbers
and simulating quantum systems. To date, research in quantum
computer engineering has focused primarily at opposite ends of
the required system stack: devising high-level programming languages and compilers to describe and optimize quantum algorithms,
and building reliable low-level quantum hardware. Relatively little
attention has been given to using the compiler output to fully control the operations on experimental quantum processors. Bridging
this gap, we propose and build a prototype of a flexible control
microarchitecture supporting quantum-classical mixed code for
a superconducting quantum processor. The microarchitecture is
based on three core elements: (i) a codeword-based event control
scheme, (ii) queue-based precise event timing control, and (iii) a
flexible multilevel instruction decoding mechanism for control. We
design a set of quantum microinstructions that allows flexible control of quantum operations with precise timing. We demonstrate
the microarchitecture and microinstruction set by performing a
standard gate-characterization experiment on a transmon qubit.

",18.08858127442927,17.675154553049293,172
Middleware_17_001.txt,14.869944720559655,12.4332402283145,Middleware,10810,"
The wide adoption of SMP virtual machines (VMs) and resource consolidation present challenges to efficiently executing multi-threaded
programs in the cloud. An important problem is the semantic gaps
between the guest OS and the hypervisor. The well-known lockholder preemption (LHP) and lock-waiter preemption (LWP) problems are examples of such semantic gaps, in which the hypervisor
is unaware of the activities in the guest OS and adversely deschedules virtual CPUs (vCPUs) that are executing in critical sections.
Existing studies have focused on inferring a high-level semantic
state of the guest OS to aid hypervisor-level scheduling so as to
avoid the LHP and LWP problems.

In this work, we find a reverse semantic gap — the guest OS is
oblivious of the scheduling events at the hypervisor, leaving the
potential of addressing the LHP and LWP problems in the guest
OS unexploited. Inspired by scheduler activations (SAs) in hybrid
threading, we proposed interference-resilient scheduling (IRS), a
guest-hypervisor coordinated approach to enhancing load balancing in the guest. IRS informs the guest OS before vCPU preemption
happens at the hypervisor to activate in-guest load balancing. As
such, critical threads on preempted vCPUs can be migrated to other
running vCPUs so that the LHP and LWP problems are all alleviated. Experimental results with Xen and Linux guests show as
much as 42%, 43%, and 46% performance improvement for parallel
programs with blocking, spinning synchronizations, and multithreaded server workloads, respectively.
",18.001758247042904,15.809349930843712,242
Middleware_17_002.txt,16.087722504812085,14.07477726574501,Middleware,8129,"
We investigate the use of content-based publish/subscribe for data
dissemination in large-scale applications with expressive filtering
requirements. In particular, we focus on top-k subscription filtering, where a publication is delivered only to the k best ranked subscribers, as ordered using expressive semantics such as relevance,
fairness, and diversity. The naive approach to perform filtering
early at the publisher edge works only if complete knowledge of
the subscriptions is available, which is not compatible with the
well-established covering optimization in scalable content-based
publish/subscribe systems. We propose an efficient rank-cover technique to reconcile top-k subscription filtering with covering. We
extend the covering model to support top-k and describe a novel
algorithm for forwarding subscriptions to publishers while maintaining correctness. We also establish a framework for supporting
different types of ranking semantics and propose an implementation
to support fairness. Finally, we compare our solutions to a baseline
covering system and perform sensitivity analysis to demonstrate
that our optimized rank-cover algorithm retains both covering and
fairness while achieving properties advantageous to our targeted
workloads. In a typical setting, our optimized solution is scalable,
selects fairly, and provides over 81% of the covering benefit.
",18.903936251131103,17.406000000000002,201
Middleware_17_003.txt,15.701521022279113,14.170230592745515,Middleware,9802,"
Major cloud computing operators provide powerful monitoring
tools to understand the current (and prior) state of the distributed
systems deployed in their infrastructure. While such tools provide a
detailed monitoring mechanism at scale, they also pose a significant
challenge for the application developers/operators to transform the
huge space of monitored metrics into useful insights. These insights
are essential to build effective management tools for improving the
efficiency, resiliency, and dependability of distributed systems.

This paper reports on our experience with building and deploying S1EvE—a platform to derive actionable insights from monitored
metrics in distributed systems. SIEVE builds on two core components: a metrics reduction framework, and a metrics dependency
extractor. More specifically, SIEVE first reduces the dimensionality
of metrics by automatically filtering out unimportant metrics by
observing their signal over time. Afterwards, SIEVE infers metrics
dependencies between distributed components of the system using
a predictive-causality model by testing for Granger Causality.

We implemented Sieve as a generic platform and deployed it for
two microservices-based distributed systems: OpenStack and ShareLatex. Our experience shows that (1) SIEVE can reduce the number
of metrics by at least an order of magnitude (10 — 100x), while preserving the statistical equivalence to the total number of monitored
metrics; (2) SIEVE can dramatically improve existing monitoring
infrastructures by reducing the associated overheads over the entire system stack (CPU—80%, storage—90%, and network—50%);
(3) Lastly, Steve can be effective to support a wide-range of workflows in distributed systems—we showcase two such workflows:
Orchestration of autoscaling, and Root Cause Analysis (RCA).
",19.287186520377343,19.09512820512821,261
Middleware_17_004.txt,15.6298215305271,13.915701161688009,Middleware,9146,"
Web applications hosted on the cloud are exposed to cyberattacks and can be compromised by HTTP requests that exploit
vulnerabilities. Platform as a Service (PaaS) offerings often
provide a backup service that allows restoring application
state after a serious attack, but all valid state changes since
the last backup are lost. We propose Rectify, a new approach
to recover from intrusions on applications running in a PaaS.
Rectify is a service designed to be deployed alongside the application in a PaaS container. It does not require modifications
to the software and the recovery can be performed by a system administrator. Machine learning techniques are used to
associate the requests received by the application to the statements issued to the database. Rectify was evaluated using
three widely used web applications — Wordpress, LimeSurvey
and MediaWiki — and the results show that the effects of malicious requests can be removed whilst preserving the valid
application data.
",15.343465313023842,14.46015037593985,153
Middleware_17_005.txt,17.018809657800283,15.287376687340288,Middleware,8584,"
Multipath TCP enables remarkable optimizations for throughput,
load balancing, and mobility in today’s networks. The design space
of Multipath TCP scheduling, i.e., the application-aware mapping of
packets to paths, is largely unexplored due to its inherent complexity. Evidence in this paper suggests that an application-aware scheduling decision, if leveraged right, pushes Multipath TCP beyond
throughput optimization and thereby provides benefits for a wide
range of applications.
This paper introduces a high-level programming model that enables application-defined Multipath TCP scheduling. We provide
an efficient interpreter and eBPF-based runtime environment for
the Linux Kernel, enabling isolated application-defined schedulers
in multi-tenancy environments. In combination with a high-level
API, our work closes the gap between scheduler specification and
deployment. We show the strength of our programming model by
implementing seven novel schedulers tackling diverse objectives.
Our real world measurements, for example, of an application- and
preference-aware scheduler, show that the programming model
enables timely scheduling decisions to retain fine-grained throughput objectives. Further measurements of a novel HTTP/2-aware
scheduler show significantly improved interactions with upperlayer protocols, e.g., an optimized dependency resolution, while
preserving path preferences.
",18.83193753551143,17.620625,196
Middleware_17_006.txt,13.878777813449997,12.293014048312106,Middleware,10178,"
Traditionally, active storage techniques have been proposed to move
computation tasks to storage nodes in order to exploit data locality.
However, we argue in this paper that active storage is ill-suited for
cloud storage for two reasons: 1. Lack of elasticity: Computing can
only scale out with the number of storage nodes; and 2. Resource
Contention: Sharing compute resources can produce interferences
in the storage system. Serverless computing is now emerging as a
promising alternative for ensuring painless scalability, and also, for
simplifying the development of disaggregated computing tasks.

Here we present an innovative data-driven serverless computing
middleware for object storage. It is a lightweight compute solution
that allows users to create small, stateless functions that intercept
and operate on data flows in a scalable manner without the need
to manage a server or a runtime environment. We demonstrate
through different use cases how our solution scales with minimal
overhead, while getting rid of the resource contention problems
incurred by active storage tasks.
",15.247664890283005,14.337386363636366,166
Middleware_17_007.txt,16.422158412841384,15.307388254859529,Middleware,10215,"
As VMware are now widely used in desktop and server environ
Mobile apps make extensive use of GPUs on smartphones and
tablets to access Web content. To support pervasive Web content,
we introduce three key OS techniques for binary graphics compatibility necessary to build a real-world system to run iOS and
Android apps together on the same smartphone or tablet. First diplomat usage patterns manage resources to bridge proprietary iOS
and Android graphics implementations. Second, thread impersonation allows a single thread-specific context to be shared amongst
multiple threads using multiple iOS and Android personas. Third,
dynamic library replication allows multiple, independent instances
of the same library to be loaded in a single process to support iOS
apps on Android while using multiple graphics API versions at
the same time. We use these techniques to build a system prototype, and demonstrate that it runs widely-used iOS apps, including
apps such as Safari that use the popular GPU-accelerated WebKit
framework, using a Google Nexus tablet running Android.
",16.728156217252725,16.551213017751483,170
Middleware_17_008.txt,14.979203485386911,13.285966940177822,Middleware,8694,"
Onboarding network functions onto current clouds requires laborintensive configuration of the virtual environment. Developers
need to dimension the resources available to each virtual machine
such as CPU and memory, define thresholds for scaling dynamically
and create configuration files that operators can use to execute the
network services. This process is time consuming and dependent on
the server architecture. As resources are managed on an individual
virtual machine basis, services cannot be orchestrated end to end
without significant expertise. In this paper, we argue that much of
the manual configuration needed for onboarding services onto a
cloud can be automated. Moreover, we can automatically generate abstractions that consider services end-to-end and enable their
holistic orchestration. We propose a framework that benchmarks
network services during the onboarding process and generates an
elastic model which relates workload mixes to resource requirements, identifies component dependencies and automates service
operation on heterogeneous stacks. We have evaluated our framework using a real-time communication service that handles multiple
classes of workloads. Results show that underprovisioning can be
eliminated for regular daily traffic, reducing resource provisioning time by at least 5X for the most stressing traffic surges, while
improving key performance indicators by at least 40%.
",18.36309006607702,17.483184079601987,202
Middleware_17_009.txt,14.683439415763992,13.178922001320245,Middleware,10057,"
Distributed Complex Event Processing (DCEP) is a paradigm to infer the occurrence of complex situations in the surrounding world
from basic events like sensor readings. In doing so, DCEP operators detect event patterns on their incoming event streams. To
yield high operator throughput, data parallelization frameworks
divide the incoming event streams of an operator into overlapping
windows that are processed in parallel by a number of operator
instances. In doing so, the basic assumption is that the different
windows can be processed independently from each other. However, consumption policies enforce that events can only be part of
one pattern instance; then, they are consumed, ie., removed from
further pattern detection. That implies that the constituent events
of a pattern instance detected in one window are excluded from all
other windows as well, which breaks the data parallelism between
different windows. In this paper, we tackle this problem by means
of speculation: Based on the likelihood of an event’s consumption
in a window, subsequent windows may speculatively suppress that
event. We propose the SPECTRE framework for speculative processing of multiple dependent windows in parallel. Our evaluations
show an up to linear scalability of SPECTRE with the number of
CPU cores.
",15.470042427545799,14.854438860971523,201
Middleware_17_010.txt,15.163406101031889,13.482108036358913,Middleware,9108,"
Data analytic applications built upon big data processing frameworks such as Apache Spark are an important class of applications.
Many of these applications are not latency-sensitive and thus can
run as batch jobs in data centers. By running multiple applications
on a computing host, task co-location can significantly improve the
server utilization and system throughput. However, effective task
co-location is a non-trivial task, as it requires an understanding of
the computing resource requirement of the co-running applications,
in order to determine what tasks, and how many of them, can be
co-located. State-of-the-art co-location schemes either require the
user to supply the resource demands which are often far beyond
what is needed; or use a one-size-fits-all function to estimate the requirement, which, unfortunately, is unlikely to capture the diverse
behaviors of applications.

In this paper, we present a mixture-of-experts approach to model
the memory behavior of Spark applications. We achieve this by
learning, off-line, a range of specialized memory models on a range
of typical applications; we then determine at runtime which of the
memory models, or experts, best describes the memory behavior of
the target application. We show that by accurately estimating the
resource level that is needed, a co-location scheme can effectively
determine how many applications can be co-located on the same
host to improve the system throughput, by taking into consideration the memory and CPU requirements of co-running application
tasks. Our technique is applied to a set of representative data analytic applications built upon the Apache Spark framework. We
evaluated our approach for system throughput and average normalized turnaround time on a multi-core cluster. Our approach
achieves over 83.9% of the performance delivered using an ideal
memory predictor. We obtain, on average, 8.69x improvement on
system throughput and a 49% reduction on turnaround time over
executing application tasks in isolation, which translates to a 1.28x
and 1.68x improvement over a state-of-the-art co-location scheme
for system throughput and turnaround time respectively.
",18.062587368997235,16.79950867052023,351
Middleware_17_011.txt,15.444363715015214,13.651351491210054,Middleware,13264,"
Disaster Recovery (DR) is a crucial feature to ensure availability
and data protection in modern information systems. A common DR
approach requires the replication of services in a set of virtual machines running in the cloud as backups. This leads to considerable
monetary costs and managing efforts to keep such cloud VMs. We
present GINJA, a DR solution for transactional database management systems (DBMS) that uses only cloud storage services such
as Amazon S3. GINJA works at file-system level to efficiently capture and replicate data updates to a remote cloud storage service,
achieving three important goals: (1) reduces the costs for maintaining a cloud-based DR to less than one dollar per month for relevant
databases’ sizes and workloads (up to 222 x less than the traditional
approach of having a DBMS replica in a cloud VM); (2) allows a
precise control of the operational costs, durability and performance
trade-offs; and (3) introduces a small performance overhead to the
DBMS (e.g., less than 5% overhead for the TPC-C workload with ~
10 seconds of data loss in case of disasters).
",20.267338824336647,22.24157103825137,184
Middleware_17_012.txt,16.6595541958867,15.277934786135791,Middleware,10591,"
Big Data applications suffer from unpredictable and unacceptably
high pause times due to bad memory management (Garbage Collection, GC) decisions. This is a problem for all applications but
it is even more important for applications with low pause time
requirements such as credit-card fraud detection or targeted website advertisement systems, which can easily fail to comply with
Service Level Agreements due to long GC cycles (during which the
application is stopped). This problem has been previously identified
and is related to Big Data applications keeping in memory (for a
long period of time, from the GC’s perspective) massive amounts
of data objects.

Memory management approaches have been proposed to reduce
the GC pause time by allocating objects with similar lifetimes close
to each other. However, they either do not provide a general solution
for all types of Big Data applications (thus only solving the problem
for a specific set of applications), and/or require programmer effort
and knowledge to change/annotate the application code.

This paper proposes POLM2, a profiler that automatically: i) estimates application allocation profiles based on execution records,
and ii) instruments application bytecode to help the GC taking
advantage of the profiling information. Thus, no programmer effort
is required to change the source code to allocate objects according
to their lifetimes. POLM2 is implemented for the OpenJDK HotSpot
Java Virtual Machine 8 and uses NG2C, a recently proposed GC
which supports multi-generational pretenuring. Results show that
POLM2 is able to: i) achieve pauses as low as NG2C (which requires
manual source code modification), and ii) significantly reduce application pauses by up to 80% when compared to G1 (default collector
in OpenJDK). POLM2 does not negatively impact neither application throughput nor memory utilization.
",18.35118385865746,17.229480836236934,289
Middleware_17_013.txt,14.652188523379792,13.336585839643977,Middleware,8159,"
The exploitation of user search queries by search engines is at the
heart of their economic model. As consequence, offering private
Web search functionalities is essential to the users who care about
their privacy. Nowadays, there exists no satisfactory approach to
enable users to access search engines in a privacy-preserving way.
Existing solutions are either too costly due to the heavy use of
cryptographic mechanisms (e.g., private information retrieval protocols), subject to attacks (e.g., Tor, TrackMeNot, GooPIR) or rely
on weak adversarial models (e.g., PEAS). This paper introduces
X-Sgarcu, a novel private Web search mechanism building on the
disruptive Software Guard Extensions (SGX) proposed by Intel. We
compare X-Sgarcx to its closest competitors, Tor and PEAS, using
a dataset of real web search queries. Our evaluation shows that: (1)
X-Sgarcu offers stronger privacy guarantees than its competitors as
it operates under a stronger adversarial model; (2) it better resists
state-of-the-art re-identification attacks; and (3) from the performance perspective, X-Szarcu outperforms its competitors both in
terms of latency and throughput by orders of magnitude.
",16.785175570968402,15.945604395604398,186
Middleware_17_014.txt,15.854893603158455,14.611478579062481,Middleware,8518,"
Rivulet is a fault-tolerant distributed platform for running
smart-home applications; it can tolerate failures typical for
a home environment (e.g., link losses, network partitions,
sensor failures, and device crashes). In contrast to existing
cloud-centric solutions, which rely exclusively on a home
gateway device, Rivulet leverages redundant smart consumer
appliances (e.g., TVs, Refrigerators) to spread sensing and
actuation across devices local to the home, and avoids making the Smart-Home Hub a single point of failure. Rivulet
ensures event delivery in the presence of link loss, network
partitions and other failures in the home, to enable applications with reliable sensing in the case of sensor failures,
and event processing in the presence of device crashes. In
this paper, we present the design and implementation of
Rivulet, and evaluate its effective handling of failures in a
smart home.
",18.7741,19.44019784172662,142
Middleware_17_015.txt,14.746735268596431,13.279441905743287,Middleware,9604,"
Approximate computing aims for efficient execution of workflows
where an approximate output is sufficient instead of the exact
output. The idea behind approximate computing is to compute
over a representative sample instead of the entire input dataset.
Thus, approximate computing — based on the chosen sample size —
can make a systematic trade-off between the output accuracy and
computation efficiency.

Unfortunately, the state-of-the-art systems for approximate computing primarily target batch analytics, where the input data remains unchanged during the course of computation. Thus, they are
not well-suited for stream analytics. This motivated the design of
STREAMAPPROX— a stream analytics system for approximate computing. To realize this idea, we designed an online stratified reservoir sampling algorithm to produce approximate output with rigorous error bounds. Importantly, our proposed algorithm is generic
and can be applied to two prominent types of stream processing systems: (1) batched stream processing such as Apache Spark Streaming, and (2) pipelined stream processing such as Apache Flink.

To showcase the effectiveness of our algorithm, we implemented
STREAMAPPROx as a fully functional prototype based on Apache
Spark Streaming and Apache Flink. We evaluated STREAMAPPROX
using a set of microbenchmarks and real-world case studies. Our
results show that Spark- and Flink-based SrREAMAPPROX systems
achieve a speedup of 1.15x—3x compared to the respective native
Spark Streaming and Flink executions, with varying sampling fraction of 80% to 10%. Furthermore, we have also implemented an
improved baseline in addition to the native execution baseline — a
Spark-based approximate computing system leveraging the existing sampling modules in Apache Spark. Compared to the improved
baseline, our results show that STREAMAPPROX achieves a speedup
of 1.1x—2.4x while maintaining the same accuracy level.
",16.758888586881195,15.290812720848056,287
Middleware_17_016.txt,16.29882785191662,14.30835303195694,Middleware,8555,"
The rapid adoption of smartphones with different types of
advanced sensors has led to an increasing trend in the usage
of mobile crowdsensing applications, e.g. , to create hyperlocal weather maps. However, the high energy consumption
of crowdsensing, chiefly due to expensive network communication, has been found to be detrimental to the wide-spread
adoption. We propose a framework, called SENSE-AID, that
can provide energy-efficient mobile crowdsensing service,
coexisting with the cellular network. There are two key
innovations in SENSE-AID beyond prior work (Piggyback
Crowdsensing-Sensys13)—the middleware running on the
cellular network edge to orchestrate multiple devices present
in geographical proximity to suppress redundant data collection and communication. It understands the state of each
device (radio state, battery state, etc.) to decide which ones
should be selected for crowdsensing activities at any point
in time. It also provides a simple programming abstraction
to help with the development of crowdsensing applications.
We show the benefit of SENsE-Arp by conducting a user
study consisting of 60 students in our campus, compared
to a baseline periodic data collection method and Piggyback Crowdsensing. We find that energy saving is 93.3%
for SENSE-AID compared with Piggyback Crowdsensing in
a representative case which requires 2 devices to provide
barometric values within an area of a circle whose radius
is 1 kilometer and requires periodic data collection every
5 minutes for a 90-minute test. The selection algorithm of
SENSE-AID also ensures reasonable fairness in the use of the
different devices.
",19.174585242480724,17.285733333333337,253
Middleware_17_017.txt,16.352321843912826,15.308208207899138,Middleware,10326,"
High performance computing (HPC) applications, such as metagenomics and other big data systems, need to store and analyze huge
volumes of semi-structured data. Such applications often rely on
NoSQL-based datastores, and optimizing these databases is a challenging endeavor, with over 50 configuration parameters in Cassandra alone. As the application executes, database workloads can
change rapidly from read-heavy to write-heavy ones, and a system
tuned with a read-optimized configuration becomes suboptimal
when the workload becomes write-heavy.

In this paper, we present a method and a system for optimizing
NoSQL configurations for Cassandra and ScyllaDB when running
HPC and metagenomics workloads. First, we identify the significance of configuration parameters using ANOVA. Next, we apply
neural networks using the most significant parameters and their
workload-dependent mapping to predict database throughput, as
a surrogate model. Then, we optimize the configuration using
genetic algorithms on the surrogate to maximize the workloaddependent performance. Using the proposed methodology in our
system (RAFIKI), we can predict the throughput for unseen workloads and configuration values with an error of 7.5% for Cassandra
and 6.9-7.8% for ScyllaDB. Searching the configuration spaces using
the trained surrogate models, we achieve performance improvements of 41% for Cassandra and 9% for ScyllaDB over the default
configuration with respect to a read-heavy workload, and also
significant improvement for mixed workloads. In terms of searching speed, RAFIKI, using only 1/10000-th of the searching time of
exhaustive search, reaches within 15% and 9.5% of the theoretically best achievable performances for Cassandra and ScyllaDB,
respectively—supporting optimizations for highly dynamic workloads.
",17.693802365651003,17.680242424242426,269
Middleware_17_018.txt,18.067816930339237,16.437277395433828,Middleware,8967,"
Derivative clouds, light weight application containers provisioned
in virtual machines, are becoming viable and cost-effective options
for infrastructure and software-based services. Ubiquitous dynamic
memory management techniques in virtualized systems are centralized at the hypervisor and are ineffective in nested derivative
cloud setups. In this paper, we highlight the challenges in management of memory resources in derivative cloud systems. Hypervisor
caching, an enabler of centralized disk cache management, provides flexible memory or non-volatile memory management at the
hypervisor to improve the resource usage efficiency and performance of applications. Existing hypervisor caching solutions have
limited effectiveness in nested setups due to their nesting agnostic
design, centralized management model and lack of holistic view of
memory management. We propose DoubleDecker, a decentralized
disk caching framework, realized through guest OS and hypervisor
cooperation, with support for efficient memory management in derivative clouds. The DoubleDecker hypervisor caching framework,
an integral part of our proposed solution, provides interfaces for
differentiated cache partitioning and management in nested setups
and is equipped to handle both memory and SSD based caching
stores. We demonstrate the flexibility of DoubleDecker to handle
dynamic and changing memory provisioning requirements and
its capability to simultaneously provision memory across multiple levels. Such multi-level configurations cannot be explored by
centralized designs and are a key feature of DoubleDecker. Our
experimentation with DoubleDecker demonstrates that application
performance can be consistently improved due to the flexible policy
framework for disk caching. With our setup, we report an average
performance improvement of 4x and a maximum of 11x.
",19.739881575394175,18.502068718682896,255
Middleware_17_019.txt,15.230210517274656,14.024004845146187,Middleware,8854,"
Developers use Machine Learning (ML) platforms to train ML models and then deploy these ML models as web services for inference
(prediction). A key challenge for platform providers is to guarantee
response-time Service Level Agreements (SLAs) for inference workloads while maximizing resource efficiency. Swayam is a fully distributed autoscaling framework that exploits characteristics of production ML inference workloads to deliver on the dual challenge of
resource efficiency and SLA compliance. Our key contributions are
(1) model-based autoscaling that takes into account SLAs and ML
inference workload characteristics, (2) a distributed protocol that
uses partial load information and prediction at frontends to provision new service instances, and (3) a backend self-decommissioning
protocol for service instances. We evaluate Swayam on 15 popular
services that were hosted on a production ML-as-a-service platform,
for the following service-specific SLAs: for each service, at least
99% of requests must complete within the response-time threshold.
Compared to a clairvoyant autoscaler that always satisfies the SLAs
(Le., even if there is a burst in the request rates), Swayam decreases
resource utilization by up to 27%, while meeting the service-specific
SLAs over 96% of the time during a three hour window. Microsoft
Azure’s Swayam-based framework was deployed in 2016 and has
hosted over 100,000 services.
",17.77360955136429,17.422063492063497,219
Middleware_17_020.txt,16.508648264988167,15.31734832118416,Middleware,10953,"
The quality of machine learning (ML) and deep learning (DL) models are very sensitive to many different adjustable parameters that
are set before training even begins, commonly called hyperparameters. Efficient hyperparameter exploration is of great importance
to practitioners in order to find high-quality models with affordable time and cost. This is however a challenging process due to
a huge search space, expensive training runtime, sparsity of good
configurations, and scarcity of time and resources. We develop a
scheduling algorithm POP that quickly identifies among promising,
opportunistic and poor configurations of hyperparameters. It infuses
probabilistic model-based classification with dynamic scheduling
and early termination to jointly optimize quality and cost. We also
build a comprehensive hyperparameter exploration infrastructure,
HyperDrive, to support existing and future scheduling algorithms
for a wide range of usage scenarios across different ML/DL frameworks and learning domains. We evaluate POP and HyperDrive
using complex and deep models. The results show that we speedup
the training process by up to 6.7x compared with basic approaches
like random/grid search and up to 2.1x compared with state-of-theart approaches while achieving similar model quality compared
with prior work.
",18.10804710084791,17.07412958115184,194
MobiCom_17_001.txt,15.198301895330275,14.15635568275475,MobiCom,7994,"

Recent advances in Cross-Technology Communication (CTC) have
improved efficient coexistence and cooperation among heterogeneous wireless devices (e.g., WiFi, ZigBee, and Bluetooth) operating in the same ISM band. However, until now the effectiveness of
existing CTCs, which rely on packet-level modulation, is limited
due to their low throughput (e.g., tens of bps). Our work, named
WEBee, opens a promising direction for high-throughput CTC via
physical-level emulation. WEBee uses a high-speed wireless radio
(e.g., WiFi OFDM) to emulate the desired signals of a low-speed
radio (e.g., ZigBee). Our unique emulation technique manipulates
only the payload of WiFi packets, requiring neither hardware nor
firmware changes in commodity technologies — a feature allowing
zero-cost fast deployment on existing WiFi infrastructure. We designed and implemented WEBee with commodity devices (Atheros
AR2425 WiFi card and MicaZ CC2420) and the USRP-N210 platform (for PHY layer evaluation). Our comprehensive evaluation
reveals that WEBee can achieve a more than 99% reliable parallel
CTC between WiFi and ZigBee with 126 Kbps in noisy environments, a throughput about 16,000x faster than current state-of-theart CTCs.

",17.122413403193683,17.177472527472528,188
MobiCom_17_002.txt,14.92658621030569,13.284704725115322,MobiCom,7165,"

Despite years of innovative research and development, gigabit-speed
60 GHz wireless networks are still not mainstream. The main concern
for network operators and vendors is the unfavorable propagation
characteristics due to short wavelength and high directionality, which
renders the 60 GHz links highly vulnerable to blockage and mobility.
However, the advent of multi-band chipsets opens the possibility of
leveraging the more robust WiFi technology to assist 60 GHz in order
to provide seamless, Gbps connectivity. In this paper, we design and
implement MUST, an IEEE 802.11-compliant system that provides
seamless, high-speed connectivity over multi-band 60 GHz and WiFi
devices. MUST has two key design components: (1) a WiFi-assisted
60 GHz link adaptation algorithm, which can instantaneously predict
the best beam and PHY rate setting, with zero probing overhead; and
(2) a proactive blockage detection and switching algorithm which can
re-direct ongoing user traffic to the robust interface within sub-10 ms
latency. Our experiments with off-the-shelf 802.11 hardware show
that MUST can achieve 25-60% throughput gain over state-of-the-art
solutions, while bringing almost 2 orders of magnitude cross-band
switching latency improvement.

",17.315433740611066,18.187010309278353,197
MobiCom_17_003.txt,14.270740704251981,12.683697605752261,MobiCom,9865,"
60 GHz millimeter-wave networking has emerged as the next frontier technology to provide multi-Gbps wireless connectivity. However, the intrinsic directionality and limited field-of-view of 60 GHz
antennas make the links extremely sensitive to user mobility and
orientation change. Hence, seamless coverage, even at room level,
becomes challenging. In this paper, we propose Pia, a robust 60
GHz network architecture that can provide seamless coverage and
mobility support at multi-Gbps bitrate. Pia comprises multiple cooperating access points (APs). It leverages the pose information on
mobile clients to proactively select the AP and manage multi-link
spatial reuse. These decisions require a model of the pose/location
of the APs and ambient reflectors. We address these challenges
through a set of AP-pose sensing and compressive angle estimation
algorithms that fuse the pose measurement with link quality measurement on the client. We have implemented Pia using commodity
60 GHz platforms. Our experiments show that Pia reduces the occurrence of link outage by 6.3× and improves the spatial sharing
capacity by 76%, compared to conventional schemes that only use
in-band information for adaptation.
",15.381575749822971,13.914764705882359,188
MobiCom_17_004.txt,13.20958240333125,10.931560365077374,MobiCom,10231,"

Control-plane operations are indispensable to providing data access to mobile devices in the 4G LTE networks. They provision
necessary control states at the device and network nodes to enable
data access. However, the current design may suffer from long data
access latency even under good radio conditions. The fundamental
problem is that, data-plane packet delivery cannot start or resume
until all control-plane procedures are completed, and these control
procedures run sequentially by design. We show both are more than
necessary under popular use cases. We design DPCM, which reduces
data access latency through parallel processing approaches and exploiting device-side state replica. We implement DPCM and validate
its effectiveness with extensive evaluations.

",15.151101081350806,13.391055900621119,116
MobiCom_17_005.txt,16.25945539922039,15.063059908389587,MobiCom,5453,"

Open experimentation with operational Mobile Broadband (MBB)
networks in the wild is currently a fundamental requirement of the
research community in its endeavor to address the need of innovative solutions for mobile communications. Even more, there is a
strong need for objective data about stability and performance of
MBB (e.g., 3G/4G) networks, and for tools that rigorously and scientifically assess their status. In this paper, we introduce the MONROE
measurement platform: an open access and flexible hardware-based
platform for measurements and custom experimentation on operational MBB networks. The MONROE platform enables accurate,
realistic and meaningful assessment of the performance and reliability of 11 MBB networks in Europe. We report on our experience
designing, implementing and testing the solution we propose for
the platform. We detail the challenges we overcame while building and testing the MONROE testbed and argue our design and
implementation choices accordingly. We describe and exemplify
the capabilities of the platform and the wide variety of experiments
that external users already perform using the system.

",17.122413403193683,17.076601859678785,171
MobiCom_17_006.txt,13.588439017168167,11.71278601262749,MobiCom,7254,"
In an increasingly mobile connected world, our user experience of
mobile applications more and more depends on the performance of
cellular radio access networks (RAN). To achieve high quality of
experience for the user, it is imperative that operators identify and
diagnose performance problems quickly. In this paper, we describe
our experience in understanding the challenges in automating the
diagnosis of RAN performance problems. Working with a major
cellular network operator on a part of their RAN that services more
than 2 million users, we demonstrate that fine-grained modeling
and analysis could be the key towards this goal. We describe our
methodology in analyzing RAN problems, and highlight a few of
our findings, some previously unknown. We also discuss lessons
from our attempt at building automated diagnosis solutions.

",16.32212239822248,15.297325581395352,130
MobiCom_17_007.txt,14.914038782414746,13.24062386002242,MobiCom,7241,"

The near ubiquitous availability and success of mobile broadband
networks has motivated verticals that range from public safety
communication to intelligent transportation systems and beyond to
consider choosing them as the communication mean of choice. Several of these verticals, however, expect high availability of multiple
nines. This paper leverages end-to-end measurements to investigate the potential of current mobile broadband networks to support
these expectations. We conduct a large-scale measurement study
of network availability in four networks in Norway. This study is
based on three years of measurements from hundreds of stationary
measurement nodes and several months of measurements from four
mobile nodes. We find that the mobile network centralized architecture and infrastructure sharing between operators are responsible
for a non-trivial fraction of network failures. Most episodes of degraded availability, however, are uncorrelated. We also find that
using two networks simultaneously can result in more than five
nines of availability for stationary nodes and three nines of availability for mobile nodes. Our findings point to potential avenues
for enhancing the availability of future mobile networks.

",15.903189008614273,15.458164794007494,179
MobiCom_17_008.txt,13.32416622857005,10.673798242335995,MobiCom,8674,"

Tick is anew SDR system that provides programmability and ensures low latency at both PHY and MAC. It supports modular design and element-based programming, similar to the Click router
framework [23]. It uses an accelerator-rich architecture, where an
embedded processor executes control flows and handles various
MAC events. User-defined accelerators offload those tasks, which
are either computation-intensive or communication-heavy, or require fine-grained timing control, from the processor, and accelerate
them in hardware. Tick applies a number of hardware and software
co-design techniques to ensure low latency, including multi-clockdomain pipelining, field-based processing pipeline, separation of
data and control flows, etc. We have implemented Tick and validated its effectiveness through extensive evaluations as well as two
prototypes of 802.11ac SISO/MIMO and 802.11a/g full-duplex.

",16.32212239822248,15.324172932330828,136
MobiCom_17_009.txt,12.720725146813717,10.840506097194687,MobiCom,10451,"

Full duplex techniques can potentially double the channel capacity
and achieve lower delays by empowering two radios to simultaneously transmit in the same frequency band. However, full duplex is
only available between two adjacent nodes within the communication range. In this paper, we present BiPass to break this limitation.
With the help of full duplex capable relays, we enable simultaneous bidirectional in-band cut-through transmissions between two
far apart nodes, so they can do full duplex communications as if
they were within each other’s transmission range. To design such a
system, we analyze interference patterns and propose a loop-back
interference cancellation strategy. We identify the power amplification problem at relay nodes and develop an algorithm to solve it.
We also develop a routing algorithm, an opportunistic forwarding
scheme, and a real-time feedback strategy to leverage this system
in ad-hoc networks. To evaluate the real world performance of
BiPass, we build a prototype and conduct experiments using software defined radios. We show that BiPass can achieve 1.6x median
throughput gain over state-of-the-art one-way cut-through systems, and 4.09x gain over the decode-and-forward scheme. Our
simulations further reveal that even when the data traffic is not
bidirectional, BiPass has 1.36x throughput gain and 47% delay reduction over one-way cut-through systems in large networks.

",14.836745963215662,14.114174887892379,228
MobiCom_17_010.txt,18.365727301458342,17.198585309800922,MobiCom,10570,"
 example, ITU [29] identifies three broad classes of 5G services (en
Emerging 5G mobile networks are envisioned to become multiservice environments, enabling the dynamic deployment of services
with a diverse set of performance requirements, accommodating the
needs of mobile network operators, verticals and over-the-top (OTT)
service providers. Virtualizing the mobile network in a flexible way
is of paramount importance for a cost-effective realization of this
vision. While virtualization has been extensively studied in the case
of the mobile core, virtualizing the radio access network (RAN) is
still at its infancy. In this paper, we present Orion, a novel RAN slicing system that enables the dynamic on-the-fly virtualization of base
stations, the flexible customization of slices to meet their respective
service needs and which can be used in an end-to-end network slicing
setting. Orion guarantees the functional and performance isolation of
slices, while allowing for the efficient use of RAN resources among
them. We present a concrete prototype implementation of Orion for
LTE, with experimental results, considering alternative RAN slicing
approaches, indicating its efficiency and highlighting its isolation capabilities. We also present an extension to Orion for accommodating
the needs of OTT providers.

",19.7143461543385,18.32628858578607,200
MobiCom_17_011.txt,14.973952276038585,12.982507305297933,MobiCom,8944,"

Simultaneously using multiple network paths (e.g., WiFi and cellular) is an attractive feature on mobile devices. A key component in
a multipath system such as MPTCP is the scheduler, which determines how to distribute the traffic over multiple paths. In this paper,
we propose DEMS, a new multipath scheduler aiming at reducing
the data chunk download time. DEMS consists of three key design
decisions: (1) being aware of the chunk boundary and strategically
decoupling the paths for chunk delivery, (2) ensuring simultaneous
subflow completion at the receiver side, and (3) allowing a path to
trade a small amount of redundant data for performance. We have
implemented DEMS on smartphones and evaluated it over both
emulated and real cellular/WiFi networks. DEMS is robust to diverse network conditions and brings significant performance boost
compared to the default MPTCP scheduler (e.g., median download
time reduction of 33%—-48% for fetchingfi les and median loading
time reduction of 6%-43% for fetching web pages), and even more
benefits compared to other state-of-the-art schedulers.

",18.243605946275583,16.39026011560694,176
MobiCom_17_012.txt,14.813371553149928,13.101179160028625,MobiCom,8564,"
Expected increase in cellular demand has pushed recent interest
in picocell networks which have reduced cell sizes (100-200m or
less). For ease of deployment of such networks, a wireless backhaul
network is highly desired. Since RF-based technologies are unlikely
to provide the desired multi-gigabit data rates, we motivate and
explore use of free space optics (FSO) for picocell backhaul. In particular, we present a novel network architecture based on steerable
links and sufficiently many robust short-range links, to help circumvent the key challenge of outdoor effects in reliable operation
of outdoor FSO links. Our architecture is motivated by the fact that,
due to the high density of picocells, many short-range links will
occur naturally in a picocell backhaul. Moreover, use of steerable
FSO links facilitates networks with sufficient redundancy while using only a small number of interfaces per node. We address the key
problems that arise in the context of such a backhaul architecture,
viz., an FSO link design with desired characteristics, and related
network design and management problems. We develop and evaluate a robust 100m FSO link prototype, and simulate the proposed
architecture in many metro US cities while show its viability via
evaluation of key performance metrics.
",16.52667757954773,15.352058823529415,205
MobiCom_17_013.txt,15.81931358415682,13.738982549797775,MobiCom,7612,"

Near-field communication (NFC) plays a crucial role in the
operation of mobile devices to enhance applications such as
payment, social networks, private communication, gaming,
and etc. Despite of the convenience, existing NFC standards
like ISO-13157 require additional hardware (e.g., loop antenna
and dedicated chip) and thereby hindering their wide-scale applications. In this work, we seek to propose a novel near-field
communication protocol, MagneComm, which utilizes Magnetic Induction (MI) signals emitted from CPUs and captured
by magnetometers on mobile devices for communication. Since
CPUs and magnetometers are readily available components in
mobile devices, MagneComm eliminates the requirement for
special hardware and complements existing near-field communication protocols by providing additional bandwidth. We
systematically analyze the characteristics of magnetic signals
of CPUs and facilitate MagneComm with one-way communication, full-duplex communication, and multi-transmitter
schemes in accordance with the hardware availability on devices. We prototype MagneComm on both laptops and smartphones. Extensive evaluation results show that MagneComm
achieves up to 110 bps within 10 cm.

",19.287186520377343,18.21285714285715,170
MobiCom_17_014.txt,14.859188103807103,13.126081765468644,MobiCom,5780,"

This paper investigates the feasibility of practical backscatter communication using visible light for battery-free IoT applications.
Based on the idea of modulating the light retroreflection with a
commercial LCD shutter, we effectively synthesize these off-theshelf optical components into a sub-mW low power visible light
passive transmitter along with a retroreflecting uplink design dedicated for power constrained mobile/IoT devices. On top of that, we
design, implement and evaluate PassiveVLC, a novel visible light
backscatter communication system. PassiveVLC system enables
a battery-free tag device to perform passive communication with
the illuminating LEDs over the same light carrier and thus offers
several favorable features including battery-free, sniff-proof, and
biologically friendly for human-centric use cases. Experimental results from our prototyped system show that PassiveVLC is flexible
with tag orientation, robust to ambient lighting conditions, and
can achieve up to 1 kbps uplink speed. Link budget analysis and
two proof-of-concept applications are developed to demonstrate
PassiveVLC’s efficacy and practicality.

",19.946969662950774,19.317560975609755,166
MobiCom_17_015.txt,16.190718739347417,14.743928534469834,MobiCom,9555,"
As a popular approach to implementing Visible Light Communication (VLC) on commercial-off-the-shelf devices, LED-Camera VLC
has attracted substantial attention recently. While such systems
initially used reflected light as the communication media, direct
light becomes the dominant media for the purpose of combating
interference. Nonetheless, the data rate achievable by direct light
LED-Camera VLC systems has hit its bottleneck: the dimension of
the transmitters. In order to further improve the performance, we
revisit the reflected light approach and we innovate in converting
the potentially destructive interferences into collaborative transmissions. Essentially, our ReflexCode system codes information by
superposing light emissions from multiple transmitters. It combines
traditional amplitude demodulation with slope detection to “decode”
the grayscale modulated signal, and it tunes decoding thresholds
dynamically depending on the spatial symbol distribution. In addition, ReflexCode re-engineers the balanced codes to avoid flicker
from individual transmitters. We implement ReflexCode as two
prototypes and demonstrate that it can achieve a throughput up to
3.2kb/s at a distance of 3m.
",18.903936251131103,16.737507396449704,171
MobiCom_17_016.txt,14.885834840832597,13.189869651411211,MobiCom,8747,"
The past decade’s research in visible light positioning (VLP) has led
to technologies with high location precision. However, existing VLP
systems either require specialized LEDs which hinder large-scale
deployment, or need cameras which preclude continuous localization because of high power consumption and short coverage. In this
paper, we propose Pulsar, which uses a compact photodiode sensor, readily fit into a mobile device, to discriminate existing ceiling
lights—either fluorescent lights or conventional LEDs—based on
their intrinsic optical emission features. To overcome the photodiode’s lack of spatial resolution, we design a novel sparse photogrammetry mechanism, which resolves the light source’s angle-of-arrival,
and triangulates the device’s 3D location and even orientation. To
facilitate ubiquitous deployment, we further develop a light registration mechanism that automatically registers the ceiling lights’
locations as landmarks on a building’s floor map. Our experiments
demonstrate that Pulsar can reliably achieve decimeter precision
in both controlled environment and large-scale buildings.
",19.454632303725965,18.528205128205133,162
MobiCom_17_017.txt,15.416572895510072,14.365293840679549,MobiCom,4293,"

Long-range low-power localization is a key technology that enables
a host of new applications of wireless sensor nodes. We present
RF-Echo, a new low-power RF localization solution that achieves
decimeter accuracy in long range indoor non-line-of-sight (NLOS)
scenarios. RF-Echo introduces a custom-designed active RF reflector
ASIC (application specific integrated circuit) fabricated in a 180nm
CMOS process which echoes a frequency-shifted orthogonal frequency division multiplexing (OFDM) signal originally generated
from an anchor. The proposed technique is based on time-of-flight
(ToF) estimation in the frequency domain that effectively eliminates
inter-carrier and inter-symbol interference in multipath-rich indoor
NLOS channels. RF-Echo uses a relatively narrow bandwidth of <80
MHz which does not require an expensive very high sampling rate
analog-to-digital converter (ADC). Unlike ultra-wideband (UWB)
systems, the active reflection scheme is designed to operate at a relatively low carrier frequency that can penetrate building walls and
other blocking objects for challenging NLOS scenarios. Since the
bandwidth at lower frequencies (2.4 GHz and sub-1 GHz) is severely
limited, we propose novel signal processing algorithms as well as
machine learning techniques to significantly enhance the localization resolution given the bandwidth constraint of the proposed
system. The newly fabricated tag IC consumes 62.8 mW active
power. The software defined radio (SDR) based anchor prototype
is rapidly deployable without the need for accurate synchronization among anchors and tags. Field trials conducted in a university
building confirm up to 85 m operation with decimeter accuracy for
robust 2D localization.

",17.693802365651003,17.375206896551727,263
MobiCom_17_018.txt,15.820766093800078,14.269529066865008,MobiCom,10113,"
The current mechanisms for locating spectrum offenders are time
consuming, human-intensive, and expensive. In this paper, we propose a novel approach to locate spectrum offenders using crowdsourcing. In such a participatory sensing system, privacy and bandwidth concerns preclude distributed mobile sensing devices from
reporting raw signal samples to a central agency; instead, devices
would be limited to measurements of received power. However,
this limit enables a smart attacker to evade localization by simultaneously transmitting from multiple infected devices. Existing
localization methods are insufficient or incapable of locating multiple sources when the powers from each source cannot be separated
at the receivers. In this paper, we first propose a simple and efficient method that simultaneously locates multiple transmitters
using the received power measurements from mobile devices. Second, we build sampling approaches to select mobile sensing devices
required for localization. Next, we enhance our sampling to also
take into account incentives for participation in crowdsourcing. We
experimentally evaluate our localization framework under a variety
of settings and find that we are able to localize multiple sources
transmitting simultaneously with reasonably high accuracy in a
timely manner.
",17.5058628484301,17.302342342342346,186
MobiCom_17_019.txt,14.91580806107439,12.759309721762499,MobiCom,5039,"

State-of-the-art RFID localization systems fall under two categories.
The first category operates with off-the-shelf narrowband RFID
tags but makes restrictive assumptions on the environment or the
tag’s movement patterns. The second category does not make such
restrictive assumptions; however, it requires designing new ultrawideband hardware for RFIDs and uses the large bandwidth to
directly compute a tag’s 3D location. Hence, while the first category
is restrictive, the second one requires replacing the billions of RFIDs
already produced and deployed annually.

This paper presents RFind, a new technology that brings the
benefits of ultra-wideband localization to the billions of RFIDs in
today’s world. RFind does not require changing today’s passive narrowband RFID tags. Instead, it leverages their underlying physical
properties to emulate a very large bandwidth and uses it for localization. Our empirical results demonstrate that RFind can emulate
over 220MHz of bandwidth on tags designed with a communication bandwidth of only tens to hundreds of kHz, while remaining
compliant with FCC regulations. This, combined with a new superresolution algorithm over this bandwidth, enables RFind to perform
3D localization with sub-centimeter accuracy in each of the x/y/z
dimensions, without making any restrictive assumptions on the
tag’s motion or the environment.

",16.99224021665606,15.378141025641025,214
MobiCom_17_020.txt,13.63655155881623,11.630339668119031,MobiCom,9804,"
In this paper, we design and develop R��, a novel battery-free touch
sensing user interface (UI) primitive for future IoT and smart spaces.
R�� enables UIs to be constructed using o�-the-shelf RFID readers
and tags, and provides a unique approach to designing smart IoT
spaces. With R��, any surface can be turned into a touch-aware
surface by simply attaching RFID tags to them. R�� also supports
custom-designed RFID tags, and thus allows specially customized
UIs to be easily deployed into a real-world environment.
R�� is built using the technique of impedance tracking: when a
human �nger touches the surface of an RFID tag, the impedance
of the antenna changes. This change manifests as a change in the
phase of the RFID backscattered signal, and is used by R�� to track
�ne-grained touch movement over both o�-the-shelf and custombuilt tags. We study this impedance behavior in-depth and show
how R�� is a reliable UI primitive that is robust even within a multitag environment. We leverage this primitive to build a prototype of
R�� that can continuously locate a �nger during a swipe movement
to within 3mm of its actual position. We also show how customdesign RFID tags can be built and used with R��, and provide two
example applications that demonstrate its real-world use.
",14.232682905230785,12.74167341430499,225
MobiCom_17_021.txt,14.072644863515443,12.029840350866248,MobiCom,8793,"

With parallel decoding for backscatter communication, tags are
allowed to transmit concurrently and more efficiently. Existing parallel decoding mechanisms, however, assume that signals of the
tags are highly stable, and hence may not perform optimally in
the naturally dynamic backscatter systems. This paper introduces
FlipTracer, a practical system that achieves highly reliable parallel
decoding even in hostile channel conditions. FlipTracer is designed
with a key insight: although the collided signal is time-varying
and irregular, transitions between signals’ combined states follow
highly stable probabilities, which offers important clues for identifying the collided signals, and provides us with an opportunity to
decode the collided signals without relying on stable signals. Motivated by this observation, we propose a graphical model, called
one-flip-graph (OFG), to capture the transition pattern of collided
signals, and design a reliable approach to construct the OFG in a
manner robust to the diversity in backscatter systems. Then FlipTracer can resolve the collided signals by tracking the OFG. We
have implemented FlipTracer and evaluated its performance with
extensive experiments across a wide variety of scenarios. Our experimental results have shown that FlipTracer achieves a maximum
aggregated throughput that approaches 2 Mbps, which is 6x higher
than the state-of-the-art.

",19.430816780756558,18.91296918767507,205
MobiCom_17_022.txt,14.443242374586497,12.73580242783974,MobiCom,9364,"

Target imaging and material identification play an important role
in many real-life applications. This paper introduces TagScan, a
system that can identify the material type and image the horizontal cut of a target simultaneously with cheap commercial offthe-shelf (COTS) RFID devices. The key intuition is that different
materials and target sizes cause different amounts of phase and
RSS (Received Signal Strength) changes when radio frequency (RF)
signal penetrates through the target. Multiple challenges need to
be addressed before we can turn the idea into a functional system
including (i) indoor environments exhibit rich multipath which
breaks the linear relationship between the phase change and the
propagation distance inside a target; (ii) without knowing either
material type or target size, trying to obtain these two information
simultaneously is challenging; and (iii) stitching pieces of the propagation distances inside a target for an image estimate is non-trivial.

We propose solutions to all the challenges and evaluate the
system’s performance in three different environments. TagScan is
able to achieve higher than 94% material identification accuracies
for 10 liquids and differentiate even very similar objects such as
Coke and Pepsi. TagScan can accurately estimate the horizontal cut
images of more than one target behind a wall.

",18.848423458724294,18.568013937282235,207
MobiCom_17_023.txt,12.374306866263186,10.107466999338921,MobiCom,7470,"

Deployment of billions of Commercial off-the-shelf (COTS) RFID
tags has drawn much of the attention of the research community
because of the performance gaps of current systems. In particular,
hash-enabled protocol (HEP) is one of the most thoroughly studied
topics in the past decade. HEPs are designed for a wide spectrum of
notable applications (e.g., missing detection) without need to collect
all tags. HEPs assume that each tag contains a hash function, such
that a tag can select a random but predicable time slot to reply witha
one-bit presence signal that shows its existence. However, the hash
function has never been implemented in COTS tags in reality, which
makes HEPs a 10-year untouchable mirage. This work designs and
implements a group of analog on-tag hash primitives (called Tash)
for COTS Gen2-compatible RFID systems, which moves prior HEPs
forward from theory to practice. In particular, we design three
types of hash primitives, namely, tash function, tash table function
and tash operator. All of these hash primitives are implemented
through selective reading, which is a fundamental and mandatory
functionality specified in Gen2 protocol, without any hardware
modification and fabrication. We further apply our hash primitives
in two typical HEP applications (i.e., cardinality estimation and
missing detection) to show the feasibility and effectiveness of Tash.
Results from our prototype, which is composed of one ImpinJ reader
and 3, 000 Alien tags, demonstrate that the new design lowers 60%
of the communication overhead in the air. The tash operator can
additionally introduce an overhead drop of 29.7%.

",15.549919911452193,14.051258741258742,264
MobiCom_17_024.txt,15.478515440852622,14.032488505747128,MobiCom,10198,"

Continuous authentication is of great importance to maintain the
security level of a system throughout the login session. The goal
of this work is to investigate a trustworthy, continuous, and noncontact user authentication approach based on a heart-related biometric that works in a daily-life environment. To this end, we
present a novel, continuous authentication system, namely Cardiac Scan, based on geometric and non-volitional features of the
cardiac motion. Cardiac motion is an automatic heart deformation
caused by self-excitement of the cardiac muscle, which is unique
to each user and is difficult (if not impossible) to counterfeit. Cardiac Scan features intrinsic liveness detection, unobtrusiveness,
cost-effectiveness, and high usability. We prototype a remote, highresolution cardiac motion sensing system based on the smart DCcoupled continuous-wave radar. Fiducial-based invariant identity
descriptors of cardiac motion are extracted after the radar signal
demodulation. We conduct a pilot study with 78 subjects to evaluate
Cardiac Scan in accuracy, authentication time, permanence, evaluation in complex conditions, and vulnerability. Specifically, Cardiac
Scan achieves 98.61% balanced accuracy (BAC) and 4.42% equal
error rate (EER) in a real-world setup. We demonstrate that Cardiac
Scan is a robust and usable continuous authentication system.

",17.122413403193683,16.245371859296483,202
MobiCom_17_025.txt,15.828209658037853,13.871493153256218,MobiCom,8453,"

The ubiquity of mobile camera devices has been triggering an outcry of privacy concerns, whereas privacy protection still relies on
the cooperation of the photographer or camera hardware, which
can hardly be guaranteed in practice. In this paper, we introduce
LiShield, which automatically protects a physical scene against
photographing, by illuminating it with smart LEDs flickering in
specialized waveforms. We use a model-driven approach to optimize
the waveform, so as to ensure protection against the (uncontrollable) cameras and potential image-processing based attacks. We
have also designed mechanisms to unblock authorized cameras and
enable graceful degradation under strong ambient light interference.
Our prototype implementation and experiments show that LiShield
can effectively destroy unauthorized capturing while maintaining
robustness against potential attacks.

",19.287186520377343,18.325685950413227,122
MobiCom_17_026.txt,14.894173769609257,13.256734964334058,MobiCom,9281,"

Voice has become an increasingly popular User Interaction (UI)
channel, mainly contributing to the current trend of wearables,
smart vehicles, and home automation systems. Voice assistants
such as Alexa, Siri, and Google Now, have become our everyday
fixtures, especially when/where touch interfaces are inconvenient
or even dangerous to use, such as driving or exercising. The open
nature of the voice channel makes voice assistants difficult to secure,
and hence exposed to various threats as demonstrated by security
researchers. To defend against these threats, we present VAuth,
the first system that provides continuous authentication for voice
assistants. VAuth is designed to fit in widely-adopted wearable
devices, such as eyeglasses, earphones/buds and necklaces, where
it collects the body-surface vibrations of the user and matches it
with the speech signal received by the voice assistant’s microphone.
VAuth guarantees the voice assistant to execute only the commands
that originate from the voice of the owner. We have evaluated VAuth
with 18 users and 30 voice commands and find it to achieve 97%
detection accuracy and less than 0.1% false positive rate, regardless
of VAuth’s position on the body and the user’s language, accent or
mobility. VAuth successfully thwarts various practical attacks, such
as replay attacks, mangled voice attacks, or impersonation attacks.
It also incurs low energy and latency overheads and is compatible
with most voice assistants.

",16.860833078287435,15.192713864306786,231
MobiCom_17_027.txt,13.781424861263844,11.181347185599272,MobiCom,8505,"

Today’s mobile devices contain sensitive data, which raises concerns
about data security. This paper discusses a covert channel threat
on existing mobile systems. Through it, malware can wirelessly
leak information without making network connections or emitting
signals, such as sound, EMR, vibration, etc., that we can feel or are
aware of.

The covert channel is built on a communication method that
we call NICScatter. NICScatter transmitter malware forces mobile
devices, such as mobile phones, tablets or laptops, to reflect surrounding RF signals to covertly convey information. The operation
is achieved by controlling the impedance of a device’s wireless
network interface card (NIC). Importantly, the operation requires
no special privileges on current mobile OSs, which allows the malware to stealthily pass sensitive data to an attacker’s nearby mobile
device, which can then decode the signal and thus effectively gather
the guarded data. Our experiments with different mobile devices
show that the covert channel can achieve 1.6 bps and transmit as
far as 2 meters. In a through-the-wall scenario, it can transmit up
to 70 cm.

",14.554592549557764,13.35712121212121,181
MobiCom_17_028.txt,15.273867223339067,13.361909982988262,MobiCom,5012,"

Wearable devices such as smartwatches offer exciting new opportunities for users to interact with their applications. However, the
current wearable programming model requires the developer to
write a custom companion app for each wearable form factor; the
companion app extends the smartphone display onto the wearable, relays user interactions from the wearable to the phone, and
updates the wearable display as needed. The development effort
required to write a companion app is significant and will not scale
to an increasing diversity of form factors. This paper argues for a
different programming model for wearable devices. The developer
writes an application for the smartphone, but only specifies a UI design for the wearable. Our UlWear system abstracts a logical model
of the smartphone GUI, re-tailors the GUI for the wearable device
based on the specified UI design, and compiles it into a companion
app that we call the UICompanion app. We implemented UI Wear
on Android smartphones, AndroidWear smartwatches, and Sony
SmartEyeGlasses. We evaluate 20 developer-written companion
apps from the AndroidWear category on Google Play against the
UlIWear-created UlCompanion apps. The lines-of-code required for
the developer to specify the UI design in UIWear is an order-ofmagnitude smaller compared to the companion app lines-of-code.
Further, in most cases, the UICompanion app performed comparably
or better than the corresponding companion app both in terms of
qualitative metrics, including latency and energy, and quantitative
metrics, including look-and-feel.

",18.946978176291534,16.888971193415646,244
MobiCom_17_029.txt,15.216551707007177,13.134248714652955,MobiCom,9456,"

Rapid development is essential for IoT (Internet of Things)
application developers to obtain first-mover advantages and reduce
the development cost. In this paper, we present TinyLink, a holistic
system for rapid development of IoT applications. The key idea
of TinyLink is to use a top-down approach for designing both
the hardware and the software of IoT applications. Developers
write the application code in a C-like language to specify the
key logic of their applications, without dealing with the details
of the specific hardware components. Taking the application
code as input, TinyLink automatically generates the hardware
configuration as well as the binary program executable on the
target hardware platform. TinyLink provides unified APIs for
applications to interact with the underlying hardware components.
We implement TinyLink and evaluate its performance using realworld IoT applications. Results show that: (1) TinyLink achieves
rapid development of IoT applications, reducing 52.58% of lines
of code in average compared with traditional approaches; (2)
TinyLink searches a much larger design space and thus can generate
a superior solution for the hardware configuration, compared
with the state-of-the-art approach; (3) TinyLink incurs acceptable
overhead in terms of execution time and program memory.

",18.243605946275583,16.768560606060607,200
MobiCom_17_030.txt,15.862828436925408,14.408103221494521,MobiCom,7404,"

In this paper, we design a pluggable data management solution
for modern mobile platforms (e.g., Android). Our goal is to allow
data management mechanisms and policies to be implemented
independently of core app logic. Our design allows a user to install
data management solutions as apps, install multiple such solutions
on a single device, and choose a suitable solution each for one or
more apps. It allows app developers to focus their effort on app
logic and helps the developers of data management solutions to
achieve wider deployability. It also gives increased control of data
management to end users and allows them to use different solutions
for different apps.

We present a prototype implementation of our design called BlueMountain, and implement several data management solutions for
file and database management to demonstrate the utility and ease
of using our design. We perform detailed microbenchmarks as well
as end-to-end measurements for files and databases to demonstrate
the performance overhead incurred by our implementation.

",16.954822765917157,15.492264808362368,166
MobiCom_17_031.txt,16.53652801656669,15.316418570535593,MobiCom,8038,"

In this paper, we perform a systematic design study of the “elephant in the room” facing the VR industry — is it feasible to enable
high-quality VR apps on untethered mobile devices such as smartphones? Our quantitative, performance-driven design study makes
two contributions. First, we show that the QoE achievable for highquality VR applications on today’s mobile hardware and wireless
networks via local rendering or offloading is about 10X away from
the acceptable QoE, yet waiting for future mobile hardware or
next-generation wireless networks (e.g., 5G) is unlikely to help,
because of power limitation and the higher CPU utilization needed
for processing packets under higher data rate.

Second, we present FurIon, a VR framework that enables highquality, immersive mobile VR on today’s mobile devices and wireless networks. FuRION exploits a key insight about the VR workload
that foreground interactions and background environment have
contrasting predictability and rendering workload, and employs
a split renderer architecture running on both the phone and the
server. Supplemented with video compression, use of panoramic
frames, and parallel decoding on multiple cores on the phone, we
demonstrate Furion can support high-quality VR apps on today’s
smartphones over WiFi, with under 14ms latency and 60 FPS (the
phone display refresh rate).

",18.946978176291534,19.64084541062802,212
MobiCom_17_032.txt,14.721198316581642,12.732537673687613,MobiCom,8323,"

High-end mobile GPUs are now becoming an integral part of mobile devices. However, a mobile GPU constitutes a major portion of
power consumption on the devices, and mobile games top as the
most popular class of graphics applications. This paper presents
the design and implementation of RAVEN, a novel, on-the-fly frame
rate scaling system for mobile gaming applications. RAVEN utilizes
human visual perception of graphics change to opportunistically
achieve power saving without degrading user experiences. The system develops a light-weight frame comparison technique to measure and predict perception-aware frame similarity. It also builds
a low resolution virtual display which clones the device screen
for performing similarity measurement at a low-power cost. It is
able to work on an existing commercial smartphone and support
applications from app stores without any modifications. It has been
implemented on Nexus 5X, and its performance has been measured
with 13 games. The system effectively reduces the overall power
consumption of mobile devices while maintaining satisfactory user
experiences. The power consumption is reduced by 21.78% on average and up to 34.74%.

",14.975302809339372,13.504359116022101,184
MobiCom_17_033.txt,15.235564472996568,13.437603337722447,MobiCom,7316,"

The most important step in an empirical computer scientist’s research is gathering sufficient real-world data to validate a system.
Unfortunately, it is also one of the most time-consuming and expensive tasks: placing measurement tools in remote networks or
end-clients requires one to marshal resources from different administrative domains, devices, populations, and countries. Often
such efforts culminate in a trace that is deficient in multiple ways: a
small set of test subjects, a short time frame, missing ground truth
for device IDs, networking environments lacking in diversity and
geographic spread, or highly biased sampling.

We present a method of addressing these challenges by leveraging the most open and globally accessible test and measurement
platform: digital advertising. Digital advertising instantly provides
a window into 7 billion devices spanning every county for an extremely low cost. We propose Advertising as a Platform (AaaP), an
ad-based system to perform massive-scale mobile measurement
studies. In contrast with measurements made by large media companies who own platforms, ad networks, and apps, we concentrate on
the opportunities and challenges for researchers that are end-users
of advertising systems. We evaluate a prototype system, discuss
ethical guidelines, and demonstrate its use in four scenarios: IP2Geo
databases, bandwidth measurement, energy management, and the
identifiability of mobile users. We show the efficacy and ease-of-use
of AaaP, and illuminate key challenges and the great promise of
using AaaP to study a wide variety of mobile phenomena.

",18.243605946275583,16.741294765840227,244
MobiCom_17_034.txt,14.774782872870603,14.007804948611604,MobiCom,10212,"

Despite much recent progress, Web page latencies over cellular networks remain much higher than those over wired networks. Proxies
that execute Web page JavaScript (JS) and push objects needed by
the client can reduce latency. However, a key concern is the scalability of the proxy which must execute JS for many concurrent users.
In this paper, we propose to scale the proxies, focusing on a design
where the proxy’s execution is solely to push the needed objects
and the client completely executes the page as normal. Such redundant execution is a simple, yet effective approach to cutting network
latencies, which dominate page load delays in cellular settings. We
develop whittling, a technique to identify and execute in the proxy
only the JS code necessary to identify and push the objects required
for the client page load, while skipping other code. Whittling is
closely related to program slicing, but with the important distinction that it is acceptable to approximate the program slice in the
proxy given the client’s complete execution. Experiments with top
Alexa Web pages show NuiShell can sustain, on average, 27% more
user requests per second than a proxy performing fully redundant
execution, while preserving, and sometimes enhancing, the latency
benefits.

",16.218646115125612,15.170371287128717,205
MobiCom_17_035.txt,15.015029642508686,13.701527950636734,MobiCom,10031,"

Traditional signal processing design (e.g., frequency offset and channel estimation) at a receiver treats each packet arrival as an independent process to facilitate decoding and interpreting packet data.
In this paper, we enhance the performance of this process in the
wireless network domain. We propose STAteful inter-Packet signaL procEssing (STAPLE), a framework of stateful signal processing residing between the physical and link layers. STAPLE transforms the signal processing procedure into a lightweight stateful
process that caches in a small-sized memory table physical and
link layer header fields as packet state information. The similarity of such information among packets serves as prior knowledge
to further enhance the reliability of signal processing and thus
improve the wireless network performance. We implement STAPLE on USRP X300-series devices with adapted configurations for
802.11a/b/g/n/ac and 802.15.4. The STAPLE prototype is of low
processing complexity and does not change any wireless standard
specification. Comprehensive experimental results show that the
benefit from STAPLE is universal in various wireless networks.

",16.061879428645646,15.480702662721896,174
NDSS_17_001.txt,13.64587246437003,12.0418075721395,NDSS,8892,"
Free cloud-based services are powerful candidates
for deploying ubiquitous encryption for messaging. In the case of
email and increasingly chat, users expect the ability to store and
search their messages persistently. Using data from a major mail
provider, we confirm that for a searchable encryption scheme
to scale to millions of users, it should be highly 1O-efficient
docality) and handle a very dynamic message corpi. We observe
that existing solutions fail to achieve both properties simultaneously. We then design, build, and evaluate a provably secure
Dynamic Searchable Symmetric Encryption (DSSE) scheme with
significant reduction in IO cost compared to preceding works
when used for email or other highly dynamic material.

",17.58133193835471,15.271,113
NDSS_17_002.txt,13.820485956694295,12.473204512481335,NDSS,9320,"
Oblivious RAM (ORAM) protocols are powerful
techniques that hide a client’s data as well as access patterns from
untrusted service providers. We present an oblivious cloud storage system, ObliviSync, that specifically targets one of the most
widely-used personal cloud storage paradigms: synchronization
and backup services, popular examples of which are Dropbox,
iCloud Drive, and Google Drive. This setting provides a unique
opportunity because the above privacy properties can be achieved
with a simpler form of ORAM called write-only ORAM, which
allows for dramatically increased efficiency compared to related
work. Our solution is asymptotically optimal and practically
efficient, with a small constant overhead of approximately 4x
compared with non-private file storage, depending only on the
total data size and parameters chosen according to the usage
rate, and not on the number or size of individual files. Our
construction also offers protection against timing-channel attacks,
which has not been previously considered in ORAM protocols.
We built and evaluated a full implementation of ObliviSync
that supports multiple simultaneous read-only clients and a
single concurrent read/write client whose edits automatically
and seamlessly propagate to the readers. We show that our
system functions under high work loads, with realistic file size
distributions, and with small additional latency (as compared to
a baseline encrypted file system) when paired with Dropbox as
the synchronization service.

",18.699421769314853,19.33107142857143,226
NDSS_17_003.txt,12.565423004934086,10.5423216782767,NDSS,20718,"
This paper presents TumbleBit, a new unidirectional unlinkable payment hub that is fully compatible with today’s Bitcoin protocol. TumbleBit allows
parties to make fast, anonymous, off-blockchain payments
through an untrusted intermediary called the Tumbler.
TumbleBit’s anonymity properties are similar to classic
Chaumian eCash: no one, not even the Tumbler, can link
a payment from its payer to its payee. Every payment
made via TumbleBit is backed by bitcoins, and comes with
a guarantee that Tumbler can neither violate anonymity,
nor steal bitcoins, nor “print money” by issuing payments
to itself. We prove the security of TumbleBit using the
real/ideal world paradigm and the random oracle model.
Security follows from the standard RSA assumption and
ECDSA unforgeability. We implement TumbleBit, mix
payments from 800 users and show that TumbleBit’s offblockchain payments can complete in seconds.

",14.955608360458719,14.025378151260504,140
NDSS_17_004.txt,15.64742272434031,13.770623115720166,NDSS,10363,"
Starting with Dining Cryptographers networks
(DC-nets), several peer-to-peer (P2P) anonymous communication
protocols have been proposed. However, despite their strong
anonymity guarantees, none of them have been employed in
practice so far: Most protocols fail to simultaneously address
the crucial problems of slot collisions and disruption by malicious peers, while the remaining ones handle f malicious peers
with O(f?) communication rounds. We conceptualize these P2P
anonymous communication protocols as P2P mixing, and present
a novel P2P mixing protocol, DiceMix, that needs only four
communication rounds in the best case, and 4+ 2f rounds
in the worst case with f malicious peers. As every individual
malicious peer can force a restart of a P2P mixing protocol by
simply omitting his messages, we find DiceMix with its worst-case
complexity of O(/) rounds to be an optimal P2P mixing solution.

On the application side, we employ DiceMix to improve
anonymity in crypto-currencies such as Bitcoin. The public
verifiability of their pseudonymous transactions through publicly
available ledgers (or blockchains) makes these systems highly
vulnerable to a variety of linkability and deanonymization attacks.
We use DiceMix to define CoinShuffle++, a coin mixing protocol
that enables pseudonymous peers to perform unlinkable transactions in a manner fully compatible with the current Bitcoin
system. Moreover, we demonstrate the efficiency of our protocols with a proof-of-concept implementation. In our evaluation,
DiceMix requires less than eight seconds to mix 50 messages
(160 bits, i.e., Bitcoin addresses), while the best protocol in the
literature requires almost three minutes in the same setting.

Finally, we present a deanonymization attack on existing P2P
mixing protocols that guarantee termination in the presence of
disruptive peers. We generalize the attack to demonstrate that
no P2P mixing protocol simultaneously supports arbitrary input
messages, provides anonymity, and terminates in the presence of
disruptive peers. DiceMix resists this attack by requiring fresh
input messages, e.g., cryptographic keys never used before.

",17.99237782463571,16.717204968944106,322
NDSS_17_005.txt,14.855860499964574,13.483162943905196,NDSS,11869,"
Credit networks model transitive trust (or credit)
between users in a distributed environment and have recently
seen a rapid increase of popularity due to their flexible design
and robustness against intrusion. They serve today as a backbone
of real-world JOweYou transaction settlement networks such
as Ripple and Stellar, which are deployed by various banks
worldwide, as well as several other systems, such as spamresistant communication protocols and Sybil-tolerant social networks. Current solutions, however, raise serious privacy concerns,
as the network topology as well as the credit value of the links are
made public for apparent transparency purposes and any changes
are logged. In payment scenarios, for instance, this means that
all transactions have to be public and everybody knows who paid
what to whom.

In this work, we question the necessity of a privacy-invasive
transaction ledger. In particular, we present SilentWhispers, the
first distributed, privacy-preserving credit network that does
not require any ledger to protect the integrity of transactions.
Yet, SilentWhispers guarantees integrity and privacy of link
values and transactions even in the presence of distrustful users
and malicious neighbors, whose misbehavior in changing link
values is detected and such users can be held accountable. We
formalize these properties as ideal functionalities in the universal composability framework and present a secure realization
based on a novel combination of secret-sharing-based multiparty computation and digital signature chains. SilentWhispers
can handle network churn, and it is efficient as demonstrated
with a prototype implementation evaluated using payments data
extracted from the currently deployed Ripple payment system.

",18.83193753551143,18.321007751937987,259
NDSS_17_006.txt,16.276087086728225,15.082919991756992,NDSS,6745,"
Developing a systematic understanding of the attack
surface of emergent networks, such as software-defined networks
(SDNs), is necessary and arguably the starting point toward
making it more secure. Prior studies have largely relied on
ad hoc empirical methods to evaluate the security of various
SDN elements from different perspectives. However, they have
stopped short of converging on a systematic methodology or
developing automated systems to rigorously test for security flaws
in SDNs. Thus, conducting security assessments of new SDN
software remains a non-replicable and unregimented process.
This paper makes the case for automating and standardizing
the vulnerability identification process in SDNs. As a first step,
we developed a security assessment framework, DELTA, that
reinstantiates published SDN attacks in diverse test environments.
Next, we enhanced our tool with a protocol-aware fuzzing module
to automatically discover new vulnerabilities. In our evaluation,
DELTA successfully reproduced 20 known attack scenarios across
diverse SDN controller environments and discovered seven novel
SDN application mislead attacks.

",17.693802365651003,17.58280864197531,163
NDSS_17_007.txt,15.995966180524189,14.259072714166262,NDSS,9841,"
Despite soaring investments in IT infrastructure,
the state of operational network security continues to be abysmal.
We argue that this is because existing enterprise security approaches fundamentally lack precision in one or more dimensions: (1) isolation to ensure that the enforcement mechanism does
not induce interference across different principals; (2) context
to customize policies for different devices; and (3) agility to
rapidly change the security posture in response to events. To
address these shortcomings, we present PSI, a new enterprise
network security architecture that addresses these pain points.
PSI enables fine-grained and dynamic security postures for
different network devices. These are implemented in isolated
enclaves and thus provides precise instrumentation on these above
dimensions by construction. To this end, PSI leverages recent
advances in software-defined networking (SDN) and network
functions virtualization (NFV). We design expressive policy abstractions and scalable orchestration mechanisms to implement
the security postures. We implement PSI using an industry-grade
SDN controller (OpenDaylight) and integrate several commonly
used enforcement tools (e.g., Snort, Bro, Squid). We show that
PSI is scalable and is an enabler for new detection and prevention capabilities that would be difficult to realize with existing
solutions.

",19.287186520377343,16.903264604810996,196
NDSS_17_008.txt,14.345824882216437,12.104944034547533,NDSS,9620,"
Traditional hardware-based firewall appliances are
placed at fixed locations with fixed capacity. Such nature makes
them difficult to protect today’s prevailing virtualized environments. Two emerging networking paradigms, Network Function
Virtualization (NFV) and Software-Defined Networking (SDN),
offer the potential to address these limitations. NFV envisions
to implement firewall function as software instance (a.k.a virtual
firewall). Virtual firewalls provide great flexibility and elasticity,
which are necessary to protect virtualized environments. In
this paper, we propose to build an innovative virtual firewall
controller, VFW Controller, to enable safe, efficient and costeffective virtual firewall elasticity control. VFW Controller
addresses four key challenges with respect to semantic consistency,
correct flow update, buffer overflow avoidance, and optimal scaling
in virtual firewall scaling. To demonstrate the feasibility of our approach, we implement the core components of VFW Controller
on top of NFV and SDN environments. Our experimental results
demonstrate that VFW Controller is efficient to provide safe
elasticity control of virtual firewalls.

",17.631426480028413,16.617154989384286,161
NDSS_17_009.txt,14.272637045436174,11.976062186034333,NDSS,13124,"
Hypervisors have quickly become essential but are
vulnerable to attack. Unfortunately, efficiently hardening hypervisors is challenging because they lack a privileged security monitor
and decomposition strategies. In this work we systematically
analyze the 191 Xen hypervisor vulnerabilities from Xen Security
Advisories, revealing that the majority (144) are in the core
hypervisor not Dom0. We then use the analysis to provide
a novel deconstruction of Xen, called Nexen, into a security
monitor, a shared service domain, and per-VM Xen slices that
are isolated by a least-privileged sandboxing framework. We
implement Nexen using the Nested Kernel architecture, efficiently
nesting itself within the Xen address space, and extend the
Nested Kernel design by adding services for arbitrarily many
protection domains along with dynamic allocators, data isolation,
and cross-domain control-flow integrity. The effect is that Nexen
confines VM-based hypervisor compromises to single Xen VM
instances, thwarts 74% (107/144) of known Xen vulnerabilities,
and enforces Xen code integrity (defending against all code
injection compromises) while observing negligible overhead (1.2%
on average). Overall, we believe that Nexen is uniquely positioned
to provide a fundamental need for hypervisor hardening at
minimal performance and implementation costs.

",19.99310930976973,19.365994108983802,196
NDSS_17_010.txt,17.082342470521912,15.372129453399154,NDSS,9899,"
Web developers routinely rely on third-party JavaScript libraries such as jQuery to enhance the functionality of
their sites. However, if not properly maintained, such dependencies can create attack vectors allowing a site to be compromised.

In this paper, we conduct the first comprehensive study of
client-side JavaScript library usage and the resulting security
implications across the Web. Using data from over 133k websites,
we show that 37 % of them include at least one library with a
known vulnerability; the time lag behind the newest release of
a library is measured in the order of years. In order to better
understand why websites use so many vulnerable or outdated
libraries, we track causal inclusion relationships and quantify
different scenarios. We observe sites including libraries in ad hoc
and often transitive ways, which can lead to different versions of
the same library being loaded into the same document at the same
time. Furthermore, we find that libraries included transitively, or
via ad and tracking code, are more likely to be vulnerable. This
demonstrates that not only website administrators, but also the
dynamic architecture and developers of third-party services are
to blame for the Web’s poor state of library management.

The results of our work underline the need for more thorough
approaches to dependency management, code maintenance and
third-party code inclusion on the Web.

",16.99224021665606,15.085333333333338,227
NDSS_17_011.txt,15.52897104605378,14.942027895694363,NDSS,13095,"
In this paper, we present ChromePic, a web browser
equipped with a novel forensic engine that aims to greatly enhance
the browser’s logging capabilities. ChromePic’s main goal is to
enable a fine-grained post-mortem reconstruction and trace-back
of web attacks without incurring the high overhead of record-andreplay systems. In particular, we aim to enable the reconstruction
of attacks that target users and have a significant visual component,
such as social engineering and phishing attacks. To this end,
ChromePic records a detailed snapshot of the state of a web
page, including a screenshot of how the page is rendered and a
“deep” DOM snapshot, at every significant interaction between
the user and the page. If an attack is later suspected, these finegrained logs can be used to reconstruct the attack and trace back
the sequence of steps the user followed to reach the attack page.

We develop ChromePic by implementing several careful
modifications and optimizations to the Chromium code base, to
minimize overhead and make always-on logging practical. We
then demonstrate that ChromePic can successfully capture and
aid the reconstruction of attacks on users. Our evaluation includes
the analysis of an in-the-wild social engineering download attack
on Android, a phishing attack, and two different clickjacking
attacks, as well as a user study aimed at accurately measuring the
overhead introduced by our forensic engine. The experimental results show that browsing snapshots can be logged very efficiently,
making the logging events practically unnoticeable to users.

",16.860833078287435,16.234892473118283,251
NDSS_17_012.txt,13.671512792727292,12.00556445722469,NDSS,12882,"
In this paper, we propose a browser fingerprinting
technique that can track users not only within a single browser
but also across different browsers on the same machine. Specifically, our approach utilizes many novel OS and hardware level
features, such as those from graphics cards, CPU, and installed
writing scripts. We extract these features by asking browsers
to perform tasks that rely on corresponding OS and hardware
functionalities.

Our evaluation shows that our approach can successfully
identify 99.24% of users as opposed to 90.84% for state of the
art on single-browser fingerprinting against the same dataset.
Further, our approach can achieve higher uniqueness rate than
the only cross-browser approach in the literature with similar
stability.

",13.348371206891418,14.51377777777778,120
NDSS_17_013.txt,15.567089340810082,14.854129455967254,NDSS,6836,"
Recommender systems have become an essential
component in a wide range of web services. It is believed that
recommender systems recommend a user items (e.g., videos on
YouTube, products on Amazon) that match the user’s preference.
In this work, we propose new attacks to recommender systems.
Our attacks exploit fundamental vulnerabilities of recommender
systems and can spoof a recommender system to make recommendations as an attacker desires. Our key idea is to inject fake
co-visitations to the system. Given a bounded number of fake
co-visitations that an attacker can inject, two key challenges are
1) which items the attacker should inject fake co-visitations to,
and 2) how many fake co-visitations an attacker should inject
to each item. We address these challenges via modelling our
attacks as constrained linear optimization problems, by solving
which the attacker can perform attacks with maximal threats. We
demonstrate the feasibility and effectiveness of our attacks via
evaluations on both synthetic data and real-world recommender
systems on several popular web services including YouTube,
eBay, Amazon, Yelp, and LinkedIn. We also discuss strategies
to mitigate our attacks.

",16.678067442207542,15.62550675675676,188
NDSS_17_014.txt,14.832645929483483,13.548000695679061,NDSS,7576,"
In this work we present a systematic presentation
attack against ECG biometrics. We demonstrate the attack’s
effectiveness using the Nymi Band, a wrist band that uses electrocardiography (ECG) as a biometric to authenticate the wearer.
We instantiate the attack using a hardware-based Arbitrary
Waveform Generator (AWG), an AWG software using a computer
sound card, and the playback of ECG signals encoded as .wav
files using an off-the-shelf audio player. In two sets of experiments
we collect data from a total of 41 participants using a variety of
ECG monitors, including a medical monitor, a smartphone-based
mobile monitor and the Nymi Band itself.

We use the first dataset to understand the statistical differences in
biometric features that arise from using different measurement
devices and modes. Such differences are addressed through
the automated derivation of so-called mapping functions, whose
purpose is to transform ECG signals from any device in order to
resemble the morphology of the signals recorded with the Nymi
Band.

As part of our second dataset, we enroll users into the Nymi Band
and test whether data from any of our sources can be used for a
signal injection attack. Using data collected directly on the Nymi
Band we achieve a success rate of 81%. When only using data
gathered on other devices, this rate decreases to 43% when using
raw data, and 62% after applying the mapping function. While
we demonstrate the attack on the Nymi Band, we expect other
ECG-based authentication systems to most likely suffer from the
same, fundamental weaknesses.

",15.6451,15.19516988416989,261
NDSS_17_015.txt,14.920652073942598,13.335525293508294,NDSS,8082,"
Selecting and remembering secure passwords puts
a high cognitive burden on the user, which has adverse effects
on usability and security. Authentication schemes based on
implicit memory can relieve the user of the burden of actively
remembering a secure password. In this paper, we propose a
new authentication scheme (Mooney Auth) that relies on implicitly
remembering the content of previously seen Mooney images.
These images are thresholded two-tone images derived from
images containing single objects. Our scheme has two phases:
In the enrollment phase, a user is presented with Mooney images,
their corresponding original images, and labels. This creates an
implicit link between the Mooney image and the object in the
user’s memory that serves as the authentication secret. In the
authentication phase, the user has to label a set of Mooney images,
a task that gets performed with substantially fewer mistakes if
the images have been seen in the enrollment phase. We applied
an information-theoretical approach to compute the eligibility of
the user, based on which images were labeled correctly. This new
dynamic scoring is substantially better than previously proposed
static scoring by considering the surprisal of the observed events.
We built a prototype and performed three experiments with
230 and 70 participants over the course of 264 and 21 days,
respectively. We show that MooneyAuth outperforms current
implicit memory-based schemes, and demonstrates a promising
new approach for fallback authentication procedures on the Web.

",15.903189008614273,14.51456837280367,240
NDSS_17_016.txt,14.894920296744225,13.490258123602146,NDSS,10374,"
Accelerometer-based gait recognition for mobile
healthcare systems has became an attractive research topic in
the past years. However, a major bottleneck of such system is
it requires continuous sampling of accelerometer, which reduces
battery life of wearable sensors. In this paper, we present KEHGait, which advocates use of output voltage signal from kinetic
energy harvester (KEH) as the source for gait recognition. KEHGait is motivated by the prospect of significant power saving by
not having to sample the accelerometer at all. Indeed, our measurements show that, compared to conventional accelerometerbased gait detection, KEH-Gait can reduce energy consumption
by 78.15%. The feasibility of KEH-Gait is based on the fact
that human gait has distinctive movement patterns for different
individuals, which is expected to leave distinctive patterns for
KEH as well. We evaluate the performance of KEH-Gait using
two different types of KEH hardware on a data set of 20 subjects.
Our experiments demonstrate that, although KEH-Gait yields
slightly lower accuracy than accelerometer-based gait detection
when single step is used, the accuracy problem can be overcome
by the proposed Multi-Step Sparse Representation Classification
(MSSRC). We discuss the advantages and limitations of our
approach in detail and give practical insights to the use of KEH
in a real-world environment.

",16.18397175987059,15.353426791277261,216
NDSS_17_017.txt,14.217855583891009,12.336890197095173,NDSS,10269,"
How to choose a strong but still easily memorable
password? An often recommended advice is to memorize a
random sentence (the mnemonic) and to concatenate the words’
initials: a so-called mnemonic password. The paper in hand
analyzes the effectiveness of this advice—in terms of the obtained
password strength—and sheds light on various related aspects.
While it is infeasible to obtain a sufficiently large sample of
human-chosen mnemonics, the password strength depends only
on the distribution of certain character probabilities. We provide
several pieces of evidence that these character probabilities
are approximately the same for human-chosen mnemonics and
sentences from a web crawl and exploit this connection for our
analyses. The presented analyses are independent of cracking
software, avoid privacy concerns, and allow full control over
the details of how passwords are generated from sentences. In
particular, the paper introduces the following original research
contributions: (1) construction of one of the largest corpora of
human-chosen mnemonics, (2) construction of two web sentence
corpora from the 27.3 TB ClueWeb12 web crawl, (3) demonstration of the suitability of web sentences as substitutes for
mnemonics in password strength analyses, (4) improved estimation of password probabilities by position-dependent language
models, and (5) analysis of the obtained password strength using
web sentence samples of different sentence complexity and using
18 generation rules for mnemonic password construction.

Our findings include both expected and less expected results,
among others: mnemonic passwords from lowercase letters only
provide comparable strength to mnemonic passwords that exploit
the 7-bit visible ASCII character set, less complex mnemonics
reduce password strength in offline scenarios by less than expected, and longer mnemonic passwords provide more security
in an offline but not necessarily in an online scenario. When
compared to passwords generated by uniform sampling from a
dictionary, distributions of mnemonic passwords can reach the
same strength against offline attacks with less characters.

",20.161218678152366,20.536190476190473,317
NDSS_17_018.txt,13.99909708044013,12.475589532184106,NDSS,11486,"
Pattern lock is widely used as a mechanism for
authentication and authorization on Android devices. In this
paper, we demonstrate a novel video-based attack to reconstruct
Android lock patterns from video footage filmed using a mobile
phone camera. Unlike prior attacks on pattern lock, our approach
does not require the video to capture any content displayed on the
screen. Instead, we employ a computer vision algorithm to track
the fingertip movements to infer the pattern. Using the geometry
information extracted from the tracked fingertip motions, our approach is able to accurately identify a small number of (often one)
candidate patterns to be tested by an adversary. We thoroughly
evaluated our approach using 120 unique patterns collected from
215 independent users, by applying it to reconstruct patterns from
video footage filmed u sing s martphone c ameras. Experimental
results show that our approach can break over 95% of the
patterns in five attempts before the device is automatically locked
by the Android system. We discovered that, in contrast to many
people’s belief, complex patterns do not offer stronger protection
under our attacking scenarios. This is demonstrated by the fact
that we are able to break all but one complex patterns (with a
97.5% success rate) as opposed to 60% of the simple patterns in
the first attempt. Since our threat model is common in day-to-day
lives, our work calls for the community to revisit the risks of
using Android pattern lock to protect sensitive information.

",14.836745963215662,14.728829959514172,250
NDSS_17_019.txt,15.963369337078237,15.480504468980026,NDSS,10154,"
In technical support scams, cybercriminals attempt
to convince users that their machines are infected with malware
and are in need of their technical support. In this process, the
victims are asked to provide scammers with remote access to their
machines, who will then “diagnose the problem’, before offering
their support services which typically cost hundreds of dollars.
Despite their conceptual simplicity, technical support scams are
responsible for yearly losses of tens of millions of dollars from
everyday users of the web.

In this paper, we report on the first systematic study of
technical support scams and the call centers hidden behind them.
We identify malvertising as a major culprit for exposing users
to technical support scams and use it to build an automated
system capable of discovering, on a weekly basis, hundreds of
phone numbers and domains operated by scammers. By allowing
our system to run for more than 8 months we collect a large
corpus of technical support scams and use it to provide insights
on their prevalence, the abused infrastructure, the illicit profits,
and the current evasion attempts of scammers. Finally, by setting
up a controlled, IRB-approved, experiment where we interact
with 60 different scammers, we experience first-hand their social
engineering tactics, while collecting detailed statistics of the
entire process. We explain how our findings can be used by
law-enforcing agencies and propose technical and educational
countermeasures for helping users avoid being victimized by
technical support scams.

",16.678067442207542,16.833119834710747,243
NDSS_17_020.txt,15.841000111198408,13.801038024027843,NDSS,9973,"
This paper proposes a technique for automatically
learning semantic malware signatures for Android from very
few samples of a malware family. The key idea underlying our
technique is to look for a maximally suspicious common subgraph
(MSCS) that is shared between all known instances of a malware
family. An MSCS describes the shared functionality between
multiple Android applications in terms of inter-component call
relations and their semantic metadata (e.g., data-flow properties).
Our approach identifies such maximally suspicious common
subgraphs by reducing the problem to maximum satisfiability.
Once a semantic signature is learned, our approach uses a
combination of static analysis and a new approximate signature
matching algorithm to determine whether an Android application
matches the semantic signature characterizing a given malware
family.

We have implemented our approach in a tool called ASTROID
and show that it has a number of advantages over state-of-theart malware detection techniques. First, we compare the semantic
malware signatures automatically synthesized by ASTROID with
manually-written signatures used in previous work and show that
the signatures learned by ASTROID perform better in terms of accuracy as well as precision. Second, we compare ASTROID against
two state-of-the-art malware detection tools and demonstrate its
advantages in terms of interpretability and accuracy. Finally,
we demonstrate that ASTROID’s approximate signature matching
algorithm is resistant to behavioral obfuscation and that it can
be used to detect zero-day malware. In particular, we were able
to find 22 instances of zero-day malware in Google Play that are
not reported as malware by existing tools.

",18.026119701940384,17.06076923076923,263
NDSS_17_021.txt,13.02633155566079,11.087448087643683,NDSS,8614,"
The rise in popularity of the Android platform
has resulted in an explosion of malware threats targeting it. As
both Android malware and the operating system itself constantly
evolve, it is very challenging to design robust malware mitigation
techniques that can operate for long periods of time without
the need for modifications or costly re-training. In this paper,
we present MAMADROID, an Android malware detection system
that relies on app behavior. MAMADROID builds a behavioral
model, in the form of a Markov chain, from the sequence of
ABSTRACT
",14.554592549557764,13.505477528089887,90
NDSS_17_022.txt,15.285583974216582,13.402632584104598,NDSS,11321,"
Our understanding of exploit documents as a vector
to deliver targeted malware is limited to a handful of studies
done in collaboration with the Tibetans, Uyghurs, and political
dissidents in the Middle East. In this measurement study, we
present a complementary methodology relying only on publicly
available data to capture and analyze targeted attacks with both
greater scale and depth. In particular, we detect exploit documents uploaded over one year to a large anti-virus aggregator
(VirusTotal) and then mine the social engineering information
they embed to infer their likely targets and contextual information
of the attacks. We identify attacks against two ethnic groups
(Tibet and Uyghur) as well as 12 countries spanning America,
Asia, and Europe. We then analyze the exploit documents
dynamically in sandboxes to correlate and compare the exploited
vulnerabilities and malware families targeting different groups.
Finally, we use machine learning to infer the role of the uploaders
of these documents to VirusTotal (i.e., attacker, targeted victim, or
third-party), which enables their classification based only on their
metadata, without any dynamic analysis. We make our datasets
available to the academic community.

",18.848423458724294,18.18957528957529,187
NDSS_17_023.txt,14.654222194248383,12.867322166973086,NDSS,9221,"
The growing commoditization of the underground
economy has given rise to malware delivery networks, which
charge fees for quickly delivering malware or unwanted software
to a large number of hosts. A key method to provide this service
is through the orchestration of silent delivery campaigns. These
campaigns involve a group of downloaders that receive remote
commands and then deliver their payloads without any user
interaction. These campaigns can evade detection by relying on
inconspicuous downloaders on the client side and on disposable
domain names on the server side.

We describe Beewolf, a system for detecting silent delivery
campaigns from Internet-wide records of download events. The
key observation behind our system is that the downloaders
involved in these campaigns frequently retrieve payloads in
lockstep. Beewolf identifies such locksteps in an unsupervised
and deterministic manner, and can operate on streaming data.
We utilize Beewolf to study silent delivery campaigns at scale, on a
data set of 33.3 million download events. This investigation yields
novel findings, e.g. malware distributed through compromised
software update channels, a substantial overlap between the
delivery ecosystems for malware and unwanted software, and
several types of business relationships within these ecosystems.
Beewolf achieves over 92% true positives and fewer than 5% false
positives. Moreover, Beewolf can detect suspicious downloaders
a median of 165 days ahead of existing anti-virus products and
payload-hosting domains a median of 196 days ahead of existing
blacklists.

",15.062637766997835,14.735009671179888,238
NDSS_17_024.txt,14.482151218341794,13.261819208770174,NDSS,12197,"
 —Several recent standards, including NIST SP 80056A and RFC 5114, advocate the use of “DSA” parameters
for Diffie-Hellman key exchange. While it is possible to use
such parameters securely, additional validation checks are
necessary to prevent well-known and potentially devastating
attacks. In this paper, we observe that many Diffie-Hellman
implementations do not properly validate key exchange inputs.
Combined with other protocol properties and implementation
choices, this can radically decrease security. We measure the
prevalence of these parameter choices in the wild for HTTPS,
POP3S, SMTP with STARTTLS, SSH, IKEv1, and IKEv2,
finding millions of hosts using DSA and other non-“safe”
primes for Diffie-Hellman key exchange, many of them in
combination with potentially vulnerable behaviors. We examine
over 20 open-source cryptographic libraries and applications
and observe that until January 2016, not a single one validated
subgroup orders by default. We found feasible full or partial
key recovery vulnerabilities in OpenSSL, the Exim mail server,
the Unbound DNS client, and Amazon’s load balancer, as well
as susceptibility to weaker attacks in many other applications.

",16.785175570968402,16.402423756019264,180
NDSS_17_025.txt,15.083296314225315,12.974978318308953,NDSS,9893,"
Software implementations of discrete logarithm
based cryptosystems over finite fields typically make the assumption that any domain parameters they encounter define cyclic
groups for which the discrete logarithm problem is assumed to be
hard. In this paper we explore this trust assumption and examine
situations where it may not be justified. In particular we focus on
groups for which the order is unknown and not easily determined,
and explore the scenario in which the modulus is trapdoored to
make computing discrete logarithms efficient for an entity with
knowledge of the trapdoor, while simultaneously leaving its very
existence as matter of speculation to everyone else.

We conducted an investigation of discrete logarithm domain
parameters in use across the Internet and discovered a multitude
of instances of groups of unknown order in use in TLS and
STARTTLS spanning numerous countries, organizations, and
implementations. Although our disclosures resulted in a number
of organizations taking down their suspicious parameters, none
were able or willing to rule out the possibility that their
parameters were trapdoors, and obtaining conclusive evidence
in each case could be as hard as factoring an RSA modulus,
highlighting a key feature of this attack method—deniability.

",21.374148478970945,21.88871794871795,196
NDSS_17_026.txt,15.508451927288071,14.632624343920721,NDSS,10145,"
WireGuard is a secure network tunnel, operating
at layer 3, implemented as a kernel virtual network interface for
Linux, which aims to replace both IPsec for most use cases, as well
as popular user space and/or TLS-based solutions like OpenVPN,
while being more secure, more performant, and easier to use.
The virtual tunnel interface is based on a proposed fundamental
principle of secure tunnels: an association between a peer public
key and a tunnel source IP address. It uses a single round trip
key exchange, based on NoiseIK, and handles all session creation
transparently to the user using a novel timer state machine
mechanism. Short pre-shared static keys—Curve25519 points—
are used for mutual authentication in the style of OpenSSH. The
protocol provides strong perfect forward secrecy in addition to a
high degree of identity hiding. Transport speed is accomplished
using ChaCha20Poly1305 authenticated-encryption for encapsulation of packets in UDP. An improved take on IP-binding cookies
is used for mitigating denial of service attacks, improving greatly
on IKEv2 and DTLS’s cookie mechanisms to add encryption
and authentication. The overall design allows for allocating no
resources in response to received packets, and from a systems
perspective, there are multiple interesting Linux implementation
techniques for queues and parallelism. Finally, WireGuard can
be simply implemented for Linux in less than 4,000 lines of code,
making it easily audited and verified.
",16.32212239822248,15.772380952380953,234
NDSS_17_027.txt,16.106462988628074,14.80274355557422,NDSS,10348,"
As HTTPS deployment grows, middlebox and antivirus products are increasingly intercepting TLS connections to
retain visibility into network traffic. In this work, we present a
comprehensive study on the prevalence and impact of HTTPS interception. First, we show that web servers can detect interception
by identifying a mismatch between the HTTP User-Agent header
and TLS client behavior. We characterize the TLS handshakes
of major browsers and popular interception products, which
we use to build a set of heuristics to detect interception and
identify the responsible product. We deploy these heuristics at
three large network providers: (1) Mozilla Firefox update servers,
(2) a set of popular e-commerce sites, and (3) the Cloudflare
content distribution network. We find more than an order of
magnitude more interception than previously estimated and with
dramatic impact on connection security. To understand why
security suffers, we investigate popular middleboxes and clientside security software, finding that nearly all reduce connection
security and many introduce severe vulnerabilities. Drawing on
our measurements, we conclude with a discussion on recent
proposals to safely monitor HTTPS and recommendations for
the security community.

",17.833180683606166,16.86676912568306,184
NDSS_17_028.txt,13.323984736798653,10.66154509383503,NDSS,11090,"
Oblivious Transfer (OT) is one of the most fundamental cryptographic primitives with wide-spread application in
general secure multi-party computation (MPC) as well as in a
number of tailored and special-purpose problems of interest such
as private set intersection (PSI), private information retrieval
(PIR), contract signing to name a few. Often the instantiations
of OT require prohibitive communication and computation complexity. OT extension protocols are introduced to compute a very
large number of OTs referred as extended OTs at the cost of a
small number of OTs referred as seed OTs.

We present a fast OT extension protocol for small secrets
in active setting. Our protocol when used to produce 1-out-ofn OTs outperforms all the known actively secure OT extensions.
Our protocol is built on the semi-honest secure extension protocol
of Kolesnikov and Kumaresan of CRYPTO’13 (referred as KK13
protocol henceforth) which is the best known OT extension
for short secrets. At the heart of our protocol lies an efficient
consistency checking mechanism that relies on the linearity of
Walsh-Hadamard (WH) codes. Asymptotically, our protocol adds
a communication overhead of O(log «) bits over KK13 protocol
irrespective of the number of extended OTs, where « and yj refer
to computational and statistical security parameter respectively.
Concretely, our protocol when used to generate a large enough
number of OTs adds only 0.011-0.028% communication overhead
and 4-6% runtime overhead both in LAN and WAN over KK13
extension. The runtime overheads drop below 2% when in
addition the number of inputs of the sender in the extended
OTs is large enough.

As an application of our proposed extension protocol, we show
that it can be used to obtain the most efficient PSI protocol secure
against a malicious receiver and a semi-honest sender.

",17.64278748958908,16.12042098840757,301
NDSS_17_029.txt,14.31882978962457,12.730764557717205,NDSS,12713,"
Secure two-party computation (S2PC) allows two
parties to compute a function on their joint inputs while leaking
only the output of the function. At TCC 2009 Orlandi and Nielsen
proposed the LEGO protocol for maliciously secure 2PC based
on cut-and-choose of Yao’s garbled circuits at the gate level and
showed that this is asymptotically more efficient than on the
circuit level. Since then the LEGO approach has been improved
upon in several theoretical works, but never implemented. In
this paper we describe further concrete improvements and
provide the first implementation of a protocol from the LEGO
family. Our protocol has a constant number of rounds and is
optimized for the offline/online setting with function-independent
preprocessing. We have benchmarked our prototype and find
that our protocol can compete with all existing implementations
and that it is often more efficient. As an example, in a LAN
setting we can evaluate an AES-128 circuit with online latency
down to 1.13 ms, while if evaluating 128 AES-128 circuits in
parallel the amortized cost is 0.09 ms per AES-128. This online
performance does not come at the price of offline inefficiency
as we achieve comparable performance to previous, less general
protocols, and significantly better if we ignore the cost of the
function-independent preprocessing. Also, as our protocol has an
optimal 2-round online phase it is significantly more efficient than
previous protocols when considering a high latency network.

Keywords—Secure Two-party Computation, Implementation,
LEGO, XOR-Homomorphic Commitments, Selective OT-Attack

",16.15616582465906,15.224803921568629,259
NDSS_17_030.txt,14.466712497583387,12.783658500280257,NDSS,11921,"
Secure two-party computation has witnessed significant efficiency improvements in the recent years. Current
implementations of protocols with security against passive adversaries generate and process data much faster than it can be
sent over the network, even with a single thread. This paper
introduces novel methods to further reduce the communication
bottleneck and round complexity of semi-honest secure two-party
computation. Our new methodology creates a trade-off between
communication and computation, and we show that the added
computing cost for each party is still feasible and practicable in
light of the new communication savings. We first improve communication for Boolean circuits with 2-input gates by factor 1.9x
when evaluated with the protocol of Goldreich-Micali-Wigderson
(GMW). As a further step, we change the conventional Boolean
circuit representation from 2-input gates to multi-input/multioutput lookup tables (LUTs) which can be programmed to realize
arbitrary functions. We construct two protocols for evaluating
LUTs offering a trade-off between online communication and
total communication. Our most efficient LUT-based protocol
reduces the communication and round complexity by a factor
2-4x for several basic and complex operations. Our proposed
scheme results in a significant overall runtime decrease of up to
a factor of 3x on several benchmark functions.

",16.45884130781739,16.258546603475516,213
NDSS_17_031.txt,13.939241396957868,11.312267238452606,NDSS,10601,"
In recent years, there has been a growing trend
towards outsourcing of computational tasks with the development
of cloud services. The Gentry’s pioneering work of fully homomorphic encryption (FHE) and successive works have opened
a new vista for secure and practical cloud computing. In this
paper, we consider performing statistical analysis on encrypted
data. To improve the efficiency of the computations, we take
advantage of the batched computation based on the ChineseRemainder-Theorem. We propose two building blocks that work
with FHE: a novel batch greater-than primitive, and matrix
primitive for encrypted matrices. With these building blocks, we
construct secure procedures and protocols for different types of
statistics including the histogram (count), contingency table (with
cell suppression) for categorical data; k-percentile for ordinal
data; and principal component analysis and linear regression for
numerical data. To demonstrate the effectiveness of our methods,
we ran experiments in five real datasets. For instance, we can
compute a contingency table with more than 50 cells from 4000
of data in just 5 minutes, and we can train a linear regression
model with more than 40k of data and dimension as high as
6 within 15 minutes. We show that the FHE is not as slow as
commonly believed and it becomes feasible to perform a broad
range of statistical analysis on thousands of encrypted data.

",17.122413403193683,14.974678624813155,225
NDSS_17_032.txt,15.752299065969407,14.426259209401238,NDSS,12342,"
Hidden sensitive operations (HSO) such as stealing
privacy user data upon receiving an SMS message are increasingly
utilized by mobile malware and other potentially-harmful apps
(PHAs) to evade detection. Identification of such behaviors is
hard, due to the challenge in triggering them during an app’s
runtime. Current static approaches rely on the trigger conditions
or hidden behaviors known beforehand and therefore cannot
capture previously unknown HSO activities. Also these techniques
tend to be computationally intensive and therefore less suitable for
analyzing a large number of apps. As a result, our understanding
of real-world HSO today is still limited, not to mention effective
means to mitigate this threat.

In this paper, we present HSOMINER, an innovative machinelearning based program analysis technique that enables a largescale discovery of unknown HSO activities. Our approach leverages a set of program features that characterize an HSO branch’
and can be relatively easy to extract from an app. These features
summarize a set of unique observations about an HSO condition,
its paths and the relations between them, and are designed to be
general for finding hidden suspicious behaviors. Particularly, we
found that a trigger condition is less likely to relate to the path
of its branch through data flows or shared resources, compared
with a legitimate branch. Also, the behaviors exhibited by the
two paths of an HSO branch tend to be conspicuously different
(innocent on one side and sinister on the other). Most importantly,
even though these individual features are not sufficiently accurate
for capturing HSO on their own, collectively they are shown to be
highly effective in identifying such behaviors. This differentiating
power is harnessed by HSOMINER to classify Android apps,
which achieves a high precision (>98%) and coverage (>94%),
and is also efficient as discovered in our experiments. The new tool
was further used in a measurement study involving 338,354 realworld apps, the largest one ever conducted on suspicious hidden
operations. Our research brought to light the pervasiveness of
HSO activities, which are present in 18.7% of the apps we
analyzed, surprising trigger conditions (e.g., click on a certain
region of a view) and behaviors (e.g., hiding operations in a
dynamically generated receiver), which help better understand
the problem and contribute to more effective defense against this
new threat to the mobile platform.

",18.160329797683527,17.256041666666665,388
NDSS_17_033.txt,14.162744176799645,12.88309455308892,NDSS,11584,"
The massive growth of transaction via third-party
cashier has attracted numerous mobile apps to embed in-app
payment functionality. Although this feature makes the payment
easy within apps, transactions via current third-party in-app
payment involve more sophisticated interactions between multiple
participants compared to those using traditional payments. The
implementations in mobile apps also lack security considerations.
Therefore, such transaction exposes new attack vectors and could
be exploited more easily, leading to serious deceptions such as
payment forging.

To investigate current third-party mobile payment ecosystem
and find potential security threats, we conduct an in-depth analysis on world’s largest mobile payment market—China’s mobile
payment market. We study four mainstream third-party mobile
payment cashiers, and conclude unified security rules that must
be regulated by both cashier and merchant. We also illustrate
the serious consequences of violating these security rules, which
may cause up to four types of attacks against online and offline
transactions. Besides, we detect the seven security rule violations
to the payment in Android apps. Our detection result shows not
only the prevalence of third-party in-app payment, but also the
awful status quo of its security. Over 37% Android apps with
at least 100,000 users embed third-party payment functionality.
Hundreds of them violate security rule(s) and face with various
potential security risks, allowing an attacker to consume almost
every aspect of commodities or services in life without actually
purchasing them or deceiving others to pay for them. Our further
investigation reveals that the cashiers not only have improperly
designed SDK, which may expand the attack effects, but also
release ambiguous documents and even vulnerable sample codes,
directly leading to the mistakes committed by merchants. Besides
the cashiers’ ignorance for security, our successful exploits to
several apps show that these flawed implementations can cause
financial loss in real world. We have reported these findings to
all the related parties and received positive feedbacks.

",15.719379583869454,15.404223602484475,326
NDSS_17_034.txt,14.997867933668577,13.961342149751925,NDSS,9714,"
Android graphic user interface (GUI) system plays
an important role in rendering app GUIs on display and
interacting with users. However, the security of this critical subsystem remains under-investigated. In fact, Android GUI has been
plagued by a variety of GUI attacks in recent years. GUI attack
refers to any harmful behavior that attempts to adversely affect
the integrity or availability of the GUIs belonging to other apps.
These attacks are real threats and can cause severe consequences,
such as sensitive user information leakage, user device denial
of service, etc. Given the seriousness and rapid growth of GUI
attacks, we are in a pressing need for a comprehensive defense
solution. Nevertheless, existing defense methods fall short in
defense coverage, effectiveness and practicality.

To overcome these challenges, we systematically scrutinize the
security implications of Android GUI system design and propose
a new security model, Android Window Integrity (AWI), to comprehensively protect the system against GUI attacks. The AWI
model defines the user session to be protected and the legitimacy
of GUI system states in the unique mobile GUI environment. By
doing so, it can protect a normal user session against arbitrary
manipulation by attackers, and still preserve the original user
experience. Our implementation, WindowGuard, enforces the
AWI model and responds to a suspicious behavior by briefing the
user about a security event and asking for the final decision from
the user. This design not only improves the detection accuracy,
but also makes WindowGuard more usable and practical to meet
diverse user needs. WindowGuard is implemented as an Xposed
module, making it practical to be quickly deployed on a large
number of user devices. Our evaluation shows that WindowGuard
can successfully detect all known GUI attacks, while yielding
small impacts on user experience and system performance.

",15.532846611407376,15.107857142857146,296
NDSS_17_035.txt,16.683762507886456,15.351645181452863,NDSS,9185,"
Mobile apps are notorious for collecting a wealth
of private information from users. Despite significant effort from
the research community in developing privacy leak detection tools
based on data flow tracking inside the app or through network
traffic analysis, it is still unclear whether apps and ad libraries
can hide the fact that they are leaking private information. In
fact, all existing analysis tools have limitations: data flow tracking
suffers from imprecisions that cause false positives, as well as false
negatives when the data flow from a source of private information
to a network sink is interrupted; on the other hand, network
traffic analysis cannot handle encryption or custom encoding.

We propose a new approach to privacy leak detection that
is not affected by such limitations, and it is also resilient to
obfuscation techniques, such as encoding, formatting, encryption,
or any other kind of transformation performed on private
information before it is leaked. Our work is based on blackbox differential analysis, and it works in two steps: first, it
establishes a baseline of the network behavior of an app; then,
it modifies sources of private information, such as the device
ID and location, and detects leaks by observing deviations in
the resulting network traffic. The basic concept of black-box
differential analysis is not novel, but, unfortunately, it is not
practical enough to precisely analyze modern mobile apps. In fact,
their network traffic contains many sources of non-determinism,
such as random identifiers, timestamps, and _ server-assigned
session identifiers, which, when not handled properly, cause too
much noise to correlate output changes with input changes.

The main contribution of this work is to make black-box differential analysis practical when applied to modern Android apps.
In particular, we show that the network-based non-determinism
can often be explained and eliminated, and it is thus possible to
reliably use variations in the network traffic as a strong signal
to detect privacy leaks. We implemented this approach in a
tool, called AGRIGENTO, and we evaluated it on more than one
thousand Android apps. Our evaluation shows that our approach
works well in practice and outperforms current state-of-the-art
techniques. We conclude our study by discussing several case
studies that show how popular apps and ad libraries currently
exfiltrate data by using complex combinations of encoding and
encryption mechanisms that other approaches fail to detect. Our
results show that these apps and libraries seem to deliberately
hide their data leaks from current approaches and clearly
demonstrate the need for an obfuscation-resilient approach such
as ours.

",17.99237782463571,18.103145539906105,428
NDSS_17_036.txt,15.529440392309397,13.235137288110725,NDSS,12003,"
Mobile apps have to satisfy various privacy requirements. Notably, app publishers are often obligated to provide a
privacy policy and notify users of their apps’ privacy practices.
But how can a user tell whether an app behaves as its policy
promises? In this study we introduce a scalable system to help
analyze and predict Android apps’ compliance with privacy
requirements. We discuss how we customized our system in a
collaboration with the California Office of the Attorney General.
Beyond its use by regulators and activists our system is also meant
to assist app publishers and app store owners in their internal
assessments of privacy requirement compliance.

Our analysis of 17,991 free Android apps shows the viability
of combining machine learning-based privacy policy analysis with
static code analysis of apps. Results suggest that 71% of apps that
lack a privacy policy should have one. Also, for 9,050 apps that
have a policy, we find many instances of potential inconsistencies
between what the app policy seems to state and what the code
of the app appears to do. In particular, as many as 41% of
these apps could be collecting location information and 17%
could be sharing such with third parties without disclosing so in
their policies. Overall, each app exhibits a mean of 1.83 potential
privacy requirement inconsistencies.

",16.35954948731386,13.14092577147623,222
NDSS_17_037.txt,15.340776996646387,13.515867511316731,NDSS,10614,"
Modern browsers such as Chrome and Edge deploy
constant blinding to remove attacker-controlled constants from
the JIT-compiled code. Without such a defense, attackers can
encode arbitrary shellcode in constants that get compiled to
executable code. In this paper, we review the security and
completeness of current constant blinding implementations. We
develop DACHSHUND, a fuzzing-driven framework to find userspecified constants in JIT-compiled code. DACHSHUND reveals
several cases in which JIT compilers of modern browsers fail
to blind constants, ranging from constants passed as function
parameters to blinded constants that second-stage code optimizers
revert to a non-protected form. To tackle this problem, we
then propose a JavaScript rewriting mechanism that removes
all constants from JavaScript code. We prototype this crossbrowser methodology as part of a Web proxy and show that
it can successfully remove all constants from JavaScript code.

",13.707050652182112,13.089030969030972,144
NDSS_17_038.txt,15.468761117006338,14.487719556519146,NDSS,7677,"
T U X 2 is a new distributed graph engine that bridges
graph computation and distributed machine learning.
T U X 2 inherits the benefits of an elegant graph computation model, efficient graph layout, and balanced parallelism to scale to billion-edge graphs; we extend and
optimize it for distributed machine learning to support
heterogeneity, a Stale Synchronous Parallel model, and
a new MEGA (Mini-batch, Exchange, GlobalSync, and
Apply) model.
We have developed a set of representative distributed
machine learning algorithms in T U X 2 , covering both supervised and unsupervised learning. Compared to implementations on distributed machine learning platforms,
writing these algorithms in T U X 2 takes only about 25%
of the code: Our graph computation model hides the detailed management of data layout, partitioning, and parallelism from developers. Our extensive evaluation of
T U X 2 , using large data sets with up to 64 billion edges,
shows that T U X 2 outperforms state-of-the-art distributed
graph engines PowerGraph and PowerLyra by an order of
magnitude, while beating two state-of-the-art distributed
machine learning systems by at least 48%.
",18.878054631974784,20.26124064171123,188
NDSS_17_039.txt,15.527207262804271,13.54850713968958,NDSS,10790,"
Reverse engineering of binary executables is a
difficult task which gets more involved by the way compilers
translate high-level concepts used in paradigms such as objectoriented programming into native code, as it is the case for C++.
Such code is harder to grasp than, e.g., traditional procedural
code, since it is generally more verbose and adds complexity
through features such as polymorphism or inheritance. Hence, a
deep understanding of interactions between instantiated objects,
their corresponding classes, and the connection between classes
would vastly reduce the time it takes an analyst to understand
the application. The growth in complexity in contemporary C++
applications only amplifies the effect.

In this paper, we introduce Marx, an analysis framework
to reconstruct class hierarchies of C++ programs and resolve
virtual callsites. We have evaluated the results on a diverse set
of large, real-world applications. Our experimental results show
that our approach achieves a high precision (93.2% of the hierarchies reconstructed accurately for Node,js, 88.4% for MySQL
Server) while keeping analysis times practical. Furthermore, we
show that, despite any imprecision in the analysis, the derived
information can be reliably used in classic software security
hardening applications without breaking programs. We showcase
this property for two applications built on top of the output of
our framework: viable protection and type-safe object reuse. This
demonstrates that, in addition to traditional reverse engineering
applications, Marx can aid in implementing concrete, valuable
tools e.g., in the domain of exploit mitigations.

",17.353723509956247,16.108,249
NDSS_17_040.txt,15.05375534093124,13.272172005949194,NDSS,10349,"
Kernel exploits constitute a powerful attack class
allowing attackers to gain full control over a system. Various
kernel hardening solutions have been proposed or deployed in
practice to protect the kernel against code injection (e.g., DEP)
or code-reuse exploits (e.g., CFI). However, the security of all
these hardening techniques relies heavily on the assumption that
kernel page tables cannot be manipulated, e.g., by means of dataonly attacks. Ensuring kernel page tables integrity is not only
essential for kernel security but also a challenging task in practice
since existing solutions require hardware trust anchors, costly
hypervisors, or inefficient integrity checks.

In this paper, we first motivate the importance of protecting
kernel page tables by presenting a data-only attack against page
tables to bypass the recently released CFI-based (Linux) kernel
hardening technique RAP. Thereafter, we present the design and
implementation of PT-Rand, the first practical solution to protect
kernel page tables that does not suffer from the mentioned deficiencies of previous proposals. PT-Rand randomizes the location
of page tables and tackles a number of challenges to ensure that
the location of page tables is not leaked. This effectively prevents
the attacker from manipulating access permissions of code pages,
thereby enabling secure enforcement of kernel exploit mitigation
technologies such as CFI. We extensively evaluate our prototype
implementation of PT-Rand for the current Linux kernel on the
popular Linux distribution Debian and report a low overhead
of 0.22% for common benchmarks. Moreover, we combine RAP
with PT-Rand to protect RAP against data-only attacks on kernel
page tables.

",17.122413403193683,16.380570881226053,266
NDSS_17_041.txt,15.748189266646701,14.85445109594512,NDSS,10083,"
Privilege separation has long been considered as a
fundamental principle in software design to mitigate the potential
damage of a security attack. Much effort has been given to
develop various privilege separation schemes where a monolithic
OS or hypervisor is divided into two privilege domains where
one domain is logically more privileged than the other even
if both run at an identical processor privilege level. We say
that privilege separation is intra-level if it is implemented for
software of a certain privilege level without any involvement
or assistance of more privileged software. In general, realizing
intra-level privilege separation mandates developers to rely on
certain security features of the underlying hardware. So far,
such development efforts however have been much less focused
on ARM architectures than on the Intel x86 family mainly
because the architectural provision of ARM security features
was relatively insufficient. Unlike on x86, as a result, there exists
no full intra-level scheme that can be universally applied to any
privilege level on ARM. However, as malware and attacks increase
against virtually every level of privileged software including
an OS, a hypervisor and even the highest privileged software
armored by TrustZone, we have been motivated to develop a
technique, named as Hilps, to realize true intra-level privilege
separation in all these levels of privileged software on ARM.
Pivotal to the success of Hilps is the support from a new hardware
feature of ARM’s latest 64-bit architecture, called TxSZ, which
we manipulate to elastically adjust the accessible virtual address
range for a program. In our experiments, we have applied Hilps
to retrofit the core software mechanisms for privilege separation
into existing system software and evaluated the performance of
the resulting system. According to the experimental results, the
system incurs on average just less than 1 % overhead; hence, we
conclude that Hilps is quite promising for practical use in real
deployments.

",19.48791578843652,18.908968253968258,317
NDSS_17_042.txt,13.832435717658147,11.708494258180746,NDSS,12541,"
Covert channels evade isolation mechanisms be-
tween multiple parties in the cloud. Especially cache covert
channels allow the transmission of several hundred kilobits
per second between unprivileged user programs in separate
virtual machines. However, caches are small and shared and
thus cache-based communication is susceptible to noise from
any system activity and interrupts. The feasibility of a reliable
cache covert channel under a severe noise scenario has not
been demonstrated yet. Instead, previous work relies on either
of the two contradicting assumptions: the assumption of direct
applicability of error-correcting codes, or the assumption that
noise effectively prevents covert channels.

In this paper, we show that both assumptions are wrong.
First, error-correcting codes cannot be applied directly, due
to the noise characteristics. Second, even with extraordinarily
high system activity, we demonstrate an error-free and high-
throughput covert channel. We provide the ﬁrst comprehensive
characterization of noise on cache covert channels due to cache
activity and interrupts. We build the ﬁrst robust covert channel
based on established techniques from wireless transmission proto-
cols, adapted for our use in microarchitectural attacks. Our error-
correcting and error-handling high-throughput covert channel
can sustain transmission rates of more than 45 KBps on Amazon
EC2, which is 3 orders of magnitude higher than previous covert
channels demonstrated on Amazon EC2. Our robust and error-
free channel even allows us to build an SSH connection between
two virtual machines, where all existing covert channels fail.
",15.021129683784007,14.256471193415642,244
NDSS_17_043.txt,18.925532839513412,17.73418103006929,NDSS,9519,"
Location privacy continues to attract significant
attentions in recent years, fueled by the rapid growth of locationbased services (LBSs) and smart mobile devices. Location obfuscation has been the dominating location privacy preserving
approach, which transforms the exact location of a mobile user
to a perturbed location before its public release. The notion
of location privacy has evolved from user-defined location kanonymity to two statistical quantification based privacy notions:
geo-indistinguishability and expected inference error. The former
promotes differential location privacy but does not protect location against inference attacks of Bayesian adversary with using
prior information, whereas the latter promotes the background
inference resilient location privacy but does not guarantee differential location privacy with respect to geo-indistinguishability. In
this paper we argue that geo-indistinguishability and expected
inference error are two complementary notions for location
privacy. We formally study the relationship between two privacy
notions. By leveraging this relationship and a personalized error
bound, we can effectively combine the two privacy notions. We
develop PIVE, a two-phase dynamic differential location privacy
framework. In Phase I, we take into account the user-defined
inference error threshold and the prior knowledge about the
user’s location to determine a subset of locations as the protection
location set for protecting the actual location by increasing
adversary’s expected location inference error. In Phase II, we
generate pseudo-locations (i.e., perturbed locations) in the way
that achieves differential privacy over the protection location set.
This two-phase location obfuscation is constructed dynamically
by leveraging the relationship between two privacy notions
based on adversary’s current prior information and user-specific
privacy requirements on different locations and at different
times. Experiments with real-world datasets demonstrate that
our PIVE approach effectively guarantees the two privacy notions
simultaneously and outperforms the existing mechanisms in terms
of adaptive privacy protection in presence of skewed locations and
computation efficiency.

",20.65956897547239,19.64564516129032,315
NDSS_17_044.txt,15.402429352523804,13.809155868595443,NDSS,5624,"
 The Resource Public Key Infrastructure (RPKI)
binds IP address blocks to owners’ public keys. RPKI enables
routers to perform Route Origin Validation (ROV), thus preventing devastating attacks such as IP prefix hijacking. Yet, despite
extensive effort, RPKI’s deployment is frustratingly sluggish,
leaving the Internet largely insecure. We tackle fundamental
questions regarding today’s RPKI’s deployment and security:
What is the adoption status of RPKI and ROV? What are the
implications for global security of partial adoption? What are the
root-causes for slow adoption? How can deployment be pushed
forward? We address these questions through a combination of
empirical analyses, a survey of over 100 network practitioners,
and extensive simulations. Our main contributions include the
following. We present the first study measuring ROV enforcement,
revealing disappointingly low adoption at the core of the Internet.
We show, in contrast, that without almost ubiquitous ROV
adoption by large ISPs significant security benefits cannot be
attained. We next expose a critical security vulnerability: about a
third of RPKI authorizations issued for IP prefixes do not protect
the prefix from hijacking attacks. We examine potential reasons
for scarce adoption of RPKI and ROV, including human error
in issuing RPKI certificates and inter-organization dependencies,
and present recommendations for addressing these challenges.

",15.195765016291148,13.67446601941748,210
NDSS_17_045.txt,15.721848024160572,14.573402329666887,NDSS,12612,"
Miulti-tenancy in the cloud usually leads to security
concerns over network isolation around each cloud tenant’s
virtual resources. However, verifying network isolation in cloud
virtual networks poses several unique challenges. The sheer size
of virtual networks implies a prohibitive complexity, whereas
the constant changes in virtual resources demand a short response time. To make things worse, such networks typically
allow fine-grained (e.g., VM-level) and distributed (e.g., security
groups) network access control. Those challenges can either
invalidate existing approaches or cause an unacceptable delay
which prevents runtime applications. In this paper, we present
TenantGuard, a scalable system for verifying cloud-wide, VMlevel network isolation at runtime. We take advantage of the
hierarchical nature of virtual networks, efficient data structures,
incremental verification, and parallel computation to reduce the
performance overhead of security verification. We implement our
approach based on OpenStack and evaluate its performance both
in-house and on Amazon EC2, which confirms its scalability and
efficiency (13 seconds for verifying 168 millions of VM pairs).
We further integrate TenantGuard with Congress, an OpenStack
policy service, to verify compliance with respect to isolation
requirements based on tenant-specific high-level security policies.

",18.243605946275583,16.984732297063903,197
NDSS_17_046.txt,14.428618550526132,13.0372899897127,NDSS,9048,"
Bridges are onion routers in the Tor Network whose
IP addresses are not public. So far, no global security analysis of
Tor bridges has been performed. Leveraging public data sources,
and two known Tor issues, we perform the first systematic study
on the security of the Tor bridges infrastructure. Our study covers
both the public infrastructure available to all Tor users, and the
previously unreported private infrastructure, comprising private
nodes for the exclusive use of those who know their existence.

Our analysis of the public infrastructure is twofold. First,
we examine the security implications of the public data in the
CollecTor service, identifying several pieces of data that may
be detrimental for the security of bridges. Then, we measure
security relevant properties of public bridges. Our results show
that the 55% of public bridges that carry clients are vulnerable to
aggressive blocking; that 90% of bridge clients use default bridges
that are trivial to identify; that the concurrent deployment of
Pluggable Transports in bridges reduces the security of the most
secure transports; and that running non-Tor services in the same
host as a bridge may harm its anonymity.

To study the private infrastructure, we use an approach to
discover 694 private bridges on the Internet and a novel technique
to track bridges across IP changes. We are first to measure the
size of the private bridge population (35% discovered bridges are
private) and to report existence of infrastructures that use private
proxies to forward traffic to backend bridges or relays. We use a
novel clustering approach to analyze the different infrastructures
using proxies and bridges, examining its hosting and security
properties. We provide an extensive discussion on the security
implications of our findings.

",15.579741850924794,14.913239436619719,285
NDSS_17_047.txt,15.008054331084871,13.788631678041892,NDSS,12350,"
Previous attacks that link the sender and receiver of
trafﬁc in the Tor network (“correlation attacks”) have generally
relied on analyzing trafﬁc from TCP connections. The TCP
connections of a typical client application, however, are often
accompanied by DNS requests and responses. This additional
trafﬁc presents more opportunities for correlation attacks. This
paper quantiﬁes how DNS trafﬁc can make Tor users more vul-
nerable to correlation attacks. We investigate how incorporating
DNS trafﬁc can make existing correlation attacks more powerful
and how DNS lookups can leak information to third parties
about anonymous communication. We (i) develop a method to
identify the DNS resolvers of Tor exit relays; (ii) develop a new
set of correlation attacks (DefecTor attacks) that incorporate DNS
trafﬁc to improve precision; (iii) analyze the Internet-scale effects
of these new attacks on Tor users; and (iv) develop improved
methods to evaluate correlation attacks. First, we ﬁnd that there
exist adversaries that can mount DefecTor attacks: for example,
Google’s DNS resolver observes almost 40% of all DNS requests
exiting the Tor network. We also ﬁnd that DNS requests often
traverse ASes that the corresponding TCP connections do not
transit, enabling additional ASes to gain information about Tor
users’ trafﬁc. We then show that an adversary that can mount a
DefecTor attack can often determine the website that a Tor user
is visiting with perfect precision, particularly for less popular
websites where the set of DNS names associated with that website
may be unique to the site. We also use the Tor Path Simulator
(TorPS) in combination with traceroute data from vantage points
co-located with Tor exit relays to estimate the power of AS-level
adversaries that might mount DefecTor attacks in practice.
",18.669449996058646,17.551666666666666,295
NDSS_17_048.txt,16.01975299936353,14.851723736458418,NDSS,12174,"
Tor users are vulnerable to deanonymization by
an adversary that can observe some Tor relays or some parts
of the network. We demonstrate that previous network-aware
path-selection algorithms that propose to solve this problem are
vulnerable to attacks across multiple Tor connections. We suggest
that users use trust to choose the paths through Tor that are
less likely to be observed, where trust is flexibly modeled as a
probability distribution on the location of the user’s adversaries,
and we present the Trust-Aware Path Selection algorithm for Tor
that helps users avoid traffic-analysis attacks while still choosing
paths that could have been selected by many other users. We
evaluate this algorithm in two settings using a high-level map of
Internet routing: (@) users try to avoid a single global adversary
that has an independent chance to control each Autonomous
System organization, Internet Exchange Point organization, and
Tor relay family, and (i) users try to avoid deanonymization by
any single country. We also examine the performance of TrustAware Path selection using the Shadow network simulator.

",18.243605946275583,19.616000000000003,179
NDSS_17_049.txt,14.874080194980653,12.93818210914868,NDSS,8240,"
A large volume of existing research attempts to
understand who uses Tor and how the network is used (and
misused). However, conducting measurements on the live Tor
network, if done improperly, can endanger the security and
anonymity of the millions of users who depend on the network
to enhance their online privacy. Indeed, several existing measurement studies of Tor have been heavily criticized for unsafe
research practices.

Tor needs privacy-preserving methods of gathering statistics.
The recently proposed PrivEx system demonstrates how data
can be safely collected on Tor using techniques from differential
privacy. However, as we demonstrate in this paper, the integrity
of the statistics reported by PrivEx is brittle under realistic
deployment conditions. An adversary who operates even a single
relay in the volunteer-operated anonymity network can arbitrarily influence the result of PrivEx queries. We argue that a safe
and useful data collection mechanism must provide both privacy
and integrity protections.

This paper presents HisTorc, a privacy-preserving statistics
collection scheme based on (c, 5)-differential privacy that is robust
against adversarial manipulation. We formalize the security
guarantees of HisTor€ and show using historical data from the
Tor Project that HisTor€ provides useful data collection and
reporting with low bandwidth and processing overheads.

",17.238542476582836,15.55469230769231,207
NDSS_17_050.txt,15.162239785752025,13.598859122920235,NDSS,9791,"
Traditional execution environments deploy Address
Space Layout Randomization (ASLR) to defend against memory
corruption attacks. However, Intel Software Guard Extension
(SGX), a new trusted execution environment designed to serve
security-critical applications on the cloud, lacks such an effective,
well-studied feature. In fact, we find that applying ASLR to SGX
programs raises non-trivial issues beyond simple engineering for
a number of reasons: 1) SGX is designed to defeat a stronger
adversary than the traditional model, which requires the address
space layout to be hidden from the kernel; 2) the limited memory
uses in SGX programs present a new challenge in providing a
sufficient degree of entropy; 3) remote attestation conflicts with
the dynamic relocation required for ASLR; and 4) the SGX
specification relies on known and fixed addresses for key data
structures that cannot be randomized.

This paper presents SGX-Shield, a new ASLR scheme designed for SGX environments. SGX-Shield is built on a secure
in-enclave loader to secretly bootstrap the memory space layout
with a finer-grained randomization. To be compatible with SGX
hardware (e.g., remote attestation, fixed addresses), SGX-Shield
is designed with a software-based data execution protection
mechanism through an LLVM-based compiler. We implement
SGX-Shield and thoroughly evaluate it on real SGX hardware. It
shows a high degree of randomness in memory layouts and stops
memory corruption attacks with a high probability. SGX-Shield
shows 7.61% performance overhead in running common microbenchmarks and 2.25% overhead in running a more realistic
workload of an HTTPS server.

",17.37919286519448,16.56117647058824,259
NDSS_17_051.txt,14.886041532666209,13.06254304105455,NDSS,8230,"
Intel Software Guard Extensions (SGX) is a
hardware-based Trusted Execution Environment (TEE) that enables secure execution of a program in an isolated environment,
called an enclave. SGX hardware protects the running enclave
against malicious software, including the operating system, hypervisor, and even low-level firmware. This strong security property
allows trustworthy execution of programs in hostile environments,
such as a public cloud, without trusting anyone (e.g., a cloud
provider) between the enclave and the SGX hardware. However,
recent studies have demonstrated that enclave programs are
vulnerable to accurate controlled-channel attacks conducted by a
malicious OS. Since enclaves rely on the underlying OS, curious
and potentially malicious OSs can observe a sequence of accessed
addresses by intentionally triggering page faults.

In this paper, we propose T-SGX, a complete mitigation
solution to the controlled-channel attack in terms of compatibility,
performance, and ease of use. T-SGX relies on a commodity
component of the Intel processor (since Haswell), called Transactional Synchronization Extensions (TSX), which implements a
restricted form of hardware transactional memory. As TSX is
implemented as an extension (i.e., snooping the cache protocol),
any unusual event, such as an exception or interrupt, that should
be handled in its core component, results in an abort of the
ongoing transaction. One interesting property is that the TSX
abort suppresses the notification of errors to the underlying OS.
This means that the OS cannot know whether a page fault has
occurred during the transaction. T-SGX, by utilizing this property
of TSX, can carefully isolate the effect of attempts to tap running
enclaves, thereby completely eradicating the known controlledchannel attack.

We have implemented T-SGX as a compiler-level scheme
to automatically transform a normal enclave program into a
secured enclave program without requiring manual source code
modification or annotation. We not only evaluate the security
properties of T-SGX, but also demonstrate that it could be
applied to all the previously demonstrated attack targets, such as
libjpeg, Hunspell, and FreeType. To evaluate the performance of
T-SGX, we ported 10 benchmark programs of nbench to the SGX
environment. Our evaluation results look promising. T-SGX is

an order of magnitude faster than the state-of-the-art mitigation
schemes. On our benchmarks, T-SGX incurs on average 50%
performance overhead and less than 30% storage overhead.

",16.49080716021325,15.21451691557452,389
NDSS_17_052.txt,17.371458375614885,15.921862072852722,NDSS,9614,"
In the past decade, we have come to rely on computers for various safety and security-critical tasks, such as securing
our homes, operating our vehicles, and controlling our finances.
To facilitate these tasks, chip manufacturers have begun including
trusted execution environments (TEEs) in their processors, which
enable critical code (e.g., cryptographic functions) to run in
an isolated hardware environment that is protected from the
traditional operating system (OS) and its applications. While
code in the untrusted environment (e.g., Android or Linux) is
forbidden from accessing any memory or state within the TEE,
the code running in the TEE, by design, has unrestricted access
to the memory of the untrusted OS and its applications. However,
due to the isolation between these two environments, the TEE has
very limited visibility into the untrusted environment’s security
mechanisms (e.g., kernel vs. application memory).

In this paper, we introduce BOOMERANG, a class of vulnerabilities that arises due to this semantic separation between the
TEE and the untrusted environment. These vulnerabilities permit
untrusted user-level applications to read and write any memory
location in the untrusted environment, including security-sensitive
kernel memory, by leveraging the TEE’s privileged position to
perform the operations on its behalf. BOOMERANG can be used
to steal sensitive data from other applications, bypass security
checks, or even gain full control of the untrusted OS.

To quantify the extent of this vulnerability, we developed an
automated framework for detecting BOOMERANG bugs within
the TEEs of popular mobile phones. Using this framework,
we were able to confirm the existence of BOOMERANG on
four different TEE platforms, affecting hundreds of millions
of devices on the market today. Moreover, we confirmed that,
in at least two instances, BOOMERANG could be leveraged to
completely compromise the untrusted OS (i.e., Android). While
the implications of these vulnerabilities are severe, defenses can be
quickly implemented by vendors, and we are currently in contact
with the affected TEE vendors to deploy adequate fixes. To this
end, we evaluated the two most promising defense proposals
and their inherent trade-offs. This analysis led the proposal of a
novel BOOMERANG defense, addressing the major shortcomings
of the existing defenses with minimal performance overhead. Our
findings have been reported to and verified by the corresponding
vendors, who are currently in the process of creating security
patches.

",19.7143461543385,17.87745535714286,391
NDSS_17_053.txt,14.493329413401852,12.686573244635184,NDSS,10807,"
 Program obfuscation is a central primitive in cryptography, and has important real-world applications in protecting
software from IP theft. However, well known results from the
cryptographic literature have shown that software only virtual
black box (VBB) obfuscation of general programs is impossible.
In this paper we propose HOP, a system (with matching theoretic
analysis) that achieves simulation-secure obfuscation for RAM
programs, using secure hardware to circumvent previous impossibility results. To the best of our knowledge, HOP is the first
implementation of a provably secure VBB obfuscation scheme in
any model under any assumptions.

HOP trusts only a hardware single-chip processor. We present
a theoretical model for our complete hardware design and prove
its security in the UC framework. Our goal is both provable
security and practicality. To this end, our theoretic analysis
accounts for all optimizations used in our practical design,
including the use of a hardware Oblivious RAM (ORAM), hardware scratchpad memories, instruction scheduling techniques
and context switching. We then detail a prototype hardware
implementation of HOP. The complete design requires 72% of the
area of a V7485t Field Programmable Gate Array (FPGA) chip.
Evaluated on a variety of benchmarks, HOP achieves an overhead
of 8x ~ 76x relative to an insecure system. Compared to all prior
(mot implemented) work that strives to achieve obfuscation, HOP
improves performance by more than three orders of magnitude.
We view this as an important step towards deploying obfuscation
technology in practice.

",15.091289759661642,13.855950413223141,242
NDSS_17_054.txt,14.616905787551424,12.859081438565813,NDSS,9125,"
Intel SGX, a new security capability in emerging
CPUs, allows user-level application code to execute in hardwareisolated enclaves. Enclave memory is isolated from all other
software on the system, even from the privileged OS or hypervisor. While being a promising hardware-rooted building block,
enclaves have severely limited capabilities, such as no native
access to system calls and standard OS abstractions. These OS
ABSTRACT
",14.191785591663535,13.621346153846158,66
NDSS_17_055.txt,15.341184030766492,13.715836515953715,NDSS,5337,"
Trustworthy operation of industrial control systems
(ICS) depends on secure code execution on the embedded programmable logic controllers (PLCs). The controllers monitor and
control the underlying physical plants such as electric power
grids and continuously report back the system status to human
operators.

We present HARVEY, ! a PLC rootkit that implements a
physics-aware stealthy attack against cyberphysical power grid
control systems. HARVEY sits within the PLC’s firmware below
the control logic and modifies control commands before they are
sent out by the PLC’s output modules to the physical plant’s
actuators. HARVEY replaces legitimate control commands with
malicious, adversary-optimal commands to maximize the damage
to the physical power equipment and cause large-scale failures. To
ensure system safety, the operators observe the status of the power
system by fetching system parameter values from PLC devices.
To conceal the maliciously caused anomalous behavior from
operators, HARVEY intercepts the sensor measurement inputs to
the PLC device. HARVEY simulates the power system with the legitimate control commands (which were intercepted/replaced with
malicious ones), and calculates/injects the sensor measurements
that operators would expect to see. We implemented HARVEY
on the widely spread Allen Bradley PLC and evaluated it on a
real-world electric power grid test-bed. The results empirically
prove HARVEY’s deployment feasibility in practice nowadays.

",15.429909175157395,14.848737373737379,221
NDSS_17_056.txt,17.536341497963626,17.036827837713727,NDSS,10783,"
The Internet-of-Things (IoT) has quickly evolved to
a new appified era where third-party developers can write apps
for IoT platforms using programming frameworks. Like other
appified platforms, e.g., the smartphone platform, the permission
system plays an important role in platform security. However,
design flaws in current IoT platform permission models have
been reported recently, exposing users to significant harm such as
break-ins and theft. To solve these problems, a new access control
model is needed for both current and future IoT platforms. In this
paper, we propose ContexIoT, a context-based permission system
for appified IoT platforms that provides contextual integrity by
supporting fine-grained context identification for sensitive actions,
and runtime prompts with rich context information to help users
perform effective access control. Context definition in ContexIoT
is at the inter-procedure control and data flow levels, that we
show to be more comprehensive than previous context-based
permission systems for the smartphone platform. ContexloT is
designed to be backward compatible and thus can be directly
adopted by current IoT platforms.

We prototype ContexIoT on the Samsung SmartThings platform, with an automatic app patching mechanism developed to
support unmodified commodity SmartThings apps. To evaluate
the system’s effectiveness, we perform the first extensive study
of possible attacks on appified IoT platforms by reproducing
reported IoT attacks and constructing new IoT attacks based
on smartphone malware classes. We categorize these attacks
based on lifecycle and adversary techniques, and build the first
taxonomized IoT attack app dataset. Evaluating ContexIoT on
this dataset, we find that it can effectively distinguish the attack
context for all the tested apps. The performance evaluation on
283 commodity IoT apps shows that the app patching adds nearly
negligible delay to the event triggering latency, and the permission
request frequency is far below the threshold that is considered
to risk user habituation or annoyance.

",17.315433740611066,16.143709677419356,313
NDSS_17_057.txt,14.477340192721428,12.52708219806345,NDSS,7713,"
Base stations constitute the basic infrastructure of
today’s cellular networks. Unfortunately, vulnerabilities in the
GSM (2G) network protocol enable the creation of fake base
stations (FBSes) that are not authorized by network operators.
Criminal gangs are using FBSes to directly attack users by
sending spam and fraud SMS messages, even if the users have
access to 3G/4G networks. In this paper, we present the design,
deployment, and evolution of an FBS detection system called
FBS-Radar, based on crowdsourced data of nearly 100M users.
In particular, we evaluate five different metrics for identifying
FBSes in the wild, and find that FBSes can be precisely identified
without sacrificing user privacy. Additionally, we present a
novel method for accurately geolocating FBSes while incurring
negligible impact on end-user devices. Our system protects users
from millions of spam and fraud SMS messages per day, and has
helped the authorities arrest hundreds of FBS operators.

",16.084390811093357,14.848308270676693,154
NDSS_17_058.txt,18.28143956227639,17.369441890210904,NDSS,7411,"
Although the security of Cyber-Physical Systems

(CPS) has been recently receiving significant attention from the
research community, undoubtedly, there still exists a substantial
lack of a comprehensive and a holistic understanding of attackers’
malicious strategies, aims and intentions. To this end, this
paper uniquely exploits passive monitoring and analysis of a
newly deployed network telescope IP address space in a first
attempt ever to build broad notions of real CPS maliciousness.
Specifically, we approach this problem by inferring, investigating,
characterizing and reporting large-scale probing activities that
specifically target more than 20 diverse, heavily employed CPS
protocols. To permit such analysis, we initially devise and evaluate
a novel probabilistic model that aims at filtering noise that is
embedded in network telescope traffic. Subsequently, we generate
amalgamated statistics, inferences and insights characterizing
such inferred scanning activities in terms of their probe types, the
distribution of their sources and their packets’ headers, among
numerous others, in addition to examining and visualizing the
co-occurrence patterns of such events. Further, we propose and
empirically evaluate an innovative hybrid approach rooted in
time-series analysis and context triggered piecewise hashing to
infer, characterize and cluster orchestrated and well-coordinated
probing activities targeting CPS protocols, which are generated
from Internet-scale unsolicited sources.
Our analysis and evaluations, which draw upon extensive network
telescope data observed over a recent one month period, demonstrate a staggering 33 thousand probes towards ample of CPS
protocols, the lack of interest in UDP-based CPS services, and the
prevalence of probes towards the ICCP and Modbus protocols.
Additionally, we infer a considerable 74% of CPS probes that
were persistent throughout the entire analyzed period targeting
prominent protocols such as DNP3 and BACnet. Further, we
uncover close to 9 thousand large-scale, stealthy, previously
undocumented orchestrated probing events targeting a number
of such CPS protocols. We validate the various outcomes through
cross-validations against publicly available threat repositories. We
concur that the devised approaches, techniques, and methods
provide a solid first step towards better comprehending real CPS
unsolicited objectives and intents.

",21.19438992294339,20.965102639296187,342
NDSS_17_059.txt,15.039948325099658,13.225372426086427,NDSS,3827,"
Drones are becoming increasingly popular for hobbyists and recreational use. But with this surge in popularity
comes increased risk to privacy as the technology makes it easy
to spy on people in otherwise-private environments, such as
an individual’s home. An attacker can fly a drone over fences
and walls in order to observe the inside of a house, without
having physical access. Existing drone detection systems require
specialist hardware and expensive deployment efforts; making
them inaccessible to the general public.

In this work we present a drone detection system that requires
minimal prior configuration and uses inexpensive commercial offthe-shelf (COTS) hardware to detect drones that are carrying out
privacy invasion attacks. We use a model of the attack structure
to derive statistical metrics for movement and proximity, that are
then applied to received communications between a drone and its
controller. We tested our system in real world experiments with
two popular consumer drone models mounting privacy invasion
attacks using a range of flight patterns. We were able to both
detect the presence of a drone and identify which phase of
the privacy attack was in progress. Even in our worst-case we
detected an attack before the drone was within 48m of its target.

",15.470042427545799,14.129870550161815,208
NDSS_17_060.txt,15.027259280440592,13.400813574143609,NDSS,9007,"
Address space layout randomization (ASLR) is an
important first line of defense against memory corruption attacks
and a building block for many modern countermeasures. Existing
attacks against ASLR rely on software vulnerabilities and/or on
repeated (and detectable) memory probing.

In this paper, we show that neither is a hard requirement
and that ASLR is fundamentally insecure on modern cachebased architectures, making ASLR and caching conflicting
requirements (ASLR@®Cache, or simply AnC). To support
this claim, we describe a new EVICT+TIME cache attack
on the virtual address translation performed by the memory
management unit (MMU) of modern processors. Our AnC attack
relies on the property that the MMU’s page-table walks result
in caching page-table pages in the shared last-level cache (LLC).
As a result, an attacker can derandomize virtual addresses of a
victim’s code and data by locating the cache lines that store the
page-table entries used for address translation.

Relying only on basic memory accesses allows AnC to be
implemented in JavaScript without any specific instructions or
software features. We show our JavaScript implementation can
break code and heap ASLR in two major browsers running on
the latest Linux operating system with 28bits of entropy in 150
seconds. We further verify that the AnC attack is applicable to
every modern architecture that we tried, including Intel, ARM
and AMD. Mitigating this attack without naively disabling caches
is hard, since it targets the low-level operations of the MMU.
We conclude that ASLR is fundamentally flawed in sandboxed
environments such as JavaScript and future defenses should not
rely on randomized virtual addresses as a building block.

",16.35954948731386,15.095684315684316,274
NDSS_17_061.txt,16.108183835382306,15.227952962340428,NDSS,9588,"
A common type of memory error in the Linux
kernel is using uninitialized variables (uninitialized use). Uninitialized uses not only cause undefined behaviors but also impose a
severe security risk if an attacker takes control of the uninitialized
variables. However, reliably exploiting uninitialized uses on the
kernel stack has been considered infeasible until now since the
code executed prior to triggering the vulnerability must leave an
attacker-controlled pattern on the stack. Therefore, uninitialized
uses are largely overlooked and regarded as undefined behaviors,
rather than security vulnerabilities. In particular, full memorysafety techniques (e.g., SoftBound+CETS) exclude uninitialized
use as a prevention target, and widely used systems such as
OpenSSL even use uninitialized memory as a randomness source.

In this paper, we propose a fully automated targeted stackspraying approach for the Linux kernel that reliably facilitates
the exploitation of uninitialized uses. Our targeted stack-spraying
includes two techniques: (1) a deterministic stack spraying
technique that suitably combines tailored symbolic execution and
guided fuzzing to identify kernel inputs that user-mode programs
can use to deterministically guide kernel code paths and thereby
leave attacker-controlled data on the kernel stack, and (2) an
exhaustive memory spraying technique that uses memory occupation and pollution to reliably control a large region of the kernel
stack. We show that our targeted stack-spraying approach allows
attackers to reliably control more than 91% of the Linux kernel
stack, which, in combination with uninitialized-use vulnerabilities,
suffices for a privilege escalation attack. As a countermeasure, we
propose a compiler-based mechanism that initializes potentially

unsafe pointer-type fields with almost no performance overhead.

Our results show that uninitialized use is a severe attack vector
that can be readily exploited with targeted stack-spraying, so
future memory-safety techniques should consider it a prevention
target, and systems should not use uninitialized memory as a
randomness source.

",19.38786093064905,20.67200323624596,310
NDSS_17_062.txt,15.284279164573256,13.761109069627398,NDSS,10183,"
Memory corruption vulnerabilities not only allow
modification of control data and injection of malicious payloads;
they also allow adversaries to reconnoiter a diversified program,
customize a payload, and ultimately bypass code randomization
defenses. In response, researchers have proposed and built various
leakage-resilient defenses against code reuse. Leakage-resilient
defenses use memory protection techniques to prevent adversaries
from directly reading code as well as pointer indirection or
encryption techniques to decouple code pointers from the randomized code layout, avoiding indirect leakage. In this paper, we
show that although current code pointer protections do prevent
leakage per se, they are fundamentally unable to stop code
reuse. Specifically, we demonstrate a new class of attacks we
call address-oblivious code reuse that bypasses state-of-the-art
leakage-resilience techniques by profiling and reusing protected
code pointers, without leaking the code layout. We show that
an attacker can accurately identify protected code pointers of
interest and mount code-reuse attacks at the abstraction level
of pointers without requiring any knowledge of code addresses.
We analyze the prevalence of opportunities for such attacks in
popular code bases and build three real-world exploits against
Nginx and Apache to demonstrate their practicality. We analyze
recently proposed leakage resilient defenses and show that they
are vulnerable to address oblivious code reuse. Our findings
indicate that because of the prevalence of code pointers in realistic
programs and the fundamental need to expose them to “Tread”
operations (even indirectly), diversity defenses face a fundamental
design challenge in mitigating such attacks.

",19.174585242480724,18.460289855072464,254
NDSS_17_063.txt,15.810407999011357,14.167412728249197,NDSS,9360,"
Dynamic loading is a core feature used on current
systems to (i) enable modularity and reuse, (ii) reduce memory
footprint by sharing code pages of libraries and executables
among processes, and (iii) simplify update procedures by eliminating the need to recompile executables when a library is
updated. The Executable and Linkable Format (ELF) is a generic
specification that describes how executable programs are stitched
together from object files produced from source code to libraries
and executables. Programming languages allow fine-grained control over variables, including access and memory protections, so
programmers may write defense mechanisms assuming that the
permissions specified at the source and/or compiler level will hold
at runtime.

Unfortunately, information about memory protection is lost
during compilation. We identify one case that has significant
security implications: when instantiating a process, constant
external variables that are referenced in executables are forcefully
relocated to a writable memory segment without warning. The
loader trades security for compatibility due to the lack of memory
protection information on the relocated external variables. We call
this new attack vector COREV for Copy Relocation Violation.
An adversary may use a memory corruption vulnerability to
modify such “read-only” constant variables like vtables, function
pointers, format strings, and file names to bypass defenses (like
FORTIFY_SOURCE or CFI) and to escalate privileges.

We have studied all Ubuntu 16.04 LTS packages and found
that out of 54,045 packages, 4,570 packages have unexpected
copy relocations that change read-only permissions to read-write,
presenting new avenues for attack. The attack surface is broad
with 29,817 libraries exporting relocatable read-only variables.
The set of 6,399 programs with actual copy relocation violations
includes ftp servers, apt-get, and gettext. We discuss the cause,
effects, and a set of possible mitigation strategies for the COREV
attack vector.

",19.032712561301913,17.861153198653202,303
NDSS_17_064.txt,15.271374199147076,13.433608559668283,NDSS,11429,"
Object bounds overflow errors are a common
source of security vulnerabilities. In principle, bounds check
instrumentation eliminates the problem, but this introduces high
overheads and is further hampered by limited compatibility
against un-instrumented code. On 64-bit systems, low-fat pointers
are a recent scheme for implementing efficient and compatible
bounds checking by transparently encoding meta information
within the native pointer representation itself. However, low-fat
pointers are traditionally used for heap objects only, where the
allocator has sufficient control over object location necessary for
the encoding. This is a problem for stack allocation, where there
exist strong constraints regarding the location of stack objects
that is apparently incompatible with the low-fat pointer approach.
To address this problem, we present an extension of low-fat
pointers to stack objects by using a collection of techniques,
such as pointer mirroring and memory aliasing, thereby allowing
stack objects to enjoy bounds error protection from instrumented
code. Our extension is compatible with common special uses of
the stack, such as alloca, set jmp and longjmp, exceptions, and
multi-threading, which rely on direct manipulation of the stack
pointer. Our experiments show that we successfully extend the
advantages of the low-fat pointer encoding to stack objects. The
end result is a competitive bounds checking instrumentation for
the stack and heap with low memory and runtime overheads,
and high compatibility with un-instrumented legacy code.

",17.251386760058843,16.688640915593712,234
NDSS_17_065.txt,14.141348181002822,12.32005808020774,NDSS,9118,"
Fuzzing is an effective software testing technique
to find bugs. Given the size and complexity of real-world
applications, modern fuzzers tend to be either scalable, but not
effective in exploring bugs that lie deeper in the execution, or
capable of penetrating deeper in the application, but not scalable.

In this paper, we present an application-aware evolutionary
fuzzing strategy that does not require any prior knowledge of the
application or input format. In order to maximize coverage and
explore deeper paths, we leverage control- and data-flow features
based on static and dynamic analysis to infer fundamental properties of the application. This enables much faster generation of
interesting inputs compared to an application-agnostic approach.
We implement our fuzzing strategy in VUzzer and evaluate it
on three different datasets; DARPA Grand Challenge binaries
(CGC), a set of real-world applications (binary input parsers),
and the recently released LAVA dataset. On all of these datasets,
VUzzer yields significantly better results than state-of-the-art
fuzzers, by quickly finding several existing and new bugs.

",17.28802050969988,16.212331691297212,175
NDSS_17_066.txt,13.315176970875676,11.050479366511532,NDSS,12423,"
Malicious payload injection attacks have been a
serious threat to software for decades. Unfortunately, protection against these attacks remains challenging due to the ever
increasing diversity and sophistication of payload injection and
triggering mechanisms used by adversaries. In this paper, we
develop A2C, a system that provides general protection against
payload injection attacks. A2C is based on the observation that
payloads are highly fragile and thus any mutation would likely
break their functionalities. Therefore, A2C mutates inputs from
untrusted sources. Malicious payloads that reside in these inputs
are hence mutated and broken. To assure that the program
continues to function correctly when benign inputs are provided,
A2C divides the state space into exploitable and post-exploitable
sub-spaces, where the latter is much larger than the former, and
decodes the mutated values only when they are transmitted from
the former to the latter. A2C does not rely on any knowledge of
malicious payloads or their injection and triggering mechanisms.
Hence, its protection is general. We evaluate A2C with 30 realworld applications, including apache on a real-world work-load,
and our results show that A2C effectively prevents a variety of
payload injection attacks on these programs with reasonably low
overhead (6.94%).

",15.381575749822971,13.676860696517412,203
NDSS_17_067.txt,17.539056732778224,16.4354497106921,NDSS,12592,"
JIT spraying allows an attacker to subvert a JustIn-Time compiler, introducing instruction sequences useful to the
attacker into executable regions of the victim program’s address
space as a side effect of compiling seemingly innocuous code in
a safe language like JavaScript.

We present new JIT spraying attacks against Google’s V8
and Mozilla’s SpiderMonkey JavaScript engines on ARM. The
V8 attack is the first JIT spraying attack not to rely on
instruction decoding ambiguity, and the SpiderMonkey attack
uses the first ARM payload that executes unintended instructions
derived from intended instruction bytes without resynchronizing
to the intended instruction stream. We review the JIT spraying
defenses proposed in the literature and their currently-deployed
implementations and conclude that the current state of JIT
spraying mitigation, which prioritizes low performance overhead,
leaves many exploitable attacker options unchecked.

We perform an empirical evaluation of mitigations with low
but non-zero overhead in a unified framework and find that full,
robust defense implementations of diversification defenses can
effectively mitigate JIT spraying attacks in the literature as well
as our new attacks with a combined average overhead of 4.56%
on x86-64 and 4.88% on ARM32.

",21.012824612059983,21.857738219895293,197
NDSS_17_068.txt,16.03029750255766,14.253297075875981,NDSS,12011,"
Static binary rewriting has many important ap-
plications in reverse engineering, such as patching, code reuse,
and instrumentation. Binary reassembling is an efﬁcient solution
for static binary rewriting. While there has been a proposed
solution to the reassembly of binaries, an evaluation on a real-
world binary dataset shows that it suffers from some problems
that lead to breaking binaries. Those problems include incorrect
symbolization of immediates, failure in identifying symbolizable
constants, lack of pointer safety checks, and other issues. Failure
in addressing those problems makes the existing approach un-
suitable for real-world binaries, especially those compiled with
optimizations enabled.

In this paper, we present a new systematic approach for
binary reassembling. Our new approach is implemented in a tool
called Ramblr. We evaluate Ramblr on 106 real-world programs
on Linux x86 and x86-64, and 143 programs collected from the
Cyber Grand Challenge Qualiﬁcation Event. All programs are
compiled to binaries with a set of different compilation ﬂags
in order to cover as many real-world scenarios as possible.
Ramblr successfully reassembles most of the binaries, which
is an improvement over the state-of-the-art approach. It should
be noted that our reassembling procedure yields no execution
overhead and no size expansion.
",15.429909175157395,13.19421888790821,209
NSDI_17_001.txt,14.54078530276205,13.012930487393856,NSDI,8782,"
We report the design, implementation, and deployment
of Lepton, a fault-tolerant system that losslessly compresses JPEG images to 77% of their original size on average. Lepton replaces the lowest layer of baseline JPEG
compression—a Huffman code—with a parallelized arithmetic code, so that the exact bytes of the original JPEG
file can be recovered quickly. Lepton matches the compression efficiency of the best prior work, while decoding
more than nine times faster and in a streaming manner.
Lepton has been released as open-source software and
has been deployed for a year on the Dropbox file-storage
backend. As of February 2017, it had compressed more
than 203 PiB of user JPEG files, saving more than 46 PiB.
",13.968273953766033,12.748333333333338,121
NSDI_17_002.txt,17.132668672729373,16.195789369079126,NSDI,8852,"

The performance characteristics of modern non-volatile
storage devices have led to storage becoming a shared,
rack-scale resource. System designers have an imperative to rethink existing storage abstractions in light of this
development. This paper proposes a lightweight storage abstraction that encapsulates full-system hardware
resources and focuses only on isolating and sharing storage devices across multiple, remote tenants. In support
of this, it prototypes a shared-nothing runtime capable
of managing hardware and scheduling this abstraction in
a manner that avoids performance interference and preserves the performance of the storage devices for tenants.
",18.243605946275583,16.651447368421056,96
NSDI_17_003.txt,15.403283621119357,14.238388903881276,NSDI,8255,"

This paper presents vCorfu, a strongly consistent cloudscale object store built over a shared log. vCorfu augments the traditional replication scheme of a shared log
to provide fast reads and leverages a new technique, composable state machine replication, to compose large state
machines from smaller ones, enabling the use of state
machine replication to be used to efficiently in huge data
stores. We show that vCorfu outperforms Cassandra, a
popular state-of-the art NOSQL stores while providing
strong consistency (opacity, read-own-writes), efficient
transactions, and global snapshots at cloud scale.
",20.267338824336647,22.486956521739135,93
NSDI_17_004.txt,14.971048589212362,13.643877114366827,NSDI,9768,"
Modern cluster storage systems perform a variety of
background tasks to improve the performance, availability, durability, and cost-efficiency of stored data. For
example, cleaners compact fragmented data to generate
long sequential runs, tiering services automatically migrate data between solid-state and hard disk drives based
on usage, recovery mechanisms replicate data to improve
availability and durability in the face of failures, cost saving techniques perform data transformations to reduce
the storage costs, and so on.

In this work, we present Curator, a background
MapReduce-style execution framework for cluster management tasks, in the context of a distributed storage system used in enterprise clusters. We describe Curator’s
design and implementation, and evaluate its performance
using a handful of relevant metrics. We further report experiences and lessons learned from its five-year construction period, as well as thousands of customer deployments. Finally, we propose a machine learning-based
model to identify an efficient execution policy for Curator’s management tasks that can adapt to varying workload characteristics.
",18.243605946275583,18.4489393939394,168
NSDI_17_005.txt,15.394003857507673,13.952636224193636,NSDI,9041,"

Recent hardware switch architectures make it feasible
to perform flexible packet processing inside the network. This allows operators to configure switches to
parse and process custom packet headers using flexible match+action tables in order to exercise control over
how packets are processed and routed. However, flexible
switches have limited state, support limited types of operations, and limit per-packet computation in order to be
able to operate at line rate.

Our work addresses these limitations by providing a
set of general building blocks that mask these limitations using approximation techniques and thereby enabling the implementation of realistic network protocols. In particular, we use these building blocks to tackle
the network resource allocation problem within datacenters and realize approximate variants of congestion control and load balancing protocols, such as XCP, RCP,
and CONGA, that require explicit support from the network. Our evaluations show that these approximations
are accurate and that they do not exceed the hardware
resource limits associated with these flexible switches.
We demonstrate their feasibility by implementing RCP
with the production Cavium CNX880xx switch. This
implementation provides significantly faster and lowervariance flow completion times compared with TCP.

",17.267425705330172,16.775657894736842,190
NSDI_17_006.txt,16.03130395397726,14.254661648049215,NSDI,8514,"

Many research works have recently experimented with
GPU to accelerate packet processing in network applications. Most works have shown that GPU brings a significant performance boost when it is compared to the CPUonly approach, thanks to its highly-parallel computation
capacity and large memory bandwidth. However, a recent
work argues that for many applications, the key enabler
for high performance is the inherent feature of GPU that
automatically hides memory access latency rather than its
parallel computation power. It also claims that CPU can
outperform or achieve a similar performance as GPU if
its code is re-arranged to run concurrently with memory
access, employing optimization techniques such as group
prefetching and software pipelining.

In this paper, we revisit the claim of the work and see if
it can be generalized to a large class of network applications. Our findings with eight popular algorithms widely
used in network applications show that (a) there are many
compute-bound algorithms that do benefit from the parallel computation capacity of GPU while CPU-based optimizations fail to help, and (b) the relative performance
advantage of CPU over GPU in most applications is due to
data transfer bottleneck in PCIe communication of discrete
GPU rather than lack of capacity of GPU itself. To avoid
the PCIe bottleneck, we suggest employing integrated
GPU in recent APU platforms as a cost-effective packet
processing accelerator. We address a number of practical
issues in fully exploiting the capacity of APU and show
that network applications based on APU achieve multi-10
Gbps performance for many compute/memory-intensive
algorithms.

",19.53771442962202,18.890896946564883,263
NSDI_17_007.txt,15.555850496968517,14.778600865232814,NSDI,7759,"

In this paper we present Stateless Network Functions,
a new architecture for network functions virtualization,
where we decouple the existing design of network functions into a stateless processing component along with
a data store layer. In breaking the tight coupling, we
enable a more elastic and resilient network function infrastructure. Our StatelessNF processing instances are
architected around efficient pipelines utilizing DPDK
for high performance network I/O, packaged as Docker
containers for easy deployment, and a data store interface optimized based on the expected request patterns to efficiently access a RAMCloud-based data store.
A network-wide orchestrator monitors the instances for
load and failure, manages instances to scale and provide
resilience, and leverages an OpenFlow-based network to
direct traffic to instances. We implemented three example network functions (network address translator, firewall, and load balancer). Our evaluation shows (i) we are
able to reach a throughput of 10Gbit/sec, with an added
latency overhead of between 100us and 500s, (ii) we
are able to have a failover which does not disrupt ongoing traffic, and (11) when scaling out and scaling in we
are able to match the ideal performance.

",18.243605946275583,18.062105263157893,191
NSDI_17_008.txt,15.841751444227683,14.451951486411044,NSDI,9138,"

Stateful middleboxes, such as intrusion detection systems
and application-level firewalls, have provided key functionalities in operating modern IP networks. However,
designing an efficient middlebox is challenging due to
the lack of networking stack abstraction for TCP flow
processing. Thus, middlebox developers often write the
complex flow management logic from scratch, which is
not only prone to errors, but also wastes efforts for similar
functionalities across applications.

This paper presents the design and implementation of
mOS, a reusable networking stack for stateful flow processing in middlebox applications. Our API allows developers to focus on the core application logic instead
of dealing with low-level packet/flow processing themselves. Under the hood, it implements an efficient event
system that scales to monitoring millions of concurrent
flow events. Our evaluation demonstrates that mOS enables modular development of stateful middleboxes, often
significantly reducing development efforts represented by
the source lines of code, while introducing little performance overhead in multi-10Gbps network environments.
",18.848423458724294,17.53649595687332,160
NSDI_17_009.txt,15.438247294518643,13.72719820297863,NSDI,8270,"

We perform the first Internet study of the cryptographic security of DNSSEC-signed domains. To that
end, we collected 2.14@ DNSSEC keys for popular
signed domains out of these 1.9M are RSA keys. We
analyse the RSA keys and show that a large fraction
of signed domains are using vulnerable keys: 35% are
signed with RSA keys that share their moduli with some
other domain and 66% use keys that are too short (1024
bit or less) or keys which modulus has a GCD > 1 with
the modulus of some other domain. As we show, to a
large extent the vulnerabilities are due to poor key generation practices, but also due to potential faulty hardware
or software bugs.

The DNSSEC keys collection and analysis is performed on a daily basis with the DNSSEC Keys Validation Engine which we developed. The statistics as well as
the DNSSEC Keys Validation Engine are made available
online, as a service for Internet users.

",14.06817628641468,12.510000000000002,162
NSDI_17_010.txt,14.934753982814549,13.543601861744726,NSDI,9785,"

With Tor being a popular anonymity network, many attacks have been proposed to break its anonymity or leak
information of a private communication on Tor. However,
guaranteeing complete privacy in the face of an adversary on Tor is especially difficult because Tor relays are
under complete control of world-wide volunteers. Currently, one can gain private information, such as circuit
identifiers and hidden service identifiers, by running Tor
relays and can even modify their behaviors with malicious
intent.

This paper presents a practical approach to effectively
enhancing the security and privacy of Tor by utilizing
Intel SGX, a commodity trusted execution environment.
We present a design and implementation of Tor, called
SGX-Tor, that prevents code modification and limits the
information exposed to untrusted parties. We demonstrate
that our approach is practical and effectively reduces the
power of an adversary to a traditional network-level adversary. Finally, SGX-Tor incurs moderate performance overhead; the end-to-end latency and throughput overheads
for HTTP connections are 3.9% and 11.9%, respectively.

",18.08858127442927,17.006779374471687,172
NSDI_17_011.txt,14.406465580055897,12.519932677067697,NSDI,7988,"

Today, search for dashcam video evidences is conducted
manually and its procedure does not guarantee privacy. In
this paper, we motivate, design, and implement ViewMap,
an automated public service system that enables sharing
of private dashcam videos under anonymity. ViewMap
takes a profile-based approach where each video is represented in a compact form called a view profile (VP), and
the anonymized VPs are treated as entities for search, verification, and reward instead of their owners. ViewMap
exploits the line-of-sight (LOS) properties of dedicated
short-range communications (DSRC) such that each vehicle makes VP links with nearby ones that share the
same sight while driving. ViewMap uses such LOS-based
VP links to build a map of visibility around a given incident, and identifies VPs whose videos are worth reviewing. Original videos are never transmitted unless they are
verified to be taken near the incident and anonymously
solicited. ViewMap offers untraceable rewards for the
provision of videos whose owners remain anonymous.
We demonstrate the feasibility of ViewMap via field experiments on real roads using our DSRC testbeds and
trace-driven simulations.

",16.218646115125612,15.190266393442624,184
NSDI_17_012.txt,15.612308821779425,13.533766597252939,NSDI,8928,"

60 GHz millimeter-wave networks represent the next frontier in high-speed wireless access technologies. Due to
the use of highly directional and electronically steerable
beams, the performance of 60 GHz networks becomes
a sensitive function of environment structure and reflectivity, which cannot be handled by existing networking
paradigms. In this paper, we propose E-Mi, a framework that harnesses 60 GHz radios’ sensing capabilities
to boost network performance. E-Mi uses a single pair
of 60 GHz transmitter and receiver to sense the environment. It can resolve all dominant reflection paths between the two nodes, from which it reconstructs a coarse
outline of major reflectors in the environment. It then
feeds the reflector information into a ray-tracer to predict the channel and network performance of arbitrarily
located links. Our experiments on a custom-built 60 GHz
testbed verify that E-Mi can accurately sense a given
environment, and predict the channel quality of different links with 2.8 dB median error. The prediction is
then used to optimize the deployment of 60 GHz access
points, with 2.2x to 4.5x capacity gain over empirical
approaches.

",16.373557378465907,14.349831081081081,189
NSDI_17_013.txt,14.584699301485738,12.731191403857075,NSDI,8022,"
Carrier sensing is a key mechanism that enables decentralized sharing of unlicensed spectrum. However,
carrier sensing in its current form is fundamentally unsuitable when devices transmit at different power levels, a scenario increasingly common given the diversity of Wi-Fi APs in the market and the need for WiFi’s co-existence with new upcoming standards such as
LAA/LWA. The primary contribution of this paper is a
novel carrier sensing mechanism – skip correlation – that
extends carrier sensing to accommodate multiple transmit power levels. Through an FPGA based implementation on the WARP platform, we demonstrate the effectiveness of our technique in a variety of scenarios including support for backward compatibility.
",19.032712561301913,18.52924311926606,111
NSDI_17_014.txt,15.10617948719538,13.072843651652029,NSDI,9971,"
 — This paper enables connectivity on everyday objects by transforming them into FM radio stations.
To do this, we show for the first time that ambient FM
radio signals can be used as a signal source for backscatter
communication. Our design creates backscatter transmissions that can be decoded on any FM receiver including
those in cars and smartphones. This enables us to achieve
a previously infeasible capability: backscattering information to cars and smartphones in outdoor environments.

Our key innovation is a modulation technique that transforms backscatter, which is a multiplication operation on
RF signals, into an addition operation on the audio signals
output by FM receivers. This enables us to embed both
digital data as well as arbitrary audio into ambient analog
FM radio signals. We build prototype hardware of our
design and successfully embed audio transmissions over
ambient FM signals. Further, we achieve data rates of up
to 3.2 kbps and ranges of 5—60 feet, while consuming as
little as 11.07 «W of power. To demonstrate the potential of our design, we also fabricate our prototype on a
cotton t-shirt by machine sewing patterns of a conductive
thread to create a smart fabric that can transmit data to
a smartphone. We also embed FM antennas into posters
and billboards and show that they can communicate with
FM receivers in cars and smartphones.

",16.647925096878797,15.060285714285715,227
NSDI_17_015.txt,13.860381772631197,12.503518656938027,NSDI,10327,"
This paper presents Prio, a privacy-preserving
system for the collection of aggregate statistics. Each Prio
client holds a private data value (e.g., its current location),
and a small set of servers compute statistical functions
over the values of all clients (e.g., the most popular location). As long as at least one server is honest, the Prio
servers learn nearly nothing about the clients’ private data,
except what they can infer from the aggregate statistics
that the system computes. To protect functionality in the
face of faulty or malicious clients, Prio uses secret-shared
non-interactive proofs (SNIPs), a new cryptographic technique that yields a hundred-fold performance improvement over conventional zero-knowledge approaches. Prio
extends classic private aggregation techniques to enable
the collection of a large class of useful statistics. For
example, Prio can perform a least-squares regression on
high-dimensional client-provided data without ever seeing the data in the clear.
",15.247664890283005,14.725194805194807,157
NSDI_17_016.txt,14.671213545499029,13.081843855377763,NSDI,9040,"
Many systems run rich analytics on sensitive data in the
cloud, but are prone to data breaches. Hardware enclaves
promise data confidentiality and secure execution of arbitrary computation, yet still suffer from access pattern
leakage. We propose Opaque, a distributed data analytics
platform supporting a wide range of queries while providing strong security guarantees. Opaque introduces new
distributed oblivious relational operators that hide access
patterns, and new query planning techniques to optimize
these new operators. Opaque is implemented on Spark
SQL with few changes to the underlying system. Opaque
provides data encryption, authentication and computation verification with a performance ranging from 52%
faster to 3.3x slower as compared to vanilla Spark SQL;
obliviousness comes with a 1.6–46x overhead. Opaque
provides an improvement of three orders of magnitude
over state-of-the-art oblivious protocols, and our query
optimization techniques improve performance by 2–5x.
",16.26309291913925,16.08857142857143,148
NSDI_17_017.txt,13.705138340740621,12.064521899742157,NSDI,9230,"
Many online services let users query public datasets
such as maps, flight prices, or restaurant reviews. Unfortunately, the queries to these services reveal highly sensitive
information that can compromise users’ privacy. This paper presents Splinter, a system that protects users’ queries
on public data and scales to realistic applications. A user
splits her query into multiple parts and sends each part to
a different provider that holds a copy of the data. As long
as any one of the providers is honest and does not collude
with the others, the providers cannot determine the query.
Splinter uses and extends a new cryptographic primitive
called Function Secret Sharing (FSS) that makes it up to
an order of magnitude more efficient than prior systems
based on Private Information Retrieval and garbled circuits. We develop protocols extending FSS to new types
of queries, such as MAX and TOPK queries. We also provide an optimized implementation of FSS using AES-NI
instructions and multicores. Splinter achieves end-to-end
latencies below 1.6 seconds for realistic workloads including a Yelp clone, flight search, and map routing.
",14.06817628641468,12.658858195211788,183
NSDI_17_018.txt,15.202456328274696,13.765770853127894,NSDI,8859,"

Many modern scalable cloud networking architectures
rely on host networking for implementing VM network
policy - e.g. tunneling for virtual networks, NAT for
load balancing, stateful ACLs, QoS, and more. We
present the Virtual Filtering Platform (VFP) - a
programmable virtual switch that powers Microsoft
Azure, a large public cloud, and provides this policy.
We define several major goals for a programmable
virtual switch based on our operational experiences,
including support for multiple independent network
controllers, policy based on connections rather than
only on packets, efficient caching and classification
algorithms for performance, and efficient offload of
flow policy to programmable NICs, and demonstrate
how VFP achieves these goals. VFP has been deployed
on >1M hosts running IaaS and PaaS workloads for
over 4 years. We present the design of VFP and its API,
its flow language and compiler used for flow
processing, performance results, and experiences
deploying and using VFP in Azure over several years.

",19.686211704642208,18.33628571428571,155
NSDI_17_019.txt,16.291547565932635,15.197521929391637,NSDI,10119,"

We consider the following question: what consistency
model is appropriate for coordinating the actions of a
replicated set of SDN controllers? We first argue that
the conventional requirement of strong consistency, typically achieved through the use of Paxos or other consensus algorithms, is conceptually unnecessary to handle unplanned network updates. We present an alternate
approach, based on the weaker notion of eventual correctness, and describe the design of a simple coordination layer (SCL) that can seamlessly turn a set of singleimage SDN controllers (that obey certain properties) into
a distributed SDN system that achieves this goal (whereas
traditional consensus mechanisms do not). We then show
through analysis and simulation that our approach provides faster responses to network events. While our primary focus is on handling unplanned network updates, our
coordination layer also handles policy updates and other
situations where consistency is warranted. Thus, contrary
to the prevailing wisdom, we argue that distributed SDN
control planes need only be slightly more complicated
than single-image controllers.

",18.7741,18.878030303030304,166
NSDI_17_020.txt,15.394792076995067,14.152639956796367,NSDI,9359,"

A key challenge confronting wide-area network architects is validating that their network designs provide assurable performance in the face of variable traffic demands and failures. Validation is hard because of the
exponential, and possibly non-enumerable, set of scenarios that must be considered. Current theoretical tools
provide overly conservative bounds on network performance since to remain tractable, they do not adequately
model the flexible routing strategies that networks employ in practice to adapt to failures and changing traffic demands. In this paper, we develop an optimizationtheoretic framework to derive the worst-case network
performance across scenarios of interest by modeling
flexible routing adaptation strategies. We present an
approach to tackling the resulting intractable problems,
which can achieve tighter bounds on network performance than current techniques. While our framework
is general, we focus on bounding worst-case link utilizations, and case studies involving topology design,
and MPLS tunnels, chosen both for their practical importance and to illustrate key aspects of our framework.
Evaluations over real network topologies and traffic data
show the promise of the approach.

",17.122413403193683,17.078214285714285,177
NSDI_17_021.txt,15.152063341814298,13.478613405615079,NSDI,8266,"
We describe ExCamera, a system that can edit, transform,
and encode a video, including 4K and VR material, with
low latency. The system makes two major contributions.
First, we designed a framework to run general-purpose
parallel computations on a commercial “cloud function”
service. The system starts up thousands of threads in
seconds and manages inter-thread communication.
Second, we implemented a video encoder intended for
fine-grained parallelism, using a functional-programming
style that allows computation to be split into thousands of
tiny tasks without harming compression efficiency. Our
design reflects a key insight: the work of video encoding
can be divided into fast and slow parts, with the “slow”
work done in parallel, and only “fast” work done serially.
",15.247664890283005,13.339462809917357,122
NSDI_17_022.txt,14.80842382686832,12.844665008981764,NSDI,8854,"

Video cameras are pervasively deployed for security
and smart city scenarios, with millions of them in large
cities worldwide. Achieving the potential of these cameras requires efficiently analyzing the live videos in realtime. We describe VideoStorm, a video analytics system
that processes thousands of video analytics queries on
live video streams over large clusters. Given the high
costs of vision processing, resource management is crucial. We consider two key characteristics of video analytics: resource-quality tradeoff with multi-dimensional
configurations, and variety in quality and lag goals.
VideoStorm’s offline profiler generates query resourcequality profile, while its online scheduler allocates resources to queries to maximize performance on quality
and lag, in contrast to the commonly used fair sharing
of resources in clusters. Deployment on an Azure cluster of 101 machines shows improvement by as much as
80% in quality of real-world queries and 7x better lag,
processing video from operational traffic cameras.

",18.3970566412798,16.47857142857143,154
NSDI_17_023.txt,14.896327243492834,13.988468022276908,NSDI,8799,"
Content providers are increasingly using data-driven
mechanisms to optimize quality of experience (QoE).
Many existing approaches formulate this process as a
prediction problem of learning optimal decisions (e.g.,
server, bitrate, relay) based on observed QoE of recent sessions. While prediction-based mechanisms have
shown promising QoE improvements, they are necessarily incomplete as they: (1) suffer from many known biases (e.g., incomplete visibility) and (2) cannot respond
to sudden changes (e.g., load changes). Drawing a parallel from machine learning, we argue that data-driven
QoE optimization should instead be cast as a real-time
exploration and exploitation (E2) process rather than as
a prediction problem. Adopting E2 in network applications, however, introduces key architectural (e.g., how
to update decisions in real time with fresh data) and algorithmic (e.g., capturing complex interactions between
session features vs. QoE) challenges. We present Pytheas, a framework which addresses these challenges using
a group-based E2 mechanism. The insight is that application sessions sharing the same features (e.g., IP prefix, location) can be grouped so that we can run E2 algorithms at a per-group granularity. This naturally captures the complex interactions and is amenable to realtime control with fresh measurements. Using an endto-end implementation and a proof-of-concept deployment in CloudLab, we show that Pytheas improves video
QoE over a state-of-the-art prediction-based system by
up to 31% on average and 78% on 90th percentile of persession QoE.
",17.37919286519448,16.441143654114367,246
NSDI_17_024.txt,13.686856293727683,11.775972126465089,NSDI,8375,"

Datacenter networks require efficient multi-path load
balancing to achieve high bisection bandwidth. Despite
much progress in recent years towards addressing this
challenge, a load balancing design that is both simple
to implement and resilient to network asymmetry has
remained elusive. In this paper, we show that flowlet
switching, an idea first proposed more than a decade ago,
is a powerful technique for resilient load balancing with
asymmetry. Flowlets have a remarkable elasticity property: their size changes automatically based on traffic
conditions on their path. We use this insight to develop
LetFlow, a very simple load balancing scheme that is resilient to asymmetry. LetFlow simply picks paths at random for flowlets and lets their elasticity naturally balance the traffic on different paths. Our extensive evaluation with real hardware and packet-level simulations
shows that LetFlow is very effective. Despite being much
simpler, it performs significantly better than other traffic
oblivious schemes like WCMP and Presto in asymmetric scenarios, while achieving average flow completions
time within 10-20% of CONGA in testbed experiments
and 2x of CONGA in simulated topologies with large
asymmetry and heavy traffic load.

",16.52667757954773,15.681801075268819,187
NSDI_17_025.txt,13.93545276122337,12.21291791448147,NSDI,8268,"
Rapid convergence to a desired allocation of network
resources to endpoint traffic is a difficult problem. The
reason is that congestion control decisions are distributed
across the endpoints, which vary their offered load in
response to changes in application demand and network
feedback on a packet-by-packet basis. We propose a different approach for datacenter networks, flowlet control,
in which congestion control decisions are made at the
granularity of a flowlet, not a packet. With flowlet control, allocations have to change only when flowlets arrive
or leave. We have implemented this idea in a system
called Flowtune using a centralized allocator that receives
flowlet start and end notifications from endpoints. The
allocator computes optimal rates using a new, fast method
for network utility maximization, and updates endpoint
congestion-control parameters. Experiments show that
Flowtune outperforms DCTCP, pFabric, sfqCoDel, and
XCP on tail packet delays in various settings, converging
to optimal rates within a few packets rather than over
several RTTs. Benchmarks on an EC2 deployment show
a fairer rate allocation than Linux’s Cubic. A data aggregation benchmark shows 1.61 lower p95 coflow completion time.
",14.867677710551934,14.176936936936936,188
NSDI_17_026.txt,14.809698965919647,13.25985055934866,NSDI,9374,"

Flexplane enables users to program data plane algorithms
and conduct experiments that run real application traffic
over them at hardware line rates. Flexplane explores an
intermediate point in the design space between past work
on software routers and emerging work on programmable
hardware chipsets. Like software routers, Flexplane enables users to express resource management schemes in
a high-level language (C++), but unlike software routers,
Flexplane runs at close to hardware line rates. To achieve
these two goals, a centralized emulator faithfully emulates, in real-time on a multi-core machine, the desired
data plane algorithms with very succinct representations
of the original packets. Real packets traverse the network
when notified by the emulator, sharing the same fate and
relative delays as their emulated counterparts.

Flexplane accurately predicts the behavior of several
network schemes such as RED and DCTCP, sustains aggregate throughput of up to 760 Gbits/s on a 10-core machine (* 20x faster than software routers), and enables
experiments with real-world operating systems and applications (e.g., Spark) running on diverse network schemes
at line rate, including those such as HULL and pFabric
that are not available in hardware today.

",17.315433740611066,17.57876288659794,194
NSDI_17_027.txt,15.623174969744397,14.555042226728546,NSDI,9111,"

We describe the design, implementation, and evaluation
of Occult (Observable Causal Consistency Using Lossy
Timestamps), the first scalable, geo-replicated data store that
provides causal consistency to its clients without exposing
the system to the possibility of slowdown cascades, a key
obstacle to the deployment of causal consistency at scale.
Occult supports read/write transactions under PC-PSI, a
variant of Parallel Snapshot Isolation that contributes to
Occult’s immunity to slowdown cascades by weakening how
PSI replicates transactions committed at the same replica.
While PSI insists that they all be totally ordered, PC-PSI
simply requires total order Per Client session. Nonetheless,
Occult guarantees that all transactions read from a causally
consistent snapshot of the datastore without requiring any coordination in how transactions are asynchronously replicated.
",20.503739492662863,20.66960317460317,128
NSDI_17_028.txt,15.467752615404827,13.625190439710106,NSDI,8490,"
 — Picking the right cloud configuration for
recurring big data analytics jobs running in clouds is
hard, because there can be tens of possible VM instance
types and even more cluster sizes to pick from. Choosing poorly can significantly degrade performance and increase the cost to run a job by 2-3x on average, and as
much as 12x in the worst-case. However, it is challenging to automatically identify the best configuration for a
broad spectrum of applications and cloud configurations
with low search cost. CherryPick is a system that leverages Bayesian Optimization to build performance models for various applications, and the models are just accurate enough to distinguish the best or close-to-the-best
configuration from the rest with only a few test runs. Our
experiments on five analytic applications in AWS EC2
show that CherryPick has a 45-90% chance to find optimal configurations, otherwise near-optimal, saving up to
75% search cost compared to existing solutions.
",17.80541091248751,16.950000000000006,161
NSDI_17_029.txt,13.825055346262005,12.285620400680894,NSDI,8689,"

Most major content providers use content delivery networks (CDNs) to serve web and video content to their
users. A CDN is a large distributed system of servers
that caches and delivers content to users. The first-level
cache in a CDN server is the memory-resident Hot Object
Cache (HOC). A major goal of a CDN is to maximize the
object hit ratio (OHR) of its HOCs. But, the small size
of the HOC, the huge variance in the requested object
sizes, and the diversity of request patterns make this goal
challenging.

We propose AdaptSize, the first adaptive, size-aware
cache admission policy for HOCs that achieves a high
OHR, even when object size distributions and request
characteristics vary significantly over time. At the core of
AdaptSize is a novel Markov cache model that seamlessly
adapts the caching parameters to the changing request
patterns. Using request traces from one of the largest
CDNs in the world, we show that our implementation of
AdaptSize achieves significantly higher OHR than widelyused production systems: 30-48% and 47-91% higher
OHR than Nginx and Varnish, respectively. AdaptSize
also achieves 33-46% higher OHR than state-of-the-art
research systems. Further, AdaptSize is more robust to
changing request patterns than the traditional tuning approach of hill climbing and shadow queues studied in
other contexts.

",13.348371206891418,12.731262443438911,222
NSDI_17_030.txt,14.115997612912686,12.355743850340058,NSDI,8102,"

This paper explores the possibility of bringing IoT to
sports analytics, particularly to the game of Cricket. We
develop solutions to track a ball’s 3D trajectory and spin
with inexpensive sensors and radios embedded in the
ball. Unique challenges arise rendering existing localization and motion tracking solutions inadequate. Our system, iBall, mitigates these problems by fusing disparate
sources of partial information — wireless, inertial sensing,
and motion models — into a non-linear error minimization
framework. Measured against a mm-level ground truth,
the median ball location error is at 8cm while rotational
error remains below 12° even at the end of the flight. The
results do not rely on any calibration or training, hence
we expect the core techniques to extend to other sports
like baseball, with some domain-specific modifications.

",15.247664890283005,14.903625954198475,132
NSDI_17_031.txt,14.993376951241384,13.66107883488866,NSDI,9050,"
 — Data-driven techniques help boost agricultural productivity by increasing yields, reducing losses
and cutting down input costs. However, these techniques
have seen sparse adoption owing to high costs of manual
data collection and limited connectivity solutions. In this
paper, we present FarmBeats, an end-to-end IoT platform
for agriculture that enables seamless data collection from
various sensors, cameras and drones. FarmBeats’s system design that explicitly accounts for weather-related
power and Internet outages has enabled six month long
deployments in two US farms.

",16.52667757954773,14.795238095238094,86
NSDI_17_032.txt,13.640227806195984,12.110773917006323,NSDI,8651,"

Today’s virtual reality (VR) headsets require a cable connection to a PC or game console. This cable significantly
limits the player’s mobility and, hence, her VR experience. The high data rate requirement of this link (multiple Gbps) precludes its replacement by WiFi. Thus, in
this paper, we focus on using mmWave technology to deliver multi-Gbps wireless communication between VR
headsets and their game consoles. We address the two
key problems that prevent existing mmWave links from
being used in VR systems. First, mmWave signals suffer from a blockage problem, i.e., they operate mainly
in line-of-sight and can be blocked by simple obstacles
such as the player lifting her hand in front of the headset.
Second, mm Wave radios use highly directional antennas
with very narrow beams; they work only when the transmitter’s beam is aligned with the receiver’s beam. Any
small movement of the headset can break the alignment
and stall the data stream. We present MoVR, a novel
system that allows mmWave links to sustain high data
rates even in the presence of a blockage and mobility.
MoVR does this by introducing a smart mmWave mirror and leveraging VR headset tracking information. We
implement MoVR and empirically demonstrate its performance using an HTC VR headset.

",12.563389971838223,11.354928229665074,215
NSDI_17_033.txt,15.533820332538635,13.874436914172637,NSDI,8670,"
Existing wired optical interconnects face a challenge of
supporting wide-spread communications in production
clusters. Initial proposals are constrained, as they only
support connections among a small number of racks
(e.g., 2 or 4) at a time, with switching time of milliseconds. Recent efforts on reducing optical circuit reconfiguration time to microseconds partially mitigate this problem by rapidly time-sharing optical circuits across more
nodes, but are still limited by the total number of parallel
circuits available simultaneously.
In this paper, we seek an optical interconnect that can
enable unconstrained communications within a computing cluster of thousands of servers. We present MegaSwitch, a multi-fiber ring optical fabric that exploits
space division multiplexing across multiple fibers to
deliver rearrangeably non-blocking communications to
30+ racks and 6000+ servers. We have implemented
a 5-rack 40-server MegaSwitch prototype with commercial optical devices, and used testbed experiments as well
as large-scale simulations to explore MegaSwitch’s architectural benefits and tradeoffs.
",19.117987234576393,18.035248447204967,162
NSDI_17_034.txt,15.718944949206897,14.390611247124962,NSDI,10439,"

Datacenters are characterized by their large scale,
stringent reliability requirements, and significant application diversity. However, the realities of employing
hardware with small but non-zero failure rates mean that
datacenters are subject to significant numbers of failures,
impacting the performance of the services that rely on
them. To make matters worse, these failures are not always obvious; network switches and links can fail partially, dropping or delaying various subsets of packets
without necessarily delivering a clear signal that they are
faulty. Thus, traditional fault detection techniques involving end-host or router-based statistics can fall short
in their ability to identify these errors.

We describe how to expedite the process of detecting
and localizing partial datacenter faults using an end-host
method generalizable to most datacenter applications. In
particular, we correlate transport-layer flow metrics and
network-I/O system call delay at end hosts with the path
that traffic takes through the datacenter and apply statistical analysis techniques to identify outliers and localize
the faulty link and/or switch(es). We evaluate our approach in a production Facebook front-end datacenter.

",17.613555460941566,16.788461538461537,183
NSDI_17_035.txt,16.46706310239799,15.488511361473758,NSDI,9756,"
Machine learning is being deployed in a growing number
of applications which demand real-time, accurate, and
robust predictions under heavy query load. However, most
machine learning frameworks and systems only address
model training and not deployment.
In this paper, we introduce Clipper, a general-purpose
low-latency prediction serving system. Interposing between end-user applications and a wide range of machine
learning frameworks, Clipper introduces a modular architecture to simplify model deployment across frameworks
and applications. Furthermore, by introducing caching,
batching, and adaptive model selection techniques, Clipper reduces prediction latency and improves prediction
throughput, accuracy, and robustness without modifying
the underlying machine learning frameworks. We evaluate Clipper on four common machine learning benchmark
datasets and demonstrate its ability to meet the latency,
accuracy, and throughput demands of online serving applications. Finally, we compare Clipper to the Tensorflow Serving system and demonstrate that we are able
to achieve comparable throughput and latency while enabling model composition and online learning to improve
accuracy and render more robust predictions.
",17.613555460941566,17.738237810094102,168
NSDI_17_036.txt,15.312172587149576,14.56160986911053,NSDI,10628,"

Machine learning (ML) is widely used to derive useful
information from large-scale data (such as user activities,
pictures, and videos) generated at increasingly rapid rates,
all over the world. Unfortunately, it is infeasible to move
all this globally-generated data to a centralized data center
before running an ML algorithm over it—moving large
amounts of raw data over wide-area networks (WANs) can
be extremely slow, and is also subject to the constraints
of privacy and data sovereignty laws. This motivates the
need for a geo-distributed ML system spanning multiple
data centers. Unfortunately, communicating over WANs
can significantly degrade ML system performance (by as
much as 53.7 in our study) because the communication
overwhelms the limited WAN bandwidth.

Our goal in this work is to develop a geo-distributed
ML system that (1) employs an intelligent communication
mechanism over WANs to efficiently utilize the scarce
WAN bandwidth, while retaining the accuracy and correctness guarantees of an ML algorithm; and (2) is generic
and flexible enough to run a wide range of ML algorithms,
without requiring any changes to the algorithms.

To this end, we introduce a new, general geo-distributed
ML system, Gaia, that decouples the communication
within a data center from the communication between
data centers, enabling different communication and consistency models for each. We present a new ML synchronization model, Approximate Synchronous Parallel
(ASP), whose key idea is to dynamically eliminate insignificant communication between data centers while
still guaranteeing the correctness of ML algorithms. Our
experiments on our prototypes of Gaia running across
11 Amazon EC2 global regions and on a cluster that
emulates EC2 WAN bandwidth show that Gaia provides
1.8-53.5 x speedup over two state-of-the-art distributed
ML systems, and is within 0.94—1.40 of the speed of
running the same ML algorithm on machines on a local
area network (LAN).

",20.14790713358019,21.805333601286176,317
NSDI_17_037.txt,15.150213619563797,13.256396071168908,NSDI,8195,"

Memory-intensive applications suffer large performance
loss when their working sets do not fully fit in memory.
Yet, they cannot leverage otherwise unused remote memory when paging out to disks even in the presence of large
imbalance in memory utilizations across a cluster. Existing proposals for memory disaggregation call for new architectures, new hardware designs, and/or new programming models, making them infeasible.

This paper describes the design and implementation of
INFINISWAP, a remote memory paging system designed
specifically for an RDMA network. INFINIS WAP opportunistically harvests and transparently exposes unused
memory to unmodified applications by dividing the swap
space of each machine into many slabs and distributing
them across many machines’ remote memory. Because
one-sided RDMA operations bypass remote CPUs, INFINISWAP leverages the power of many choices to perform decentralized slab placements and evictions.

We have implemented and deployed INFINISWAP on
an RDMA cluster without any modifications to user applications or the OS and evaluated its effectiveness using multiple workloads running on unmodified VoltDB,
Memcached, PowerGraph, GraphX, and Apache Spark.
Using INFINISWAP, throughputs of these applications
improve between 4x (0.94x) to 15.4x (7.8) over disk
(Mellanox nbdX), and median and tail latencies between
5.4x (2x) and 61x (2.3x). INFINIS WAP achieves these
with negligible remote CPU usage, whereas nbdX becomes CPU-bound. INFINISWAP increases the overall
memory utilization of a cluster and works well at scale.

",17.353723509956247,16.643228070175443,234
NSDI_17_038.txt,14.652667245821302,13.367405267954258,NSDI,7027,"

TuX? is a new distributed graph engine that bridges
graph computation and distributed machine learning.
TuxX? inherits the benefits of an elegant graph computation model, efficient graph layout, and balanced parallelism to scale to billion-edge graphs; we extend and
optimize it for distributed machine learning to support
heterogeneity, a Stale Synchronous Parallel model, and
a new MEGA (Mini-batch, Exchange, GlobalSync, and
Apply) model.

We have developed a set of representative distributed
machine learning algorithms in TUX”, covering both supervised and unsupervised learning. Compared to implementations on distributed machine learning platforms,
writing these algorithms in TUX? takes only about 25%
of the code: Our graph computation model hides the detailed management of data layout, partitioning, and parallelism from developers. Our extensive evaluation of
TuX?, using large data sets with up to 64 billion edges,
shows that TUX? outperforms state-of-the-art distributed
graph engines PowerGraph and PowerLyra by an order of
magnitude, while beating two state-of-the-art distributed
machine learning systems by at least 48%.

",14.867677710551934,14.006706114398426,170
NSDI_17_039.txt,15.951045008732898,14.742878951553664,NSDI,8254,"
Building software-defined network controllers is an
exercise in software development and, as such, likely to
introduce bugs. We present Cocoon, a framework for
SDN development that facilitates both the design and
verification of complex networks using stepwise refinement to move from a high-level specification to the final
network implementation.

A Cocoon user specifies intermediate design levels in
a hierarchical design process that delineates the modularity in complicated network forwarding and makes verification extremely efficient. For example, an enterprise
network, equipped with VLANs, ACLs, and Level 2 and
Level 3 Routing, can be decomposed cleanly into abstractions for each mechanism, and the resulting stepwise verification is over 200x faster than verifying the final implementation. Cocoon further separates static network design from its dynamically changing configuration. The former is verified at design time, while the latter is checked at run time using statically defined invariants. We present six different SDN use cases including
B4 and F10. Our performance evaluation demonstrates
that Cocoon is not only faster than existing verification
tools but can also find many bugs statically before the
network design has been fully specified.

",17.122413403193683,17.028750000000006,186
NSDI_17_040.txt,16.770364204819547,15.454482432986818,NSDI,9450,"

Recent work has made great progress in verifying the forwarding correctness of networks [26—28, 35]. However,
these approaches cannot be used to verify networks containing middleboxes, such as caches and firewalls, whose
forwarding behavior depends on previously observed traffic. We explore how to verify reachability properties for
networks that include such “mutable datapath” elements,
both for the original network and in the presence of failures.
The main challenge lies in handling large and complicated
networks. We achieve scaling by developing and leveraging the concept of slices, which allow network-wide
verification to only require analyzing small portions of the
network. We show that with slices the time required to
verify an invariant on many production networks is independent of the size of the network itself.

",15.903189008614273,14.233333333333338,127
NSDI_17_041.txt,14.41981605532245,12.784963871214341,NSDI,11212,"

When debugging an SDN application, diagnosing the
problem is merely the first step: the operator must still
find a fix that solves the problem, without causing new
problems elsewhere. However, most existing debuggers
focus exclusively on diagnosis and offer the network operator little or no help with finding an effective fix. Finding a suitable fix is difficult because the number of candidates can be enormous.

In this paper, we propose a step towards automated
repair for SDN applications. Our approach consists of
two elements. The first is a data structure that we call
meta provenance, which can be used to efficiently find
good candidate repairs. Meta provenance is inspired by
the provenance concept from the database community;
however, whereas standard provenance can only reason
about changes to data, meta provenance can also reason
about changes to programs. The second element is a system that can efficiently backtest a set of candidate repairs
using historical data from the network. This is used to
eliminate candidate repairs that do not work well, or that
cause other problems.

We have implemented a system that maintains meta
provenance for SDNs, as well as a prototype debugger
that uses the meta provenance to automatically suggest
repairs. Results from several case studies show that, for
problems of moderate complexity, our debugger can find
high-quality repairs within one minute.

",15.062637766997835,13.535197717081125,224
NSDI_17_042.txt,15.254902015377333,13.93967174728759,NSDI,8787,"

Real-time network verification promises to automatically
detect violations of network-wide reachability invariants
on the data plane. To be useful in practice, these violations need to be detected in the order of milliseconds,
without raising false alarms. To date, most real-time
data plane checkers address this problem by exploiting
at least one of the following two observations: (i) only
small parts of the network tend to be affected by typical
changes to the data plane, and (ii) many different packets tend to share the same forwarding behaviour in the
entire network. This paper shows how to effectively exploit a third characteristic of the problem, namely: simlarity among forwarding behaviour of packets through
parts of the network, rather than its entirety. We propose
the first provably amortized quasi-linear algorithm to do
so. We implement our algorithm in a new real-time data
plane checker, Delta-net. Our experiments with SDN-IP,
a globally deployed ONOS software-defined networking
application, and several hundred million IP prefix rules
generated using topologies and BGP updates from realworld deployed networks, show that Delta-net checks a
rule insertion or removal in approximately 40 microseconds on average, a more than 10x improvement over the
state-of-the-art. We also show that Delta-net eliminates
an inherent bottleneck in the state-of-the-art that restricts
its use in answering Datalog-style “what if” queries.

",17.267425705330172,16.624310344827588,233
OOPSLA_17_001.txt,13.885384341853804,12.127689573038506,OOPSLA,12011,"
In JavaScript programs, asynchrony arises in situations such as web-based user-interfaces, communicating with
servers through HTTP requests, and non-blocking I/O. Event-based programming is the most popular approach
for managing asynchrony, but suffers from problems such as lost events and event races, and results in code
that is hard to understand and debug. Recently, ECMAScript 6 has added support for promises, an alternative
mechanism for managing asynchrony that enables programmers to chain asynchronous computations while
supporting proper error handling. However, promises are complex and error-prone in their own right, so
programmers would benefit from techniques that can reason about the correctness of promise-based code.

Since the ECMAScript 6 specification is informal and intended for implementers of JavaScript engines, it
does not provide a suitable basis for formal reasoning. This paper presents Ap, a core calculus that captures the
essence of ECMAScript 6 promises. Based on Ap, we introduce the promise graph, a program representation that
can assist programmers with debugging of promise-based code. We then report on a case study in which we
investigate how the promise graph can be helpful for debugging errors related to promises in code fragments
posted to the StackOverflow website.
",18.243605946275583,17.044483734087695,203
OOPSLA_17_002.txt,12.98402036519095,10.754678360923197,OOPSLA,13549,"
Dependent Object Types (DOT) is intended to be a core calculus for modelling Scala. Its distinguishing feature
is abstract type members, fields in objects that hold types rather than values. Proving soundness of DOT has
been surprisingly challenging, and existing proofs are complicated, and reason about multiple concepts at
the same time (e.g. types, values, evaluation). To serve as a core calculus for Scala, DOT should be easy to
experiment with and extend, and therefore its soundness proof needs to be easy to modify.
This paper presents a simple and modular proof strategy for reasoning in DOT. The strategy separates
reasoning about types from other concerns. It is centred around a theorem that connects the full DOT type
system to a restricted variant in which the challenges and paradoxes caused by abstract type members are
eliminated. Almost all reasoning in the proof is done in the intuitive world of this restricted type system. Once
we have the necessary results about types, we observe that the other aspects of DOT are mostly standard and
can be incorporated into a soundness proof using familiar techniques known from other calculi.
Our paper comes with a machine-verified version of the proof in Coq.
",14.06817628641468,12.54666666666667,202
OOPSLA_17_003.txt,14.279509643744078,12.641072582339145,OOPSLA,10771,"
Developing a small but useful set of inputs for tests is challenging. We show that a domain-specific language
backed by a constraint solver can help the programmer with this process. The solver can generate a set of test
inputs and guarantee that each input is different from other inputs in a way that is useful for testing.

This paper presents Iorek: a tool that empowers the programmer with the ability to express to any SMT
solver what it means for inputs to be different. The core of Iorek is a rich language for constraining the set of
inputs, which includes a novel bounded enumeration mechanism that makes it easy to define and encode a
flexible notion of difference over a recursive structure. We demonstrate the flexibility of this mechanism for
generating strings.

We use Iorek to test real services and find that it is effective at finding bugs. We also build Iorek into a
random testing tool and show that it increases coverage.
",13.227904075235838,11.328170731707317,165
OOPSLA_17_004.txt,15.679701382705435,14.732901024742429,OOPSLA,11818,"
We present a concurrent-read exclusive-write buffer system with strong correctness and security properties.
Our motivating application for this system is the distribution of sensor values in a multicomponent vehiclecontrol system, where some components are unverified and possibly malicious, and other components are
vehicle-control-critical and must be verified. Valid participants are guaranteed correct communication (i.e., the
writer is always able to write to an unused buffer, and readers always read the most recently published value),
while invalid readers or writers cannot compromise the correctness or liveness of valid participants. There is
only one writer, all operations are wait-free, and there is no extra process or thread mediating communication.
We prove the correctness of the system with valid participants by formally verifying a C implementation of
the system in Coq, using the Verified Software Toolchain extended with an atomic exchange operation. The
result is the first C-level mechanized verification of a nonblocking communication protocol.
",18.599290044081553,17.906582278481014,160
OOPSLA_17_005.txt,16.85232297380101,15.396083351262671,OOPSLA,9699,"
A memory consistency model (or simply memory model) defines the possible values that a shared-memory
read may return in a multithreaded programming language. Choosing a memory model involves an inherent
performance-programmability tradeoff. The Java language has adopted a relaxed (or weak) memory model
that is designed to admit most traditional compiler optimizations and obviate the need for hardware fences on
most shared-memory accesses. The downside, however, is that programmers are exposed to a complex and
unintuitive semantics and must carefully declare certain variables as volatile in order to enforce program
orderings that are necessary for proper behavior.
This paper proposes a simpler and stronger memory model for Java through a conceptually small change:
every variable has volatile semantics by default, but the language allows a programmer to tag certain
variables, methods, or classes as relaxed and provides the current Java semantics for these portions of code.
This volatile-by-default semantics provides sequential consistency (SC) for all programs by default. At the
same time, expert programmers retain the freedom to build performance-critical libraries that violate the SC
semantics.
At the outset, it is unclear if the volatile-by-default semantics is practical for Java, given the cost of
memory fences on today’s hardware platforms. The core contribution of this paper is to demonstrate, through
comprehensive empirical evaluation, that the volatile-by-default semantics is arguably acceptable for a
predominant use case for Java today Ð server-side applications running on Intel x86 architectures. We present
VBD-HotSpot, a modification to Oracle’s widely used HotSpot JVM that implements the volatile-by-default
semantics for x86. To our knowledge VBD-HotSpot is the first implementation of SC for Java in the context of
a modern JVM. VBD-HotSpot incurs an average overhead versus the baseline HotSpot JVM of 28% for the Da
Capo benchmarks, which is significant though perhaps less than commonly assumed. Further, VBD-HotSpot
incurs average overheads of 12% and 19% respectively on standard benchmark suites for big-data analytics
and machine learning in the widely used Spark framework.
",17.30067935935376,16.476676384839653,346
OOPSLA_17_006.txt,15.996318472518144,15.20459406188225,OOPSLA,11370,"
In this paper, we consider the problem of source code abridgment, where the goal is to remove statements
from a source code in order to display the source code in a small space, while at the same time leaving the
“important” parts of the source code intact, so that an engineer can read the code and quickly understand
purpose of the code. To this end, we develop an algorithm that looks at a number of example, human-created
source code abridgments, and learns how to remove lines from the code in order to mimic the human abridger.
The learning algorithm takes into account syntactic features of the code, as well as semantic features such as
control flow and data dependencies. Through a comprehensive user study, we show that the abridgments
that our system produces can decrease the time that a user must look at code in order to understand its
functionality, as well as increase the accuracy of the assessment, while displaying the code in a greatly reduced
area.
",17.122413403193683,18.831878698224852,170
OOPSLA_17_007.txt,16.324590921473238,14.579062184938554,OOPSLA,13584,"
The emergence of energy harvesting devices creates the potential for batteryless sensing and computing
devices. Such devices operate only intermittently, as energy is available, presenting a number of challenges for
software developers. Programmers face a complex design space requiring reasoning about energy, memory
consistency, and forward progress. This paper introduces Alpaca, a low-overhead programming model for
intermittent computing on energy-harvesting devices. Alpaca programs are composed of a sequence of userdefined tasks. The Alpaca runtime preserves execution progress at the granularity of a task. The key insight
in Alpaca is the privatization of data shared between tasks. Shared values written in a task are detected using
idempotence analysis and copied into a buffer private to the task. At the end of the task, modified values from
the private buffer are atomically committed to main memory, ensuring that data remain consistent despite
power failures. Alpaca provides a familiar programming interface, a highly efficient runtime model, and places
few restrictions on a target device’s hardware. We implemented a prototype of Alpaca as an extension to C with
an LLVM compiler pass. We evaluated Alpaca, and directly compared to two systems from prior work. Alpaca
eliminates checkpoints, which improves performance up to 15x, and Alpaca avoids static multi-versioning,
which improves memory consumption by up to 5.5x.
",16.47975071879961,14.569767441860467,218
OOPSLA_17_008.txt,15.732876657290916,13.808330362615546,OOPSLA,11245,"
Today’s cloud services extensively rely on replication techniques to ensure availability and reliability. In
complex datacenter network architectures, however, seemingly independent replica servers may inadvertently
share deep dependencies (e.g., aggregation switches). Such unexpected common dependencies may potentially
result in correlated failures across the entire replication deployments, invalidating the efforts. Although
existing cloud management and diagnosis tools have been able to offer post-failure forensics, they, nevertheless,
typically lead to quite prolonged failure recovery time in the cloud-scale systems. In this paper, we propose a
novel language framework, named RepAudit, that manages to prevent correlated failure risks before service
outages occur, by allowing cloud administrators to proactively audit the replication deployments of interest.
In particular, RepAudit consists of three new components: 1) a declarative domain-specific language, RAL, for
cloud administrators to write auditing programs expressing diverse auditing tasks; 2) a high-performance
RAL auditing engine that generates the auditing results by accurately and efficiently analyzing the underlying
structures of the target replication deployments; and 3) an RAL-code generator that can automatically produce
complex RAL programs based on easily written specifications. Our evaluation result shows that RepAudit
uses 80× less lines of code than state-of-the-art efforts in expressing the auditing task of determining the
top-20 critical correlated-failure root causes. To the best of our knowledge, RepAudit is the first effort capable
of simultaneously offering expressive, accurate and efficient correlated failure auditing to the cloud-scale
replication systems.
",21.08112680061502,20.968934426229513,246
OOPSLA_17_009.txt,15.839597749137202,14.6671089594705,OOPSLA,13879,"
We present an automated technique for finding defects in compilers for graphics shading languages. A key
challenge in compiler testing is the lack of an oracle that classifies an output as correct or incorrect; this is
particularly pertinent in graphics shader compilers where the output is a rendered image that is typically underspecified. Our method builds on recent successful techniques for compiler validation based on metamorphic
testing, and leverages existing high-value graphics shaders to create sets of transformed shaders that should
be semantically equivalent. Rendering mismatches are then indicative of shader compilation bugs. Deviant
shaders are automatically minimized to identify, in each case, a minimal change to an original high-value
shader that induces a shader compiler bug. We have implemented the approach as a tool, GLFuzz, targeting the
OpenGL shading language, GLSL. Our experiments over a set of 17 GPU and driver configurations, spanning
the main 7 GPU designers, have led to us finding and reporting more than 60 distinct bugs, covering all tested
configurations. As well as defective rendering, these issues identify security-critical vulnerabilities that affect
WebGL, including a significant remote information leak security bug where a malicious web page can capture
the contents of other browser tabs, and a bug whereby visiting a malicious web page can lead to a “blue
screen of death” under Windows 10. Our findings show that shader compiler defects are prevalent, and that
metamorphic testing provides an effective means for detecting them automatically.
",17.755912252390015,16.93753086419753,244
OOPSLA_17_010.txt,14.48271832472561,12.604471187096635,OOPSLA,11656,"
We present a technique for automatically generating features for data-driven program analyses. Recently datadriven approaches for building a program analysis have been developed, which mine existing codebases and
automatically learn heuristics for finding a cost-effective abstraction for a given analysis task. Such approaches
reduce the burden of the analysis designers, but they do not remove it completely; they still leave the nontrivial
task of designing so called features to the hands of the designers. Our technique aims at automating this
feature design process. The idea is to use programs as features after reducing and abstracting them. Our
technique goes through selected program-query pairs in codebases, and it reduces and abstracts the program
in each pair to a few lines of code, while ensuring that the analysis behaves similarly for the original and the
new programs with respect to the query. Each reduced program serves as a boolean feature for program-query
pairs. This feature evaluates to true for a given program-query pair when (as a program) it is included in
the program part of the pair. We have implemented our approach for three real-world static analyses. The
experimental results show that these analyses with automatically-generated features are cost-effective and
consistently perform well on a wide range of programs.
",15.112257680678326,13.595930232558139,216
OOPSLA_17_011.txt,14.182272029125581,12.311985824506703,OOPSLA,12652,"
Bounded exhaustive testing is an effective methodology for detecting bugs in a wide range of applications. A
well-known approach for bounded exhaustive testing is Korat which generates all test inputs up to a given
small size based on a formal specification that characterizes properties of desired test inputs. This specification
is written as an executable predicate and Korat executes the predicate on candidate inputs to implement a
backtracking search based on pruning to systematically explore the space of all possible inputs and generate
only those that satisfy the specification.

This paper presents a novel approach for speeding up test generation for bounded exhaustive testing using
Korat. The novelty of our approach is two-fold. One, we introduce a new approach for writing the specification
predicate based on an abstract representation of candidate inputs, so that the predicate executes directly
on these abstract structures and each execution has a lower cost. Two, we use the abstract representation
as the basis to define the first technique for utilizing GPUs for systematic test generation using executable
predicates. Moreover, we present a suite of optimizations that are necessary to enable effective utilization of
the computational resources offered by modern GPUs. We use our prototype tool to experimentally evaluate
our approach using a suite of 7 data structures that were used in prior studies on bounded exhaustive testing.
Our results show that our abstract representation can speed up test generation by 5.68 x on a standard CPU,
while execution on a GPU speeds up the generation, on average, by 17.46x.
",17.122413403193683,16.4268125,259
OOPSLA_17_012.txt,14.906911339159258,13.38118337972707,OOPSLA,10802,"
We present a new data-driven approach to achieve highly cost-effective context-sensitive points-to analysis
for Java. While context-sensitivity has greater impact on the analysis precision and performance than any
other precision-improving techniques, it is difficult to accurately identify the methods that would benefit the
most from context-sensitivity and decide how much context-sensitivity should be used for them. Manually
designing such rules is a nontrivial and laborious task that often delivers suboptimal results in practice. To
overcome these challenges, we propose an automated and data-driven approach that learns to effectively apply
context-sensitivity from codebases. In our approach, points-to analysis is equipped with a parameterized and
heuristic rules, in disjunctive form of properties on program elements, that decide when and how much to apply
context-sensitivity. We present a greedy algorithm that efficiently learns the parameter of the heuristic rules.
We implemented our approach in the Doop framework and evaluated using three types of context-sensitive
analyses: conventional object-sensitivity, selective hybrid object-sensitivity, and type-sensitivity. In all cases,
experimental results show that our approach significantly outperforms existing techniques.
",17.553077303434723,17.551595744680856,189
OOPSLA_17_013.txt,12.501622487370298,10.104767794175128,OOPSLA,12471,"
Futures are an elegant approach to expressing parallelism in functional programs. However, combining
futures with imperative programming (as in C++ or in Java) can lead to pernicious bugs in the form of data
races and deadlocks, as a consequence of uncontrolled data flow through mutable shared memory.

In this paper we introduce the Known Joins (KJ) property for parallel programs with futures, and relate it to
the Deadlock Freedom (DF) and the Data-Race Freedom (DRF) properties. Our paper offers two key theoretical
results: 1) DRF implies KJ, and 2) KJ implies DF. These results show that data-race freedom is sufficient to
guarantee deadlock freedom in programs with futures that only manipulate unsynchronized shared variables.
To the best of our knowledge, these are the first theoretical results to establish sufficient conditions for deadlock
freedom in imperative parallel programs with futures, and to characterize the subset of data races that can
trigger deadlocks (those that violate the KJ property).

From result 2), we developed a tool that avoids deadlocks in linear time and space when KJ holds, i.e., when
there are no data races among references to futures. When KJ fails, the tool reports the data race and optionally
falls back to a standard deadlock avoidance algorithm by cycle detection. Our tool verified a dataset of ~2,300
student’s homework solutions and found one deadlocked program. The performance results obtained from
our tool are very encouraging: a maximum slowdown of 1.06x on a 16-core machine, always outperforming
deadlock avoidance via cycle-detection. Proofs of the two main results were formalized using the Coq proof
assistant.
",15.903189008614273,15.071701492537319,270
OOPSLA_17_014.txt,14.475945776926988,12.331961484552284,OOPSLA,8628,"
Identifier names are often used by developers to convey additional information about the meaning of a program
over and above the semantics of the programming language itself. We present an algorithm that uses this
information to detect argument selection defects, in which the programmer has chosen the wrong argument to
a method call in Java programs. We evaluate our algorithm at Google on 200 million lines of internal code and
10 million lines of predominantly open-source external code and find defects even in large, mature projects
such as OpenJDK, ASM, and the MySQL JDBC. The precision and recall of the algorithm vary depending on a
sensitivity threshold. Higher thresholds increase precision, giving a true positive rate of 85%, reporting 459
true positives and 78 false positives. Lower thresholds increase recall but lower the true positive rate, reporting
2,060 true positives and 1,207 false positives. We show that this is an order of magnitude improvement on
previous approaches. By analyzing the defects found, we are able to quantify best practice advice for API
design and show that the probability of an argument selection defect increases markedly when methods have
more than five arguments.
CCS Concepts: • Software and its engineering → Software defect analysis; Automated static analysis;
Additional Key Words and Phrases: empirical study, name-based program analysis, static analysis, method
arguments
",16.99224021665606,15.824675716440428,223
OOPSLA_17_015.txt,12.609426636160553,10.362608391924997,OOPSLA,9323,"
Previous studies have shown that there is a non-trivial amount of duplication in source code. This paper
analyzes a corpus of 4.5 million non-fork projects hosted on GitHub representing over 428 million iles written
in Java, C++, Python, and JavaScript. We found that this corpus has a mere 85 million unique iles. In other
words, 70% of the code on GitHub consists of clones of previously created iles. There is considerable variation
between language ecosystems. JavaScript has the highest rate of ile duplication, only 6% of the iles are distinct.
Java, on the other hand, has the least duplication, 60% of iles are distinct. Lastly, a project-level analysis shows
that between 9% and 31% of the projects contain at least 80% of iles that can be found elsewhere. These rates
of duplication have implications for systems built on open source software as well as for researchers interested
in analyzing large code bases. As a concrete artifact of this study, we have created DéjàVu, a publicly available
map of code duplicates in GitHub repositories.
",12.340626583579944,10.576719101123597,178
OOPSLA_17_016.txt,13.853258491452568,11.960139905254714,OOPSLA,13663,"
We propose an interactive approach to resolve static analysis alarms. Our approach synergistically combines a
sound but imprecise analysis with precise but unsound heuristics, through user interaction. In each iteration, it
solves an optimization problem to find a set of questions for the user such that the expected payoff is maximized.
We have implemented our approach in a tool, Ursa, that enables interactive alarm resolution for any analysis
specified in the declarative logic programming language Datalog. We demonstrate the effectiveness of Ursa
on a state-of-the-art static datarace analysis using a suite of 8 Java programs comprising 41-194 KLOC each.
Ursa is able to eliminate 74% of the false alarms per benchmark with an average payoff of 12× per question.
Moreover, Ursa prioritizes user effort effectively by posing questions that yield high payoffs earlier.
",15.151101081350806,14.501469979296068,138
OOPSLA_17_017.txt,13.918297003230698,11.564730518519468,OOPSLA,11841,"
Non-volatile memory (NVM) technologies such as PCM, ReRAM and STT-RAM allow processors to directly
write values to persistent storage at speeds that are significantly faster than previous durable media such
as hard drives or SSDs. Many applications of NVM are constructed on a logging subsystem, which enables
operations to appear to execute atomically and facilitates recovery from failures. Writes to NVM, however,
pass through a processor’s memory system, which can delay and reorder them and can impair the correctness
and cost of logging algorithms.
Reordering arises because of out-of-order execution in a CPU and the inter-processor cache coherence
protocol. By carefully considering the properties of these reorderings, this paper develops a logging protocol
that requires only one round trip to non-volatile memory while avoiding expensive computations. We show
how to extend the logging protocol to building a persistent set (hash map) that also requires only a single
round trip to non-volatile memory for insertion, updating, or deletion.
",19.946969662950774,17.08707317073171,166
OOPSLA_17_018.txt,16.152091683566564,14.412035679933805,OOPSLA,13567,"
While unstructured merge tools rely only on textual analysis to detect and resolve conflicts, semistructured
merge tools go further by partially exploiting the syntactic structure and semantics of the involved artifacts.
Previous studies compare these merge approaches with respect to the number of reported conflicts, showing,
for most projects and merge situations, reduction in favor of semistructured merge. However, these studies do
not investigate whether this reduction actually leads to integration effort reduction (productivity) without
negative impact on the correctness of the merging process (quality). To analyze that, and better understand
how merge tools could be improved, in this paper we reproduce more than 30,000 merges from 50 open source
projects, identifying conflicts incorrectly reported by one approach but not by the other (false positives),
and conflicts correctly reported by one approach but missed by the other (false negatives). Our results and
complementary analysis indicate that, in the studied sample, the number of false positives is significantly
reduced when using semistructured merge. We also find evidence that its false positives are easier to analyze
and resolve than those reported by unstructured merge. However, we find no evidence that semistructured
merge leads to fewer false negatives, and we argue that they are harder to detect and resolve than unstructured
merge false negatives. Driven by these findings, we implement an improved semistructured merge tool that
further combines both approaches to reduce the false positives and false negatives of semistructured merge.
We find evidence that the improved tool, when compared to unstructured merge in our sample, reduces the
number of reported conflicts by half, has no additional false positives, has at least 8% fewer false negatives,
and is not prohibitively slower.
",18.71604785175511,17.84947242206235,280
OOPSLA_17_019.txt,15.030775260585745,12.754299804248454,OOPSLA,13468,"
Frameworks and libraries provide application programming interfaces (APIs) that serve as building blocks in
modern software development. As APIs present the opportunity of increased productivity, it also calls for
correct use to avoid buggy code. The usage-based specification mining technique has shown great promise in
solving this problem through a data-driven approach. These techniques leverage the use of the API in large
corpora to understand the recurring usages of the APIs and infer behavioral specifications (pre- and postconditions) from such usages. A challenge for such technique is thus inference in the presence of insufficient
usages, in terms of both frequency and richness. We refer to this as a “sparse usage problem."" This paper
presents the first technique to solve the sparse usage problem in usage-based precondition mining. Our
key insight is to leverage implicit beliefs to overcome sparse usage. An implicit belief (IB) is the knowledge
implicitly derived from the fact about the code. An IB about a program is known implicitly to a programmer
via the language’s constructs and semantics, and thus not explicitly written or specified in the code. The
technical underpinnings of our new precondition mining approach include a technique to analyze the data
and control flow in the program leading to API calls to infer preconditions that are implicitly present in the
code corpus, a catalog of 35 code elements in total that can be used to derive implicit beliefs from a program,
and empirical evaluation of all of these ideas. We have analyzed over 350 millions lines of code and 7 libraries
that suffer from the sparse usage problem. Our approach realizes 6 implicit beliefs and we have observed that
adding single-level context sensitivity can further improve the result of usage based precondition mining. The
result shows that we achieve overall 60% in precision and 69% in recall and the accuracy is relatively improved
by 32% in precision and 78% in recall compared to base usage-based mining approach for these libraries.
",14.856640023380862,14.12920371169616,333
OOPSLA_17_020.txt,14.20408703613089,12.372653429945352,OOPSLA,13427,"
With the range and sensitivity of algorithmic decisions expanding at a break-neck speed, it is imperative that
we aggressively investigate fairness and bias in decision-making programs. First, we show that a number of
recently proposed formal definitions of fairness can be encoded as probabilistic program properties. Second,
with the goal of enabling rigorous reasoning about fairness, we design a novel technique for verifying
probabilistic properties that admits a wide class of decision-making programs. Third, we present FairSquare,
the first verification tool for automatically certifying that a program meets a given fairness property. We
evaluate FairSquare on a range of decision-making programs. Our evaluation demonstrates FairSquare’s
ability to verify fairness for a range of different programs, which we show are out-of-reach for state-of-the-art
program analysis techniques.
",16.92669308720184,15.751343283582095,136
OOPSLA_17_021.txt,14.718010429554536,12.688203646953905,OOPSLA,14930,"
Parametric polymorphism and inheritance are both important, extensively explored language mechanisms
for providing code reuse and extensibility. But harmoniously integrating these apparently distinct mechanismsÐ
and powerful recent forms of them, including type classes and family polymorphismÐin a single language
remains an elusive goal. In this paper, we show that a deep unification can be achieved by generalizing the
semantics of interfaces and classes. The payoff is a significant increase in expressive power with little increase
in programmer-visible complexity. Salient features of the new programming language include retroactive
constraint modeling, underpinning both object-oriented programming and generic programming, and modulelevel inheritance with further-binding, allowing family polymorphism to be deployed at large scale. The
resulting mechanism is syntactically light, and the more advanced features are transparent to the novice
programmer. We describe the design of a programming language that incorporates this mechanism; using a
core calculus, we show that the type system is sound. We demonstrate that this language is highly expressive
by illustrating how to use it to implement highly extensible software and by showing that it can not only
concisely model state-of-the-art features for code reuse, but also go beyond them.
",18.10804710084791,17.143571428571423,197
OOPSLA_17_022.txt,14.066138265663614,11.820523381088442,OOPSLA,14160,"
In this paper we present the design and implementation of Flow, a fast and precise type checker for JavaScript
that is used by thousands of developers on millions of lines of code at Facebook every day. Flow uses
sophisticated type inference to understand common JavaScript idioms precisely. This helps it find non-trivial
bugs in code and provide code intelligence to editors without requiring significant rewriting or annotations from
the developer. We formalize an important fragment of Flow’s analysis and prove its soundness. Furthermore,
Flow uses aggressive parallelization and incrementalization to deliver near-instantaneous response times.
This helps it avoid introducing any latency in the usual edit-refresh cycle of rapid JavaScript development. We
describe the algorithms and systems infrastructure that we built to scale Flow’s analysis.
",15.903189008614273,14.342407199100112,130
OOPSLA_17_023.txt,15.20404261288548,12.980190546676301,OOPSLA,12441,"
This paper presents GLORE, a novel approach to enabling the detection and removal of large-scope redundant
computations in nested loops. GLORE works on LER-notation, a new representation of computations in both
regular and irregular loops. Together with a set of novel algorithms, it makes GLORE able to systematically
consider computation reordering at both the expression level and the loop level in a unified manner. GLORE
shows an applicability much broader than prior methods have, and frequently lowers the computational
complexities of some nested loops that are elusive to prior optimization techniques, producing significantly
larger speedups.
",17.693802365651003,16.85925257731959,98
OOPSLA_17_024.txt,13.939722382274518,12.449016296990226,OOPSLA,11176,"
Many service applications use actors as a programming model for the middle tier, to simplify synchronization,
fault-tolerance, and scalability. However, efficient operation of such actors in multiple, geographically distant
datacenters is challenging, due to the very high communication latency. Caching and replication are essential
to hide latency and exploit locality; but it is not a priori clear how to combine these techniques with the actor
programming model.

We present GEo, an open-source geo-distributed actor system that improves performance by caching
actor states in one or more datacenters, yet guarantees the existence of a single latest version by virtue of
a distributed cache coherence protocol. GEo’s programming model supports both volatile and persistent
actors, and supports updates with a choice of linearizable and eventual consistency. Our evaluation on several
workloads shows substantial performance benefits, and confirms the advantage of supporting both replicated
and single-instance coherence protocols as configuration choices. For example, replication can provide fast,
always-available reads and updates globally, while batching of linearizable storage accesses at a single location
can boost the throughput of an order processing workload by 7x.
",18.99602597827317,18.31714285714286,187
OOPSLA_17_025.txt,15.7862799137345,14.465908560830517,OOPSLA,10980,"
Static analyses aspire to explore all possible executions in order to achieve soundness. Yet, in practice, they
fail to capture common dynamic behavior. Enhancing static analyses with dynamic information is a common
pattern, with tools such as Tamiflex. Past approaches, however, miss significant portions of dynamic behavior,
due to native code, unsupported features (e.g., invokedynamic or lambdas in Java), and more. We present
techniques that substantially counteract the unsoundness of a static analysis, with virtually no intrusion
to the analysis logic. Our approach is reified in the HeapDL toolchain and consists in taking whole-heap
snapshots during program execution, that are further enriched to capture significant aspects of dynamic
behavior, regardless of the causes of such behavior. The snapshots are then used as extra inputs to the static
analysis. The approach exhibits both portability and significantly increased coverage. Heap information
under one set of dynamic inputs allows a static analysis to cover many more behaviors under other inputs.
A HeapDL-enhanced static analysis of the DaCapo benchmarks computes 99.5% (median) of the call-graph
edges of unseen dynamic executions (vs. 76.9% for the Tamiflex tool).
",15.247664890283005,14.332027027027028,189
OOPSLA_17_026.txt,13.667468762183411,11.915262079124926,OOPSLA,12513,"
Providing better supports for debugging type errors has been an active research area in the last three decades.
Numerous approaches from different perspectives have been developed. Most approaches work well under
certain conditions only, for example, when type errors are caused by single leaves and when type annotations
are correct. However, the research community is still unaware of which conditions hold in practice and what
the real debugging situations look like. We address this problem with a study of 3 program data sets, which
were written in different years, using different compilers, and were of diverse sizes. They include more than
55,000 programs, among which more than 2,700 are ill typed. We investigated all the ill-typed programs, and
our results indicate that current error debugging support is far from sufficient in practice since only about
35% of all type errors were caused by single leaves. In addition, type annotations cannot always be trusted in
error debuggers since about 30% of the time type errors were caused by wrong type annotations. Our study
also provides many insights about the debugging behaviors of students in functional programming, which
could be exploited for developing more effective error debuggers.
",14.712192995108573,12.770680272108844,199
OOPSLA_17_027.txt,13.664856823652194,11.41634389788453,OOPSLA,14439,"
Program analyses frequently track objects throughout a program, which requires reasoning about aliases.
Most dataflow analysis frameworks, however, delegate the task of handling aliases to the analysis clients,
which causes a number of problems. For instance, custom-made extensions for alias analysis are complex and
cannot easily be reused. On the other hand, due to the complex interfaces involved, off-the-shelf alias analyses
are hard to integrate precisely into clients. Lastly, for precision many clients require strong updates, and alias
abstractions supporting strong updates are often relatively inefficient.
In this paper, we present IDEal , an alias-aware extension to the framework for Interprocedural Distributive
Environment (IDE) problems. IDEal relieves static-analysis authors completely of the burden of handling
aliases by automatically resolving alias queries on-demand, both efficiently and precisely. IDEal supports
a highly precise analysis using strong updates by resorting to an on-demand, flow-sensitive, and contextsensitive all-alias analysis. Yet, it achieves previously unseen efficiency by propagating aliases individually,
creating highly reusable per-pointer summaries.
We empirically evaluate IDEal by comparing TSf , a state-of-the-art typestate analysis, to TSal , an IDEal based typestate analysis. Our experiments show that the individual propagation of aliases within IDEal enables
TSal to propagate 10.4× fewer dataflow facts and analyze 10.3× fewer methods when compared to TSf . On
the DaCapo benchmark suite, TSal is able to efficiently compute precise results.
",17.02489783603662,16.646019313304723,234
OOPSLA_17_028.txt,15.021129683784007,13.005994424447739,OOPSLA,13805,"
This paper presents Fast Instrumentation Bias (FIB), a sound and complete dynamic data race detection
algorithm that improves performance by reducing or eliminating the costs of analysis atomicity. In addition
to checking for errors in target programs, dynamic data race detectors must introduce synchronization to
guard against metadata races that may corrupt analysis state and compromise soundness or completeness.
Pessimistic analysis synchronization can account for nontrivial performance overhead in a data race detector.
The core contribution of FIB is a novel cooperative ownership-based synchronization protocol whose
states and transitions are derived purely from preexisting analysis metadata and logic in a standard data race
detection algorithm. By exploiting work already done by the analysis, FIB ensures atomicity of dynamic analysis
actions with zero additional time or space cost in the common case. Analysis of temporally thread-local or
read-shared accesses completes safely with no synchronization. Uncommon write-sharing transitions require
synchronous cross-thread coordination to ensure common cases may proceed synchronization-free.
We implemented FIB in the Jikes RVM Java virtual machine. Experimental evaluation shows that FIB
eliminates nearly all analysis atomicity costs on programs where data often experience windows of threadlocal access. Adaptive extensions to the ownership policy effectively eliminate high coordination costs of the
core ownership protocol on programs with high rates of serialized sharing. FIB outperforms a naïve pessimistic
synchronization scheme by 50% on average. Compared to a tuned optimistic metadata synchronization scheme
based on conventional one-grained atomic compare-and-swap operations, FIB is competitive overall, and up to
17% faster on some programs. Overall, FIB effectively exploits latent analysis and program invariants to bring
strong integrity guarantees to an otherwise unsynchronized data race detection algorithm at minimal cost.
",18.572220574447478,17.776478873239437,285
OOPSLA_17_029.txt,14.236457708299287,12.330687167454887,OOPSLA,13659,"
Type inference is convenient by allowing programmers to elide type annotations, but this comes at the cost
of often generating very confusing and opaque type error messages that are of little help to fix type errors.
Though there have been many successful attempts at making type error messages better in the past thirty
years, many classes of errors are still difficult to fix. In particular, current approaches still generate imprecise
and uninformative error messages for type errors arising from errors in grouping constructs like parentheses
and brackets. Worse, a recent study shows that these errors usually take more than 10 steps to fix and occur
quite frequently (around 45% to 60% of all type errors) in programs written by students learning functional
programming. We call this class of errors, nonstructural errors.

We solve this problem by developing LEARNSKELL, a type error debugger that uses machine learning
to help diagnose and deliver high quality error messages, for programs that contain nonstructural errors.
While previous approaches usually report type errors on typing constraints or on the type level, LEARNSKELL
generates suggestions on the expression level. We have performed an evaluation on more than 1,500 type
errors, and the result shows that LEARNSKELL is quite precise. It can correctly capture 86% of all nonstructural
errors and locate the error cause with a precision of 63%/87% with the first 1/3 messages, respectively. This is
several times more than the precision of state-of-the-art compilers and debuggers. We have also studied the
performance of LEARNSKELL and found out that it scales to large programs.
",15.186304673781336,13.475990321465606,265
OOPSLA_17_030.txt,14.418653644888519,12.796223282495056,OOPSLA,13539,"
1 INTRODUCTION
Types are awesome. Languages like OCaml and Haskell make the value-proposition for types
even more appealing by using constraints to automatically synthesize the types for all program
terms without troubling the programmer for any annotations. Unfortunately, this automation has
come at a price. Type annotations signify the programmer’s intent and help to correctly blame the
Localizing type errors is challenging in languages with global type inference, as the type checker must make
assumptions about what the programmer intended to do. We introduce Nate, a data-driven approach to
error localization based on supervised learning. Nate analyzes a large corpus of training data Ð pairs of
ill-typed programs and their łfixedž versions Ð to automatically learn a model of where the error is most likely
to be found. Given a new ill-typed program, Nate executes the model to generate a list of potential blame
assignments ranked by likelihood. We evaluate Nate by comparing its precision to the state of the art on
a set of over 5,000 ill-typed OCaml programs drawn from two instances of an introductory programming
course. We show that when the top-ranked blame assignment is considered, Nate’s data-driven model is
able to correctly predict the exact sub-expression that should be changed 72% of the time, 28 points higher
than OCaml and 16 points higher than the state-of-the-art SHErrLoc tool. Furthermore, Nate’s accuracy
surpasses 85% when we consider the top two locations and reaches 91% if we consider the top three.
",15.6451,13.595527559055117,259
OOPSLA_17_031.txt,15.478715520490386,13.31020469932859,OOPSLA,11326,"
Modern concurrent copying garbage collection (GC), in particular, real-time GC, uses fine-grained synchronizations with a mutator, which is the application program that mutates memory, when it moves objects in its
copy phase. It resolves a data race using a concurrent copying protocol, which is implemented as interactions
between the collector threads and the read and write barriers that the mutator threads execute. The behavioral
effects of the concurrent copying protocol rely on the memory model of the CPUs and the programming
languages in which the GC is implemented. It is difficult, however, to formally investigate the behavioral
properties of concurrent copying protocols against various memory models.
To address this problem, we studied the feasibility of the bounded model checking of concurrent copying
protocols with memory models. We investigated a correctness-related behavioral property of copying protocols
of various concurrent copying GC algorithms, including real-time GC Stopless, Clover, Chicken, Staccato, and
Schism against six memory models, total store ordering (TSO), partial store ordering (PSO), relaxed memory
ordering (RMO), and their variants, in addition to sequential consistency (SC) using bounded model checking.
For each combination of a protocol and memory model, we conducted model checking with a model of a
mutator. In this wide range of case studies, we found faults in two GC algorithms, one of which is relevant
to the memory model. We fixed these faults with the great help of counterexamples. We also modified some
protocols so that they work under some memory models weaker than those for which the original protocols
were designed, and checked them using model checking. We believe that bounded model checking is a feasible
approach to investigate behavioral properties of concurrent copying protocols under weak memory models.
",19.10251660953655,16.7472027972028,287
OOPSLA_17_032.txt,15.710370953811843,13.434075041408644,OOPSLA,11479,"
Binary rewriters are tools that are used to modify the functionality of binaries lacking source code. Binary
rewriters can be used to rewrite binaries for a variety of purposes including optimization, hardening, and
extraction of executable components. To rewrite a binary based on semantic criteria, an essential primitive to
have is a machine-code synthesizer—a tool that synthesizes an instruction sequence from a specification of the
desired behavior, often given as a formula in quantifier-free bit-vector logic (QFBV). However, state-of-the-art
machine-code synthesizers such as McSynth++ employ naive search strategies for synthesis: McSynth++
merely enumerates candidates of increasing length without performing any form of prioritization. This
inefficient search strategy is compounded by the huge number of unique instruction schemas in instruction
sets (e.g., around 43,000 in Intel’s IA-32) and the exponential cost inherent in enumeration. The effect is slow
synthesis: even for relatively small specifications, McSynth++ might take several minutes or a few hours to
find an implementation.

In this paper, we describe how we use machine learning to make the search in McSynth++ smarter and
potentially faster. We converted the linear search in McSynth++ into a best-first search over the space of
instruction sequences. The cost heuristic for the best-first search comes from two models—used together—
built from a corpus of (QFBV-formula, instruction-sequence) pairs: (i) a language model that favors useful
instruction sequences, and (ii) a regression model that correlates features of instruction sequences with features
of QFBV formulas, and favors instruction sequences that are more likely to implement the input formula.
Our experiments for IA-32 showed that our model-assisted synthesizer enables synthesis of code for 6 out of
50 formulas on which McSynth++ times out, speeding up the synthesis time by at least 526x, and for the
remaining formulas, speeds up synthesis by 4.55x.
",19.38786093064905,17.599677018633546,315
OOPSLA_17_033.txt,14.77053819791135,13.313011058560733,OOPSLA,11400,"
Achieving determinism on real software systems remains difficult. Even a batch-processing job, whose task is
to map input bits to output bits, risks nondeterminism from thread scheduling, system calls, CPU instructions,
and leakage of environmental information such as date or CPU model. In this work, we present a system for
achieving low-overhead deterministic execution of batch-processing programs that read and write the file
systemÐturning them into pure functions on files.
We allow multi-process executions where a permissions system prevents races on the file system. Process separation enables different processes to enforce permissions and enforce determinism using distinct
mechanisms. Our prototype, DetFlow, allows a statically-typed coordinator process to use shared-memory
parallelism, as well as invoking process-trees of sandboxed legacy binaries. DetFlow currently implements
the coordinator as a Haskell program with a restricted I/O type for its main function: a new monad we call
DetIO. Legacy binaries launched by the coordinator run concurrently, but internally each process schedules
threads sequentially, allowing dynamic determinism-enforcement with predictably low overhead.
We evaluate DetFlow by applying it to bioinformatics data pipelines and software build systems. DetFlow
enables determinizing these data-processing workflows by porting a small amount of code to become a
statically-typed coordinator. This hybrid approach of static and dynamic determinism enforcement permits
freedom where possible but restrictions where necessary.
",17.693802365651003,16.837444444444447,226
OOPSLA_17_034.txt,16.08900369741048,14.287997263846432,OOPSLA,13006,"
This paper presents natural synthesis, which generalizes the proof-theoretic synthesis technique to support very expressive logic theories. This approach leverages the natural proof methodology and reduces
an intractable, unbounded-size synthesis problem to a tractable, bounded-size synthesis problem, which is
amenable to be handled by modern inductive synthesis engines. The synthesized program admits a natural
proof and is a provably-correct solution to the original synthesis problem. We explore the natural synthesis approach in the domain of imperative data-structure manipulations and present a novel syntax-guided
synthesizer based on natural synthesis. The input to our system is a program template together with a rich
functional specification that the synthesized program must meet. Our system automatically produces a program implementation along with necessary proof artifacts, namely loop invariants and ranking functions,
and guarantees the total correctness with a natural proof. Experiments show that our natural synthesizer can
efficiently produce provably-correct implementations for sorted lists and binary search trees. To our knowledge, this is the first system that can automatically synthesize these programs, their functional correctness
and their termination in tandem from bare-bones control flow skeletons.
",18.377959752453624,17.41105158730159,190
OOPSLA_17_035.txt,13.133689993472995,11.263503713541315,OOPSLA,12560,"
Orca is a concurrent and parallel garbage collector for actor programs, which does not require any stop-theworld steps, or synchronisation mechanisms, and which has been designed to support zero-copy message
passing and sharing of mutable data. Orca is part of the runtime of the actor-based language Pony. Pony’s
runtime was co-designed with the Pony language. This co-design allowed us to exploit certain language
properties in order to optimise performance of garbage collection. Namely, Orca relies on the absence of race
conditions in order to avoid read/write barriers, and it leverages actor message passing for synchronisation
among actors. This paper describes Pony, its type system, and the Orca garbage collection algorithm. An
evaluation of the performance of Orca suggests that it is fast and scalable for idiomatic workloads.
",13.484332010920856,13.113233082706767,135
OOPSLA_17_036.txt,15.238470226675268,13.797605555125063,OOPSLA,11828,"
Static information-flow analysis (especially taint-analysis) is a key technique in software security, computing
where sensitive or untrusted data can propagate in a program. Points-to analysis is a fundamental static
program analysis, computing what abstract objects a program expression may point to. In this work, we
propose a deep unification of information-flow and points-to analysis. We observe that information-flow
analysis is not a mere high-level client of points-to information, but it is indeed identical to points-to analysis
on artificial abstract objects that represent different information sources. The very same algorithm can compute,
simultaneously, two interlinked but separate results (points-to and information-flow values) with changes
only to its initial conditions.
The benefits of such a unification are manifold. We can use existing points-to analysis implementations,
with virtually no modification (only minor additions of extra logic for sanitization) to compute information
flow concepts, such as value tainting. The algorithmic enhancements of points-to analysis (e.g., different
flavors of context sensitivity) can be applied transparently to information-flow analysis. Heavy engineering
work on points-to analysis (e.g., handling of the reflection API for Java) applies to information-flow analysis
without extra effort. We demonstrate the benefits in a realistic implementation that leverages the Doop
points-to analysis framework (including its context-sensitivity and reflection analysis features) to provide
an information-flow analysis with excellent precision (over 91%) and recall (over 99%) for standard Java
information-flow benchmarks.
The analysis comfortably scales to large, real-world Android applications, analyzing the Facebook Messenger
app with more than 55K classes in under 7 hours.
",18.341435485029603,17.766603582291314,272
OOPSLA_17_037.txt,15.323056128131448,13.7333079402467,OOPSLA,15869,"
Distributed protocols such as Paxos play an important role in many computer systems. Therefore, a bug
in a distributed protocol may have tremendous effects. Accordingly, a lot of effort has been invested in
verifying such protocols. However, checking invariants of such protocols is undecidable and hard in practice,
as it requires reasoning about an unbounded number of nodes and messages. Moreover, protocol actions
and invariants involve both quantifier alternations and higher-order concepts such as set cardinalities and
arithmetic.

This paper makes a step towards automatic verification of such protocols. We aim at a technique that
can verify correct protocols and identify bugs in incorrect protocols. To this end, we develop a methodology
for deductive verification based on effectively propositional logic (EPR)—a decidable fragment of first-order
logic (also known as the Bernays-Schénfinkel-Ramsey class). In addition to decidability, EPR also enjoys
the finite model property, allowing to display violations as finite structures which are intuitive for users.
Our methodology involves modeling protocols using general (uninterpreted) first-order logic, and then
systematically transforming the model to obtain a model and an inductive invariant that are decidable to
check. The steps of the transformations are also mechanically checked, ensuring the soundness of the method.
We have used our methodology to verify the safety of Paxos, and several of its variants, including Multi-Paxos,
Vertical Paxos, Fast Paxos, Flexible Paxos and Stoppable Paxos. To the best of our knowledge, this work is the
first to verify these protocols using a decidable logic, and the first formal verification of Vertical Paxos, Fast
Paxos and Stoppable Paxos.
",16.758888586881195,15.425660377358493,266
OOPSLA_17_038.txt,16.003570990436152,15.274319315485343,OOPSLA,11698,"
Event races are a common source of subtle errors in JavaScript web applications. Several automated tools for
detecting event races have been developed, but experiments show that their accuracy is generally quite low.
We present a new approach that focuses on three categories of event race errors that often appear during the
initialization phase of web applications: form-input-overwritten errors, late-event-handler-registration errors,
and access-before-definition errors. The approach is based on a dynamic analysis that uses a combination of
adverse and approximate execution. Among the strengths of the approach are that it does not require browser
modifications, expensive model checking, or static analysis.
In an evaluation on 100 widely used websites, our tool InitRacer reports 1 085 initialization races, while
providing informative explanations of their causes and effects. A manual study of 218 of these reports shows
that 111 of them lead to uncaught exceptions and at least 47 indicate errors that affect the functionality of the
websites.
",15.903189008614273,15.715968448729186,164
OOPSLA_17_039.txt,14.378303638480368,12.570829514870713,OOPSLA,12396,"
Garbage collection greatly improves programmer productivity and ensures memory safety. Manual memory
management on the other hand often delivers better performance but is typically unsafe and can lead to
system crashes or security vulnerabilities. We propose integrating safe manual memory management with
garbage collection in the NET runtime to get the best of both worlds. In our design, programmers can choose
between allocating objects in the garbage collected heap or the manual heap. All existing applications run
unmodified, and without any performance degradation, using the garbage collected heap. Our programming
model for manual memory management is flexible: although objects in the manual heap can have a single
owning pointer, we allow deallocation at any program point and concurrent sharing of these objects amongst
all the threads in the program. Experimental results from our NET CoreCLR implementation on real-world
applications show substantial performance gains especially in multithreaded scenarios: up to 3x savings in
peak working sets and 2x improvements in runtime.
",17.77360955136429,16.320372670807455,162
OOPSLA_17_040.txt,14.575272191002849,12.409537246529393,OOPSLA,12497,"
Coaxioms have been recently introduced to enhance the expressive power of inference systems, by supporting
interpretations which are neither purely inductive, nor coinductive. This paper proposes a novel approach
based on coaxioms to capture divergence in semantic definitions by allowing inductive and coinductive
semantic rules to be merged together for defining a unique semantic judgment. In particular, coinduction
is used to derive a special result which models divergence. In this way, divergent, terminating, and stuck
computations can be properly distinguished even in semantic definitions where this is typically difficult, as
in big-step style. We show how the proposed approach can be applied to several languages; in particular,
we first illustrate it on the paradigmatic example of the A-calculus, then show how it can be adopted for
defining the big-step semantics of a simple imperative Java-like language. We provide proof techniques to
show classical results, including equivalence with small-step semantics, and type soundness for typed versions
of both languages.
",19.117987234576393,16.722791411042945,164
OOPSLA_17_041.txt,17.019541405434563,15.486124639958103,OOPSLA,15246,"
This paper describes an extensible language framework, ableC, that allows programmers to import new,
domain-specific, independently-developed language features into their programming language, in this case
C. Most importantly, this framework ensures that the language extensions will automatically compose to
form a working translator that does not terminate abnormally. This is possible due to two modular analyses
that extension developers can apply to their language extension to check its composability. Specifically, these
ensure that the composed concrete syntax specification is non-ambiguous and the composed attribute grammar
specifying the semantics is well-defined. This assurance and the expressiveness of the supported extensions is
a distinguishing characteristic of the approach.
The paper describes a number of techniques for specifying a host language, in this case C at the C11 standard,
to make it more amenable to language extension. These include techniques that make additional extensions
pass these modular analyses, refactorings of the host language to support a wider range of extensions, and
the addition of semantic extension points to support, for example, operator overloading and non-local code
transformations.
",17.93193317476759,17.059952114924183,180
OOPSLA_17_042.txt,13.798313998338507,10.99596537956893,OOPSLA,11667,"

Precedence and associativity declarations in systems like yacc resolve ambiguities in context-free grammars (CFGs) by specifying restrictions on allowed parses. However, they are special purpose and do not
handle the grammatical restrictions that language designers need in order to resolve ambiguities like dangling
else, the interactions between binary operators and functional if expressions in ML, and the interactions
between object allocation and function calls in JavaScript. Often, language designers resort to restructuring
their grammars in order to encode these restrictions, but this obfuscates the designer’s intent and can make
grammars more difficult to read, write, and maintain.

In this paper, we show how tree automata can modularly and concisely encode such restrictions. We do this
by reinterpreting CFGs as tree automata and then intersecting them with tree automata encoding the desired
restrictions. The results are then reinterpreted back into CFGs that encode the specified restrictions. This
process can be used as a preprocessing step before other CFG manipulations and is well behaved. It performs
well in practice and never introduces ambiguities or LR(k) conflicts.
",16.52667757954773,15.316136363636364,178
OOPSLA_17_043.txt,15.515582700565414,13.55711220280341,OOPSLA,10911,"
In scenarios such as web programming, where code is linked together from multiple sources, object capability patterns (OCPs)
provide an essential safeguard, enabling programmers to protect the private state of their objects from corruption by unknown
and untrusted code. However, the benefits of OCPs in terms of program verification have never been properly formalized. In
this paper, building on the recently developed Iris framework for concurrent separation logic, we develop OCPL, the first
program logic for compositionally specifying and verifying OCPs in a language with closures, mutable state, and concurrency.
The key idea of OCPL is to account for the interface between verified and untrusted code by adopting a well-known idea from
the literature on security protocol verification, namely robust safety. Programs that export only properly wrapped values
to their environment can be proven robustly safe, meaning that their untrusted environment cannot violate their internal
invariants. We use OCPL to give the first general, compositional, and machine-checked specs for several commonly-used
OCPs—including the dynamic sealing, membrane, and caretaker patterns—which we then use to verify robust safety for
representative client code. All our results are fully mechanized in the Coq proof assistant.
",18.99602597827317,17.54530612244898,197
OOPSLA_17_044.txt,15.084685505800913,13.718596935489007,OOPSLA,10160,"
Direct sharing and storing of memory objects allows high-performance and low-overhead collaboration
between parallel processes or application workflows with loosely coupled programs. However, sharing
of objects is hindered by the inability to use subtype polymorphism which is common in object-oriented
programming languages. That is because implementations of subtype polymorphism in modern compilers
rely on using virtual tables stored at process-specific locations, which makes objects unusable in processes
other than the creating process.

In this paper, we present SAVI Objects, objects with Sharing and Virtuality Incorporated. SAVI Objects
support subtype polymorphism but can still be shared across processes and stored in persistent data structures.
We propose two different techniques to implement SAVI Objects and evaluate the tradeoffs between them. The
first technique is virtual table duplication which adheres to the virtual-table-based implementation of subtype
polymorphism, but duplicates virtual tables for shared objects to fixed memory addresses associated with
each shared memory region. The second technique is hashing-based dynamic dispatch which re-implements
subtype polymorphism using hashing-based look-ups to a global virtual table.

Our results show that SAVI Objects enable direct sharing and storing of memory objects that use subtype
polymorphism by adding modest overhead costs to object construction and dynamic dispatch time. SAVI
Objects thus enable faster inter-process communication, improving the overall performance of production
applications that share polymorphic objects.
",17.00531248756302,16.83794323144105,230
OOPSLA_17_045.txt,14.703951732595929,12.84896323116433,OOPSLA,10859,"
This paper presents a new technique for automatically synthesizing SQL queries from natural language (NL).
At the core of our technique is a new NL-based program synthesis methodology that combines semantic
parsing techniques from the NLP community with type-directed program synthesis and automated program
repair. Starting with a program sketch obtained using standard parsing techniques, our approach involves
an iterative refinement loop that alternates between quantitative type inhabitation and automated sketch
repair. We use the proposed idea to build an end-to-end system called Sqlizer that can synthesize SQL queries
from natural language. Our method is fully automated, works for any database without requiring additional
customization, and does not require users to know the underlying database schema. We evaluate our approach
on over 450 natural language queries concerning three different databases, namely MAS, IMDB, and YELP.
Our experiments show that the desired query is ranked within the top 5 candidates in close to 90% of the cases
and that Sqlizer outperforms Nalir, a state-of-the-art tool that won a best paper award at VLDB’14.
",16.439396014739867,16.006349206349206,181
OOPSLA_17_046.txt,15.558461951409043,13.80641662536955,OOPSLA,13641,"
Algorithms that create and mutate graph data structures are challenging to implement correctly. However,
verifying even basic properties of low-level implementations, such as referential integrity and memory safety,
remains non-trivial. Furthermore, any extension to such a data structure multiplies the complexity of its
implementation, while compounding the challenges in reasoning about correctness. We take a language
design approach to this problem. We propose Seam, a language for expressing local edits to graph-like
data structures, based on a relational data model, and such that data integrity can be verified automatically.
We present a verification method that leverages an SMT solver, and prove it sound and precise (complete
modulo termination of the SMT solver). We evaluate the verification capabilities of Seam empirically, and
demonstrate its applicability to a variety of examples, most notably a new class of verification tasks derived
from geometric remeshing operations used in scientific simulation and computer graphics. We describe our
prototype implementation of a Seam compiler that generates low-level code, which can then be integrated
into larger applications. We evaluate our compiler on a sample application, and demonstrate competitive
execution time, compared to hand-written implementations.
",17.755912252390015,17.128958333333333,193
OOPSLA_17_047.txt,14.209640425599648,12.454835410502483,OOPSLA,14518,"
With more and more web scripting languages on offer, programmers have access to increasing language
support for web scraping tasks. However, in our experiences collaborating with data scientists, we learned that
two issues still plague long-running scraping scripts: i) When a network or website goes down mid-scrape,
recovery sometimes requires restarting from the beginning, which users find frustratingly slow. ii) Websites
do not offer atomic snapshots of their databases; they update their content so frequently that output data is
cluttered with slight variations of the same information — e.g., a tweet from profile 1 that is retweeted on
profile 2 and scraped from both profiles, once with 52 responses then later with 53 responses.

We introduce the skip block, a language construct that addresses both of these disparate problems. Programmers write lightweight annotations to indicate when the current object can be considered equivalent to a
previously scraped object and direct the program to skip over the scraping actions in the block. The construct
is hierarchical, so programs can skip over long or short script segments, allowing adaptive reuse of prior work.
After network and server failures, skip blocks accelerate failure recovery by 7.9x on average. Even scripts that
do not encounter failures benefit; because sites display redundant objects, skipping over them accelerates
scraping by up to 2.1x. For longitudinal scraping tasks that aim to fetch only new objects, the second run
exhibits an average speedup of 5.2x. Our small user study reveals that programmers can quickly produce skip
block annotations.
",15.760457277294734,15.266746347941567,256
OOPSLA_17_048.txt,15.731203898148618,13.781526702667499,OOPSLA,14662,"
Recent research has identified significant performance hurdles that sound gradual typing needs to overcome.
These performance hurdles stem from the fact that the run-time checks gradual type systems insert into code
can cause significant overhead. We propose that designing a type system for a gradually typed language hand
in hand with its implementation from scratch is a possible way around these and several other hurdles on
the way to efficient sound gradual typing. Such a design process also highlights the type-system restrictions
required for efficient composition with gradual typing. We formalize the core of a nominal object-oriented
language that fulfills a variety of desirable properties for gradually typed languages, and present evidence that
an implementation of this language suffers minimal overhead even in adversarial benchmarks identified in
earlier work.
",18.458006810337128,16.965090909090915,133
OOPSLA_17_049.txt,15.37707731893937,14.10958332418608,OOPSLA,11058,"
While gradual typing has proven itself attractive to programmers, many systems have avoided sound gradual
typing due to the run time overhead of enforcement. In the context of sound gradual typing, both anecdotal
and systematic evidence has suggested that run time costs are quite high, and often unacceptable, casting
doubt on the viability of soundness as an approach.
We show that these overheads are not fundamental, and that with appropriate improvements, just-intime compilers can greatly reduce the overhead of sound gradual typing. Our study takes benchmarks
published in a recent paper on gradual typing performance in Typed Racket (Takikawa et al., POPL 2016) and
evaluates them using an experimental tracing JIT compiler for Racket, called Pycket. On typical benchmarks,
Pycket is able to eliminate more than 90% of the gradual typing overhead. While our current results are not
the final word in optimizing gradual typing, we show that the situation is not dire, and where more work is
needed.
Pycket’s performance comes from several sources, which we detail and measure individually. First, we apply a sophisticated tracing JIT compiler and optimizer, automatically generated in Pycket using the RPython
framework originally created for PyPy. Second, we focus our optimization efforts on the challenges posed
by run time checks, implemented in Racket by chaperones and impersonators. We introduce representation
improvements, including a novel use of hidden classes to optimize these data structures, and measure the
performance implications of each optimization.
",16.887214914478655,15.948573221757325,241
OOPSLA_17_050.txt,14.264440672360898,12.166744532115175,OOPSLA,13299,"
Heterogeneous architectures characterize today hardware ranging from super-computers to smartphones.
However, in spite of this importance, programming such systems is still challenging. In particular, it is
challenging to map computations to the different processors of a heterogeneous device. In this paper, we
provide a static analysis that mitigates this problem. Our contributions are two-fold: first, we provide a semicontext-sensitive algorithm, which analyzes the program’s call graph to determine the best processor for each
calling context. This algorithm is parameterized by a cost model, which takes into consideration processor’s
characteristics and data transfer time. Second, we show how to use simulated annealing to calibrate this cost
model for a given heterogeneous architecture. We have used our ideas to build Etino, a tool that annotates C
programs with OpenACC or OpenMP 4.0 directives. Etino generates code for a CPU-GPU architecture without
user intervention. Experiments on classic benchmarks reveal speedups of up to 75x. Moreover, our calibration
process lets Etino avoid slowdowns of up to 720x which trivial parallelization approaches would yield.
",15.429909175157395,13.870803993694167,177
OOPSLA_17_051.txt,14.09590529487209,12.573742406146923,OOPSLA,10453,"
Heterogeneous hardware is central to modern advances in performance and efficiency. Mainstream programming models for heterogeneous architectures, however, sacrifice safety and expressiveness in favor of
low-level control over performance details. The interfaces between hardware units consist of verbose, unsafe
APIs; hardware-specific languages make it difficult to move code between units; and brittle preprocessor
macros complicate the task of specializing general code for efficient accelerated execution. We propose a
unified low-level programming model for heterogeneous systems that offers control over performance, safe
communication constructs, cross-device code portability, and hygienic metaprogramming for specialization.
The language extends constructs from multi-stage programming to separate code for different hardware units,
to communicate between them, and to express compile-time code optimization. We introduce static staging,
a different take on multi-stage programming that lets the compiler generate all code and communication
constructs ahead of time.
To demonstrate our approach, we use static staging to implement BraidGL, a real-time graphics programming
language for CPUśGPU systems. Current real-time graphics software in OpenGL uses stringly-typed APIs for
communication and unsafe preprocessing to generate specialized GPU code variants. In BraidGL, programmers
instead write hybrid CPUśGPU software in a unified language. The compiler statically generates target-specific
code and guarantees safe communication between the CPU and the graphics pipeline stages. Example scenes
demonstrate the language’s productivity advantages: BraidGL eliminates the safety and expressiveness pitfalls
of OpenGL and makes common specialization techniques easy to apply. The case study demonstrates how
static staging can express core placement and specialization in general heterogeneous programming.
",17.219254097808864,17.20201737451737,263
OOPSLA_17_052.txt,13.288992950975462,10.97022162257808,OOPSLA,13227,"
In application domains that store data in a tabular format, a common task is to fill the values of some cells
using values stored in other cells. For instance, such data completion tasks arise in the context of missing
value imputation in data science and derived data computation in spreadsheets and relational databases.
Unfortunately, end-users and data scientists typically struggle with many data completion tasks that require
non-trivial programming expertise. This paper presents a synthesis technique for automating data completion
tasks using programming-by-example (PBE) and a very lightweight sketching approach. Given a formula sketch
(e.g., AVG(?1, ?2)) and a few input-output examples for each hole, our technique synthesizes a program to
automate the desired data completion task. Towards this goal, we propose a domain-specific language (DSL)
that combines spatial and relational reasoning over tabular data and a novel synthesis algorithm that can
generate DSL programs that are consistent with the input-output examples. The key technical novelty of our
approach is a new version space learning algorithm that is based on finite tree automata (FTA). The use of
FTAs in the learning algorithm leads to a more compact representation that allows more sharing between
programs that are consistent with the examples. We have implemented the proposed approach in a tool called
DACE and evaluate it on 84 benchmarks taken from online help forums. We also illustrate the advantages of
our approach by comparing our technique against two existing synthesizers, namely Prose and Sketch.
",14.790194502661404,13.586200000000002,252
OOPSLA_17_053.txt,14.314028922438442,13.079362022967178,OOPSLA,9120,"
System failures resulting from configuration errors are one of the major reasons for the compromised
reliability of today’s software systems. Although many techniques have been proposed for configuration error
detection, these approaches can generally only be applied after an error has occurred. Proactively verifying
configuration files is a challenging problem, because 1) software configurations are typically written in poorly
structured and untyped “languages”, and 2) specifying rules for configuration verification is challenging in
practice. This paper presents ConfigV, a verification framework for general software configurations. Our
framework works as follows: in the pre-processing stage, we first automatically derive a specification. Once we
have a specification, we check if a given configuration file adheres to that specification. The process of learning
a specification works through three steps. First, ConfigV parses a training set of configuration files (not
necessarily all correct) into a well-structured and probabilistically-typed intermediate representation. Second,
based on the association rule learning algorithm, ConfigV learns rules from these intermediate representations.
These rules establish relationships between the keywords appearing in the files. Finally, ConfigV employs
rule graph analysis to refine the resulting rules. ConfigV is capable of detecting various configuration errors,
including ordering errors, integer correlation errors, type errors, and missing entry errors. We evaluated
ConfigV by verifying public configuration files on GitHub, and we show that ConfigV can detect known
configuration errors in these files.
",15.091289759661642,15.755982532751094,231
OOPSLA_17_054.txt,14.88779525352112,13.332931168040727,OOPSLA,13004,"
Tensor algebra is a powerful tool with applications in machine learning, data analytics, engineering and the
physical sciences. Tensors are often sparse and compound operations must frequently be computed in a single
kernel for performance and to save memory. Programmers are left to write kernels for every operation of
interest, with different mixes of dense and sparse tensors in different formats. The combinations are infinite,
which makes it impossible to manually implement and optimize them all. This paper introduces the first
compiler technique to automatically generate kernels for any compound tensor algebra operation on dense
and sparse tensors. The technique is implemented in a C++ library called taco. Its performance is competitive
with best-in-class hand-optimized kernels in popular libraries, while supporting far more tensor operations.
",16.613394197324528,14.163626373626375,129
OOPSLA_17_055.txt,14.998613987315654,13.960275715204116,OOPSLA,13562,"
Programmers in dynamic languages wishing to constrain and understand the behavior of their programs
may turn to gradually-typed languages, which allow types to be specified optionally and check values at the
boundary between dynamic and static code. Unfortunately, the performance cost of these run-time checks can
be severe, slowing down execution by at least 10x when checks are present. Modern virtual machines (VMs)
for dynamic languages use speculative techniques to improve performance: If a particular value was seen once,
it is likely that similar values will be seen in the future. They combine optimization-relevant properties of
values into cacheable łshapesž, then use a single shape check to subsume checks for each property. Values with
the same memory layout or the same field types have the same shape. This greatly reduces the amount of type
checking that needs to be performed at run-time to execute dynamic code. While very valuable to the VM’s
optimization, these checks do little to benefit the programmer aside from improving performance. We present
in this paper a design for intrinsic object contracts, which makes the obligations of gradually-typed languages’
type checks an intrinsic part of object shapes, and thus can subsume run-time type checks into existing shape
checks, eliminating redundant checks entirely. With an implementation on a VM for JavaScript used as a
target for SafeTypeScript’s soundness guarantees, we demonstrate slowdown averaging 7% in fully-typed
code relative to unchecked code, and no more than 45% in pessimal configurations.
",17.37919286519448,15.435341365461849,252
OOPSLA_17_056.txt,14.177025497576459,12.204041673097702,OOPSLA,11380,"
We present TiML (Timed ML), an ML-like functional language with time-complexity annotations in types. It
uses indexed types to express sizes of data structures and upper bounds on running time of functions; and
refinement kinds to constrain these indices, expressing data-structure invariants and pre/post-conditions. Indexed types are flexible enough that TiML avoids a built-in notion of “size,” and the programmer can choose
to index user-defined datatypes in any way that helps her analysis. TiML’s distinguishing characteristic is
supporting highly automated time-bound verification applicable to data structures with nontrivial invariants.
The programmer provides type annotations, and the typechecker generates verification conditions that are
discharged by an SMT solver. Type and index inference are supported to lower annotation burden, and, furthermore, big-O complexity can be inferred from recurrences generated during typechecking by a recurrence
solver based on heuristic pattern matching (e.g. using the Master Theorem to handle divide-and-conquerlike recurrences). We have evaluated TiML’s usability by implementing a broad suite of case-study modules,
demonstrating that TiML, though lacking full automation and theoretical completeness, is versatile enough
to verify worst-case and/or amortized complexities for algorithms and data structures like classic list operations, merge sort, Dijkstra’s shortest-path algorithm, red-black trees, Braun trees, functional queues, and
dynamic tables with bounds like mn log n. The learning curve and annotation burden are reasonable, as we
argue with empirical results on our case studies. We formalized TiML’s type-soundness proof in Coq.
",17.631426480028413,17.049733333333332,256
OOPSLA_17_057.txt,15.305807069022237,14.570932064706657,OOPSLA,17104,"
Localizing failure-inducing code is essential for software debugging. Manual fault localization can be quite
tedious, error-prone, and time-consuming. Therefore, a huge body of research efforts have been dedicated to
automated fault localization. Spectrum-based fault localization, the most intensively studied fault localization
approach based on test execution information, may have limited effectiveness, since a code element executed
by a failed tests may not necessarily have impact on the test outcome and cause the test failure. To bridge the
gap, mutation-based fault localization has been proposed to transform the programs under test to check the
impact of each code element for better fault localization. However, there are limited studies on the effectiveness
of mutation-based fault localization on sufficient number of real bugs. In this paper, we perform an extensive
study to compare mutation-based fault localization techniques with various state-of-the-art spectrum-based
fault localization techniques on 357 real bugs from the Defects4J benchmark suite. The study results firstly
demonstrate the effectiveness of mutation-based fault localization, as well as revealing a number of guidelines
for further improving mutation-based fault localization. Based on the learnt guidelines, we further transform
test outputs/messages and test code to obtain various mutation information. Then, we propose TraPT, an
automated Learning-to-Rank technique to fully explore the obtained mutation information for effective fault
localization. The experimental results show that TraPT localizes 65.12% and 94.52% more bugs within Top-1
than state-of-the-art mutation and spectrum based techniques when using the default setting of LIBSVM.
",16.018793980428352,14.86818181818182,263
OOPSLA_17_058.txt,15.411734904736267,13.26385349835741,OOPSLA,13279,"
Series of traversals of tree structures arise in numerous contexts: abstract syntax tree traversals in compiler
passes, rendering traversals of the DOM in web browsers, kd-tree traversals in computational simulation codes.
In each of these settings, a tree is traversed multiple times to compute various values and modify various
portions of the tree. While it is relatively easy to write these traversals as separate small updates to the tree,
for efficiency reasons, traversals are often manually fused to reduce the number of times that each portion of
the tree is traversed: by performing multiple operations on the tree simultaneously, each node of the tree can
be visited fewer times, increasing opportunities for optimization and decreasing cache pressure and other
overheads. This fusion process is often done manually, requiring careful understanding of how each of the
traversals of on tree interact.

This paper presents an automatic approach to traversal fusion: tree traversals can be written independently,
and then our framework analyzes the dependences between the traversals to determine how they can be
fused to reduce the number of visits to each node in the tree. A critical aspect of our framework is that it
exploits two opportunities to increase the amount of fusion: i) it automatically integrates code motion, and ii)
it supports partial fusion, where portions of one traversal can be fused with another, allowing for a reduction
in node visits without requiring that two traversals be fully fused. We implement our framework as Clang
tool, and show across several case studies that we can successfully fuse complex tree traversals, reducing the
overall number of traversals and substantially improving locality and performance.
",20.267338824336647,20.756263736263737,274
OOPSLA_17_059.txt,15.930819554926678,14.29389750482073,OOPSLA,12214,"
TypeScript applications often use untyped JavaScript libraries. To support static type checking of such
applications, the typed APIs of the libraries are expressed as separate declaration files. This raises the challenge
of checking that the declaration files are correct with respect to the library implementations. Previous work
has shown that mismatches are frequent and cause TypeScript’s type checker to misguide the programmers
by rejecting correct applications and accepting incorrect ones.
This paper shows how feedback-directed random testing, which is an automated testing technique that has
mostly been used for testing Java libraries, can be adapted to effectively detect such type mismatches. Given a
JavaScript library with a TypeScript declaration file, our tool TStest generates a type test script, which is an
application that interacts with the library and tests that it behaves according to the type declarations. Compared
to alternative solutions that involve static analysis, this approach finds significantly more mismatches in
a large collection of real-world JavaScript libraries with TypeScript declaration files, and with fewer false
positives. It also has the advantage that reported mismatches are easily reproducible with concrete executions,
which aids diagnosis and debugging.
",18.10804710084791,15.787771164021166,191
OOPSLA_17_060.txt,17.232575846934264,16.037740185297533,OOPSLA,15077,"
Java 8 retrofitted lambda expressions, a core feature of functional programming, into a mainstream objectoriented language with an imperative paradigm. However, we do not know how Java developers have adapted
to the functional style of thinking, and more importantly, what are the reasons that motivate Java developers to
adopt functional programming. Without such knowledge, researchers miss opportunities to improve the state
of the art, tool builders use unrealistic assumptions, language designers fail to improve upon their designs,
and developers are unable to explore efficient and effective use of lambdas.
We present the first large-scale, quantitative and qualitative empirical study to shed light on how imperative
programmers use lambda expressions as a gateway into functional thinking. Particularly, we statically scrutinize
the source code of 241 open-source projects with 19,770 contributors, to study the characteristics of 100,540
lambda expressions. Moreover, we investigate the historical trends and adoption rates of lambdas in the
studied projects. To get a complementary perspective, we seek the underlying reasons on why developers
introduce lambda expressions, by surveying 97 developers who are introducing lambdas in their projects,
using the firehouse interview method.
Among others, our findings revealed an increasing trend in the adoption of lambdas in Java: in 2016, the
ratio of lambdas introduced per added line of code increased by two-fold compared to 2015. Lambdas are
used for various reasons, including but not limited to (i) making existing code more succinct and readable,
(ii) avoiding code duplication, and (iii) simulating lazy evaluation of functions. Interestingly, we found out
that developers are using Java’s built-in functional interfaces inefficiently, i.e., they prefer to use generic
functional interfaces over the specialized ones, overlooking the performance overheads that might be imposed.
Furthermore, developers are not adopting techniques from functional programming, e.g., currying. Finally, we
present the implications of our findings for researchers, tool builders, language designers, and developers.
",18.68694054745388,17.3207154340836,317
OOPSLA_17_061.txt,13.604126419630902,10.690550928356508,OOPSLA,13425,"
In recent years dependent types have become a hot topic in programming language research. A key reason
why dependent types are interesting is that they allow unifying types and terms, which enables both additional
expressiveness and economy of concepts. Unfortunately there has been much less work on dependently typed
calculi for object-oriented programming. This is partly because it is widely acknowledged that the combination
between dependent types and subtyping is particularly challenging.
This paper presents λI ≤ , which is a dependently typed generalization of System F ≤ . The resulting calculus
follows the style of Pure Type Systems, and contains a single unified syntactic sort that accounts for expressions,
types and kinds. To address the challenges posed by the combination of dependent types and subtyping, λI ≤
employs a novel technique that unifies typing and subtyping. In λI ≤ there is only a judgment that is akin
to a typed version of subtyping. Both the typing relation, as well as type well-formedness are just special
cases of the subtyping relation. The resulting calculus has a rich metatheory and enjoys of several standard
and desirable properties, such as subject reduction, transitivity of subtyping, narrowing as well as standard
substitution lemmas. All the metatheory of λI ≤ is mechanically proved in the Coq theorem prover. Furthermore,
(and as far as we are aware) λI ≤ is the first dependently typed calculus that completely subsumes System F ≤ ,
while preserving various desirable properties.
",15.903189008614273,13.296441908713692,235
OOPSLA_17_062.txt,14.246796139914036,12.640138103737161,OOPSLA,11517,"
We introduce canonical sequentialization, a new approach to verifying unbounded, asynchronous, messagepassing programs at compile-time. Our approach builds upon the following observation: due the combinatorial
explosion in complexity, programmers do not reason about their systems by case-splitting over all the possible
execution orders. Instead, correct programs tend to be well-structured so that the programmer can reason about
a small number of representative executions, which we call the program’s canonical sequentialization. We have
implemented our approach in a tool called Brisk that synthesizes canonical sequentializations for programs
written in Haskell, and evaluated it on a wide variety of distributed systems including benchmarks from
the literature and implementations of MapReduce, two-phase commit, and a version of the Disco distributed
file-system. Brisk verifies unbounded versions of the benchmarks in tens of milliseconds, yielding the first
concurrency verification tool that is fast enough to be integrated into a design-implement-check cycle.
CCS Concepts: · Theory of computation → Program verification; Program analysis; Distributed
computing models; Concurrency; · Software and its engineering → Software notations and tools;
Additional Key Words and Phrases: canonical sequentialization, asynchronous programs, concurrency, distributed programs, reductions, program verification, parameterized systems, message passing
",20.736966565827903,22.560897435897434,195
OOPSLA_17_063.txt,15.298027417714664,13.903322728701244,OOPSLA,13685,"
Arrays computations are at the core of numerical modelling and computational science applications. However,
low-level manipulation of array indices is a source of program error. Many practitioners are aware of the need
to ensure program correctness, yet very few of the techniques from the programming research community
are applied by scientists. We aim to change that by providing targetted lightweight verification techniques for
scientific code. We focus on the all too common mistake of array offset errors as a generalisation of off-by-one
errors. Firstly, we report on a code analysis study on eleven real-world computational science code base,
identifying common idioms of array usage and their spatial properties. This provides much needed data on
array programming idioms common in scientific code. From this data, we designed a lightweight declarative
specification language capturing the majority of array access patterns via a small set of combinators. We detail
a semantic model, and the design and implementation of a verification tool for our specification language,
which both checks and infers specifications. We evaluate our tool on our corpus of scientific code. Using the
inference mode, we found roughly 87,000 targets for specification across roughly 1.1 million lines of code,
showing that the vast majority of array computations read from arrays in a pattern with a simple, regular,
static shape. We also studied the commit logs of one of our corpus packages, finding past bug fixes for which
our specification system distinguishes the change and thus could have been applied to detect such bugs.
",14.434950587195996,14.406732283464567,257
OOPSLA_17_064.txt,16.90173024988328,15.87921645021645,OOPSLA,13335,"
Data replication is used in distributed systems to maintain up-to-date copies of shared data across multiple
computers in a network. However, despite decades of research, algorithms for achieving consistency in
replicated systems are still poorly understood. Indeed, many published algorithms have later been shown to
be incorrect, even some that were accompanied by supposed mechanised proofs of correctness. In this work,
we focus on the correctness of Conflict-free Replicated Data Types (CRDTs), a class of algorithm that provides
strong eventual consistency guarantees for replicated data. We develop a modular and reusable framework
in the Isabelle/HOL interactive proof assistant for verifying the correctness of CRDT algorithms. We avoid
correctness issues that have dogged previous mechanised proofs in this area by including a network model
in our formalisation, and proving that our theorems hold in all possible network behaviours. Our axiomatic
network model is a standard abstraction that accurately reflects the behaviour of real-world computer networks.
Moreover, we identify an abstract convergence theorem, a property of order relations, which provides a formal
definition of strong eventual consistency. We then obtain the first machine-checked correctness theorems for
three concrete CRDTs: the Replicated Growable Array, the Observed-Remove Set, and an Increment-Decrement
Counter. We find that our framework is highly reusable, developing proofs of correctness for the latter two
CRDTs in a few hours and with relatively little CRDT-specific code.
",17.353723509956247,16.077025641025642,235
OOPSLA_17_065.txt,14.923659157485474,13.774445263066657,OOPSLA,13145,"
Virtual Machines (VMs) with Just-In-Time (JIT) compilers are traditionally thought to execute programs in
two phases: the initial warmup phase determines which parts of a program would most benefit from dynamic
compilation, before JIT compiling those parts into machine code; subsequently the program is said to be at a
steady state of peak performance. Measurement methodologies almost always discard data collected during
the warmup phase such that reported measurements focus entirely on peak performance. We introduce a fully
automated statistical approach, based on changepoint analysis, which allows us to determine if a program has
reached a steady state and, if so, whether that represents peak performance or not. Using this, we show that
even when run in the most controlled of circumstances, small, deterministic, widely studied microbenchmarks
often fail to reach a steady state of peak performance on a variety of common VMs. Repeating our experiment
on 3 different machines, we found that at most 43.5% of ⟨VM, benchmark⟩ pairs consistently reach a steady
state of peak performance.
",20.503739492662863,21.53088235294118,172
OOPSLA_17_066.txt,17.39368413792379,16.104724505869637,OOPSLA,11441,"
Software and hardware are increasingly being formally verified against specifications, but how can we verify
the specifications themselves? This paper explores what it means to formally verify a specification. We solve
three challenges: (1) How to create a secondary, higher-level specification that can be effectively reviewed
by processor designers who are not experts in formal verification; (2) How to avoid common-mode failures
between the specifications; and (3) How to automatically verify the two specifications against each other.

One of the most important specifications for software verification is the processor specification since it
defines the behaviour of machine code and of hardware protection features used by operating systems. We
demonstrate our approach on ARM’s v8-M Processor Specification, which is intended to improve the security
of Internet of Things devices. Thus, we focus on establishing the security guarantees the architecture is
intended to provide. Despite the fact that the ARM v8-M specification had previously been extensively tested,
we found twelve bugs (including two security bugs) that have all been fixed by ARM.
",17.613555460941566,16.687044334975372,176
PACT_17_001.txt,14.600130763325627,12.301234179042837,PACT,6584,"
In this paper we introduce RCU-HTM, a technique
that combines Read-Copy-Update (RCU) with Hardware Transactional Memory (HTM) to implement highly efficient concurrent
Binary Search Trees (BSTs). Similarly to RCU-based algorithms,
we perform the modifications of the tree structure in private
copies of the affected parts of the tree rather than in-place.
This allows threads that traverse the tree to proceed without
any synchronization and without being affected by concurrent
modifications. The novelty of RCU-HTM lies at leveraging HTM
to permit multiple updating threads to execute concurrently.
After appropriately modifying the private copy, we execute an
HTM transaction, which atomically validates that all the affected
parts of the tree have remained unchanged since they’ve been
read and, only if this validation is successful, installs the copy in
the tree structure.
We apply RCU-HTM on AVL and Red-Black balanced BSTs
and compare their performance to state-of-the-art lock-based,
non-blocking, RCU- and HTM-based BSTs. Our experimental evaluation reveals that BSTs implemented with RCU-HTM
achieve high performance, not only for read-only operations,
but also for update operations. More specifically, our evaluation
includes a diverse range of tree sizes and operation workloads
and reveals that BSTs based on RCU-HTM outperform other
alternatives by more than 18%, on average, on a multi-core server
with 44 hardware threads
",16.373557378465907,15.812719298245614,230
PACT_17_002.txt,15.172626615295595,13.095651385728207,PACT,8421,"
The Go language lacks built-in data structures
that allow fine-grained concurrent access. In particular, its map
data type, one of only two generic collections in Go, limits
concurrency to the case where all operations are read-only;
any mutation (insert, update, or remove) requires exclusive
access to the entire map. The tight integration of this map into
the Go language and runtime precludes its replacement with
known scalable map implementations.

This paper introduces the Interlocked Hash Table (IHT). The
THT is the result of language-driven data structure design: it
requires minimal changes to the Go map API, supports the
full range of operations available on the sequential Go map,
and provides a path for the language to evolve to become more
amenable to scalable computation over shared data structures.
The IHT employs a novel optimistic locking protocol to avoid
the risk of deadlock, and allows large critical sections that
access a single IHT element, and can easily support multikey atomic operations. These features come at the cost of
relaxed, though still straightforward, iteration semantics. In
experimentation in both Java and Go, the IHT performs well,
reaching up to 7x the performance of the state of the art in Go
at 24 threads. In Java, the IHT performs on par with the best
Java maps in the research literature, while providing iteration
and other features absent from other maps.
",15.470042427545799,14.393160173160172,232
PACT_17_003.txt,15.264277050090246,13.226004127294448,PACT,10198,"
High-level GPU graph processing frameworks are an at
tractive alternative for achieving both high productivity and
high performance. Hence, several high-level frameworks for
graph processing on GPUs have been developed. In this paper, we develop an approach to graph processing on GPUs
that seeks to overcome some of the performance limitations
of existing frameworks. It uses multiple data representation
and execution strategies for dense versus sparse vertex frontiers, dependent on the fraction of active graph vertices. A
two-phase edge processing approach trades off extra data
movement for improved load balancing across GPU threads,
by using a 2D blocked representation for edge data. Experimental results demonstrate performance improvement over
current state-of-the-art GPU graph processing frameworks
for many benchmark programs and data sets.
",15.688483145680365,13.663425196850394,128
PACT_17_004.txt,16.013178779124885,13.94140518923832,PACT,9579,"
Accurate, real-time Automatic Speech Recognition
(ASR) comes at a high energy cost, so accuracy has often to
be sacrificed in order to fit the strict power constraints of
mobile systems. However, accuracy is extremely important for
the end-user, and today’s systems are still unsatisfactory for
many applications. The most critical component of an ASR
system is the acoustic scoring, as it has a large impact on
the accuracy of the system and takes up the bulk of execution
time. The vast majority of ASR systems implement the acoustic
scoring by means of Gaussian Mixture Models (GMMs), where
the acoustic scores are obtained by evaluating multidimensional
Gaussian distributions.

In this paper, we propose a hardware accelerator for GMM
evaluation that reduces the energy required for acoustic scoring
by three orders of magnitude compared to solutions based on
CPUs and GPUs. Our accelerator implements a lazy evaluation
scheme where Gaussians are computed on demand, avoiding
50% of the computations. Furthermore, it employs a novel
clustering scheme to reduce the size of the acoustic model,
which results in 8x memory bandwidth savings with a negligible
impact on accuracy. Finally, it includes a novel memoization
scheme that avoids 74.88% of floating-point operations. The
end design provides a 164x speedup and 3532x energy reduction
when compared with a highly-tuned implementation running
on a modern mobile CPU. Compared to a state-of-the-art
mobile GPU, the GMM accelerator achieves 5.89x speedup
over a highly optimized CUDA implementation, while reducing
energy by 241x.

",16.887214914478655,15.824498007968128,255
PACT_17_005.txt,13.00175556030899,10.501346265859159,PACT,7935,"
Recent studies showed that DRAM restore time
degrades as technology scales, which imposes large performance
and energy overheads. This problem, prolonged restore time
(PRT), has been identified by the DRAM industry as one of
three major scaling challenges.

This paper proposes DrMP, a novel fine-grained precisionaware DRAM restore scheduling approach, to mitigate PRT. The
approach exploits process variations (PVs) within and across
DRAM rows to save data with mixed precision. The paper
describes three variants of the approach: DrMP-A, DrMP-P,
and DrMP-U. DrMP-A supports approximate computing by
mapping important data bits to fast row segments to reduce
restore time for improved performance at a low application error
rate. DrMP-P pairs memory rows together to reduce the average
restore time for precise computing. DrMP-U combines DrMP-A
and DrMP-P to better trade performance, energy consumption,
and computation precision. Our experimental results show that,
on average, DrMP achieves 20% performance improvement
and 15% energy reduction over a precision-oblivious baseline.
Further, DrMP achieves an error rate less than 1% at the
application level for a suite of benchmarks, including applications
that exhibit unacceptable error rates under simple approximation
that does not differentiate the importance of different bits.

",16.04434344847333,14.307159277504109,204
PACT_17_006.txt,13.810818255274171,11.860379232934307,PACT,9488,"
This work studies the interplay between multithreaded cores and speculative parallelism (e.g., transactional
memory or thread-level speculation). These techniques are often
used together, yet they have been developed independently. This
disconnect causes major performance pathologies: increasing the
number of threads per core adds conflicts and wasted work,
and puts pressure on speculative execution resources. These
pathologies often squander the benefits of multithreading.

We present speculation-aware multithreading (SAM), a simple policy that addresses these pathologies. By coordinating
instruction dispatch and conflict resolution priorities, SAM focuses execution resources on work that is more likely to commit,
avoiding aborts and using speculation resources more efficiently.
We design SAM variants for in-order and out-of-order cores.
SAM is cheap to implement and makes multithreaded cores
much more beneficial on speculative parallel programs.

We evaluate SAM on systems with up to 64 SMT cores.
With SAM, 8-threaded cores outperform single-threaded cores
by 2.33x on average, while a speculation-oblivious policy yields
a 1.85x speedup. SAM also reduces wasted work by 52%.
",15.186304673781336,13.284926004228332,176
PACT_17_007.txt,15.461089983547453,13.593163893589026,PACT,4319,"

This work proposes and discusses the implications of adding a new
feature to hardware transactional memory, allowing a program to
specify that a transaction should always abort (even if it executes
a commit instruction), and is thus guaranteed to be free of side
effects. Perhaps counterintuitively, we believe such a primitive can
be useful.

Prior art has already noted that HTM transactions, even in failure, can accelerate the subsequent execution of their contents by
warming up the branch predictor and caches. However, traditional
HTM requires that the programmer properly coordinate between
HTM and other synchronization primitives, otherwise data races
can occur. With always-abort HTM (AAHTM), no such synchronization is necessary, because there is no risk of accidentally committing a transaction that has seen inconsistent state. We can therefore use AAHTM in scenarios where traditional HTM would be
unsafe. In this paper, we present several designs that use AAHTM,
discuss preliminary results, and identify other situations in which
the new primitive might be useful.

",16.954822765917157,15.636167247386762,165
PACT_17_008.txt,14.964349313877026,13.129957418110653,PACT,11958,"

In spite of the multicore revolution, high single thread performance still plays an important role in ensuring a decent
overall gain. Look-ahead is a proven strategy in uncovering
implicit parallelism; however, a conventional out-of-order
core quickly becomes resource-inefficient when looking beyond a short distance. An effective approach is to use an independent look-ahead thread running on a separate context
guided by a program slice known as the skeleton. We observe
that fixed heuristics to generate skeletons are often suboptimal. As a consequence, look-ahead agent is not able to target
sufficient bottlenecks to reap all the benefits it should.

In this paper, we present DRUT, a holistic hardwaresoftware solution, which achieves good single thread performance by tuning the look-ahead skeleton efficiently. First,
we propose a number of dynamic transformations to branch
based code modules (we call them Do-It-Yourself or DIY)
that enable a faster look-ahead thread without compromising
the quality of the look-ahead. Second, we extend our tuning
mechanism to any arbitrary code region and use a profiledriven technique to tune the skeleton for the whole program.

Assisted by the aforementioned techniques, look-ahead
thread improves the performance of a baseline decoupled
look-ahead by up to 1.93 with a geometric mean of 1.15x.
Our techniques, combined with the weak dependence removal technique, improve the performance of a baseline
look-ahead by up to 2.12 with a geometric mean of 1.20x.
This is an impressive performance gain of 1.61 over the
single-thread baseline, which is much better compared to
conventional Turbo Boost with a comparable energy budget.
Keywords: Implicit parallelism, Turbo Boost, Decoupled
look-ahead, Do-It-Yourself branches, Skeleton tuning

",15.688483145680365,14.2040780141844,288
PACT_17_009.txt,17.473764655118174,17.20820652109452,PACT,1422,"
Karly design space evaluation of computer systems
is usually performed using performance models (e.g., detailed
simulators, RTL-based models, etc.). However, it is very challenging (often impossible) to run many emerging applications on
detailed performance models owing to their complex softwarestacks and long run times. To overcome such challenges in
benchmarking these complex applications, we propose a proxy
generation methodology, PerfProx to generate miniature proxy
benchmarks, which are representative of the performance of realworld applications and yet, converge to results quickly and do not
need any complex software-stack support. Past proxy generation
research utilizes detailed micro-architecture independent metrics
derived from detailed simulators, which are often difficult to
generate for many emerging applications. PerfProx enables fast
and efficient proxy generation using performance metrics derived
primarily from hardware performance counters. We evaluate
the proxy generation framework on three modern databases
(Cassandra, MongoDB and MySQL) running data-serving and
data-analytics applications. The proxy benchmarks mimic the
performance (IPC) of the original applications with ~94%
accuracy, while significantly reducing the instruction count.

",18.699421769314853,19.10215946843854,173
PACT_17_010.txt,14.717323933564032,12.699251357413178,PACT,9241,"
Provenance describes detailed information about
the history of a piece of data, containing the relationships
among elements such as users, processes, jobs, and workflows
that contribute to the existence of data. Provenance is key
to supporting many data management functionalities that are
increasingly important in operations such as identifying data
sources, parameters, or assumptions behind a given result;
auditing data usage; or understanding details about how inputs
are transformed into outputs. Despite its importance, however,
provenance support is largely underdeveloped in highly parallel
architectures and systems. One major challenge is the demanding
requirements of providing provenance service in situ. The need
to remain lightweight and to be always on often conflicts
with the need to be transparent and offer an accurate catalog
of details regarding the applications and systems. To tackle
this challenge, we introduce a lightweight provenance service,
called LPS, for high-performance computing (HPC) systems. LPS
leverages a kernel instrument mechanism to achieve transparency
and introduces representative execution and flexible granularity to
capture comprehensive provenance with controllable overhead.
Extensive evaluations and use cases have confirmed its efficiency
and usability. We believe that LPS can be integrated into current
and future HPC systems to support a variety of data management
needs.

",17.879347455551382,17.172244224422446,203
PACT_17_011.txt,13.965727454505291,12.299726180726907,PACT,8721,"
Last-level caches are increasingly distributed, consisting of many small banks. To perform well, most accesses
must be served by banks near requesting cores. An attractive
approach is to replicate read-only data so that a copy is
available nearby. But replication introduces a delicate tradeoff
between capacity and latency: too little replication forces cores
to access faraway banks, while too much replication wastes
cache space and causes excessive off-chip misses.

Workloads vary widely in their desired amount of replication, demanding an adaptive approach. Prior adaptive replication techniques only replicate data in each tile’s local bank, so
they focus on selecting which data to replicate. Unfortunately,
data that is not replicated still incurs a full network traversal,
limiting the performance of these techniques.

We argue that a better strategy is to let cores share replicas
and that adaptive schemes should focus on selecting how
much to replicate (i.e.. how many replicas to have across the
chip). This idea fully exploits the latency-capacity tradeoff,
achieving qualitatively higher performance than prior adaptive
replication techniques. It can be applied to many prior cache
organizations, and we demonstrate it on two: Nexus-R extends
R-NUCA, and Nexus-J extends Jigsaw. We evaluate Nexus on
HPC and server workloads running on a 144-core chip, where
it outperforms prior adaptive replication schemes and improves
performance by up to 90% and by 23% on average across all
workloads sensitive to replication.

Keywords-cache, data replication, NUCA, multicore

",15.579741850924794,13.9572131147541,247
PACT_17_012.txt,15.03178432461165,13.223849583623089,PACT,9804,"
 The looming breakdown of Moore’s Law and the
end of voltage scaling are ushering a new era where neither
transistors nor the energy to operate them is free. This calls for
a new regime in computer systems, one in which every transistor counts. Caches are essential for processor performance
and represent the bulk of modern processor’s transistor budget.
To get more performance out of the cache hierarchy, future
processors will rely on effective cache management policies.

This paper identifies variability in generational behavior of
cache blocks as a key challenge for cache management policies
that aim to identify dead blocks as early and as accurately as
possible to maximize cache efficiency. We show that existing
management policies are limited by the metrics they use to
identify dead blocks, leading to low coverage and/or low
accuracy in the face of variability. In response, we introduce
a new metric — Live Distance — that uses the stack distance to
learn the temporal reuse characteristics of cache blocks, thus
enabling a dead block predictor that is robust to variability in
generational behavior. Based on the reuse characteristics of an
application’s cache blocks, our predictor — Leeway - classifies
application’s behavior as streaming-oriented or reuse-oriented
and dynamically selects an appropriate cache management
policy. By leveraging live distance for LLC management,
Leeway outperforms state-of-the-art approaches on single- and
multi-core SPEC and manycore CloudSuite workloads.
",18.481644305966572,16.334022988505748,237
PACT_17_013.txt,15.396180212204872,13.410686157803358,PACT,8791,"
Achieving system fairness is a major design concern in current multicore processors. Unfairness arises due to
contention in the shared resources of the system, such as the
LLC and main memory. To address this problem, many research
works have proposed novel cache partitioning policies aimed
at addressing system fairness without harming performance.
Unfortunately, existing proposals targeting fairness require extra
hardware which makes them impractical in commercial processors.

Recent Intel Xeon processors feature Cache Allocation Technology (CAT), a hardware cache partitioning mechanism that can
be controlled from userspace software and that allows to create
partitions in the LLC and assign different groups of applications
to them.

In this paper we propose a family of clustering-based cache
partitioning policies to address fairness in systems that feature
Intel’s CAT. The proposal acts at two levels: applications showing
similar amount of core stalls due to LLC accesses are first
grouped into clusters, after which each cluster is given a number
of ways using a simple mathematical model. To the best of our
knowledge, this is the first attempt to address system fairness
using the cache partitioning hardware in a real product. Results
show that our best performing policy reduces system unfairness
by up to 80% (39% on average) for 8-application workloads and
by up to 45% (25% on average) for 12-application workloads
compared to a non-partitioning approach.
",17.553077303434723,16.89299008810573,229
PACT_17_014.txt,16.017878139774908,13.861334572146951,PACT,7825,"
The increasing memory requirements of big data
applications have been driving the precipitous growth of memory capacity in server systems. To maximize the efficiency of
external memory, HW-based memory compression techniques
have been proposed to increase effective memory capacity.
Although such memory compression techniques can improve
the memory efficiency significantly, a critical trade-off exists in
the HW-based compression techniques. As the memory blocks
need to be decompressed as quickly as possible to serve cache
misses, latency-optimized techniques apply compression at the
cacheline granularity, achieving the decompression latency
of less than a few cycles. However, such latency-optimized
techniques can lose the potential high compression ratios of
capacity-optimized techniques, which compress larger memory
blocks with longer latency algorithms.

Considering the fundamental trade-off in the memory compression, this paper proposes a transparent dual memory
compression (DMC) architecture, which selectively uses two
compression algorithms with distinct latency and compression
characteristics. Exploiting the locality of memory accesses,
the proposed architecture compresses less frequently accessed
blocks with a capacity-optimized compression algorithm, while
keeping recently accessed blocks compressed with a latencyoptimized one. Furthermore, instead of relying on the support
from the virtual memory system to locate compressed memory
blocks, the study advocates a HW-based translation between
the uncompressed address space and compressed physical
space. This OS-transparent approach eliminates conflicts between compression efficiency and large page support adopted to
reduce TLB misses. The proposed compression architecture is
applied to the Hybrid Memory Cube (HMC) with a logic layer
under the stacked DRAMs. The experimental results show that
the proposed compression architecture provides 54% higher
compression ratio than the state-of-the-art latency-optimized
technique, with no performance degradation over the baseline
system without compression.

Keywords-memory compression, dual compression technique,
OS transparency, locality awareness

",20.862888550776066,19.45380471380471,298
PACT_17_015.txt,14.803905376888956,13.346386139920302,PACT,5909,"
Accurate automatic optimization heuristics are
necessary for dealing with the complexity and diversity of
modern hardware and software. Machine learning is a proven
technique for learning such heuristics, but its success is bound
by the quality of the features used. These features must be
hand crafted by developers through a combination of expert
domain knowledge and trial and error. This makes the quality
of the final model directly dependent on the skill and available
time of the system architect.
Our work introduces a better way for building heuristics. We
develop a deep neural network that learns heuristics over raw
code, entirely without using code features. The neural network
simultaneously constructs appropriate representations of the
code and learns how best to optimize, removing the need for
manual feature creation. Further, we show that our neural
nets can transfer learning from one optimization problem to
another, improving the accuracy of new models, without the
help of human experts.
We compare the effectiveness of our automatically generated
heuristics against ones with features hand-picked by experts.
We examine two challenging tasks: predicting optimal mapping
for heterogeneous parallelism and GPU thread coarsening
factors. In 89% of the cases, the quality of our fully automatic heuristics matches or surpasses that of state-of-the-art
predictive models using hand-crafted features, providing on
average 14% and 12% more performance with no human effort
expended on designing features.
",15.668782140382113,14.455282131661445,233
PACT_17_016.txt,14.557875265485166,12.259058016985573,PACT,8748,"
Most GPU-based graph systems cannot handle
large-scale graphs that do not fit in the GPU memory. The
ever-increasing graph size demands a scale-up graph system,
which can run on a single GPU with optimized memory access
efficiency and well-controlled data transfer overhead. However,
existing systems either incur redundant data transfers or fail to
use shared memory. In this paper we present Graphie, a system
to efficiently traverse large-scale graphs on a single GPU. Graphie
stores the vertex attribute data in the GPU memory and streams
edge data asynchronously to the GPU for processing. Graphie’s
high performance relies on two renaming algorithms. The first
algorithm renames the vertices so that the source vertices can
be easily loaded to the shared memory to reduce global memory
accesses. The second algorithm inserts virtual vertices into the
vertex set to rename real vertices, which enables the use of
a small boolean array to track active partitions. The boolean
array also resides in shared memory and can be updated in
constant time. The renaming algorithms do not introduce any
extra overhead in the GPU memory or graph storage on disk.
Graphie’s runtime overlaps data transfer with kernel execution
and reuses transferred data in the GPU memory. The evaluation
of Graphie on 7 real-world graphs with up to 1.8 billion edges
demonstrates substantial speedups over X-Stream, a state-of-theart edge-centric graph processing framework on the CPU, and
GraphReduce, an out-of-memory graph processing systems on
GPUs.
",15.134931603849854,12.782639442231076,255
PACT_17_017.txt,13.262603137474684,11.336488201227645,PACT,8940,"
We propose a design for a fine-grained lockbased skiplist optimized for Graphics Processing Units (GPUs).
While GPUs are often used to accelerate streaming parallel
computations, it remains a significant challenge to efficiently
offload concurrent computations with more complicated datairregular access and fine-grained synchronization. Natural
building blocks for such computations would be concurrent
data structures, such as skiplists, which are widely used in
general purpose computations. Our design utilizes array-based
nodes which are accessed and updated by warp-cooperative
functions, thus taking advantage of the fact that GPUs are
most efficient when memory accesses are coalesced and execution divergence is minimized. The proposed design has
been implemented, and measurements demonstrate improved
performance of up to 11.6x over skiplist designs for the GPU
existing today.
",18.026119701940384,16.90149206349206,128
PACT_17_018.txt,12.962286322282871,10.853890894700587,PACT,10286,"
Networks-on-Chip (NoCs) in chip multiprocessors
are prone to within-die process variation as they span the
whole chip. To tolerate variation, their voltages (Via) carry
over-provisioned guardbands. As a result, prior work has
proposed to save energy by operating at reduced Via while
occasionally suffering and fixing errors. Unfortunately, these
proposals use heuristic controller designs that provide no error
bounds guarantees.

In this work, we develop a scheme that dynamically minimizes the Vaq of groups of routers in a variation-prone NoC
using formal control-theoretic methods. The scheme, called
Sthira, saves substantial energy while guaranteeing the stability
and convergence of error rates. We also enhance the scheme
with a low-cost secondary network that retransmits erroneous
packets for higher energy efficiency. The enhanced scheme is
called Sthira+. We evaluate Sthira and Sthira+ with simulations
of NoCs with 64-100 routers. In an NoC with 8 routers per Via
domain, our schemes reduce the average energy consumption
of the NoC by 27%; in a futuristic NoC with one router per
Vaa domain, Sthira+ and Sthira reduce the average energy
consumption by 36% and 32%, respectively. The performance
impact is negligible. These are significant savings over the stateof-the-art. We conclude that formal control is essential, and that
the cheaper Sthira is more cost-effective than Sthira+.
",13.757784519515319,11.780000000000001,221
PACT_17_019.txt,15.002253434291447,12.903796336915995,PACT,8305,"
Updates to a process’s page table entry (PTE)
renders any existing copies of that PTE in any of a system’s
TLBs stale. To prevent a process from making illegal memory
accesses using stale TLB entries, the operating system (OS)
performs a costly TLB shootdown operation. Rather than
explicitly issuing shootdowns, we propose a coordinated TLB
and page table management mechanism where an expiration
time is associated with each TLB entry. An expired TLB entry
is treated as invalid. For each PTE, the OS then tracks the
latest expiration time of any TLB entry potentially caching that
PTE. No shootdown is issued if the OS modifies a PTE when
its corresponding latest expiration time has already passed.

In this paper, we explain the hardware and OS support
required to support Self-invalidating TLB entries (SITE). As
an emerging use case that needs fast TLB shootdowns, we
consider memory systems consisting of different types of
memory (e.g., faster DRAM and slower non-volatile memory)
where aggressive migrations are desirable to keep frequentlyaccessed pages in faster memory, but pages cannot migrate
too often because each migration requires a PTE update and
corresponding TLB shootdown. We demonstrate that such
heterogeneous memory systems augmented with SITE can
allow an average performance improvement of 45.5% over a
similar system with traditional TLB shootdowns by avoiding
more than 65% of the shootdowns.

Keywords-Heterogeneous Memory; Self-Invalidation; Virtual
Memory; TLB; TLB Shootdown; HW/SW Co-design; NonVolatile Memories;

",16.52667757954773,15.303605809128634,246
PACT_17_020.txt,14.249857831049201,12.170423401781886,PACT,10805,"
The memory model for RISC-V, a newly developed
open source ISA, has not been finalized yet and thus, offers an
opportunity to evaluate existing memory models. We believe
RISC-V should not adopt the memory models of POWER or
ARM, because their axiomatic and operational definitions are
too complicated. We propose two new weak memory models:
WMM and WMM.-S, which balance definitional simplicity and
implementation flexibility differently. Both allow all instruction
reorderings except overtaking of loads by a store. We show
that this restriction has little impact on performance and it
considerably simplifies operational definitions. It also rules
out the out-of-thin-air problem that plagues many definitions.
WMM is simple (it is similar to the Alpha memory model),
but it disallows behaviors arising due to shared store buffers
and shared write-through caches (which are seen in POWER
processors). WMM-S, on the other hand, is more complex and
allows these behaviors. We give the operational definitions of
both models using Instantaneous Instruction Execution (7E),
which has been used in the definitions of SC and TSO. We also
show how both models can be implemented using conventional
cache-coherent memory systems and out-of-order processors,
and encompasses the behaviors of most known optimizations.

",16.45884130781739,14.867564102564106,209
PACT_17_021.txt,14.771500419527072,12.844722596875144,PACT,9039,"
Memory and logic integration on the same chip
is becoming increasingly cost effective, creating the opportunity to offload data-intensive functionality to processing units
placed inside memory chips. The introduction of memoryside processing units (MPUs) into conventional systems faces
virtual memory as the first big showstopper: without efficient
hardware support for address translation MPUs have highly
limited applicability. Unfortunately, conventional translation
mechanisms fall short of providing fast translations as contemporary memories exceed the reach of TLBs, making expensive
page walks common.

In this paper, we are the first to show that the historically
important flexibility to map any virtual page to any page frame
is unnecessary in today’s servers. We find that while limiting
the associativity of the virtual-to-physical mapping incurs no
penalty, it can break the translate-then-fetch serialization if
combined with careful data placement in the MPU’s memory,
allowing for translation and data fetch to proceed independently and in parallel. We propose the Distributed Inverted
Page Table (DIPTA), a near-memory structure in which the
smallest memory partition keeps the translation information
for its data share, ensuring that the translation completes
together with the data fetch. DIPTA completely eliminates the
performance overhead of translation, achieving speedups of up
to 3.81x and 2.13x over conventional translation using 4KB
and 1GB pages respectively.
",20.670646682091633,19.880396825396833,221
PACT_17_022.txt,14.77751250380049,12.569182518115941,PACT,7617,"
Future main memory will likely include NonVolatile Memory. Non-Volatile Main Memory (NVMM) provides an opportunity to rethink checkpointing strategies for
providing failure safety to applications. While there are many
checkpointing and logging schemes in literature, their use must
be revisited as they incur high execution time overheads as well
as a large number of additional writes to NVMM, which may
significantly impact write endurance.

In this paper, we propose a novel recompute-based failure
safety approach, and demonstrate its applicability to loopbased code. Rather than keeping a fully consistent logging
state, we only log enough state to enable recomputation. Upon
a failure, our approach recovers to a consistent state by determining which parts of the computation were not completed
and recomputing them. Effectively, our approach removes the
need to keep checkpoints or logs, thus reducing execution
time overheads and improving NVMM write endurance, at
the expense of more complex recovery. We compare our new
approach against logging and checkpointing on five scientific
workloads, including tiled matrix multiplication, on a computer
system model that was built on gem5 and supports Intel
PMEM instruction extensions. For tiled matrix multiplication,
our recompute approach incurs an execution time overhead
of only 5%, in contrast to 8% overhead with logging and
207% overhead with checkpointing. Furthermore, recompute
only adds 7% additional NVMM writes, compared to 111%
with logging and 330% with checkpointing.
",16.647925096878797,15.297614035087719,229
PACT_17_023.txt,14.743456460863424,12.48791117297229,PACT,8962,"
SIMD vectors help improve the performance of
certain applications. The code gets vectorized into SIMD
form either by hand, or automatically with auto-vectorizing
compilers. The Superword-Level Parallelism (SLP) vectoriza—
tion algorithm is a widely used algorithm for vectorizing
straight4tine code and is part of most industrial compilers.
The algorithm attempts to pack scalar instructions into vectors
starting from specific seed instructions in a bottom-up way.
This approach, however, suffers from two main problems: (i)
the algorithm may not reach instructions that could have been
vectorized, and (ii) atomically operating on individual SLP
graphs suffers from cost overestimation when consecutive SLP
graphs share data. Both issues lead to missed vectorization
opportunities even in simple code.

In this work we propose SuperGraph-SLP (SG-SLP), an
improved vectorization algorithm that overcomes these limita—
tions of the existing algorithm. SG-SLP operates on a larger
region, called the SuperGraph. This allows it to reach and
successfully vectorize code that was previously unreachable.
Moreover, the new region helps eliminate the inaccuracies
in the cost-calculation as it allows for a more holistic view
of the code. Our experiments show that SG-SLP improves
the vectorization coverage and outperforms the state-of-theart SLP across a number kernels by 36% on average, without
affecting the compilation time.

",15.903189008614273,14.20575052854123,216
PACT_17_024.txt,15.948594681431103,13.634954196932132,PACT,8117,"
Processor manufacturers have adopted SIMD for
decades because of its superior performance and power efficiency. The configurations of SIMD registers (i.e., the number
and width) have evolved and diverged rapidly through various
ISA extensions on different architectures. However, migrating
legacy or proprietary applications optimized for one guest ISA
to another host ISA that has fewer but longer SIMD registers through binary translation raises the issues of asymmetric
SIMD register configurations. To date, these issues have been
overlooked. As a result, only a small fraction of the potential
performance gain is realized due to underutilization of the
host’s SIMD parallelism and register capacity.

In this paper, we present a novel dynamic binary translation technique called spill-aware SLP (saSLP), which combines
short ARMv8 NEON instructions and registers in the guest
binary loops to fully utilize the x86 AVX host’s parallelism as
well as minimize register spilling. Our experiment results show
that saSLP improves the performance by 1.6X (2.3X) across
a number of benchmarks, and reduces spilling by 97% (99%)
for ARMv8 NEON to x86 AVX2 (AVX-512) translation.
",18.08858127442927,16.40476190476191,183
PACT_17_025.txt,16.08931345929332,14.791072126619731,PACT,8021,"

Computational scientists are typically not expert programmers, and thus work in easy to use dynamic languages.
However, they have very high performance requirements,
due to their large datasets and experimental setups. Thus,
the performance required for computational science must be
extracted from dynamic languages in a manner that is transparent to the programmer. Current approaches to optimize and
parallelize dynamic languages, such as just-in-time compilation
and highly optimized interpreters, require a huge amount
of implementation effort and are typically only effective for
a single language. However, scientists in different fields use
different languages, depending upon their needs.

This paper presents techniques to enable automatic extraction of parallelism within scripts that are universally applicable
across multiple different dynamic scripting languages. The
key insight is that combining a script with its interpreter,
through program specialization techniques, will embed any
parallelism within the script into the combined program that
can then be extracted via automatic parallelization techniques.
Additionally, this paper presents several enhancements to
existing speculative automatic parallelization techniques to
handle the dependence patterns created by the specialization
process. A prototype of the proposed technique, called Partial
Evaluation with Parallelization (PEP), is evaluated against two
Open-source script interpreters with 6 input linear algebra
kernal scripts each. The resulting geomean speedup of 5.10
on a 24-core machine shows the potential of the generalized
approach in automatic extraction of parallelism in dynamic
scripting languages.
",19.1858809040293,18.261806866952792,235
PLDI_17_001.txt,14.718390477090313,13.076137166636503,PLDI,13625,"
We present a novel approach to proving the absence of timing channels. The idea is to partition the program’s execution traces in such a way that each partition component is
checked for timing attack resilience by a time complexity
analysis and that per-component resilience implies the resilience of the whole program. We construct a partition by
splitting the program traces at secret-independent branches.
This ensures that any pair of traces with the same public input has a component containing both traces. Crucially, the
per-component checks can be normal safety properties expressed in terms of a single execution. Our approach is thus
in contrast to prior approaches, such as self-composition, that
aim to reason about multiple (k ≥ 2) executions at once.
We formalize the above as an approach called quotient
partitioning, generalized to any k-safety property, and prove
it to be sound. A key feature of our approach is a demanddriven partitioning strategy that uses a regex-like notion
called trails to identify sets of execution traces, particularly those influenced by tainted (or secret) data. We have
applied our technique in a prototype implementation tool
called Blazer, based on WALA, PPL, and the brics automaton library. We have proved timing-channel freedom of (or
synthesized an attack specification for) 24 programs written
in Java bytecode, including 6 classic examples from the literature and 6 examples extracted from the DARPA STAC
challenge problems.
",15.381575749822971,14.564392405063291,238
PLDI_17_002.txt,16.006259476267086,14.17203889020755,PLDI,9443,"
The correct compilation of block diagram languages like
Lustre, Scade, and a discrete subset of Simulink is important
since they are used to program critical embedded control
software. We describe the specification and verification in
an Interactive Theorem Prover of a compilation chain that
treats the key aspects of Lustre: sampling, nodes, and delays.
Building on CompCert, we show that repeated execution
of the generated assembly code faithfully implements the
dataflow semantics of source programs.
We resolve two key technical challenges. The first is the
change from a synchronous dataflow semantics, where programs manipulate streams of values, to an imperative one,
where computations manipulate memory sequentially. The
second is the verified compilation of an imperative language
with encapsulated state to C code where the state is realized
by nested records. We also treat a standard control optimization that eliminates unnecessary conditional statements.
",16.613394197324528,15.00734406438632,143
PLDI_17_003.txt,15.2362912933237,13.486892035796934,PLDI,9289,"
Peephole optimizations are a common source of compiler
bugs. Compiler developers typically transform an incorrect
peephole optimization into a valid one by strengthening the
precondition. This process is challenging and tedious. This
paper proposes ALIVE-INFER, a data-driven approach that
infers preconditions for peephole optimizations expressed
in Alive. ALIVE-INFER generates positive and negative
examples for an optimization, enumerates predicates on-
demand, and learns a set of predicates that separate the
positive and negative examples. ALIVE-INFER repeats this
process until it finds a precondition that ensures the validity
of the optimization. ALIVE-INFER reports both a weakest
precondition and a set of succinct partial preconditions to
the developer. Our prototype generates preconditions that are
weaker than LLVM’s preconditions for 73 optimizations in
the Alive suite. We also demonstrate the applicability of this
technique to generalize 54 optimization patterns generated
by Souper, an LLVM IR–based superoptimizer.
",16.04434344847333,15.459819819819824,150
PLDI_17_004.txt,11.308042489639696,8.629854711370182,PLDI,7657,"

Achieving high code coverage is essential in testing, which
gives us confidence in code quality. Testing floating-point
code usually requires painstaking efforts in handling floatingpoint constraints, e.g., in symbolic execution. This paper turns
the challenge of testing floating-point code into the opportunity of applying unconstrained programming — the mathematical solution for calculating function minimum points
over the entire search space. Our core insight is to derive a
representing function from the floating-point program, any of
whose minimum points is a test input guaranteed to exercise
a new branch of the tested program. This guarantee allows
us to achieve high coverage of the floating-point program by
repeatedly minimizing the representing function.

We have realized this approach in a tool called CoverMe
and conducted an extensive evaluation of it on Sun’s C math
library. Our evaluation results show that CoverMe achieves,
on average, 90.8% branch coverage in 6.9 seconds, drastically
outperforming our compared tools: (1) Random testing, (2)
AFL, a highly optimized, robust fuzzer released by Google,
and (3) Austin, a state-of-the-art coverage-based testing tool
designed to support floating-point code.
",17.451712890111917,16.21586789554532,191
PLDI_17_005.txt,14.320141291307632,12.102903050490365,PLDI,9448,"

We propose a fully-automated technique for inverting functional programs that operate over lists such as string encoders
and decoders. We consider programs that can be modeled
using symbolic extended finite transducers (s-EFTs), an expressive model that can describe complex list-manipulating
programs while retaining several decidable properties. Concretely, given a program P expressed as an s-EFT, we propose techniques for: 1) checking whether P is injective and,
if that is the case, 2) building an s-EFT P—! describing its
inverse. We first show that it is undecidable to check whether
an s-EFT is injective and propose an algorithm for checking
injectivity for a restricted, but a practical class of s-EFTs. We
then propose an algorithm for inverting s-EFTs based on the
following idea: if an s-EFT is injective, inverting it amounts
to inverting all its individual transitions. We leverage recent
advances in program synthesis and show that the transition
inversion problem can be expressed as an instance of the
syntax-guided synthesis framework. Finally, we implement
the proposed techniques in a tool called GENIC and show that
GENIC can invert 13 out of 14 real complex string encoders
and decoders, producing inverse programs that are almost
identical to manually written ones.
",16.975882523387877,15.206405502392347,210
PLDI_17_006.txt,15.14616614370954,13.351386379545982,PLDI,8972,"

GPU programming models enable and encourage massively
parallel programming with over a million threads, requiring
extreme parallelism to achieve good performance. Massive
parallelism brings significant correctness challenges by increasing the possibility for bugs as the number of thread interleavings balloons. Conventional dynamic safety analyses
struggle to run at this scale.

We present BARRACUDA, a concurrency bug detector for
GPU programs written in Nvidia’s CUDA language. BARRACUDA handles a wider range of parallelism constructs
than previous work, including branch operations, low-level
atomics and memory fences, which allows BARRACUDA to
detect new classes of concurrency bugs. BARRACUDA operates at the binary level for increased compatibility with existing code, leveraging a new binary instrumentation framework that is extensible to other dynamic analyses. BARRACUDA incorporates a number of novel optimizations that
are crucial for scaling concurrency bug detection to over a
million threads.
",17.93193317476759,17.495714285714282,142
PLDI_17_007.txt,14.572580741041524,12.590418552036201,PLDI,10099,"
Precise dynamic data race detectors provide strong correctness guarantees but have high overheads because they generally keep analysis state in a separate shadow location for each
heap memory location, and they check (and potentially update) the corresponding shadow location on each heap access.
The B IG F OOT dynamic data race detector uses a combination
of static and dynamic analysis techniques to coalesce checks
and compress shadow locations. With B IG F OOT, multiple
accesses to an object or array often induce a single coalesced
check that manipulates a single compressed shadow location,
resulting in a performance improvement over FAST T RACK
of 61%.
",20.890749979661237,19.71461538461539,105
PLDI_17_008.txt,15.525466780613751,13.880340149449221,PLDI,10976,"

The maturation of the Web platform has given rise to sophisticated and demanding Web applications such as interactive
3D visualization, audio and video software, and games. With
that, efficiency and security of code on the Web has become
more important than ever. Yet JavaScript as the only builtin language of the Web is not well-equipped to meet these
requirements, especially as a compilation target.

Engineers from the four major browser vendors have
risen to the challenge and collaboratively designed a portable
low-level bytecode called WebAssembly. It offers compact
representation, efficient validation and compilation, and safe
low to no-overhead execution. Rather than committing to a
specific programming model, WebAssembly is an abstraction over modern hardware, making it language-, hardware-,
and. platform-independent, with use cases beyond just the
Web. WebAssembly has been designed with a formal semantics from the start. We describe the motivation, design
and formal semantics of WebAssembly and provide some
preliminary experience with implementations.
",15.903189008614273,14.944897798742144,160
PLDI_17_009.txt,14.352457695267127,12.568473632011255,PLDI,9007,"

We present an approach to optimize the cache locality for
recursive programs by dynamically splicing—recursively
interleaving—the execution of distinct function invocations.
By utilizing data effect annotations, we identify concurrency
and data reuse opportunities across function invocations and
interleave them to reduce reuse distance. We present algorithms that efficiently track effects in recursive programs,
detect interference and dependencies, and interleave execution of function invocations using user-level (non-kernel)
lightweight threads. To enable multi-core execution, a program is parallelized using a nested fork/join programming
model. Our cache optimization strategy is designed to work
in the context of a random work-stealing scheduler. We
present an implementation using the MIT Cilk framework
that demonstrates significant improvements in sequential
and parallel performance, competitive with a state-of-the-art
compile-time optimizer for loop programs and a domainspecific optimizer for stencil programs.
",18.243605946275583,18.625279720279725,144
PLDI_17_010.txt,15.673006389159639,13.72415790092261,PLDI,9800,"
The problem of probabilistic modeling and inference, at a
high-level, can be viewed as constructing a (model, query,
inference) tuple, where an inference algorithm implements
a query on a model. Notably, the derivation of inference algorithms can be a difficult and error-prone task. Hence, researchers have explored how ideas from probabilistic programming can be applied. In the context of constructing
these tuples, probabilistic programming can be seen as taking a language-based approach to probabilistic modeling and
inference. For instance, by using (1) appropriate languages
for expressing models and queries and (2) devising inference techniques that operate on encodings of models (and
queries) as program expressions, the task of inference can
be automated.
In this paper, we describe a compiler that transforms a
probabilistic model written in a restricted modeling language
and a query for posterior samples given observed data into
a Markov Chain Monte Carlo (MCMC) inference algorithm
that implements the query. The compiler uses a sequence of
intermediate languages (ILs) that guide it in gradually and
successively refining a declarative specification of a probabilistic model and the query into an executable MCMC inference algorithm. The compilation strategy produces composable MCMC algorithms for execution on a CPU or GPU.
",18.7741,17.565420792079212,203
PLDI_17_011.txt,13.615983943467192,11.690620616285283,PLDI,8607,"

Many fields of study in compilers give rise to the concept
of a join point—a place where different execution paths
come together. Join points are often treated as functions
or continuations, but we believe it is time to study them
in their own right. We show that adding join points to a
direct-style functional intermediate language is a simple
but powerful change that allows new optimizations to be
performed, including a significant improvement to list fusion.
Finally, we report on recent work on adding join points to the
intermediate language of the Glasgow Haskell Compiler.
",14.191785591663535,12.966469072164948,98
PLDI_17_012.txt,14.884559084240394,12.892179145079798,PLDI,8708,"
This paper presents a novel component-based synthesis
algorithm that marries the power of type-directed search
with lightweight SMT-based deduction and partial evaluation. Given a set of components together with their overapproximate first-order specifications, our method first generates a program sketch over a subset of the components and
checks its feasibility using an SMT solver. Since a program
sketch typically represents many concrete programs, the use
of SMT-based deduction greatly increases the scalability
of the algorithm. Once a feasible program sketch is found,
our algorithm completes the sketch in a bottom-up fashion,
using partial evaluation to further increase the power of deduction for rejecting partially-filled program sketches. We
apply the proposed synthesis methodology for automating a
large class of data preparation tasks that commonly arise in
data science. We have evaluated our synthesis algorithm on
dozens of data wrangling and consolidation tasks obtained
from on-line forums, and we show that our approach can
automatically solve a large class of problems encountered
by R users.
",17.315433740611066,17.67587719298246,172
PLDI_17_013.txt,15.80034124796584,14.049855872494575,PLDI,9774,"
Compositional recurrence analysis (CRA) is a static-analysis
method based on a combination of symbolic analysis and
abstract interpretation. This paper addresses the problem
of creating a context-sensitive interprocedural version of
CRA that handles recursive procedures. The problem is
non-trivial because there is an “impedance mismatch” between CRA, which relies on analysis techniques based on
regular languages (i.e., Tarjan’s path-expression method),
and the context-free-language underpinnings of contextsensitive analysis.
We show how to address this impedance mismatch by
augmenting the CRA abstract domain with additional operations. We call the resulting algorithm Interprocedural CRA
(ICRA). Our experiments with ICRA show that it has broad
overall strength compared with several state-of-the-art software model checkers.
",16.52667757954773,15.844159663865547,122
PLDI_17_014.txt,15.227053722938145,13.869805275746128,PLDI,9848,"

Points-to analysis for Java benefits greatly from context
sensitivity. CFL-reachability and k-limited context strings
are two approaches to obtaining context sensitivity with
different advantages: CFL-reachability allows local reasoning
about data-value flow and thus is suitable for demand-driven
analyses, whereas k-limited analyses allow object sensitivity
which is a superior calling context abstraction for objectoriented languages. We combine the advantages of both
approaches to obtain a context-sensitive analysis that is as
precise as k-limited context strings, but is more efficient to
compute. Our key insight is based on a novel abstraction
of contexts adapted from CFL-reachability that represents
a relation between two calling contexts as a composition of
transformations over contexts.

We formulate pointer analysis in an algebraic structure of
context transformations, which is a set of functions over
calling contexts closed under function composition. We
show that the context representation of context-string-based
analyses is an explicit enumeration of all input and output
values of context transformations. CFL-reachability-based
pointer analysis is formulated to use call-strings as contexts,
but the context transformations concept can be applied to
any context abstraction used in k-limited analyses, including
object- and type-sensitive analysis. The result is a more
efficient algorithm for computing context-sensitive results
for a wide variety of context configurations.
",18.7741,18.885653153153154,223
PLDI_17_015.txt,14.27782632102345,12.841098140045307,PLDI,9430,"
Debugging is difficult. When software fails in production, de-
bugging is even harder, as failure reports usually provide only
an incomplete picture of the failing execution. We present a
system that answers control-flow queries posed by developers
as formal languages, indicating whether the query expresses
control flow that is possible or impossible for a given failure
report. We consider three separate approaches that trade off
precision, expressiveness for failure constraints, and scalabil-
ity. We also introduce a new subclass of regular languages,
the unreliable trace languages, which are particularly suited
to answering control-flow queries in polynomial time. Our
system answers queries remarkably efficiently when we en-
code failure constraints and user queries entirely as unreliable
trace languages.
",16.11434528070225,15.645840336134455,120
PLDI_17_016.txt,15.807151622202152,14.48935936057752,PLDI,11395,"
We introduce DEMOMATCH, a tool for API discovery that
allows the user to discover how to implement functionality
using a software framework by demonstrating the function-
ality in existing applications built with the same framework.
DEMOMATCH matches the demonstrations against a database
of execution traces called SEMERU and generates code snip-
pets explaining how to use the functionality. We evaluated
DEMOMATCH on several case studies involving Java Swing
and Eclipse RCP.
",18.946978176291534,17.07380281690141,72
PLDI_17_017.txt,13.301609884275315,11.316685692381768,PLDI,10195,"
Writing reliable concurrent software remains a huge chal-
lenge for today’s programmers. Programmers rarely reason
about their code by explicitly considering different possible
inter-leavings of its execution. We consider the problem of
detecting data races from individual executions in a sound
manner. The classical approach to solving this problem has
been to use Lamport’s happens-before (HB) relation. Un-
til now HB remains the only approach that runs in linear
time. Previous efforts in improving over HB such as causally-
precedes (CP) and maximal causal models fall short due to
the fact that they are not implementable efficiently and hence
have to compromise on their race detecting ability by limiting
their techniques to bounded sized fragments of the execution.
We present a new relation weak-causally-precedes (WCP)
that is provably better than CP in terms of being able to detect
more races, while still remaining sound. Moreover, it admits
a linear time algorithm which works on the entire execution
without having to fragment it.
",15.247664890283005,13.401295180722894,169
PLDI_17_018.txt,13.72505333045272,11.66306936828082,PLDI,9080,"
Mainstream points-to analysis techniques for object-oriented
languages rely predominantly on the allocation-site abstraction to model heap objects. We present M AHJONG, a novel
heap abstraction that is specifically developed to address
the needs of an important class of type-dependent clients,
such as call graph construction, devirtualization and mayfail casting. By merging equivalent automata representing
type-consistent objects that are created by the allocationsite abstraction, M AHJONG enables an allocation-site-based
points-to analysis to run significantly faster while achieving
nearly the same precision for type-dependent clients.
M AHJONG is simple conceptually, efficient, and drops
easily on any allocation-site-based points-to analysis. We
demonstrate its effectiveness by discussing some insights on
why it is a better alternative of the allocation-site abstraction
for type-dependent clients and evaluating it extensively on
12 large real-world Java programs with five context-sensitive
points-to analyses and three widely used type-dependent
clients. M AHJONG is expected to provide significant benefits
for many program analyses where call graphs are required.
",19.454632303725965,18.643285714285714,176
PLDI_17_019.txt,13.27947277178249,10.840991708088051,PLDI,11013,"
We describe a uniform and efficient framework for checking the satisfiability of a large class of string constraints.
The framework is based on the observation that both satisfiability and unsatisfiability of common constraints can
be demonstrated through witnesses with simple patterns.
These patterns are captured using flat automata each of
which consists of a sequence of simple loops. We build
a Counter-Example Guided Abstraction Refinement (CEGAR) framework which contains both an under- and an
over-approximation module. The flow of information between the modules allows to increase the precision in an automatic manner. We have implemented the framework as a
tool and performed extensive experimentation that demonstrates both the generality and efficiency of our method.
",14.06817628641468,13.922413793103448,117
PLDI_17_020.txt,14.561233347880478,12.898663034460345,PLDI,10375,"
We present FunTAL, the first multi-language system to
formalize safe interoperability between a high-level functional language and low-level assembly code while supporting compositional reasoning about the mix. A central
challenge in developing such a multi-language is bridging
the gap between assembly, which is staged into jumps to
continuations, and high-level code, where subterms return a
result. We present a compositional stack-based typed assembly language that supports components, comprised of one
or more basic blocks, that may be embedded in high-level
contexts. We also present a logical relation for FunTAL that
supports reasoning about equivalence of high-level components and their assembly replacements, mixed-language
programs with callbacks between languages, and assembly
components comprised of different numbers of basic blocks.
",17.410965686947208,18.141031746031747,127
PLDI_17_021.txt,14.731742533061166,12.624945218916732,PLDI,9568,"
List comprehensions provide a powerful abstraction mechanism for expressing computations over ordered collections of
data declaratively without having to use explicit iteration constructs. This paper puts forth effectful comprehensions as an
elegant way to describe list comprehensions that incorporate
loop-carried state. This is motivated by operations such as
compression/decompression and serialization/deserialization
that are common in log/data processing pipelines and require
loop-carried state when processing an input stream of data.
We build on the underlying theory of symbolic transducers
to fuse pipelines of effectful comprehensions into a single
representation, from which efficient code can be generated.
Using background theory reasoning with an SMT solver, our
fusion and subsequent reachability based branch elimination algorithms can significantly reduce the complexity of
the fused pipelines. Our implementation shows significant
speedups over reasonable hand-written code (3.4×, on average) and traditionally fused version of the pipeline (2.6×, on
average) for a variety of examples, including scenarios for
extracting fields with regular expressions, processing XML
with XPath, and running queries over encoded data.
",20.425298281703412,20.201609195402302,175
PLDI_17_022.txt,15.710118619332857,14.102838713046385,PLDI,9860,"
Futhark is a purely functional data-parallel array language
that offers a machine-neutral programming model and an
optimising compiler that generates OpenCL code for GPUs.
This paper presents the design and implementation of three
key features of Futhark that seek a suitable middle ground
with imperative approaches. First, in order to express efficient code inside the parallel constructs, we introduce a simple type system for in-place updates that ensures referential
transparency and supports equational reasoning. Second, we
furnish Futhark with parallel operators capable of expressing efficient strength-reduced code, along with their fusion
rules. Third, we present a flattening transformation aimed at
enhancing the degree of parallelism that (i) builds on loop
interchange and distribution but uses higher-order reasoning rather than array-dependence analysis, and (ii) still allows further locality-of-reference optimisations. Finally, an
evaluation on 16 benchmarks demonstrates the impact of the
language and compiler features and shows application-level
performance competitive with hand-written GPU code.
",18.062587368997235,17.520246913580248,163
PLDI_17_023.txt,14.82591557610272,13.07187113857017,PLDI,11032,"

Triangular Inequality (TD has been used in many manual algorithm designs to achieve good efficiency in solving some
distance calculation-based problems. This paper presents our
generalization of the idea into a compiler optimization technique, named 7/-based strength reduction. The generalization consists of three parts. The first is the establishment of
the theoretic foundation of this new optimization via the development of a new form of TI named Angular Triangular
Inequality, along with several fundamental theorems. The
second is the revealing of the properties of the new forms
of TI and the proposal of guided TI adaptation, a systematic
method to address the difficulties in effective deployments
of TI optimizations. The third is an integration of the new
optimization technique in an open-source compiler. Experiments on a set of data mining and machine learning algorithms show that the new technique can speed up the standard implementations by as much as 134X and 46X on average for distance-related problems, outperforming previous
Tl-based optimizations by 2.35X on average. It also extends
the applicability of TI-based optimizations to vector related
problems, producing tens of times of speedup.
",16.975882523387877,16.037506613756612,191
PLDI_17_024.txt,15.41843407847599,13.810026414865195,PLDI,8743,"
Parallelizing of software improves its effectiveness and productivity. To guarantee correctness, the parallel and serial
versions of the same code must be formally verified to be
equivalent. We present a novel approach, called GRASSP,
that automatically synthesizes parallel single-pass arrayprocessing programs by treating the given serial versions
as specifications. Given arbitrary segmentation of the input
array, GRASSP synthesizes a code to determine a new segmentation of the array that allows computing partial results
for each segment and merging them. In contrast to other
parallelizers, GRASSP gradually considers several parallelization scenarios and certifies the results using constrained
Horn solving. For several classes of programs, we show that
such parallelization can be performed efficiently. The C++
translations of the GRASSP solutions sped performance by
up to 5X relative to serial code on an 8-thread machine and
Hadoop translations by up to 10X on a 10-node Amazon
EMR cluster.
",16.613394197324528,15.406963103122049,150
PLDI_17_025.txt,14.599037534174364,12.472972909530274,PLDI,9021,"
Every database system contains a query optimizer that performs query rewrites. Unfortunately, developing query optimizers remains a highly challenging task. Part of the challenges comes from the intricacies and rich features of query
languages, which makes reasoning about rewrite rules difficult. In this paper, we propose a machine-checkable denotational semantics for SQL, the de facto language for interacting with relational databases, for rigorously validating
rewrite rules. Unlike previously proposed semantics that are
either non-mechanized or only cover a small amount of SQL
language features, our semantics covers all major features
of SQL, including bags, correlated subqueries, aggregation,
and indexes. Our mechanized semantics, called HoTT SQL,
is based on K-Relations and homotopy type theory, where
we denote relations as mathematical functions from tuples to
univalent types. We have implemented HoTT SQL in Coq,
which takes only fewer than 300 lines of code, and have
proved a wide range of SQL rewrite rules, including those
from database research literature (e.g., magic set rewrites)
and real-world query optimizers (e.g., subquery elimination), where several of them have never been previously
proven correct. In addition, while query equivalence is generally undecidable, we have implemented an automated decision procedure using HoTT SQL for conjunctive queries:
a well-studied decidable fragment of SQL that encompasses
many real-world queries.
",19.287186520377343,18.540000000000003,219
PLDI_17_026.txt,15.475578220417194,13.319805798864937,PLDI,8623,"

Existing techniques for injecting probes into running code
are limited: they either fail to support probing arbitrary
locations, or to support scalable, rapid toggling of probes.
We introduce a new technique on x86-64, called instruction
punning, which allows scalable probes at any instruction. The
key idea is that when we inject a jump instruction, the relative
address of the jump serves simultaneously as data and as an
instruction sequence. We show that this approach achieves
probe invocation overheads of only a few dozen cycles, as
well as low probe activation/deactivation costs, even when all
threads in the system are both invoking probes and toggling
them. Unlike competing systems, the latent overhead when
probes are deactivated is exactly zero.
",15.6451,14.026666666666667,121
PLDI_17_027.txt,14.323200657413345,11.988444397944917,PLDI,10293,"
Parametric polymorphism is one of the linchpins of modern
typed programming, but it comes with a real performance
penalty. We describe this penalty; offer a principled way to
reason about it (kinds as calling conventions); and propose
levity polymorphism. This new form of polymorphism allows
abstractions over calling conventions; we detail and verify
restrictions that are necessary in order to compile levitypolymorphic functions. Levity polymorphism has created
new opportunities in Haskell, including the ability to generalize nearly half of the type classes in GHC’s standard library.
",17.410965686947208,15.98337209302326,88
PLDI_17_028.txt,16.088951730372514,14.862260095274944,PLDI,9504,"
The ARMv8 architecture introduced AArch64, a 64-bit exe-
cution mode with a new instruction set, while retaining binary
compatibility with previous versions of the ARM architec-
ture through AArch32, a 32-bit execution mode. Most hard-
ware implementations of ARMv8 processors support both
AArch32 and AArch64, which comes at a cost in hardware
complexity.

We present MAMBO-X64, a dynamic binary translator
for Linux which executes 32-bit ARM binaries using only the
AArch64 instruction set. We have evaluated the performance
of MAMBO-X64 on three existing ARMv8 processors which
support both AArch32 and AArch64 instruction sets. The
performance was measured by comparing the running time
of 32-bit benchmarks running under MAMBO-X64 with the
same benchmark running natively. On SPEC CPU2006, we
achieve a geometric mean overhead of less than 7.5 % on
in-order Cortex-A53 processors and a performance improve-
ment of 1 % on out-of-order X-Gene 1 processors.

MAMBO-X64 achieves such low overhead by novel
optimizations to map AArch32 floating-point registers to
AArch64 registers dynamically, handle overflowing address
calculations efficiently, generate traces that harness hardware
return address prediction, and handle operating system signals accurately.
",17.28802050969988,15.828142116950406,195
PLDI_17_029.txt,14.634847522548526,12.818003899006644,PLDI,10674,"
We present the scalable, elastic operator scheduler in IBM
Streams 4.2. Streams is a distributed stream processing system used in production at many companies in a wide range
of industries. The programming language for Streams, SPL,
presents operators, tuples and streams as the primary abstractions. A fundamental SPL optimization is operator fusion, where multiple operators execute in the same process.
Streams 4.2 introduces automatic submission-time fusion
to simplify application development and deployment. However, potentially thousands of operators could then execute
in the same process, with no user guidance for thread placement. We needed a way to automatically figure out how
many threads to use, with arbitrarily sized applications on
a wide variety of hardware, and without any input from programmers. Our solution has two components. The first is
a scalable operator scheduler that minimizes synchronization, locks and global data, while allowing threads to execute any operator and dynamically come and go. The second is an elastic algorithm to dynamically adjust the number of threads to optimize performance, using the principles
of trusted measurements to establish trends. We demonstrate
our scheduler’s ability to scale to over a hundred threads, and
our elasticity algorithm’s ability to adapt to different workloads on an Intel Xeon system with 176 logical cores, and an
IBM Power8 system with 184 logical cores.
",17.015998829145012,14.887968160871392,222
PLDI_17_030.txt,16.81036355837962,15.591884487436598,PLDI,8221,"
Production compilers commonly perform dozens of transformations on an intermediate representation. Running those
transformations in separate passes harms performance. One
approach to recover performance is to combine transformations by hand in order to reduce number of passes. Such an
approach harms modularity, and thus makes it hard to maintain and evolve a compiler over the long term, and makes
reasoning about performance harder. This paper describes a
methodology that allows a compiler writer to define multiple
transformations separately, but fuse them into a single traversal of the intermediate representation when the compiler
runs. This approach has been implemented in the Dotty compiler for the Scala language. Our performance evaluation indicates that this approach reduces the running time of tree
transformations by 35% and shows that this is due to improved cache friendliness. At the same time, the approach
improves total memory consumption by reducing the object tenuring rate by 50%. This approach enables compiler
writers to write transformations that are both modular and
fast at the same time.
",15.322241378113624,13.39357142857143,169
PLDI_17_031.txt,14.42726029089528,12.844551798124474,PLDI,10762,"
We develop Propane/AT, a system to synthesize provably-
correct BGP (border gateway protocol) configurations for
large, evolving networks from high-level specifications of
topology, routing policy, and fault-tolerance requirements.
Propane/AT is based on new abstractions for capturing pa-
rameterized network topologies and their evolution, and al-
gorithms to analyze the impact of topology and routing pol-
icy on fault tolerance. Our algorithms operate entirely on ab-
stract topologies. We prove that the properties established
by our analyses hold for every concrete instantiation of the
given abstract topology. Propane/AT also guarantees that
only incremental changes to existing device configurations
are required when the network evolves to add or remove de-
vices and links. Our experiments with real-world topologies
and policies show that our abstractions and algorithms are
effective, and that, for large networks, Propane/AT synthe-
sizes configurations two orders of magnitude faster than sys-
tems that operate on concrete topologies.
",18.599290044081553,17.780483870967746,156
PLDI_17_032.txt,15.263580867984583,13.703198225417292,PLDI,9103,"
Most high-performance dynamic language virtual machines
duplicate language semantics in the interpreter, compiler,
and runtime system. This violates the principle to not repeat
yourself. In contrast, we define languages solely by writing
an interpreter. The interpreter performs specializations, e.g.,
augments the interpreted program with type information and
profiling information. Compiled code is derived automatically using partial evaluation while incorporating these specializations. This makes partial evaluation practical in the
context of dynamic languages: It reduces the size of the
compiled code while still compiling all parts of an operation
that are relevant for a particular program. When a speculation fails, execution transfers back to the interpreter, the program re-specializes in the interpreter, and later partial evaluation again transforms the new state of the interpreter to
compiled code. We evaluate our approach by comparing our
implementations of JavaScript, Ruby, and R with best-inclass specialized production implementations. Our generalpurpose compilation system is competitive with production
systems even when they have been heavily optimized for the
one language they support. For our set of benchmarks, our
speedup relative to the V8 JavaScript VM is 0.83x, relative
to JRuby is 3.8x, and relative to GNU R is 5x.
",16.647925096878797,15.533591836734697,200
PLDI_17_033.txt,16.1464282600799,14.416063173089707,PLDI,11341,"
Application-level energy management is an important di-
mension of energy optimization. In this paper, we introduce
ENT, a novel programming language for enabling proactive
and adaptive mode-based energy management at the ap-
plication level. The proactive design allows programmers
to apply their application knowledge to energy manage-
ment, by characterizing the energy behavior of different
program fragments with modes. The adaptive design allows
such characterization to be delayed until run time, useful
for capturing dynamic program behavior dependent on pro-
gram states, configuration settings, external battery levels,
or CPU temperatures. The key insight is both proactiveness
and adaptiveness can be unified under a type system com-
bined with static typing and dynamic typing. ENT has been
implemented as an extension to Java, and successfully ported
to three energy-conscious platforms: an Intel-based laptop, a
Raspberry Pi, and an Android phone. Evaluation shows ENT
improves the programmability, debuggability, and energy ef-
ficiency of battery-aware and temperature-aware programs.
",18.243605946275583,17.126583850931677,162
PLDI_17_034.txt,14.690220016528908,12.530209403770737,PLDI,11301,"

The C/C++11 memory model defines the semantics of concurrent memory accesses in C/C++, and in particular supports
racy “atomic” accesses at a range of different consistency
levels, from very weak consistency (“relaxed’’) to strong, sequential consistency (“SC”). Unfortunately, as we observe in
this paper, the semantics of SC atomic accesses in C/C++11,
as well as in all proposed strengthenings of the semantics, is
flawed, in that (contrary to previously published results) both
suggested compilation schemes to the Power architecture are
unsound. We propose a model, called RC11 (for Repaired
C11), with a better semantics for SC accesses that restores the
soundness of the compilation schemes to Power, maintains
the DRF-SC guarantee, and provides stronger, more useful,
guarantees to SC fences. In addition, we formally prove, for
the first time, the correctness of the proposed stronger compilation schemes to Power that preserve load-to-store ordering
and avoid “out-of-thin-air” reads.
",19.78447435784618,20.114634146341462,159
PLDI_17_035.txt,13.939560859741363,11.69602934810749,PLDI,10731,"
Competitive and cooperative threading are widely used abstractions in computing. In competitive threading, threads
are scheduled preemptively with the goal of minimizing response time, usually of interactive applications. In cooperative threading, threads are scheduled non-preemptively with
the goal of maximizing throughput or minimizing the completion time, usually in compute-intensive applications, e.g.
scientific computing, machine learning and AI.
Although both of these forms of threading rely on the
same abstraction of a thread, they have, to date, remained
largely separate forms of computing. Motivated by the recent
increase in the mainstream use of multicore computers, we
propose a threading model that aims to unify competitive
and cooperative threading. To this end, we extend the classic
graph-based cost model for cooperative threading to allow for
competitive threading, and describe how such a cost model
may be used in a programming language by presenting a
language and a corresponding cost semantics. Finally, we
show that the cost model and the semantics are realizable
by presenting an operational semantics for the language that
specifies the behavior of an implementation, as well as an
implementation and a small empirical evaluation.
",17.93193317476759,17.103434650455927,190
PLDI_17_036.txt,15.909558537311568,14.548041887544198,PLDI,10082,"
CPU caches introduce variations into the execution time of
programs that can be exploited by adversaries to recover
private information about users or cryptographic keys.

Establishing the security of countermeasures against this
threat often requires intricate reasoning about the interac-
tions of program code, memory layout, and hardware archi-
tecture and has so far only been done for restricted cases.

In this paper we devise novel techniques that provide sup-
port for bit-level and arithmetic reasoning about memory ac-
cesses in the presence of dynamic memory allocation. These
techniques enable us to perform the first rigorous analysis of
widely deployed software countermeasures against cache at-
tacks on modular exponentiation, based on executable code.
",19.287186520377343,18.81447368421053,115
PLDI_17_037.txt,17.250399153025217,15.861857142857144,PLDI,10277,"

We present a scalable approach for establishing similarity
between stripped binaries (with no debug information). The
main challenge in binary similarity, is to establish similarity
even when the code has been compiled using different compilers, with different optimization levels, or targeting different architectures. Overcoming this challenge, while avoiding
false positives, is invaluable to the process of reverse engineering and the process of locating vulnerable code.

We present a technique that is scalable and precise, as
it alleviates the need for heavyweight semantic comparison
by performing out-of-context re-optimization of procedure
fragments. It works by decomposing binary procedures to
comparable fragments and transforming them to a canonical, normalized form using the compiler optimizer, which
enables finding equivalent fragments through simple syntactic comparison. We use a statistical framework built by analyzing samples collected “in the wild” to generate a global
context that quantifies the significance of each pair of fragments, and uses it to lift pairwise fragment equivalence to
whole procedure similarity.

We have implemented our technique in a tool called GitZ
and performed an extensive evaluation. We show that GitZ is
able to perform millions of comparisons efficiently, and find
similarity with high accuracy.
",18.903936251131103,17.576762820512823,196
PLDI_17_038.txt,15.032095135338079,13.101810642343278,PLDI,10832,"

Safe programming languages are readily available, but many
applications continue to be written in unsafe languages because of efficiency. As a consequence, many applications
continue to have exploitable memory safety bugs. Since
garbage collection is a major source of inefficiency in the
implementation of safe languages, replacing it with safe
manual memory management would be an important step
towards solving this problem.

Previous approaches to safe manual memory management use programming models based on regions, unique
pointers, borrowing of references, and ownership types. We
propose a much simpler programming model that does not
require any of these concepts. Starting from the design of an
imperative type safe language (like Java or C#), we just add
a delete operator to free memory explicitly and an exception which is thrown if the program dereferences a pointer
to freed memory. We propose an efficient implementation
of this programming model that guarantees type safety. Experimental results from our implementation based on the C#
native compiler show that this design achieves up to 3x reduction in peak working set and run time.
",17.267425705330172,15.105416666666667,178
PLDI_17_039.txt,13.824875077858035,11.586445763799066,PLDI,10580,"

A program can be viewed as a syntactic structure P (syntactic skeleton) parameterized by a collection of identifiers
V (variable names). This paper introduces the skeletal program enumeration (SPE) problem: Given a syntactic skeleton
P and a set of variables V, enumerate a set of programs P
exhibiting all possible variable usage patterns within P. It proposes an effective realization of SPE for systematic, rigorous
compiler testing by leveraging three important observations:
(1) Programs with different variable usage patterns exhibit diverse control- and data-dependence, and help exploit different
compiler optimizations; (2) most real compiler bugs were revealed by small tests (7.¢., small-sized P) — this “small-scope”
observation opens up SPE for practical compiler validation;
and (3) SPE is exhaustive wrt. a given syntactic skeleton
and variable set, offering a level of guarantee absent from all
existing compiler testing techniques.

The key challenge of SPE is how to eliminate the enormous amount of equivalent programs w.rt. a-conversion. Our
main technical contribution is a novel algorithm for computing the canonical (and smallest) set of all non-a-equivalent
programs. To demonstrate its practical utility, we have applied
the SPE technique to test C/C++ compilers using syntactic
skeletons derived from their own regression test-suites. Our
evaluation results are extremely encouraging. In less than six
months, our approach has led to 217 confirmed GCC/Clang
bug reports, 119 of which have already been fixed, and the
majority are long latent despite extensive prior testing efforts.
Our SPE algorithm also provides six orders of magnitude
reduction. Moreover, in three weeks, our technique has found
29 CompCert crashing bugs and 42 bugs in two Scala optimizing compilers. These results demonstrate our SPE technique’s
generality and further illustrate its effectiveness.
",17.879347455551382,16.04825601374571,291
PLDI_17_040.txt,13.661748728158042,11.284904122722917,PLDI,9523,"
Asynchronous programming is a standard approach for designing responsive applications. Modern languages such as
C! provide async/await primitives for the disciplined use
of asynchrony. In spite of this, programs can deadlock because of incorrect use of blocking operations along with
non-blocking (asynchronous) operations. While developers
are aware of this problem, there is no automated technique
to detect deadlocks in asynchronous programs.

We present a novel representation of control flow and
scheduling of asynchronous programs, called continuation
scheduling graph and formulate necessary conditions for a
deadlock to occur in a program. We design static analyses
to construct continuation scheduling graphs of asynchronous
C! programs and to identify deadlocks in them.

We have implemented the static analyses in a tool called
DeadWait. Using DeadWait, we found 43 previously unknown deadlocks in 11 asynchronous C! libraries. We reported the deadlocks to the library developers. They have
confirmed and fixed 40 of them.
",12.766815935922704,11.524105960264905,152
PLDI_17_041.txt,14.852057507078406,13.078794427737972,PLDI,12087,"
Real-time decision making in emerging IoT applications typically relies on computing quantitative summaries of large
data streams in an efficient and incremental manner. To simplify the task of programming the desired logic, we propose
StreamQRE, which provides natural and high-level constructs
for processing streaming data. Our language has a novel integration of linguistic constructs from two distinct programming paradigms: streaming extensions of relational query
languages and quantitative extensions of regular expressions.
The former allows the programmer to employ relational constructs to partition the input data by keys and to integrate data
streams from different sources, while the latter can be used to
exploit the logical hierarchy in the input stream for modular
specifications.
We first present the core language with a small set of
combinators, formal semantics, and a decidable type system.
We then show how to express a number of common patterns
with illustrative examples. Our compilation algorithm translates the high-level query into a streaming algorithm with
precise complexity bounds on per-item processing time and
total memory footprint. We also show how to integrate approximation algorithms into our framework. We report on
an implementation in Java, and evaluate it with respect to
existing high-performance engines for processing streaming
data. Our experimental evaluation shows that (1) StreamQRE
allows more natural and succinct specification of queries
compared to existing frameworks, (2) the throughput of our
implementation is higher than comparable systems (for ex-
",17.122413403193683,16.4065864978903,238
PLDI_17_042.txt,15.024997010612672,13.007758140339519,PLDI,10115,"
Divide-and-conquer is a common parallel programming
skeleton supported by many cross-platform multithreaded
libraries, and most commonly used by programmers for parallelization. The challenges of producing (manually or automatically) a correct divide-and-conquer parallel program
from a given sequential code are two-fold: (1) assuming that
a good solution exists where individual worker threads execute a code identical to the sequential one, the programmer
has to provide the extra code for dividing the tasks and combining the partial results (i.e. joins), and (2) the sequential
code may not be suitable for divide-and-conquer parallelization as is, and may need to be modified to become a part of
a good solution. We address both challenges in this paper.
We present an automated synthesis technique to synthesize
correct joins and an algorithm for modifying the sequential
code to make it suitable for parallelization when modifications are necessary. We focus on a class of loops that traverse a read-only collection and compute a scalar function
over that collection. We present theoretical results for when
the necessary modifications to sequential code are possible, theoretical guarantees for the algorithmic solutions presented here, and experimental evaluation of the approach’s
success in practice and the quality of the produced parallel
programs.
",20.581828153500815,20.296435406698567,212
PLDI_17_043.txt,16.52667757954773,16.16231321335359,PLDI,9329,"
SQL is the de facto language for manipulating relational
data. Though powerful, SQL queries can be difficult to
write due to their highly expressive constructs. Using the
programming-by-example paradigm to help users write SQL
queries presents an attractive proposition, as evidenced by online help forums such as Stack Overflow. However, developing techniques to synthesize SQL queries from input-output
(I/O) examples has been difficult due to SQL’s rich set of
operators.
In this paper, we present a new scalable and efficient algorithm to synthesize SQL queries from I/O examples. Our
key innovation is the development of a language for abstract
queries, i.e., queries with uninstantiated operators, that can express a large space of SQL queries efficiently. Using abstract
queries to represent the search space nicely decomposes the
synthesis problem into two tasks: (1) searching for abstract
queries that can potentially satisfy the given I/O examples,
and (2) instantiating the found abstract queries and ranking
the results. We implemented the algorithm in a new tool,
called S CYTHE, and evaluated it on 193 benchmarks collected from Stack Overflow. Our results showed that S CYTHE
efficiently solved 74% of the benchmarks, most in just a few
seconds. Queries synthesized by S CYTHE range from simple
ones involving a single selection to complex ones with six
levels of nested queries.
",15.903189008614273,14.329261261261262,225
PLDI_17_044.txt,15.809644991740445,14.315594583281044,PLDI,10590,"
A memory consistency model specifies which writes to shared
memory a given read may see. Ambiguities or errors in these
specifications can lead to bugs in both compilers and applications. Yet architectures usually define their memory models
with prose and litmus tests—small concurrent programs that
demonstrate allowed and forbidden outcomes. Recent work
has formalized the memory models of common architectures
through substantial manual effort, but as new architectures
emerge, there is a growing need for tools to aid these efforts.
This paper presents MemSynth, a synthesis-aided system for reasoning about axiomatic specifications of memory
models. MemSynth takes as input a set of litmus tests and
a framework sketch that defines a class of memory models.
The sketch comprises a set of axioms with missing expressions (or holes). Given these inputs, MemSynth synthesizes
a completion of the axioms—i.e., a memory model—that
gives the desired outcome on all tests. The MemSynth engine
employs a novel embedding of bounded relational logic in
a solver-aided programming language, which enables it to
tackle complex synthesis queries intractable to existing relational solvers. This design also enables it to solve new kinds
of queries, such as checking if a set of litmus tests unambiguously defines a memory model within a framework sketch.
We show that MemSynth can synthesize specifications for
x86 in under two seconds, and for PowerPC in 12 seconds
from 768 litmus tests. Our ambiguity check identifies missing
tests from both the Intel x86 documentation and the validation
suite of a previous PowerPC formalization. We also used
MemSynth to reproduce, debug, and automatically repair a
paper on comparing memory models in just two days.
",15.299343439825492,14.119781021897811,276
PLDI_17_045.txt,14.25122946022558,12.049330700099318,PLDI,10291,"

We present an algorithm for synthesizing a context-free
grammar encoding the language of valid program inputs
from a set of input examples and blackbox access to the
program. Our algorithm addresses shortcomings of existing
grammar inference algorithms, which both severely overgeneralize and are prohibitively slow. Our implementation,
GLADE, leverages the grammar synthesized by our algorithm to fuzz test programs with structured inputs. We show
that GLADE substantially increases the incremental coverage on valid inputs compared to two baseline fuzzers.
",16.218646115125612,16.105000000000004,81
PLDI_17_046.txt,16.06611590312124,14.76577155684365,PLDI,7696,"
Web applications, such as collaborative editors that allow
multiple clients to concurrently interact on a shared resource,
are difficult to implement correctly. Existing techniques for
analyzing concurrent software do not scale to such complex
systems or do not consider multiple interacting clients. This
paper presents Simian, the first fully automated technique
for systematically analyzing multi-client web applications.
Naively exploring all possible interactions between a
set of clients of such applications is practically infeasible.
Simian scales to real-world applications by using a twophase black-box approach. The first phase systematically
explores the application with a single client to infer potential conflicts between client events. The second phase
synthesizes multi-client interactions targeted at triggering
misbehavior that may result from the potential conflicts, and
reports an inconsistency if the clients do not converge to a
consistent state.
We evaluate the analysis on three widely used systems,
Google Docs, Firepad, and ownCloud Documents, where it
reports a variety of inconsistencies, such as incorrect formatting and misplaced text fragments. Moreover, we find that
the two-phase approach runs 10x faster than exhaustive exploration, making systematic analysis feasible.
",17.251386760058843,16.09045045045045,186
PLDI_17_047.txt,15.241251254009338,13.354620292514976,PLDI,9844,"
A central concern for an optimizing compiler is the design of
its intermediate representation (IR) for code. The IR should
make it easy to perform transformations, and should also
afford efficient and precise static analysis.
In this paper we study an aspect of IR design that has received little attention: the role of undefined behavior. The IR
for every optimizing compiler we have looked at, including
GCC, LLVM, Intel’s, and Microsoft’s, supports one or more
forms of undefined behavior (UB), not only to reflect the
semantics of UB-heavy programming languages such as C
and C++, but also to model inherently unsafe low-level operations such as memory stores and to avoid over-constraining
IR semantics to the point that desirable transformations become illegal. The current semantics of LLVM’s IR fails to
justify some cases of loop unswitching, global value numbering, and other important “textbook” optimizations, causing
long-standing bugs.
We present solutions to the problems we have identified
in LLVM’s IR and show that most optimizations currently in
LLVM remain sound, and that some desirable new transformations become permissible. Our solutions do not degrade
compile time or performance of generated code.
",17.451712890111917,16.203107511045655,197
PODC_17_001.txt,13.916322803839371,12.465115155917363,PODC,8135,"

We study two fundamental communication primitives: broadcasting and leader election in the classical model of multi-hop radio
networks with unknown topology and without collision detection mechanisms. It has been known for almost 20 years that in
undirected networks with n nodes and diameter D, randomized
broadcasting requires Q(D log # + log? n) rounds in expectation,
assuming that uninformed nodes are not allowed to communicate (until they are informed). Only very recently, Haeupler and
Wajc (PODC’2016) showed that this bound can be slightly improved for the model with spontaneous transmissions, providing an

o(D Ser weret + log) n)-time broadcasting algorithm. In this

paper, we give a new and faster algorithm that completes broadcasting in ows + log) n) time, with high probability. This yields
the first optimal O(D)-time broadcasting algorithm whenever D is
polynomial in n.

Furthermore, our approach can be applied to design a new leader
election algorithm that matches the performance of our broadcasting algorithm. Previously, all fast randomized leader election algorithms have been using broadcasting as their subroutine and their
complexity have been asymptotically strictly bigger than the complexity of broadcasting. In particular, the fastest previously known
randomized leader election algorithm of Ghaffari and Haeupler
(SODA’2013) requires O(D log #§ min {log log n, log $}+ log n)time with high probability. Our new algorithm requires O(D 2 5 +
log? n) time with high probability, and it achieves the optimal
O(D) time whenever D is polynomial in n.
",16.908762484321528,14.696926406926412,247
PODC_17_002.txt,12.097827082522315,9.20305494303565,PODC,10081,"

We consider packet forwarding in acyclic networks with bounded
adversarial packet injections. We focus on the model of adversarial
queuing theory, where each packet is injected into the network
with a prescribed path to its destination, and both the long-range
average rate and the short-range burst size are bounded. Each edge
has an associated buffer that stores packets while they wait to cross
the edge. Our goal is to minimize the buffer space required to avoid
overflows.

Previous results for local forwarding protocols required buffers
of size Q(n). In the case of single destination trees, it is known that
for centralized protocols, buffers of size O(1) are sufficient. We show
that for local protocols, buffers of size @(log n) are necessary and
sufficient for single destination trees. The upper bound is achieved
by a novel protocol which we call Odd-Even Downhill forwarding
(OED). We also show that even slightly more general networks—
such as path graphs with multiple destinations, or DAGs with a
single destination—require buffers of size Q(n) to avoid overflows,
even if forwarding is done by centralized, offline protocols.

",14.554592549557764,12.139645390070925,189
PODC_17_003.txt,12.0056644718461,9.47974281393503,PODC,11686,"

Cognitive radio networks are a new type of multi-channel wireless
network in which different nodes can have access to different sets
of channels. By providing multiple channels, they improve the
efficiency and reliability of wireless communication. However, the
heterogeneous nature of cognitive radio networks also brings new
challenges to the design and analysis of distributed algorithms.

In this paper, we focus on two fundamental problems in cognitive radio networks: neighbor discovery, and global broadcast.
We consider a synchronous network containing n nodes, each of
which has access to c channels. We assume the network has diameter D, and each pair of neighbors have at least k > 1, and at
most Kkmax < c, shared channels. We also assume each node has at
most A neighbors. For the neighbor discovery problem, we design
a randomized algorithm called CSEEK which has time complexity
O((c?/k) + (kKmax /k)- A). CSEEK is flexible and robust, which allows
us to use it as a generic “filter” to find “well-connected” neighbors
with an even shorter running time. We then move on to the global
broadcast problem, and propose CGCastT, a randomized algorithm
which takes O((c?/k) + (kmax/k)- A + D- A) time. CGCast uses
CSEEK to achieve communication among neighbors, and uses edge
coloring to establish an efficient deterministic schedule for fast
message dissemination.

Towards the end of the paper, we give lower bounds for solving
the two problems. These lower bounds demonstrate that in many
cases CSEEK and CGCasT are near optimal.
",12.68835289967788,10.614117647058826,251
PODC_17_004.txt,12.07259150569843,10.342677761044609,PODC,8215,"

The widely-studied radio network model [Chlamtac and Kutten, 1985] is a graph-based description that captures the
inherent impact of collisions in wireless communication. In
this model, the strong assumption is made that node v receives a, message from a neighbor if and only if exactly one
of its neighbors broadcasts.

We relax this assumption by introducing a new noisy radio
network model in which random faults occur at senders or receivers. Specifically, for a constant noise parameter p € [0, 1),
either every sender has probability p of transmitting noise
or every receiver of a single transmission in its neighborhood
has probability p of receiving noise.

We first study single-message broadcast algorithms in noisy
radio networks and show that the Decay algorithm [BarYehuda et al., 1992] remains robust in the noisy model while
the diameter-linear algorithm of Gasieniec et al., 2007 does
not. We give a modified version of the algorithm of Gasieniec
et al., 2007 that is robust to sender and receiver faults, and
extend both this modified algorithm and the Decay algorithm
to robust multi-message broadcast algorithms, broadcasting

© (cogmigtogn) and 2 (sah)
tively.

We next investigate the extent to which (network) coding
improves throughput in noisy radio networks. In particular,
we study the coding cap — the ratio of the throughput of
coding to that of routing — in noisy radio networks. We
address the previously perplexing result of Alon et al. 2014
that worst case coding throughput is no better than worst
case routing throughput up to constants: we show that the
worst case throughput performance of coding is, in fact,
superior to that of routing — by a O(log(n)) gap — provided
receiver faults are introduced. However, we show that sender
faults have little effect on throughput. In particular, we show
that any coding or routing scheme for the noiseless setting
can be transformed to be robust to sender faults with only a
constant throughput overhead. These transformations imply
that the results of Alon et al., 2014 carry over to noisy radio
networks with sender faults as well. As a result, if sender
faults are introduced then there exist topologies for which
there is a O(loglogn) gap, but the worst case throughput

across all topologies is for both coding and routing.
",15.604920121030428,15.466315789473686,379
PODC_17_005.txt,14.900395291015464,13.46182724578036,PODC,6917,"
 KEYWORDS

We study a distributed learning process observed in human groups
and other social animals. This learning process appears in settings
in which each individual in a group is trying to decide over time, in
a distributed manner, which option to select among a shared set of
options. Specifically, we consider a stochastic dynamics in a group
in which every individual selects an option in the following twostep process: (1) select a random individual and observe the option
that individual chose in the previous time step, and (2) adopt that
option if its stochastic quality was good at that time step. Various
instantiations of such distributed learning appear in nature, and
have also been studied in the social science literature. From the
perspective of an individual, an attractive feature of this learning
process is that it is a simple heuristic that requires extremely limited
computational capacities. But what does it mean for the group —
could such a simple, distributed and essentially memoryless process
lead the group as a whole to perform optimally? We show that the
answer to this question is yes — this distributed learning is highly
effective at identifying the best option and is close to optimal for
the group overall. Our analysis also gives quantitative bounds that
show fast convergence of these stochastic dynamics. We prove our
result by first defining a (stochastic) infinite population version
of these distributed learning dynamics and then combining its
strong convergence properties along with its relation to the finite
population dynamics. Prior to our work the only theoretical work
related to such learning dynamics has been either in deterministic
special cases or in the asymptotic setting. Finally, we observe that
our infinite population dynamics is a stochastic variant of the classic
multiplicative weights update (MWU) method. Consequently, we
arrive at the following interesting converse: the learning dynamics
on a finite population considered here can be viewed as a novel
distributed and low-memory implementation of the classic MWU
method,
",17.5058628484301,16.76104740061162,328
PODC_17_006.txt,13.786311094055005,11.26518902484479,PODC,10639,"

Randomized backoff protocols have long been used to reduce contention on shared resources. They are heavily used in communication channels and radio networks, and have also been shown to
greatly improve the performance of shared memory algorithms in
real systems. However, while backoff protocols are well understood
in many settings, their effect in shared memory has never been
theoretically analyzed. This discrepency may be due to the difficulty
of modeling asynchrony without eliminating the advantage gained
by local delays.

In this paper, we introduce a new cost model for contention in
shared memory. Our model allows for adversarial asynchrony, but
also provides a clear notion of time, thus enabling easy calculation
of contention costs and delays. We then consider a simple use case
in which n processes try to update a single memory location. Using
our model, we first show that a naive protocol, without any backoff,
requires Q(n*) work until all processes successfully update that
location. We then analyze the commonly used exponential delay
protocol, and show that it requires @(n? logn) work with high
probability. Finally, we show that the exponential delay protocol
is suboptimal, by introducing a new backoff protocol based on
adaptive probabilities and showing that, for the same use case, it
requires only O(n?) work with high probability.
",15.359359093739592,13.07656976744186,216
PODC_17_007.txt,13.856590876301826,12.483456760142587,PODC,9707,"

Motivated by emerging applications to the edge computing paradigm, we introduce a two-layer erasure-coded fault-tolerant distributed storage system offering atomic access for read and write
operations. In edge computing, clients interact with an edge-layer
of servers that is geographically near; the edge-layer in turn interacts with a back-end layer of servers. The edge-layer provides
low latency access and temporary storage for client operations,
and uses the back-end layer for persistent storage. Our algorithm,
termed Layered Data Storage (LDS) algorithm, offers several features suitable for edge-computing systems, works under asynchronous message-passing environments, supports multiple readers
and writers, and can tolerate fj < m,/2 and f2 < n2/3 crash
failures in the two layers having n; and ng servers, respectively.
We use a class of erasure codes known as regenerating codes for
storage of data in the back-end layer. The choice of regenerating codes, instead of popular choices like Reed-Solomon codes,
not only optimizes the cost of back-end storage, but also helps
in optimizing communication cost of read operations, when the
value needs to be recreated all the way from the back-end. The
two-layer architecture permits a modular implementation of atomicity and erasure-code protocols; the implementation of erasurecodes is mostly limited to interaction between the two layers. We
prove liveness and atomicity of LDS, and also compute performance
costs associated with read and write operations. In a system with
ny = O(n2), fi = O(n1), fo = O(nz2), the write and read costs are
respectively given by O(n,) and @(1) + m1£(6 > 0). Here 6 is a
parameter closely related to the number of write operations that
are concurrent with the read operation, and J(6 > 0) is 1 if 6 > 0,
and 0 if 5 = 0. The cost of persistent storage in the back-end layer is
©(1). The impact of temporary storage is minimally felt in a multiobject system running N independent instances of LDS, where only
a small fraction of the objects undergo concurrent accesses at any
point during the execution. For the multi-object system, we identify
a condition on the rate of concurrent writes in the system such that
the overall storage cost is dominated by that of persistent storage
in the back-end layer, and is given by O(N).
",17.03242331605538,16.5863523573201,392
PODC_17_008.txt,16.853613784370044,15.512227060915055,PODC,8277,"

This paper introduces the first state-based formalization of
isolation guarantees. Our approach is premised on a simple
observation: applications view storage systems as black-boxes
that transition through a series of states, a subset of which are
observed by applications. Defining isolation guarantees in terms
of these states frees definitions from implementation-specific
assumptions. It makes immediately clear what anomalies, if any,
applications can expect to observe, thus bridging the gap that
exists today between how isolation guarantees are defined and
how they are perceived. The clarity that results from definitions
based on client-observable states brings forth several benefits.
First, it allows us to easily compare the guarantees of distinct,
but semantically close, isolation guarantees. We find that several
well-known guarantees, previously thought to be distinct, are in
fact equivalent, and that many previously incomparable flavors of
snapshot isolation can be organized in a clean hierarchy. Second,
freeing definitions from implementation-specific artefacts can
suggest more efficient implementations of the same isolation
guarantee. We show how a client-centric implementation of
parallel snapshot isolation can be more resilient to slowdown
cascades, a common phenomenon in large-scale datacenters.
",16.99224021665606,15.79242582897033,192
PODC_17_009.txt,14.583702131894412,12.720840681951795,PODC,6407,"

Driven by the rising popularity of cloud storage, the costs
associated with implementing reliable storage services from
a collection of fault-prone servers have recently become an
actively studied question. The well-known ABD result shows
that an f-tolerant register can be emulated using a collection
of 2f +1 fault-prone servers each storing a single read-modifywrite object, which is known to be optimal. In this paper
we generalize this bound: we investigate the inherent space
complexity of emulating reliable multi-writer registers as
a function of the type of the base objects exposed by the
underlying servers, the number of writers to the emulated
register, the number of available servers, and the failure
threshold.

We establish a sharp separation between registers, and both
max-registers (the base object type assumed by ABD) and
CAS in terms of the resources (i.e., the number of base objects
of the respective types) required to support the emulation;
we show that no such separation exists between max-registers
and CAS. Our main technical contribution is lower and upper
bounds on the resources required in case the underlying base
objects are fault-prone read/write registers. We show that the
number of required registers is directly proportional to the
number of writers and inversely proportional to the number
of servers.
",19.454632303725965,19.443173515981737,220
PODC_17_010.txt,13.503114625170664,11.228218886935064,PODC,9381,"
LCLs or locally checkable labelling problems (e.g. maximal independent set, maximal matching, and vertex colouring) in the LOCAL
model of computation are very well-understood in cycles (toroidal
1-dimensional grids): every problem has a complexity of O (1),
Θ(log∗ n), or Θ(n), and the design of optimal algorithms can be
fully automated.
This work develops the complexity theory of LCL problems for
toroidal 2-dimensional grids. The complexity classes are the same
as in the 1-dimensional case: O (1), Θ(log∗ n), and Θ(n). However,
given an LCL problem it is undecidable whether its complexity is
Θ(log∗ n) or Θ(n) in 2-dimensional grids.
Nevertheless, if we correctly guess that the complexity of a problem is Θ(log∗ n), we can completely automate the design of optimal
algorithms. For any problem we can find an algorithm that is of a
normal form A′ ◦Sk , where A′ is a finite function, Sk is an algorithm
for finding a maximal independent set in kth power of the grid, and
k is a constant.
Finally, partially with the help of automated design tools, we
classify the complexity of several concrete LCL problems related to
colourings and orientations.
",17.77360955136429,15.992968099861304,196
PODC_17_011.txt,14.132463457807589,13.120970036499454,PODC,11315,"
 tester might ask whether a given edge is in the graph, or what

In this paper we initiate the study of property testing in multi-party
communication complexity, focusing on testing triangle-freeness
in graphs. We consider the coordinator model, where we have k
players receiving private inputs, and a coordinator who receives
no input; the coordinator can communicate with all the players,
but the players cannot communicate with each other. In this model,
we ask: if an input graph is divided between the players, with each
player receiving some of the edges, how many bits do the players
and the coordinator need to exchange to determine if the graph is
triangle-free, or far from triangle-free? We are especially interested
in simultaneous communication protocols, where there is only one
communication round.

For general communication protocols, we show that O(k(nd)!/ 44
k’) bits are sufficient to test triangle-freeness in graphs of size n
with average degree d. We also give a simultaneous protocol using
O(k-n) bits when d = O(-yn) and O(k(nd)!/3) when d = Q(n). We
show that for average degree d = O(1), our simultaneous protocol
is asymptotically optimal up to logarithmic factors. For higher
degrees, we are not able to give lower bounds on testing trianglefreeness, but we give evidence that the problem is hard by showing
that finding an edge that participates in a triangle is hard, even
when promised that the graph is far from triangle-free.

",15.112257680678326,13.92072440944882,252
PODC_17_012.txt,14.919050287332755,13.548612836438924,PODC,8853,"
The local computation of Linial [FOCS’87] and Naor and Stockmeyer [STOC’93] concerns with the question of whether a locally
definable distributed computing problem can be solved locally: more
specifically, for a given local CSP (Constraint Satisfaction Problem)
whether a CSP solution can be constructed by a distributed algorithm using local information. In this paper, we consider the
problem of sampling a uniform CSP solution by distributed algorithms, and ask whether a locally definable joint distribution can
be sampled from locally. More broadly, we consider sampling from
Gibbs distributions induced by weighted local CSPs, especially the
Markov random fields (MRFs), in the LOCAL model.

We give two Markov chain based distributed algorithms which
we believe to represent two fundamental approaches for sampling
from Gibbs distributions via distributed algorithms. The first algorithm generically parallelizes the single-site sequential Markov
chain by updating in each step the variables from a random independent set in parallel, and achieves an O(A log n) time upper
bound in the LOCAL model, where A is the maximum degree, when
the Dobrushin’s condition for the Gibbs distribution is satisfied.
The second algorithm is a novel parallel Markov chain which proposes to update all variables simultaneously yet still guarantees to
converge correctly with no bias. It surprisingly parallelizes an intrinsically sequential process: stabilizing to a joint distribution with
massive local dependencies, and may achieve an optimal O(log n)
time upper bound independent of the maximum degree A under a
stronger mixing condition.

We also show a strong Q(diam) lower bound for sampling: in
particular for sampling independent set in graphs with maximum
degree A > 6. Independent sets are trivial to construct locally and
the sampling lower bound holds even when every node is aware of
the entire graph. This gives a strong separation between sampling
and constructing locally checkable labelings.
",18.9813275721742,18.368667752443,308
PODC_17_013.txt,12.556696611668343,10.369518950113228,PODC,10693,"
 Background: The MST problem has played a central role in dis
We present a randomized distributed algorithm that computes a
minimum spanning tree in Tmix(G) - 20 Viog 7 log log "")) rounds, in
any n-node graph G with mixing time tpix(G). This result provides
a sub-polynomial complexity for a wide range of graphs of practical
interest, and goes below the celebrated O(D + Vn) lower bound
of Das Sarma et al. [STOC’11] which holds for some worst-case
general graphs.

The core novelty in this result is a distributed method for permutation routing. In this problem, one is given a number of sourcedestination pairs, and we should deliver one packet from each
source to its destination, all in parallel, in the shortest span of time
possible. Our algorithm allows us to route and deliver all these
packets in tmix(G) - 206 Viog mloglog 2) rounds, assuming that each
node v is the source or destination for at most dg(v) packets. The
main technical ingredient in this routing result is a certain hierarchical embedding of good-expansion random graphs on the base
graph, which we believe can be of interest well beyond this work.
",14.554592549557764,13.463977027997128,199
PODC_17_014.txt,11.58411153163022,9.036260140244742,PODC,9901,"
Computing a Maximal Independent Set (MIS) is a central problem
in distributed graph algorithms. This paper presents an improved
randomized distributed algorithm for computing an MIS in an allto-all communication distributed model, known as the congested
clique model, defined as follows: Given a graph G = (V, £), initially
each node knows only its neighbors. Communication happens in
synchronous rounds over a complete graph, and per round each
node can send O(log n) bits to each other node.

We present a randomized algorithm that computes an MIS in
O(log A/,/logn +1) < O(,flog A) rounds of congested clique, with
high probability. Here A denotes the maximum degree in the graph.
This improves quadratically on the O(log A) algorithm of [Ghaffari,
SODA’16]. The core technical novelty in this result is a certain
local sparsification technique for MIS, which we believe to be of
independent interest.
",14.554592549557764,12.752255639097744,149
PODC_17_015.txt,12.551186320827206,9.97571768839526,PODC,6228,"

Distributed minimum spanning tree (MST) problem is one of the
most central and fundamental problems in distributed graph algorithms. Kutten and Peleg [KP98] devised an algorithm with running
time O(D + -Yn- log* n), where D is the hop-diameter of the input
n-vertex m-edge graph, and with message complexity O(m + n3/2),
Peleg and Rubinovich [PR99] showed that the running time of the
algorithm of [KP98] is essentially tight, and asked if one can achieve
near-optimal running time together with near-optimal message complexity.

In a recent breakthrough, Pandurangan et al. [PRS16] answered
this question in the affirmative, and devised a randomized algorithm
with time O(D + -Yn) and message complexity O(m). They asked if
such a simultaneous time- and message-optimality can be achieved
by a deterministic algorithm.

In this paper, building upon the work of [PRS16], we answer
this question in the affirmative, and devise a deterministic algorithm that computes MST in time O((D + Yn) - logan), using O(mlog n+ nlog n-log* n) messages. The polylogarithmic factors in the
time and message complexities of our algorithm are significantly
smaller than the respective factors in the result of [PRS16]. Also,
our algorithm and its analysis are very simple and self-contained, as
opposed to rather complicated previous sublinear-time algorithms
[GKP98, KP98, Elk04b, PRS16].
",15.742502247213078,15.657456140350877,224
PODC_17_016.txt,13.337516959317767,11.242415910084876,PODC,9269,"

We present a simple distributed A-approximation algorithm for
maximum weight independent set (MaxIS) in the CONGEST model
which completes in O(MIS(G) - log W) rounds, where A is the maximum degree, MIS(G) is the number of rounds needed to compute a
maximal independent set (MIS) on G, and W is the maximum weight
of a node. Plugging in the best known algorithm for MIS gives a
randomized solution in O(log n log W) rounds, where n is the number of nodes. We also present a deterministic O(A + log* n)-round
algorithm based on coloring.

We then show how to use our MaxIS approximation algorithms to
compute a 2-approximation for maximum weight matching without
incurring any additional round penalty in the CONGEST model.
We use a known reduction for simulating algorithms on the line
graph while incurring congestion, but we show our algorithm is
part of a broad family of local aggregation algorithms for which
we describe a mechanism that allows the simulation to run in the
CONGEST model without an additional overhead.

Next, we show that for maximum weight matching, relaxing the
approximation factor to (2 + ¢) allows us to devise a distributed
algorithm requiring O(geEe a) rounds for any constant ¢ > 0. For
the unweighted case, we can even obtain a (1 + ¢)-approximation
in this number of rounds. These algorithms are the first to achieve
the provably optimal round complexity with respect to dependency
on ∆

",17.553077303434723,16.689312248995986,242
PODC_17_017.txt,11.83946706224108,8.868820294482884,PODC,8624,"

In the distributed message-passing setting a communication network is represented by a graph whose vertices represent processors
that perform local computations and communicate over the edges
of the graph. In the distributed edge-coloring problem the processors
are required to assign colors to edges, such that all edges incident
on the same vertex are assigned distinct colors. The previouslyknown deterministic algorithms for edge-coloring employed at least
(2A — 1) colors, even though any graph admits an edge-coloring
with A + 1 colors [36]. Moreover, the previously-known deterministic algorithms that employed at most O(A) colors required superlogarithmic time [3, 6, 7, 17]. In the current paper we devise
deterministic edge-coloring algorithms that employ only A + o(A)
colors, for a very wide family of graphs. Specifically, as long as
the arboricity a of the graph is a = O(A1~®), for a constant € > 0,
our algorithm computes such a coloring within polylogarithmic
deterministic time.

We also devise significantly improved deterministic edge-coloring
algorithms for general graphs for a very wide range of parameters.
Specifically, for any value x in the range [4A, 2°0°84) . AJ, our kedge-coloring algorithm has smaller running time than the best
previously-known x-edge-coloring algorithms. Our algorithms are
actually much more general, since edge-coloring is equivalent to
vertex-coloring of line graphs. Our method is applicable to vertexcoloring of the family of graphs with bounded diversity that contains
line graphs, line graphs of hypergraphs, and many other graphs. We
significantly improve upon previous vertex-coloring of such graphs,
and as an implication also obtain the improved edge-coloring algorithms for general graphs.

Our results are obtained using a novel technique that connects
vertices or edges in a certain way that reduces clique size. The
resulting structures, which we call connectors, can be colored more
efficiently than the original graph. Moreover, the color classes constitute simpler subgraphs that can be colored even more efficiently

using appropriate connectors. We introduce several types of connectors that are useful for various scenarios. We believe that this
technique is of independent interest.
",15.752011032768014,14.415126050420174,349
PODC_17_018.txt,11.902722382323184,9.48856328062956,PODC,9840,"

Labeling schemes seek to assign a short label to each node in a
network, so that a function on two nodes (such as distance or
adjacency) can be computed by examining their labels alone. For
the particular case of trees, following a long line of research, optimal
bounds (up to low order terms) were recently obtained for adjacency
labeling [FOCS °15], nearest common ancestor labeling [SODA
*14], and ancestry labeling [SICOMP ’06]. In this paper we obtain
optimal bounds for distance labeling. We present labels of size
1/4 log? n + o(log” n), matching (up to low order terms) the recent
1/4 log? n — O(log n) lower bound [ICALP ’16].

Prior to our work, all distance labeling schemes for trees could
be reinterpreted as universal trees. A tree T is said to be universal
if any tree on n nodes can be found as a subtree of T. A universal
tree with |T| nodes implies a distance labeling scheme with label
size log |T|. In 1981, Chung et al. proved that any distance labeling
scheme based on universal trees requires labels of size 1/2 log? n —
logn - log log n + O(log n). Our scheme is the first to break this
lower bound, showing a separation between distance labeling and
universal trees.

The O(log? n) barrier for distance labeling in trees has led researchers to consider distances bounded by k. The size of such
labels was shown to be logn + O(k./logn) in [WADS ’01], and
then improved to log n + O(k? log(k log n)) in [SODA ’03] and finally to log n + O(k log(k log(n/k))) in [PODC ’07]. We show how
to construct labels whose size is the minimum between logn +
O(k log((log n)/k)) and O(log n-log(k/ log n)). We complement this
with almost tight lower bounds of logn + O(k log(log n/(k log k)))
and O(log n-log(k/ log n)). Finally, we consider (1+ ¢)-approximate
distances. We show that the recent labeling scheme of [ICALP ’16]
can be easily modified to obtain an O(log(1/e) - log n) upper bound
and we prove a matching O(log(1/e) - log n) lower bound.
",11.723238792162213,8.922132116004299,379
PODC_17_019.txt,16.04387235571731,15.063170524473133,PODC,8429,"

Recoverable mutual exclusion (RME) is a variation on the classic mutual exclusion (ME) problem that allows processes to crash
and recover. The time complexity of RME algorithms is quantified
in the same way as for ME, namely by counting remote memory references — expensive memory operations that traverse the
processor-to-memory interconnect. Prior work on the RME problem
established an upper bound of O(log N) RMRs in an asynchronous
shared memory model with N processes that communicate using
atomic read and write operations, prompting the question whether
sub-logarithmic RMR complexity is attainable using common readmodify-write primitives. We answer this question positively in
the cache-coherent model by presenting an RME algorithm that
incurs O(log N/log log N) RMRs and uses read, write, Fetch-AndStore, and Compare-And-Swap instructions. We also present an
O(1) RMRs algorithm that relies on double-word Compare-AndSwap and a double-word variation of Fetch-And-Store. Both algorithms are inspired by Mellor-Crummey and Scott’s queue lock.
",18.062587368997235,16.62103550295858,171
PODC_17_020.txt,13.848329375405488,12.26525795542225,PODC,7154,"
We present an abortable mutual exclusion algorithm for the cachecoherent (CC) model with atomic registers and CAS objects. The
algorithm has constant expected amortized RMR complexity in the
oblivious adversary model and is deterministically deadlock-free.
This is the first abortable mutual exclusion algorithm that achieves
o(log n/log log n) RMR complexity.
",17.5058628484301,16.55962962962963,55
PODC_17_021.txt,16.907492176645697,15.847740976672878,PODC,9564,"

Flat combining (FC) and transactional lock elision (TLE) are two
techniques that facilitate efficient multi-thread access to a sequentially implemented data structure protected by a lock. FC allows
threads to delegate their operations to another (combiner) thread,
and benefit from executing multiple operations by that thread under
the lock through combining and elimination optimizations tailored
to the specific data structure. TLE employs hardware transactional
memory (HTM) that allows multiple threads to apply their operations concurrently as long as they do not conflict. This paper
explores how these two radically different techniques can complement one another, and introduces the HTM-assisted Combining
Framework (HCF). HCF leverages HTM to allow multiple combiners to run concurrently with each other, as well as with other,
non-combiner threads. This makes HCF a good fit for data structures and workloads in which some operations may conflict with
each other while others may run concurrently without conflicts.
HCF achieves all that with changes to the sequential code similar to
those required by TLE and FC, and in particular, without requiring
the programmer to reason about concurrency.
",17.77360955136429,16.008097868981853,182
PODC_17_022.txt,12.491617527399725,10.64697433689993,PODC,10822,"

Even in the absence of clocks, time bounds on the duration of
actions enable the use of time for distributed coordination. This
paper initiates an investigation of coordination in such a setting. A
new communication structure called a zigzag pattern is introduced,
and shown to guarantee bounds on the relative timing of events
in this clockless model. Indeed, zigzag patterns are shown to be
necessary and sufficient for establishing that events occur in a
manner that satisfies prescribed bounds. We capture when a process
can know that an appropriate zigzag pattern exists, and use this to
provide necessary and sufficient conditions for timed coordination
of events using a full-information protocol in the clockless model.
",15.112257680678326,14.414782608695656,116
PODC_17_023.txt,13.720437462062765,11.637965101354087,PODC,9716,"

Practical algorithms for determining causality by assigning timestamps to events have focused on online algorithms, where a permanent timestamp is assigned to an event as soon as it is created. We
address the problem of reducing size of the timestamp by utilizing
the underlying topology (which is often not fully connected since
not all processes talk to each other) and deferring the assignment
of a timestamp to an event for a suitably chosen period of time
after the event occurs. Specifically, we focus on inline timestamps,
which are a generalization of offline timestamps that are assigned
after the computation terminates. We show that for a graph with
vertex cover VC, it is possible to assign inline timestamps which
contains only 2|VC|+2 elements. In particular, for a system with n
processes and K events per process, the size of a timestamp for any
event is at most log, n+(2|VC|+1) log,(K +1)) bits. By contrast, if
online timestamps are desired, then even for a star network, vector
timestamp of length n (for the case of integer elements) or n—1 (for
the case of real-valued elements) is required. Moreover, in addition
to being efficient, the inline timestamps developed can be used to
solve typical problems such as predicate detection, replay, recovery
that are solved with vector clocks.

",18.848423458724294,17.130372670807457,223
PODC_17_024.txt,13.143085157132635,11.16368950156831,PODC,13438,"

Biology and computer science intersect at the problem of symmetry breaking, which is relevant in both fields. Accordingly, in
recent years, distributed algorithm theorists have studied symmetry
breaking problems in models inspired by biology to help provide
insight into the capabilities and constraints of this natural process. A potential shortcoming of these models, however, is that
they execute distributed algorithms precisely as specified. In nature, where computation is often implemented by messy analog
systems, this precision cannot necessarily be guaranteed. Motivated
by this observation, in this paper we present a general method for
injecting “computational noise” into any distributed system model
that describes processes as interacting state machines. Our method
captures noise as a force that can cause state machines to transition
to the wrong state. We combine this formalization of noise with the
beeping models that have been a popular target of recent work on
bio-inspired symmetry breaking. We produce new upper and lower
bounds for both single hop and multihop models—studying leader
election in the former and the maximal independent set problem in
the latter. These bounds introduce new techniques for achieving
robustness to noise, and identify some fundamental limits in this
pursuit. We argue that both our general approach and specific results can help advance the productive relationship between biology
and algorithm theory.
",16.76809479433877,15.483559633027525,219
PODC_17_025.txt,13.464759141731978,10.898500248789286,PODC,9055,"

Consider the following random process: we are given n queues,
into which elements of increasing labels are inserted uniformly at
random. To remove an element, we pick two queues at random,
and remove the element of lower label (higher priority) among
the two. The cost of a removal is the rank of the label removed,
among labels still present in any of the queues, that is, the distance
from the optimal choice at each step. Variants of this strategy are
prevalent in state-of-the-art concurrent priority queue implementations. Nonetheless, it is not known whether such implementations
provide any rank guarantees, even in a sequential model.

We answer this question, showing that this strategy provides
surprisingly strong guarantees: Although the single-choice process,
where we always insert and remove from a single randomly chosen
queue, has degrading cost, going to infinity as we increase the
number of steps, in the two choice process, the expected rank of
a removed element is O(n) while the expected worst-case cost is
O(nlog n). These bounds are tight, and hold irrespective of the
number of steps for which we run the process. The argument is
based on a new technical connection between “heavily loaded"" ballsinto-bins processes and priority scheduling. Our analytic results
inspire a new concurrent priority queue implementation, which
improves upon the state of the art in terms of practical performance.

",15.903189008614273,14.207095851216028,234
PODC_17_026.txt,13.909959212145974,12.29633800965869,PODC,10787,"

Algorithms that use hardware transactional memory (HTM) must
provide a software-only fallback path to guarantee progress. The
design of the fallback path can have a profound impact on performance. If the fallback path is allowed to run concurrently with hardware transactions, then hardware transactions must be instrumented,
adding significant overhead. Otherwise, hardware transactions must
wait for any processes on the fallback path, causing concurrency
bottlenecks, or move to the fallback path. We introduce an approach
that combines the best of both worlds. The key idea is to use three
execution paths: an HTM fast path, an HTM middle path, and a softwate fallback path, such that the middle path can run concurrently
with each of the other two. The fast path and fallback path do not
run concurrently, so the fast path incurs no instrumentation overhead.
Furthermore, fast path transactions can move to the middle path
instead of waiting or moving to the software path. We demonstrate
our approach by producing an accelerated version of the tree update template of Brown et al., which can be used to implement fast
lock-free data structures based on down-trees. We used the accelerated template to implement two lock-free trees: a binary search tree
(BST), and an (a, b)-tree (a generalization of a B-tree). Experiments
show that, with 72 concurrent processes, our accelerated (a, b)-tree
performs between 4.0x and 4.2x as many operations per second as
an implementation obtained using the original tree update template.

",14.158211354625415,12.472888563049853,251
PODC_17_027.txt,14.554592549557764,13.048427253431324,PODC,7513,"

Modern cryptocurrency systems, such as Ethereum, permit complex
financial transactions through scripts called smart contracts. These
smart contracts are executed many, many times, always without
real concurrency. First, all smart contracts are serially executed
by miners before appending them to the blockchain. Later, those
contracts are serially re-executed by validators to verify that the
smart contracts were executed correctly by miners.

Serial execution limits system throughput and fails to exploit today’s concurrent multicore and cluster architectures. Nevertheless,
serial execution appears to be required: contracts share state, and
contract programming languages have a serial semantics.

This paper presents a novel way to permit miners and validators
to execute smart contracts in parallel, based on techniques adapted
from software transactional memory. Miners execute smart contracts speculatively in parallel, allowing non-conflicting contracts
to proceed concurrently, and “discovering” a serializable concurrent
schedule for a block’s transactions, This schedule is captured and
encoded as a deterministic fork-join program used by validators
to re-execute the miner’s parallel schedule deterministically but
concurrently.

Smart contract benchmarks run on a JVM with ScalaSTM show
that a speedup of 1.33x can be obtained for miners and 1.69x for
validators with just three concurrent threads.

",16.99224021665606,15.994040404040408,204
PODC_17_028.txt,15.892509818500152,15.37917663855075,PODC,10619,"
Nakamoto’s famous blockchain protocol enables achieving consensus in a so-called permissionless setting—anyone can join (or leave)
the protocol execution, and the protocol instructions do not depend
on the identities of the players. His ingenious protocol prevents
“sybil attacks” (where an adversary spawns any number of new
players) by relying on computational puzzles (a.k.a. “moderately
hard functions”) introduced by Dwork and Naor (Crypto’92). Recent work by Garay et al (EuroCrypt’15) and Pass et al (manuscript,
2016) demonstrate that this protocol provably achieves consistency
and liveness assuming a) honest players control a majority of the
computational power in the network, b) the puzzle-hardness is appropriately set as a function of the maximum network delay and
the total computational power of the network, and c) the computational puzzle is modeled as a random oracle. Assuming honest
participation, however, is a strong assumption, especially in a setting where honest players are expected to perform a lot of work
(to solve the computational puzzles). In Nakamoto’s Bitcoin application of the blockchain protocol, players are incentivized to solve
these puzzles by receiving rewards for every “block” (of transactions) they contribute to the blockchain. An elegant work by Eyal
and Sirer (FinancialCrypt’14), strengthening and formalizing an
earlier attack discussed on the Bitcoin forum, demonstrates that a
coalition controlling even a minority fraction of the computational
power in the network can gain (close to) 2 times its “fair share” of
the rewards (and transaction fees) by deviating from the protocol
instructions. In contrast, in a fair protocol, one would expect that
players controlling a ¢ fraction of the computational resources to
reap a ¢ fraction of the rewards.

We present a new blockchain protocol—the FruitChain protocol—
which satisfies the same consistency and liveness properties as
Nakamoto’s protocol (assuming an honest majority of the computing power), and additionally is 6-approximately fair: with overwhelming probability, any honest set of players controlling a ¢
fraction of computational power is guaranteed to get at least a fraction (1 — 5)¢ of the blocks (and thus rewards) in any QS) length
segment of the chain (where x is the security parameter). Consequently, if this blockchain protocol is used as the ledger underlying
a cryptocurrency system, where rewards and transaction fees are
evenly distributed among the miners of blocks in a length x segment
of the chain, no coalition controlling less than a majority of the
computing power can gain more than a factor (1 + 36) by deviating
from the protocol (i.e., honest participation is an $ -coalition-safe
35-Nash equilibrium). Finally, the FruitChain protocol enables decreasing the variance of mining rewards and as such significantly

lessens (or even obliterates) the need for mining pools.
",22.076135915942103,23.689022172949006,452
PODC_17_029.txt,14.821510019274193,12.332727327111751,PODC,9989,"

Assuming that there is an a priori agreement between processes
on the names of shared memory locations, as done in almost all
the publications on shared memory algorithms, is tantamount to
assuming that agreement has already been solved at the lower-level.
From a theoretical point of view, it is intriguing to figure out how
coordination can be achieved without relying on such lower-level
agreement. In order to better understand the new model, we have
designed new algorithms without relying on such a priori lowerlevel agreement, and proved space lower bounds and impossibility
results for several important problems, such as mutual exclusion,
consensus, election and renaming. Using these results, we identify
fundamental differences between the standard model where there
is a lower-level agreement about the shared register’s names and
the strictly weaker model where there is no such agreement.
",20.736966565827903,19.832677304964545,143
PODC_17_030.txt,12.764364058776643,10.431030673884525,PODC,7278,"

We study consensus processes on the complete graph of n nodes.
Initially, each node supports one up to n different opinions. Nodes
randomly and in parallel sample the opinions of constantly many
nodes. Based on these samples, they use an update rule to change
their own opinion. The goal is to reach consensus, a configuration
where all nodes support the same opinion.

We compare two well-known update rules: 2-CHoIcEs and 3Mayortty. In the former, each node samples two nodes and adopts
their opinion if they agree. In the latter, each node samples three
nodes: If an opinion is supported by at least two samples the node
adopts it, otherwise it randomly adopts one of the sampled opinions.
Known results for these update rules focus on initial configurations

with a limited number of colors (say ni ), or typically assume a bias,
where one opinion has a much larger support than any other. For
such biased configurations, the time to reach consensus is roughly
the same for 2-CHOICEs and 3-MajorITy.

Interestingly, we prove that this is no longer true for configurations with a large number of initial colors. In particular, we
show that 3-Majority reaches consensus with high probability
in O(n3/4 - log’/® n) rounds, while 2-Cuotces can need Q(n/log n)
rounds. We thus get the first unconditional sublinear bound for
3-Mayjorirty and the first result separating the consensus time of
these processes. Along the way, we develop a framework that allows
a fine-grained comparison between consensus processes from a
specific class. We believe that this framework might help to classify
the performance of more consensus processes.
",13.454273218885966,10.526079422382676,277
PODC_17_031.txt,12.233940628499237,9.710517193566282,PODC,10556,"
The set agreement power of a shared object O describes O’s ability to
solve set agreement problems: it is the sequence (11, n2,...,%,--)

such that, for every k > 1, using O and registers one can solve the kset agreement problem among at most nx processes. It has been
shown that the ability of an object O to implement other objects
is not fully characterized by its consensus number (the first component of its set agreement power) [1, 3, 14]. This raises the following
natural question: is the ability of an object O to implement other
objects fully characterized by its set agreement power? We prove
that the answer is no: every level n > 2 of Herlihy’s consensus
hierarchy has two objects that have the same set agreement power
but are not equivalent, ie., at least one cannot implement the other.
We also show that every level n > 2 of the consensus hierarchy
contains a deterministic object On with some set agreement power
(11, N2,...,Mz,...) such that being able to solve the k-set agreement problems among n x processes, for all k > 1, is not enough to
implement On.
",18.026119701940384,18.336845360824743,193
PODC_17_032.txt,13.91300422257589,12.46124370087275,PODC,9215,"

The algorithmic small-world phenomenon, empirically established
by Milgram’s letter forwarding experiments from the 60s [60], was
theoretically explained by Kleinberg in 2000 [47]. However, from
today’s perspective his model has several severe shortcomings that
limit the applicability to real-world networks. In order to give a
more convincing explanation of the algorithmic small-world phenomenon, we study decentralized greedy routing in a more flexible
random graph model (geometric inhomogeneous random graphs)
which overcomes all previous shortcomings. Apart from exhibiting good properties in theory, it has also been extensively experimentally validated that this model reasonably captures real-world
networks.

In this model, the greedy routing protocol is purely distributed
as each vertex only needs to know information about its direct
neighbors. We prove that it succeeds with constant probability, and
in case of success almost surely finds an almost shortest path of
length O(log log n), where our bound is tight including the leading
constant. Moreover, we study natural local patching methods which
augment greedy routing by backtracking and which do not require
any global knowledge. We show that such methods can ensure
success probability 1 in an asymptotically tight number of steps.

These results also address the question of Krioukov et al. [52]
whether there are efficient local routing protocols for the internet
graph. There were promising experimental studies, but the question
remained unsolved theoretically. Our results give for the first time
a rigorous and analytical affirmative answer.
",15.186304673781336,15.142424242424244,243
PODC_17_033.txt,12.805808877108358,10.329438226547715,PODC,6887,"

Triangle-free graphs play a central role in graph theory, and triangle
detection (or triangle finding) as well as triangle enumeration (triangle listing) play central roles in the field of graph algorithms. In
distributed computing, algorithms with sublinear round complexity
for triangle finding and listing have recently been developed in the
powerful CONGEST clique model, where communication is allowed
between any two nodes of the network. In this paper we present the
first algorithms with sublinear complexity for triangle finding and
triangle listing in the standard CONGEST model, where the communication topology is the same as the topology of the network. More
precisely, we give randomized algorithms for triangle finding and
listing with round complexity O(n?! 3(log n)?/3) and O(n3/4 log n),
respectively, where n denotes the number of nodes of the network.
We also show a lower bound Qn! 3 /log n) on the round complexity
of triangle listing, which also holds for the CONGEST clique model.
",14.90622815163357,12.698581288343558,164
PODC_17_034.txt,14.082595353938395,12.129670792690458,PODC,8924,"
In this work we address the question whether a simple shared channel could be efficiently utilized, that is, with a constant throughput
and linear packet latency. A shared channel (also called a multiple
access channel), introduced nearly 50 years ago in the context of
the Ethernet [36], is among the most popular and widely studied
models of communication and distributed computing. In a nutshell,
a number of stations is able to communicate by transmitting and listening to a shared channel, and a message is successfully delivered
to all stations if and only if its source station is the only transmitter
at a time. Despite of a vast amount of work in the last decades,
many fundamental questions remain open, such as: What is the
impact of asynchrony on channel utilization? How important is
the knowledge/estimate of the number of contenders? Could nonadaptive protocols (i.e., random codes) be asymptotically as efficient
as adaptive protocols? In this work we present a broad picture of
results answering the abovementioned questions for a fundamental
problem of contention resolution, in which each of the contending
stations needs to broadcast successfully its message. We show that
adaptive algorithms or algorithms with the knowledge of contention
size k (i.e., random codes with knowledge of k) achieve constant
channel throughput and linear message latency even for very weak
channels, i.e., with feedback restricted to simple acknowledgments
and in the absence of synchronization. This asymptotically optimal
performance cannot be extended to other settings — we prove that
there is no non-adaptive algorithm without the knowledge of contention size k achieving throughput c((log log k)? /(log k)) and/or
admitting latency o(k log k/(log log k)*). This means, in particular,
that coding (even random) with acknowledgments is not very efficient on a shared channel without synchronization or estimate
of contention size. We also present a non-adaptive algorithm with
no knowledge of contention size that almost matches these two
complexities. More specifically, it achieves latency O(k log” k) and
channel utilization Q(1/log? k) even if stations do not switch off
after successful transmissions (and thus, could disturb other stations in succeeding), and could be improved by factor O(log log k) if stations switch off after acknowledgment. Despite the absense of
a collision detection mechanism, our algorithms are also efficient
in terms of energy. The maximum number of channel accesses
(including transmissions and listenings) for our non-adaptive solutions, with and without knowledge of k, is respectively O(log k) and
O(log? k) whp. Regarding the adaptive algorithm, we argue that a
simple modification of our protocol preserves constant throughput
and linear latency while achieving O(log k) maximum number of
channel accesses per station whp.
",16.390658040692905,15.195565345080766,458
PODC_17_035.txt,15.178873994592376,13.647894889438582,PODC,6412,"

We consider an agent-based model in which two types of agents interact locally over a graph and have a common intolerance threshold
t for changing their types with exponentially distributed waiting
times. The model is equivalent to an unperturbed Schelling model
of self-organized segregation, an Asynchronous Cellular Automata
(ACA) with extended Moore neighborhoods, or a zero-temperature
Ising model with Glauber dynamics, and has applications in the
analysis of social and biological networks, and spin glasses systems.
Some rigorous results were recently obtained in the theoretical computer science literature, and this work provides several extensions.
We enlarge the intolerance interval leading to the formation of large
segregated regions of agents of a single type from the known size
€ > 0 to size ~ 0.134. Namely, we show that for 0.433 < r < 1/2
(and by symmetry 1/2 < rt < 0.567), the expected size of the largest
segregated region containing an arbitrary agent is exponential in
the size of the neighborhood. We further extend the interval leading to large segregated regions to size ~ 0.312 considering “almost
segregated” regions, namely regions where the ratio of the number
of agents of one type and the number of agents of the other type
vanishes quickly as the size of the neighborhood grows. In this
case, we show that for 0.344 < r < 0.433 (and by symmetry for
0.567 < t < 0.656) the expected size of the largest almost segregated region containing an arbitrary agent is exponential in the size
of the neighborhood. This behavior is reminiscent of supercritical
percolation, where small clusters of empty sites can be observed
within any sufficiently large region of the occupied percolation
cluster. The exponential bounds that we provide also imply that
complete segregation, where agents of a single type cover the whole
grid, does not occur with high probability for p = 1/2 and the range
of tolerance considered.
",18.36309006607702,18.59190476190476,318
PODC_17_036.txt,14.079980623815356,11.150564000222897,PODC,6846,"

Population protocols are a well established model of computation by
anonymous, identical finite state agents. A protocol is well-specified
if from every initial configuration, all fair executions of the protocol
reach a common consensus. The central verification question for
population protocols is the well-specification problem: deciding if a
given protocol is well-specified. Esparza et al. have recently shown
that this problem is decidable, but with very high complexity: it
is at least as hard as the Petri net reachability problem, which is
EXPSPACE-hard, and for which only algorithms of non-primitive
recursive complexity are currently known.

In this paper we introduce the class WS? of well-specified
strongly-silent protocols and we prove that it is suitable for automatic verification. More precisely, we show that WS? has the
same computational power as general well-specified protocols, and
captures standard protocols from the literature. Moreover, we show
that the membership problem for WS? reduces to solving boolean
combinations of linear constraints over N. This allowed us to develop the first software able to automatically prove well-specification for all of the infinitely many possible inputs.
",16.35954948731386,14.11056092843327,189
PODC_17_037.txt,15.317112822102057,13.325691455160097,PODC,10904,"

Population protocols are required to converge to the correct answer,
and are subject to a fairness condition that guarantees eventual
progress, but generally have no internal mechanism for detecting
when this progress has occurred. We define an extension to the
standard population protocol that provides each agent with a clock
signal that indicates when the agent has waited long enough. To
simplify the model, we represent “long enough” as an infinite time
interval, and treat a clocked population protocol as operating over
transfinite time. This gives a clean theoretical model that we show
how to translate back into finite real-world executions where the
clock ticks whenever the underlying protocol is looping or stuck.

Over finite time intervals, the protocol behaves as in the standard
model. At nonzero limit ordinals w, w-2, etc., corresponding to clock
ticks, the protocol switches to a limit of previous configurations
supplemented by an signal registering in an extra component in
some of the agents’ states. Using transfinite times means that we can
represent fairness over sequences of transitions that may include
clock ticks with the same definition as over smaller intervals. Using
arbitrary ordinals allows using times like w* or w? to represent
convergence that depends on detecting convergence repeatedly at
lower levels.

We show that a clocked population protocol running in less than
co* time for any fixed k > 2 is equivalent in power to a nondeterministic Turing machine with space complexity logarithmic in the
size of the population. A consequence of this equivalence is that
any symmetric predicate that can be computed in less than w* time
can be computed in less than w” time, which requires only finitely
many clock ticks.

",17.845785984406827,15.957827208252741,282
PODC_17_038.txt,13.891560413171224,11.67930734995992,PODC,11539,"

In this paper, we study the fundamental problem of gossip in the mobile telephone model: a recently introduced variation of the classical
telephone model modified to better describe the local peer-to-peer
communication services implemented in many popular smartphone
operating systems. In more detail, the mobile telephone model differs from the classical telephone model in three ways: (1) each
device can participate in at most one connection per round; (2) the
network topology can undergo a parameterized rate of change; and
(3) devices can advertise a parameterized number of bits about their
state to their neighbors in each round before connection attempts
are initiated. We begin by describing and analyzing new randomized gossip algorithms in this model under the harsh assumption of
a network topology that can change completely in every round. We
prove a significant time complexity gap between the case where
nodes can advertise 0 bits to their neighbors in each round, and the
case where nodes can advertise 1 bit. For the latter assumption, we
present two solutions: the first depends on a shared randomness
source, while the second eliminates this assumption using a pseudorandomness generator we prove to exist with a novel generalization
of a classical result from the study of two-party communication
complexity. We then turn our attention to the easier case where
the topology graph is stable, and describe and analyze a new gossip
algorithm that provides a substantial performance improvement
for many parameters. We conclude by studying a relaxed version
of gossip in which it is only necessary for nodes to each learn a
specified fraction of the messages in the system. We prove that our
existing algorithms for dynamic network topologies and a single
advertising bit solve this relaxed version up to a polynomial factor
faster (in network size) for many parameters. These are the first
known gossip results for the mobile telephone model, and they
significantly expand our understanding of how to communicate
and coordinate in this increasingly relevant setting.
",19.838815038463903,20.214360523665665,332
PODS_17_001.txt,14.167835887772657,12.13201730590195,PODS,9316,"

The evaluation of a query over a probabilistic database boils
down to computing the probability of a suitable Boolean
function, the lineage of the query over the database. The
method of query compilation approaches the task in two
stages: first, the query lineage is implemented (compiled) in
a circuit form where probability computation is tractable;
and second, the desired probability is computed over the
compiled circuit. A basic theoretical quest in query compilation is that of identifying pertinent classes of queries
whose lineages admit compact representations over increasingly succinct, tractable circuit classes.

Fostering previous work by Jha and Suciu (ICDT 2012)
and Petke and Razgon (SAT 2013), we focus on queries
whose lineages admit circuit implementations with small
treewidth, and investigate their compilability within tame
classes of decision diagrams. In perfect analogy with the
characterization of bounded circuit pathwidth by bounded
OBDD width, we show that a class of Boolean functions
has bounded circuit treewidth if and only if it has bounded
SDD width. Sentential decision diagrams (SDDs) are central in knowledge compilation, being essentially as tractable
as OBDDs but exponentially more succinct. By incorporating constant width (linear size) SDDs and polynomial size
SDDs in the picture, we refine the panorama of query compilation for unions of conjunctive queries with and without
inequalities.

",18.699421769314853,19.544748490945675,214
PODS_17_002.txt,13.281915118130254,11.423124183757945,PODS,11794,"

We study the optimal communication cost for computing
a full conjunctive query QO over p distributed servers. Two
prior results were known. First, for one-round algorithms
over skew-free data the optimal communication cost per
server is m/p! T*(Q) | where m is the size of the largest
input relation, and 7* is the fractional vertex covering
number of the query hypergraph. Second, for multi-round
algorithms and unrestricted database instances, it was
shown that any algorithm requires at least m/pl/ p*(Q)
communication cost per server, where p*(Q) is the fractional edge covering number of the query hypergraph; but
no matching algorithms were known for this case (except
for two restricted queries: chains and cycles).

In this paper we describe a multi-round algorithm that
computes any query with load m/p! p*(Q) per server, in
the case when all input relations are binary. Thus, we
prove this to be the optimal load for all queries over binary
input relations. Our algorithm represents a non-trivial
extension of previous algorithms for chains and cycles,
and exploits some unique properties of graphs, which no
longer hold for hyper-graphs.

",14.712192995108573,12.587179487179487,195
PODS_17_003.txt,13.276463006386411,11.134969771780874,PODS,14304,"
We consider the task of enumerating and counting answers
to k-ary conjunctive queries against relational databases that
may be updated by inserting or deleting tuples.

We exhibit a new notion of g-hierarchical conjunctive queries and show that these can be maintained efficiently in the
following sense. During a linear time preprocessing phase,
we can build a data structure that enables constant delay
enumeration of the query results; and when the database is
updated, we can update the data structure and restart the
enumeration phase within constant time. For the special
case of self-join free conjunctive queries we obtain a dichotomy: if a query is not q-hierarchical, then query enumeration with sublinear* delay and sublinear update time (and
arbitrary preprocessing time) is impossible.

For answering Boolean conjunctive queries and for the
more general problem of counting the number of solutions of
k-ary queries we obtain complete dichotomies: if the query’s
homomorphic core is q-hierarchical, then size of the the
query result can be computed in linear time and maintained
with constant update time. Otherwise, the size of the query
result cannot be maintained with sublinear update time.

All our lower bounds rely on the OMv-conjecture, a conjecture on the hardness of online matrix-vector multiplication that has recently emerged in the field of fine-grained
complexity to characterise the hardness of dynamic problems. The lower bound for the counting problem additionally relies on the orthogonal vectors conjecture, which in
turn is implied by the strong exponential time hypothesis.

*) By sublinear we mean O(n'—*) for some ¢ > 0, where
n is the size of the active domain of the current database.
",17.251386760058843,17.25522781774581,278
PODS_17_004.txt,13.53880420337995,11.189022313307884,PODS,12058,"

Triangle listing has been a long-standing problem, with many
heuristics, bounds, and experimental results, but not much
asymptotically accurate complexity analysis. To address
this issue, we introduce a novel stochastic framework, based
on Glivenko-Cantelli results for functions of order statistics,
that allows modeling cost of in-memory triangle enumeration in families of random graphs. Unlike prior work that
usually studies the O(.) notation, we derive the exact limits
of CPU complexity of all vertex/edge iterators under arbitrary acyclic orientations as graph size n — oo. These results are obtained in simple closed form as functions of the
degree distribution. This allows us to establish optimal orientations for all studied algorithms, compare them to each
other, and discover the best technique within each class.

",17.353723509956247,16.7216,126
PODS_17_005.txt,13.113457852444178,10.421131259686192,PODS,12723,"

Our concern is the overhead of answering OWL 2QL ontology-mediated queries (OMQs) in ontology-based data access compared to evaluating their underlying tree-shaped
and, more generally, bounded treewidth conjunctive queries
(CQs). We show that OMQs with bounded depth ontologies
have nonrecursive datalog (NDL) rewritings that can be constructed and evaluated in LOGCFL for combined complexity,
and even in NL if their CQs are tree-shaped with a bounded
number of leaves. Thus, such OMQs incur no overhead in
complexity-theoretic terms. For OMQs with arbitrary ontologies and bounded-leaf tree-shaped CQs, NDL-rewritings
are constructed and evaluated in LOGCFL. We experimentally demonstrate feasibility and scalability of our rewritings
compared to previously proposed NDL-rewritings. On the
negative side, we prove that answering OMQs with treeshaped CQs is not fixed-parameter tractable if the ontology
depth or the number of leaves in the CQs is regarded as the
parameter, and that answering OMQs with a fixed ontology
(of infinite depth) is NP-complete for tree-shaped CQs and
LOGCFL-complete for bounded-leaf CQs.
",18.243605946275583,16.902272727272727,177
PODS_17_006.txt,15.306067950941681,13.702567757358437,PODS,9337,"

Complex queries for massive data analysis jobs have become
increasingly commonplace. Many such queries contain common subexpressions, either within a single query or among
multiple queries submitted as a batch. Conventional query
optimizers do not exploit these subexpressions and produce
sub-optimal plans. The problem of multi-query optimization (MQO) is to generate an optimal combined evaluation
plan by computing common subexpressions once and reusing
them. Exhaustive algorithms for MQO explore an O(n”)
search space. Thus, this problem has primarily been tackled
using various heuristic algorithms, without providing any
theoretical guarantees on the quality of their solution.

In this paper, instead of the conventional cost minimization problem, we treat the problem as maximizing a linear
transformation of the cost function. We propose a greedy
algorithm for this transformed formulation of the problem,
which under weak, intuitive assumptions, provides an approximation factor guarantee for this formulation. We go
on to show that this factor is optimal, unless P = NP. Another noteworthy point about our algorithm is that it can be
easily incorporated into existing transformation-based optimizers. We finally propose optimizations which can be used
to improve the efficiency of our algorithm.
",15.668782140382113,14.730447479981162,193
PODS_17_007.txt,15.015989407043413,13.430721552112285,PODS,12606,"

Normal tuple-generating dependencies (NTGDs) are TGDs enriched with default negation, a.k.a. negation as failure. Query answering under NTGDs, where negation is interpreted according to
the stable model semantics, is an intriguing new problem that gave
rise to flourishing research activity in the database and KR communities. So far, all the existing works that investigate this problem,
except for one recent paper that adopts an operational semantics
based on the chase, follow the so-called logic programming (LP)
approach. According to the LP approach, the existentially quantified variables are first eliminated via Skolemization, which leads
to a normal logic program, and then the standard stable model semantics for normal logic programs is applied. However, as we discuss in the paper, Skolemization is not appropriate in the presence
of default negation since it fails to capture the intended meaning
of NTGDs, while the operational semantics mentioned above fails
to overcome the limitations of the LP approach. This reveals the
need to adopt an alternative approach to stable model semantics that
is directly applicable to NTGDs with existentially quantified. variables. We propose such an approach based on a recent characterization of stable models in terms of second-order logic, which indeed
overcomes the limitations of the LP approach. We then perform
an in-depth complexity analysis of query answering under prominent classes of NTGDs based on the main decidability paradigms
for TGDs, namely weak-acyclicity, guardedness and stickiness. Interestingly, weakly-acyclic NTGDs give rise to robust and highly
expressive query languages that allow us to solve in a declarative
way problems in the second level of the polynomial hierarchy.

",18.001758247042904,18.342546816479402,270
PODS_17_008.txt,14.461991222022053,12.450131248043963,PODS,11109,"

Data is continuously generated by modern data sources, and
a recent challenge in machine learning has been to develop
techniques that perform well in an incremental (streaming) setting. A variety of offline machine learning tasks
are known to be feasible under differential privacy, where
generic construction exist that, given a large enough input
sample, perform tasks such as PAC learning, Empirical Risk
Minimization (ERM), regression, etc. In this paper, we investigate the problem of private machine learning, where as
common in practice, the data is not given at once, but rather
arrives incrementally over time.

We introduce the problems of private incremental ERM
and private incremental regression where the general goal
is to always maintain a good empirical risk minimizer for
the history observed under differential privacy. Our first
contribution is a generic transformation of private batch
ERM mechanisms into private incremental ERM mechanisms, based on a simple idea of invoking the private batch
ERM procedure at some regular time intervals. We take
this construction as a baseline for comparison. We then
provide two mechanisms for the private incremental regression problem. Our first mechanism is based on privately
constructing a noisy incremental gradient function, which is
then used in a modified projected gradient procedure at every timestep. This mechanism has an excess empirical risk
of = Vd, where d is the dimensionality of the data. While
from the results of Bassily e¢ al. [2] this bound is tight in
the worst-case, we show that certain geometric properties of
the input and constraint set can be used to derive significantly better results for certain interesting regression problems. Our second mechanism which achieves this is based on
the idea of projecting the data to a lower dimensional space
using random projections, and then adding privacy noise in
this low dimensional space. The mechanism overcomes the
issues of adaptivity inherent with the use of random projections in online streams, and uses recent developments in
high-dimensional estimation to achieve an excess empirical
risk bound of = T'/3w?/3, where T is the length of the
stream and W is the sum of the Gaussian widths of the
input domain and the constraint set that we optimize over.

",17.388976658221566,16.672861035422347,365
PODS_17_009.txt,14.824596989420563,13.182732257258355,PODS,12148,"

The chase is a family of algorithms used in a number of data management tasks, such as data exchange, answering queries under dependencies, query reformulation with constraints, and data cleaning. It is well established as a theoretical tool for understanding
these tasks, and in addition a number of prototype systems have
been developed. While individual chase-based systems and particular optimizations of the chase have been experimentally evaluated
in the past, we provide the first comprehensive and publicly available benchmark—test infrastructure and a set of test scenarios—for
evaluating chase implementations across a wide range of assumptions about the dependencies and the data. We used our benchmark
to compare chase-based systems on data exchange and query answering tasks with one another, as well as with systems that can
solve similar tasks developed in closely related communities. Our
evaluation provided us with a number of new insights concerning
the factors that impact the performance of chase implementations.

",18.669449996058646,18.76564556962025,159
PODS_17_010.txt,13.694591171380054,11.33449044275731,PODS,13774,"

In the design of analytical procedures and machine-learning
solutions, a critical and time-consuming task is that of feature engineering, for which various recipes and tooling approaches have been developed. In this framework paper, we
embark on the establishment of database foundations for feature engineering. We propose a formal framework for classification in the context of a relational database. The goal of
this framework is to open the way to research and techniques
to assist developers with the task of feature engineering by
utilizing the database’s modeling and understanding of data
and queries, and by deploying the well studied principles of
database management. As a first step, we demonstrate the
usefulness of this framework by formally defining three key
algorithmic challenges. The first challenge is that of separability, which is the problem of determining the existence of
feature queries that agree with the training examples. The
second is that of evaluating the VC dimension of the model
class with respect to a given sequence of feature queries. The
third challenge is identifiability, which is the task of testing
for a property of independence among features that are represented as database queries. We give preliminary results
on these challenges for the case where features are defined
by means of conjunctive queries, and in particular we study
the implication of various traditional syntactic restrictions
on the inherent computational complexity.
",17.37919286519448,16.130350877192985,230
PODS_17_011.txt,12.809599089573815,10.945228093466522,PODS,11469,"

The task of finding heavy hitters is one of the best known
and well studied problems in the area of data streams. One is
given a list 41, %2,...,%m € [n] and the goal is to identify the
items among [n] that appear frequently in the list. In subpolynomial space, the strongest guarantee available is the
fa guarantee, which requires finding all items that occur at
least é||f||2 times in the stream, where the vector f € R” is
the count histogram of the stream with ith coordinate equal
to the number of times i appears fi = #{j € [m] : i; = i}.
The first algorithm to achieve the @2 guarantee was the
CountSketch of [11], which requires O(e~*logn) words of
memory and O(log 7) update time and is known to be spaceoptimal if the stream allows for deletions. The recent work
of [7] gave an improved algorithm for insertion-only streams,
using only O(e~* loge—* log log n) words of memory. In this
work, we give an algorithm BPTree for 2 heavy hitters in
insertion-only streams that achieves O(e~? loge) words
of memory and O(loge~') update time, which is the optimal dependence on n and m. In addition, we describe
an algorithm for tracking ||f|l2 at all times with O(e~”)
memory and update time. Our analyses rely on bounding the expected supremum of a Bernoulli process involving
Rademachers with limited independence, which we accomplish via a Dudley-like chaining argument that may have applications elsewhere.
",14.867677710551934,13.744477244772451,255
PODS_17_012.txt,14.25701775257458,12.216041539009424,PODS,9976,"

The atomic commit problem lies at the heart of distributed
database systems. The problem consists for a set of processes (database nodes) to agree on whether to commit or
abort a transaction (agreement property). The commit decision can only be taken if all processes are initially willing
to commit the transaction, and this decision must be taken
if all processes are willing to commit and there is no failure
(validity property). An atomic commit protocol is said to
be non-blocking if every correct process (a database node
that does not fail) eventually reaches a decision (commit or
abort) even if there are failures elsewhere in the distributed
database system (termination property).

Surprisingly, despite the importance of the atomic commit
problem, little is known about its complexity. In this paper,
we present, for the first time, a systematic study on the time
and message complexity of the problem. We measure complexity in the executions that are considered the most frequent in practice, i.e., failure-free, with all processes willing
to commit. In other words, we measure how fast a transaction can commit. Through our systematic study, we close
many open questions like the complexity of synchronous
non-blocking atomic commit. We also present optimal protocols which may be of independent interest. In particular,
we present an effective protocol which solves what we call
indulgent atomic commit that tolerates practical distributed
database systems which are synchronous “most of the time”.

",15.903189008614273,14.015017116774441,241
PODS_17_013.txt,12.872005278130374,10.593734543608878,PODS,10914,"

Recent works on bounding the output size of a conjunctive
query with functional dependencies and degree bounds have
shown a deep connection between fundamental questions in
information theory and database theory. We prove analogous output bounds for disjunctive datalog rules, and answer
several open questions regarding the tightness and looseness
of these bounds along the way. The bounds are intimately
related to Shannon-type information inequalities. We devise the notion of a “proof sequence” of a specific class of
Shannon-type information inequalities called “Shannon flow
inequalities”. We then show how a proof sequence can be
used as symbolic instructions to guide an algorithm called
PANDA, which answers disjunctive datalog rules within the
size bound predicted. We show that PANDA can be used
as a black-box to devise algorithms matching precisely the
fractional hypertree width and the submodular width runtimes for aggregate and conjunctive queries with functional
dependencies and degree bounds.
Our results improve upon known results in three ways.
First, our bounds and algorithms are for the much more general class of disjunctive datalog rules, of which conjunctive
queries are a special case. Second, the runtime of PANDA
matches precisely the submodular width bound, while the
previous algorithm by Marx has a runtime that is polynomial in this bound. Third, our bounds and algorithms work
for queries with input cardinality bounds, functional dependencies, and degree bounds.
Overall, our results showed a deep connection between
three seemingly unrelated lines of research; and, our results
on proof sequences for Shannon flow inequalities might be
of independent interest.
",16.581925556534415,14.963841528121687,258
PODS_17_014.txt,12.394437343728445,9.475302730291759,PODS,10541,"

We propose a logical framework, based on Datalog, to study
the foundations of querying JSON data. The main feature
of our approach, which we call J-Logic, is the emphasis on
paths. Paths are sequences of keys and are used to access
the tree structure of nested JSON objects. J-Logic also features “packing” as a means to generate a new key from a
path or subpath. J-Logic with recursion is computationally complete, but many queries can be expressed without
recursion, such as deep equality. We give a necessary condition for queries to be expressible without recursion. Most
of our results focus on the deterministic nature of JSON objects as partial functions from keys to values. Predicates
defined by J-Logic programs may not properly describe objects, however. Nevertheless we show that every object-toobject transformation in J-Logic can be defined using only
objects in intermediate results. Moreover we show that it is
decidable whether a positive, nonrecursive J-Logic program
always returns an object when given objects as inputs. Regarding packing, we show that packing is unnecessary if the
output does not require new keys. Finally, we show the
decidability of query containment for positive, nonrecursive
J-Logic programs.

",12.745084868956482,11.654207920792079,203
PODS_17_015.txt,13.496850093140633,11.200408700093408,PODS,12233,"
In the traditional sense, a subset repair of an inconsistent
database refers to a consistent subset of facts (tuples) that
is maximal under set containment. Preferences between
pairs of facts allow to distinguish a set of preferred repairs
based on relative reliability (source credibility, extraction
quality, recency, etc.) of data items. Previous studies explored the problem of categoricity, where one aims to determine whether preferences suffice to repair the database
unambiguously, or in other words, whether there is precisely
one preferred repair. In this paper we study the ability to
quantify ambiguity, by investigating two classes of problems.
The first is that of counting the number of subset repairs,
both preferred (under various common semantics) and traditional. We establish dichotomies in data complexity for the
entire space of (sets of) functional dependencies. The second
class of problems is that of enumerating (i.e., generating)
the preferred repairs. We devise enumeration algorithms
with efficiency guarantees on the delay between generated
repairs, even for constraints represented as general conflict
graphs or hypergraphs.
",16.678067442207542,16.318572485207103,171
PODS_17_016.txt,15.098401644808135,13.636429242298608,PODS,12412,"

We study the complexity of ontology-mediated querying when ontologies are formulated in the guarded fragment of first-order logic
(GF). Our general aim is to classify the data complexity on the
level of ontologies where query evaluation w.r.t. an ontology O is
considered to be in PTIME if all (unions of conjunctive) queries
can be evaluated in PTIME w.rt. O and CONP-hard if at least one
query is CONP-hard w.rt. O. We identify several large and relevant fragments of GF that enjoy a dichotomy between PTIME and
CONP, some of them additionally admitting a form of counting. In
fact, almost all ontologies in the BioPortal repository fall into these
fragments or can easily be rewritten to do so. We then establish a
variation of Ladner’s Theorem on the existence of NP-intermediate
problems and use this result to show that for other fragments, there
is provably no such dichotomy. Again for other fragments (such as
full GF), establishing a dichotomy implies the Feder- Vardi conjecture on the complexity of constraint satisfaction problems. We also
link these results to Datalog-rewritability and study the decidability of whether a given ontology enjoys PTIME query evaluation,
presenting both positive and negative results.
",16.52667757954773,16.04660891089109,208
PODS_17_017.txt,14.205780139436744,11.811082890061211,PODS,9655,"

We present an algorithm that enumerates all the minimal
triangulations of a graph in incremental polynomial time.
Consequently, we get an algorithm for enumerating all the
proper tree decompositions, in incremental polynomial time,
where “proper” means that the tree decomposition cannot be
improved by removing or splitting a bag. The algorithm can
incorporate any method for (ordinary, single result) triangulation or tree decomposition, and can serve as an anytime
algorithm to improve such a method. We describe an extensive experimental study of an implementation on real data
from different fields. Our experiments show that the algorithm improves upon central quality measures over the underlying tree decompositions, and is able to produce a large
number of high-quality decompositions.
",17.58133193835471,17.814000000000004,119
PODS_17_018.txt,12.464216070819607,10.27934246194329,PODS,9334,"

Parallel join algorithms have received much attention in recent years, due to the rapid development of massively parallel systems such as MapReduce and Spark. In the database
theory community, most efforts have been focused on studying worst-optimal algorithms. However, the worst-case optimality of these join algorithms relies on the hard instances
having very large output sizes. In the case of a two-relation
join, the hard instance is just a Cartesian product, with an
output size that is quadratic in the input size.

In practice, however, the output size is usually much smaller.

One recent parallel join algorithm by Beame et al. [8] has
achieved output-optimality, i.e., its cost is optimal in terms
of both the input size and the output size, but their algorithm only works for a 2-relation equi-join, and has some
imperfections. In this paper, we first improve their algorithm to true optimality. Then we design output-optimal
algorithms for a large class of similarity joins. Finally, we
present a lower bound, which essentially eliminates the possibility of having output-optimal algorithms for any join on
more than two relations.
",15.172626615295595,13.269432624113481,190
PODS_17_019.txt,13.407950540960123,11.112520879282226,PODS,11459,"

We propose a novel framework wherein probabilistic preferences can be naturally represented and analyzed in a probabilistic relational database. The framework augments the
relational schema with a special type of a relation symbol—a
preference symbol. A deterministic instance of this symbol
holds a collection of binary relations. Abstractly, the probabilistic variant is a probability space over databases of the
augmented form (i.e., probabilistic database). Effectively,
each instance of a preference symbol can be represented
as a collection of parametric preference distributions such
as Mallows. We establish positive and negative complexity results for evaluating Conjunctive Queries (CQs) over
databases where preferences are represented in the Repeated
Insertion Model (RIM), Mallows being a special case. We
show how CQ evaluation reduces to a novel inference problem (of independent interest) over RIM, and devise a solver
with polynomial data complexity.
",18.08858127442927,17.6652380952381,140
PODS_17_020.txt,13.929353993429071,11.525678722710985,PODS,11894,"

This paper investigates the problem of reverse engineering,
i.e., learning, select-project-join (SPJ) queries from a userprovided example set, containing positive and negative tuples.
The goal is then to determine whether there exists a query
returning all the positive tuples, but none of the negative tuples, and furthermore, to find such a query, if it exists. These
are called the satisfiability and learning problems, respectively. The ability to solve these problems is an important
step in simplifying the querying process for non-expert users.

This paper thoroughly investigates the satisfiability and
learning problems in a variety of settings. In particular,
we consider several classes of queries, which allow different
combinations of the operators select, project and join. In
addition, we compare the complexity of satisfiability and
learning, when the query is, or is not, of bounded size. We
note that bounded-size queries are of particular interest, as
they can be used to avoid over-fitting (i.e., tailoring a query
precisely to only the seen examples).

In order to fully understand the underlying factors which
make satisfiability and learning (in)tractable, we consider
different components of the problem, namely, the size of a
query to be learned, the size of the schema and the number
of examples. We study the complexity of our problems, when
considering these as part of the input, as constants or as
parameters (i.e., as in parameterized complexity analysis).
Depending on the setting, the complexity of satisfiability
and learning can vary significantly. Among other results, our
analysis also provides new problems that are complete for
W|3], for which few natural problems are known. Finally, by
considering a variety of settings, we derive insight on how
the different facets of our problem interplay with the size of
the database, thereby providing the theoretical foundations
necessary for a future implementation of query learning from
examples.
",16.194650593361626,15.635161290322582,313
PODS_17_021.txt,13.43373280363138,11.921546779466635,PODS,13694,"

Schema mappings are a fundamental concept in data integration and exchange, and they have been thoroughly studied in different data models. For graph data, however, mappings have been studied in a restricted context that, unlike real-life graph databases, completely disregards the data
they store. Our main goal is to understand query answering
under graph schema mappings — in particular, in exchange
and integration of graph data — for graph databases that
mix graph structure with data. We show that adding data
querying alters the picture in a significant way.

As the model, we use data graphs: a theoretical abstraction of property graphs employed by graph database implementations. We start by showing a very strong negative result: using the simplest form of nontrivial navigation in mappings makes answering even simple queries that mix navigation and data undecidable. This result suggests that for
the purposes of integration and exchange, schema mappings
ought to exclude recursively defined navigation over target
data. For such mappings and analogs of regular path queries
that take data into account, query answering becomes decidable, although intractable. To restore tractability without imposing further restrictions on queries, we propose a
new approach based on the use of null values that resemble
usual nulls of relational DBMSs, as opposed to marked nulls
one typically uses in integration and exchange tasks. If one
moves away from path queries and considers more complex
patterns, query answering becomes undecidable again, even
for the simplest possible mappings.

",15.514038796780547,15.401531120331953,242
PODS_17_022.txt,12.123468678234175,9.172268212415855,PODS,14450,"

This paper proposes a class of dependencies for graphs, referred to as graph entity dependencies (GEDs). A GED is
a combination of a graph pattern and an attribute dependency. In a uniform format, GEDs express graph functional
dependencies with constant literals to catch inconsistencies,
and keys carrying id literals to identify entities in a graph.
We revise the chase for GEDs and prove its Church-Rosser
property. We characterize GED satisfiability and implication, and establish the complexity of these problems and the
validation problem for GEDs, in the presence and absence of
constant literals and id literals. We also develop a sound
and complete axiom system for finite implication of GEDs.
In addition, we extend GEDs with built-in predicates or disjunctions, to strike a balance between the expressive power
and complexity. We settle the complexity of the satisfiability, implication and validation problems for the extensions.
",15.579741850924794,13.187773972602741,147
PODS_17_023.txt,14.493263644188946,12.677679571414956,PODS,11170,"

We introduce new dynamic set intersection data structures,
which we call 2-3 cuckoo filters and hash tables. These
structures differ from the standard cuckoo hash tables and
cuckoo filters in that they choose two out of three locations
to store each item, instead of one out of two, ensuring
that any item in an intersection of two structures will
have at least one common location in both structures. We
demonstrate the utility of these structures by using them
in improved algorithms for listing triangles and answering
set intersection queries in internal or external memory. For
a graph G of n vertices and m edges, our internal-memory
triangle listing algorithm runs in O(m[(a(G) log w)/w] + &)
expected time, where a(G) is the arboricity of G, w is
the number of bits in a machine word, and & is the number of output triangles. Our external-memory algorithm
uses O(sort(n a(G)) + sort(m[(a(G) log w)/w]) + sort(k))
expected number of I/Os.

",17.80541091248751,17.173954022988507,172
PODS_17_024.txt,13.692730961506989,11.795508356715619,PODS,11419,"

We study the classic set cover problem in the streaming
model: the sets that comprise the instance are revealed one
by one in a stream and the goal is to solve the problem by
making one or few passes over the stream while maintaining
a sublinear space o(mn) in the input size; here m denotes
the number of the sets and n is the universe size. Notice
that in this model, we are mainly concerned with the space
requirement of the algorithms and hence do not restrict their
computation time.

Our main result is a resolution of the space-approximation
tradeoff for the streaming set cover problem: we show that
any a@-approximation algorithm for the set cover problem requires O(mn/ *) space, even if it is allowed polylog(n) passes
over the stream, and even if the sets are arriving in a random order in the stream. This space-approximation tradeoff
matches the best known bounds achieved by the recent algorithm of Har-Peled et al. (PODS 2016) that requires only
O(a) passes over the stream in an adversarial order, hence
settling the space complexity of approximating the set cover
problem in data streams in a quite robust manner. Additionally, our approach yields tight lower bounds for the space
complexity of (1—<)-approximating the streaming maximum
coverage problem studied in several recent works.

",15.021129683784007,17.77469162995595,227
PODS_17_025.txt,12.363545731178743,9.498477872617379,PODS,10931,"

The skip list is an elegant dictionary data structure that is commonly deployed in RAM. A skip list with N elements supports
searches, inserts, and deletes in O(log N) operations with high
probability (w.h.p.) and range queries returning K elements in
O(log N + K) operations w.h.p.

A seemingly natural way to generalize the skip list to external
memory with block size B is to “promote” with probability 1/B,
rather than 1/2. However, there are practical and theoretical obstacles to getting the skip list to retain its efficient performance, space
bounds, and high-probability guarantees.

We give an external-memory skip list that achieves writeoptimized bounds. That is, forO < e¢ < 1, range queries take
O(logye N + K/B) VOs w..p. and insertions and deletions take
O((logze N)/B'~*) amortized /Os w.h.p.

Our write-optimized skip list inherits the virtue of simplicity
from RAM skip lists. Moreover, it matches or beats the asymptotic
bounds of prior write-optimized data structures such as B® trees or
LSM trees. These data structures are deployed in high-performance
databases and file systems.
The main technical challenge in proving our bounds comes from
the fact that there are so few levels in the skip list, an aspect of the
data structure that is essential to getting strong external-memory
bounds. We use extremal-graph coloring to show that it is possible to decompose paths in the skip list into uncorrelated groups,
regardless of the insertion/deletion pattern. Thus, we achieve our
bounds by averaging over these uncorrelated paths rather than by
averaging over uncorrelated levels, as in the standard skip list.

",15.247664890283005,12.763521505376342,279
PODS_17_026.txt,10.923149748696021,8.742157790827068,PODS,8112,"

We consider publication settings with positive user feedback, such
as, users publishing tweets and other users retweeting them, friends
posting photos and others liking them or even authors publishing research papers and others citing these publications. A wellaccepted notion of “impact” for users in these settings is the HIndex, which is the largest k such that at least k publications have
k or more (positive) feedback.

We study how to calculate H-index on large streams of user publications and feedback. If all the items can be stored, H-index of a
user can be computed by sorting. We focus on the streaming setting
where as is typical, we do not have space to store all the items.

We present the first known streaming algorithm for computing
the H-index of a user in the cash register streaming model using
space poly(1/e, log(1/8), log n); this algorithm provides an additive e approximation. For the aggregated model where feedback for
a publication is collated, we present streaming algorithms that use
much less space, either only dependent on € and even a small constant. We also address the problem of finding “heavy hitters” users
in H-index without estimating everyones’ H-index. We present
randomized streaming algorithms for finding 1 + € approximation
to heavy hitters that uses space poly(1/e, log(1/5), log n) and succeeds with probability at least 1 — 5. Again, this is the first sublinear space algorithm for this problem, despite extensive research
on heavy hitters in general. Our work initiates study of streaming
algorithms for problems that estimate impact or identify impactful
users.

",14.02287970970757,13.87937165775401,270
PODS_17_027.txt,11.787367397322019,9.502787422742632,PODS,11959,"

A sketch or synopsis of a large dataset captures vital properties of
the original data while typically occupying much less space. In this
paper, we consider the problem of computing a sketch of a massive data matrix A € R”*4, which is distributed across a large
number of s servers. Our goal is to output a matrix B € R°*4
which is significantly smaller than but still approximates A well
in terms of covariance error, i.e., ||A7 A — B* Bll2. Here, for a
matrix A, ||.A||2 is the spectral norm of A, which is defined as the
largest singular value of A. Following previous works, we call Ba
covariance sketch of A. We are mainly focused on minimizing the
communication cost, which is arguably the most valuable resource
in distributed computations. We show a gap between deterministic and randomized communication complexity for computing a
covariance sketch. More specifically, we first prove a tight deterministic lower bound, then show how to bypass this lower bound
using randomization. In Principle Component Analysis (PCA), the
goal is to find a low-dimensional subspace that captures as much
of the variance of a dataset as possible. Based on a well-known
connection between covariance sketch and PCA, we give a new algorithm for distributed PCA with improved communication cost.
Moreover, in our algorithms, each server only needs to make one
pass over the data with limited working space.
",14.314028922438442,12.167868852459016,237
PODS_17_028.txt,14.485747703481785,12.668374389300961,PODS,12727,"
Despite the fact that JSON is currently one of the most ""first"": ""John"",
popular formats for exchanging data on the Web, there are ""last"": ""Doe""
very few studies on this topic and there is no agreement upon , stu ag

a theoretical framework for dealing with JSON. Therefore
in this paper we propose a formal data model for JSON
documents and, based on the common features present in
available systems using JSON, we define a lightweight query
language allowing us to navigate through JSON documents.
We also introduce a logic capturing the schema proposal
for JSON and study the complexity of basic computational
tasks associated with these two formalisms.
",16.728156217252725,18.00740740740741,109
PODS_17_029.txt,15.010544875559571,13.691379645467752,PODS,13862,"

Query evaluation over probabilistic databases is known to
be intractable in many cases, even in data complexity, i.e.,
when the query is fixed. Although some restrictions of the
queries [20] and instances [4] have been proposed to lower
the complexity, these known tractable cases usually do not
apply to combined complexity, i-e., when the query is not
fixed. This leaves open the question of which query and
instance languages ensure the tractability of probabilistic
query evaluation in combined complexity.

This paper proposes the first general study of the combined
complexity of conjunctive query evaluation on probabilistic
instances over binary signatures, which we can alternatively
phrase as a probabilistic version of the graph homomorphism problem, or of a constraint satisfaction problem (CSP)
variant. We study the complexity of this problem depending on whether instances and queries can use features such
as edge labels, disconnectedness, branching, and edges in
both directions. We show that the complexity landscape is
surprisingly rich, using a variety of technical tools: automatabased compilation to d-DNNF lineages as in [4], 8-acyclic lineages using [11], the X-property for tractable CSP from [25],
graded DAGs [28] and various coding techniques for hardness
proofs.

",19.117987234576393,19.916522842639598,199
PPoPP_17_001.txt,14.716155058845143,12.397638087784845,PPoPP,5820,"
Massively multithreaded GPUs achieve high throughput by
running thousands of threads in parallel. To fully utilize
the hardware, workloads spawn work to the GPU in bulk
by launching large tasks, where each task is a kernel that
contains thousands of threads that occupy the entire GPU.
GPUs face severe underutilization and their performance
benefits vanish if the tasks are narrow, i.e., they contain
< 500 threads. Latency-sensitive applications in network,
signal, and image processing that generate a large number
of tasks with relatively small inputs are examples of such
limited parallelism.
This paper presents Pagoda, a runtime system that virtualizes GPU resources, using an OS-like daemon kernel
called MasterKernel. Tasks are spawned from the CPU
onto Pagoda as they become available, and are scheduled
by the MasterKernel at the warp granularity. Experimental results demonstrate that Pagoda achieves a geometric
mean speedup of 5.70x over PThreads running on a 20-core
CPU, 1.51x over CUDA-HyperQ, and 1.69x over GeMTC,
the state-of-the-art runtime GPU task scheduling system.
",15.903189008614273,14.569891395154553,175
PPoPP_17_002.txt,16.716103519487074,14.929001464916741,PPoPP,11256,"
This paper explores how fork-join parallelism, as supported
by concurrency platforms such as Cilk and OpenMP, can
be embedded into a compiler’s intermediate representation (IR). Mainstream compilers typically treat parallel linguistic constructs as syntactic sugar for function calls into
a parallel runtime. These calls prevent the compiler from
performing optimizations across parallel control constructs.
Remedying this situation is generally thought to require an
extensive reworking of compiler analyses and code transformations to handle parallel semantics.
Tapir is a compiler IR that represents logically parallel
tasks asymmetrically in the program’s control flow graph.
Tapir allows the compiler to optimize across parallel control constructs with only minor changes to its existing analyses and code transformations. To prototype Tapir in the
LLVM compiler, for example, we added or modified about
6000 lines of LLVM’s 4-million-line codebase. Tapir enables LLVM’s existing compiler optimizations for serial
code — including loop-invariant-code motion, commonsubexpression elimination, and tail-recursion elimination —
to work with parallel control constructs such as spawning
and parallel loops. Tapir also supports parallel optimizations
such as loop scheduling
",18.001758247042904,16.679999999999996,182
PPoPP_17_003.txt,15.752011032768014,14.440760471630515,PPoPP,7212,"
Nodes with multiple GPUs are becoming the platform of
choice for high-performance computing. However, most applications are written using bulk-synchronous programming
models, which may not be optimal for irregular algorithms
that benefit from low-latency, asynchronous communication.
This paper proposes constructs for asynchronous multi-GPU
programming, and describes their implementation in a thin
runtime environment called Groute. Groute also implements
common collective operations and distributed work-lists, enabling the development of irregular applications without
substantial programming effort. We demonstrate that this
approach achieves state-of-the-art performance and exhibits
strong scaling for a suite of irregular applications on 8-GPU
and heterogeneous systems, yielding over 7x speedup for
some algorithms.
",18.878054631974784,17.346123893805316,114
PPoPP_17_004.txt,13.79405999875986,11.507524875834175,PPoPP,8993,"
Fault tolerance is increasingly important in high performance
computing due to the substantial growth of system scale and
decreasing system reliability. In-memory/diskless checkpoint
has gained extensive attention as a solution to avoid the
IO bottleneck of traditional disk-based checkpoint methods.
However, applications using previous in-memory checkpoint
suffer from little available memory space. To provide high
reliability, previous in-memory checkpoint methods either
need to keep two copies of checkpoints to tolerate failures
while updating old checkpoints or trade performance for
space by flushing in-memory checkpoints into disk.
In this paper, we propose a novel in-memory checkpoint
method, called self-checkpoint, which can not only achieve
the same reliability of previous in-memory checkpoint methods, but also increase the available memory space for applications by almost 50%. To validate our method, we apply the
self-checkpoint to an important problem, fault tolerant HPL.
We implement a scalable and fault tolerant HPL based on this
new method, called SKT-HPL, and validate it on two largescale systems. Experimental results with 24,576 processes
show that SKT-HPL achieves over 95% of the performance of
the original HPL. Compared to the state-of-the-art in-memory
checkpoint method, it improves the available memory size by
47% and the performance by 5%.
",16.594172100314452,14.85716510903427,216
PPoPP_17_005.txt,15.136064176052162,13.279116528996244,PPoPP,8727,"
A core, but often neglected, aspect of a programming language design is its memory (consistency) model. Sequential consistency (SC) is the most intuitive memory model for
programmers as it guarantees sequential composition of instructions and provides a simple abstraction of shared memory as a single global store with atomic read and writes. Unfortunately, SC is widely considered to be impractical due to
its associated performance overheads.
Perhaps contrary to popular opinion, this paper demonstrates that SC is achievable with acceptable performance
overheads for mainstream languages that minimize mutable
shared heap. In particular, we modify the Glasgow Haskell
Compiler to insert fences on all writes to shared mutable
memory accessed in nonfunctional parts of the program. For
a benchmark suite containing 1,279 programs, SC adds a geomean overhead of less than 0.4% on an x86 machine.
The efficiency of SC arises primarily due to the isolation
provided by the Haskell type system between purely functional and thread-local imperative computations on the one
hand, and imperative computations on the global heap on the
other. We show how to use new programming idioms to further reduce the SC overhead; these create a virtuous cycle of
less overhead and even stronger semantic guarantees (static
data-race freedom)
",18.511140095513987,16.335392156862746,207
PPoPP_17_006.txt,14.471897909822644,12.603028800755432,PPoPP,8091,"
We explore a programming approach for concurrency that
synchronizes all accesses to shared memory by default. Synchronization takes place by ensuring that all program code
runs inside atomic sections even if the program code has external side effects. Threads are mapped to atomic sections
that a programmer must explicitly split to increase concurrency.

A naive implementation of this approach incurs a large
amount of overhead. We show how to reduce this overhead
to make the approach suitable for realistic application programs on existing hardware. We present an implementation
technique based on a special-purpose software transactional
memory system. To reduce the overhead, the technique exploits properties of managed, object-oriented programming
languages as well as intraprocedural static analyses and uses
field-level granularity locking in combination with transactional I/O to provide good scaling properties.

We implemented the synchronized-by-default (SBD) approach for the Java language and evaluate its performance
for six programs from the DaCapo benchmark suite. The
evaluation shows that, compared to explicit synchronization,
the SBD approach has an overhead between 0.4% and 102%
depending on the benchmark and the number of threads,
with a mean (geom.) of 23.9%.
",15.760457277294734,14.547708333333333,195
PPoPP_17_007.txt,16.85894672164988,15.58511228609883,PPoPP,10218,"
Concurrent data structures often provide better performance
on multi-core processors but are significantly more difficult
to design and test than their sequential counterparts. The
C/C++11 standard introduced a weak memory model with
support for low-level atomic operations such as compare
and swap (CAS). While low-level atomic operations can
significantly improve the performance of concurrent data
structures, they introduce non-intuitive behaviors that can
increase the difficulty of developing code.
In this paper, we develop a correctness model for concurrent data structures that make use of atomic operations.
Based on this correctness model, we present CDSSPEC, a
specification checker for concurrent data structures under the
C/C++11 memory model. We have evaluated CDSSPEC on
10 concurrent data structures, among which CDSSPEC detected 3 known bugs and 93% of the injected bugs.
",16.728156217252725,14.834285714285716,137
PPoPP_17_008.txt,13.499894223693222,11.322807898896055,PPoPP,9301,"
The popularity of Non-Uniform Memory Access (NUMA)
architectures has led to numerous locality-preserving hierarchical lock designs, such as HCLH, HMCS, and cohort
locks. Locality-preserving locks trade fairness for higher
throughput. Hence, some instances of acquisitions can incur long latencies, which may be intolerable for certain applications. Few locks admit a waiting thread to abandon its
protocol on a timeout. State-of-the-art abortable locks are
not fully locality aware, introduce high overheads, and unsuitable for frequent aborts. Enhancing locality-aware locks
with lightweight timeout capability is critical for their adoption. In this paper, we design and evaluate the HMCS-T
lock, a Hierarchical MCS (HMCS) lock variant that admits a
timeout. HMCS-T maintains the locality benefits of HMCS
while ensuring aborts to be lightweight. HMCS-T offers
the progress guarantee missing in most abortable queuing
locks. Our evaluations show that HMCS-T offers the timeout feature at a moderate overhead over its HMCS analog.
HMCS-T, used in an MPI runtime lock, mitigated the poor
scalability of an MPI+OpenMP BFS code and resulted in
4.3x superior scaling.


",14.554592549557764,12.417332015810278,185
PPoPP_17_009.txt,15.821756307769352,14.337869742465326,PPoPP,6555,"
This paper presents an algorithm based fault tolerance
method to harden three two-sided matrix factorizations
against soft errors: reduction to Hessenberg form, tridiagonal form, and bidiagonal form. These two sided factorizations are usually the prerequisites to computing eigenvalues/eigenvectors and singular value decomposition.

Algorithm based fault tolerance has been shown to work
on three main one-sided matrix factorizations: LU, Cholesky,
and QR, but extending it to cover two sided factorizations
is non-trivial because there are no obvious offline, problem
specific maintenance of checksums. We thus develop an online, algorithm specific checksum scheme and show how to
systematically adapt the two sided factorization algorithms
used in LAPACK and ScaLAPACK packages to introduce
the algorithm based fault tolerance.

The resulting ABFT scheme can detect and correct arithmetic errors continuously during the factorizations that allow
timely error handling. Detailed analysis and experiments are
conducted to show the cost and the gain in resilience. We
demonstrate that our scheme covers a significant portion of
the operations of the factorizations. Our checksum scheme
achieves high error detection coverage and error correction
coverage compared to the state of the art, with low overhead
and high scalability.

",17.971250198000288,17.113050518134717,194
PPoPP_17_010.txt,13.847337056692123,11.437201281778837,PPoPP,6118,"
Programming languages such as C for CUDA, OpenCL
or ISPC have contributed to increase the programmability
of SIMD accelerators and graphics processing units. However, these languages still lack the flexibility offered by lowlevel SIMD programming on explicit vectors. To close this
expressiveness gap while preserving performance, this paper introduces the notion of Call Re-Vectorization (CREV).
CREV allows changing the dimension of vectorization during the execution of a kernel, exposing it as a nested parallel
kernel call. CREV affords programmability close to dynamic
parallelism, a feature that allows the invocation of kernels
from inside kernels, but at much lower cost. In this paper,
we present a formal semantics of CREV, and an implementation of it on the ISPC compiler. We have used CREV to implement some classic algorithms, including string matching,
depth first search and Bellman-Ford, with minimum effort.
These algorithms, once compiled by ISPC to Intel-based
vector instructions, are as fast as state-of-the-art implementations, yet much simpler. Thus, CREV gives developers the
elegance of dynamic programming, and the performance of
explicit SIMD programming.
",15.760457277294734,13.789013035381753,180
PPoPP_17_011.txt,13.891524459744943,11.616731826006454,PPoPP,8055,"
While hardware transactional memory (HTM) has recently
been adopted to construct efficient concurrent search tree
structures, such designs fail to deliver scalable performance
under contention. In this paper, we first conduct a detailed
analysis on an HTM-based concurrent B+Tree, which uncovers several reasons for excessive HTM aborts induced
by both false and true conflicts under contention. Based
on the analysis, we advocate Eunomia, a design pattern
for search trees which contains several principles to reduce
HTM aborts, including splitting HTM regions with versionbased concurrency control to reduce HTM working sets,
partitioned data layout to reduce false conflicts, proactively
detecting and avoiding true conflicts, and adaptive concurrency control. To validate their effectiveness, we apply
such designs to construct a scalable concurrent B+Tree
using HTM. Evaluation using key-value store benchmarks
on a 20-core HTM-capable multi-core machine shows that
Eunomia leads to 5X-11X speedup under high contention,
while incurring small overhead under low contention.
",19.48791578843652,18.515826086956526,160
PPoPP_17_012.txt,14.088232433530138,12.228321055157174,PPoPP,6430,"
Data-structures can benefit from dynamic data layout modifications when the size or the shape of the data structure
changes during the execution, or when different phases in the
program execute different workloads. However, in a modern
multi-core environment, layout modifications involve costly
synchronization overhead. In this paper we propose a novel
layout lock that incurs a negligible overhead for reads and
a small overhead for updates of the data structure. We then
demonstrate the benefits of layout changes and also the advantages of the layout lock as its supporting synchronization
mechanism for two data structures. In particular, we propose
a concurrent binary search tree, and a concurrent array set,
that benefit from concurrent layout modifications using the
proposed layout lock. Experience demonstrates performance
advantages and integration simplicity
",16.728156217252725,15.937635658914733,130
PPoPP_17_013.txt,15.696621151553188,13.543614943307421,PPoPP,8040,"
In this paper, we present a methodology to understand GPU
microarchitectural features and improve performance for
compute-intensive kernels. The methodology relies on a
reverse engineering approach to crack the GPU ISA encodings in order to build a GPU assembler. An assembly
microbenchmark suite correlates microarchitectural features
with their performance factors to uncover instruction-level
and memory hierarchy preferences. We use SGEMM as a
running example to show the ways to achieve bare-metal
performance tuning. The performance boost is achieved by
tuning FFMA throughput by activating dual-issue, eliminating
register bank conflicts, adding non-FFMA instructions with
little penalty, and choosing proper width of global/shared
load instructions. On NVIDIA Kepler K20m, we develop a
faster SGEMM with 3.1Tflop/s performance and 88% efficiency; the performance is 15% higher than cuBLAS7.0.
Applying these optimizations to convolution, the implementation gains 39%-62% performance improvement compared
with cuDNN4.0. The toolchain is an attempt to automatically crack different GPU ISA encodings and build an assembler adaptively for the purpose of performance enhancements to applications on GPUs.
",16.827784334635936,15.475258620689655,178
PPoPP_17_014.txt,14.14245764478343,11.798793276347837,PPoPP,9454,"
Modern GPUs are broadly adopted in many multitasking environments, including data centers and smartphones. However, the current support for the scheduling of multiple GPU
kernels (from different applications) is limited, forming a
major barrier for GPU to meet many practical needs. This
work for the first time demonstrates that on existing GPUs,
efficient preemptive scheduling of GPU kernels is possible even without special hardware support. Specifically, it
presents EffiSha, a pure software framework that enables
preemptive scheduling of GPU kernels with very low overhead. The enabled preemptive scheduler offers flexible support of kernels of different priorities, and demonstrates significant potential for reducing the average turnaround time
and improving the system overall throughput of programs
that time share a modern GPU.

",19.083932058031824,16.68166666666667,121
PPoPP_17_015.txt,15.021129683784007,13.423801516912011,PPoPP,6971,"
Stencil computations are an important class of compute and
data intensive programs that occur widely in scientific and
engineering applications. A number of tools use sophisticated tiling, parallelization, and memory mapping strategies, and generate code that relies on vendor-supplied compilers. This code has a number of parameters, such as tile
sizes, that are then tuned via empirical exploration.

We develop a model that guides such a choice. Our model
is a simple set of analytical functions that predict the execution time of the generated code. It is deliberately optimistic,
since we are targeting modeling and parameter selections
yielding highly tuned codes.

We experimentally validate the model on a number of 2D
and 3D stencil codes, and show that the root mean square
error in the execution time is less than 10% for the subset
of the codes that achieve performance within 20% of the
best. Furthermore, based on using our model, we are able
to predict tile sizes that achieve a further improvement of
9% on average.
",14.554592549557764,13.320238095238093,169
PPoPP_17_016.txt,16.580529673204424,16.289098837012634,PPoPP,7371,"
We describe a SIMD technique for drawing values from multiple discrete distributions, such as sampling from the random variables of a mixture model, that avoids computing
a complete table of partial sums of the relative probabilities. A table of alternate (“butterfly-patterned”) form is faster
to compute, making better use of coalesced memory accesses; from this table, complete partial sums are computed
on the fly during a binary search. Measurements using CUDA
7.5 on an NVIDIA Titan Black GPU show that this technique makes an entire machine-learning application that uses
a Latent Dirichlet Allocation topic model with 1024 topics about about 13% faster (when using single-precision
floating-point data) or about 35% faster (when using doubleprecision floating-point data) than doing a straightforward
matrix transposition after using coalesced accesses.
",19.287186520377343,22.968244274809162,133
PPoPP_17_017.txt,14.599335908372677,12.802143842353757,PPoPP,9743,"
Modern hardware contains parallel execution resources that
are well-suited for data-parallelism—vector units—and task
parallelism—multicores. However, most work on parallel
scheduling focuses on one type of hardware or the other. In
this work, we present a scheduling framework that allows
for a unified treatment of task- and data-parallelism. Our key
insight is an abstraction, task blocks, that uniformly handles
data-parallel iterations and task-parallel tasks, allowing them
to be scheduled on vector units or executed independently
as multicores. Our framework allows us to define schedulers that can dynamically select between executing taskblocks on vector units or multicores. We show that these
schedulers are asymptotically optimal, and deliver the maximum amount of parallelism available in computation trees.
To evaluate our schedulers, we develop program transformations that can convert mixed data- and task-parallel programs into task block—based programs. Using a prototype
instantiation of our scheduling framework, we show that, on
an 8-core system, we can simultaneously exploit vector and
multicore parallelism to achieve 14x-108x speedup over
sequential baselines.
",16.827784334635936,16.455909090909092,177
PPoPP_17_018.txt,12.97346011930367,11.105721968353311,PPoPP,6385,"
On modern multi-core processors, independent workloads
often interfere with each other by competing for shared
cache space. However, for multi-threaded workloads, where
a single copy of data can be accessed by multiple threads, the
threads can cooperatively share cache. Because data sharing
consolidates the collective working set of threads, the effective size of shared cache becomes larger than it would have
been when data are not shared.

This paper presents a new theory of data sharing. It includes (1) a new metric called the shared footprint to mathematically compute the amount of data shared by any group
of threads in any size cache, and (2) a linear-time algorithm
to measure shared footprint by scanning the memory trace
of a multi-threaded program. The paper presents the practical implementation and evaluates the new theory using 14
PARSEC and SPEC OMP benchmarks, including an example use of shared footprint in program optimization.
",13.5591,13.790294117647061,154
PPoPP_17_019.txt,16.157585126926676,14.817324415874804,PPoPP,8751,"
Finite State Machine (FSM) is the key kernel behind many
popular applications, including regular expression matching, text tokenization, and Huffman decoding. Parallelizing
FSMs is extremely difficult because of the strong dependencies and unpredictable memory accesses. Previous efforts
have largely focused on multi-core parallelization, and used
different approaches, including speculative and enumerative
execution, both of which have been effective but also have
limitations. With increasing width and improving flexibility
in SIMD instruction sets, this paper focuses on combining
SIMD and multi/many-core parallelism for FSMs. We have
developed a novel strategy, called enumerative speculation.
Instead of speculating on a single state as in speculative execution or enumerating all possible states as in enumerative
execution, our strategy speculates transitions from several
possible states, reducing the prediction overheads of speculation approach and the large amount of redundant work
in the enumerative approach. A simple lookback approach
produces a set of guessed states to achieve high speculation
success rates in our enumerative speculation. We evaluate
our method with four popular FSM applications: Huffman
decoding, regular expression matching, HTML tokenization,
and Div7. We obtain up to 2.5x speedup using SIMD on one
core and up to 95x combining SIMD with 60 cores of an
Intel Xeon Phi. On a single core, we outperform the best
single-state speculative execution version by an average of
1.6x, and in combining SIMD and many-core parallelism,
outperform enumerative execution by an average of 2x.
",19.72995580123306,18.86871148459384,241
PPoPP_17_020.txt,12.95672930986047,11.059683427372175,PPoPP,8180,"
Modern big data processing platforms employ huge inmemory key-value (KV) maps. Their applications simultaneously drive high-rate data ingestion and large-scale analytics. These two scenarios expect KV-map implementations
that scale well with both real-time updates and large atomic
scans triggered by range queries.

We present KiWi, the first atomic KV-map to efficiently
support simultaneous large scans and real-time access. The
key to achieving this is treating scans as first class citizens, and organizing the data structure around them. KiWi
provides wait-free scans, whereas its put operations are
lightweight and lock-free. It optimizes memory management jointly with data structure access. We implement KiWi
and compare it to state-of-the-art solutions. Compared to
other KV-maps providing atomic scans, KiWi performs either long scans or concurrent puts an order of magnitude
faster. Its scans are twice as fast as non-atomic ones implemented via iterators in the Java skiplist.

",12.68835289967788,10.67567515923567,158
PPoPP_17_021.txt,15.210204012740999,14.469295977011498,PPoPP,6832,"
Debugging intermittently occurring bugs within MPI applications is challenging, and message races, a condition in
which two or more sends race to match with a receive,
are one of the common root causes. Many debugging tools
have been proposed to help programmers resolve them, but
their runtime interference perturbs the timing such that subtle races often cannot be reproduced with debugging tools.
We present novel noise injection techniques to expose message races even under a tool’s control. We first formalize
this race problem in the context of non-deterministic parallel applications and use this analysis to determine an effective noise-injection strategy to uncover them. We codified these techniques in NINJA (Noise INJection Agent) that
exposes these races without modification to the application.
Our evaluations on synthetic cases as well as a real-world
bug in Hypre-2.10.1 show that NINJA significantly helps expose races.
",16.11434528070225,14.912241379310348,149
PPoPP_17_022.txt,14.515445753878442,12.6010257451502,PPoPP,8663,"
Over the past two decades, many concurrent data structures
have been designed and implemented. Nearly all such work
analyzes concurrent data structures empirically, omitting
asymptotic bounds on their efficiency, partly because of the
complexity of the analysis needed, and partly because of
the difficulty of obtaining relevant asymptotic bounds: when
the analysis takes into account important practical factors,
such as contention, it is difficult or even impossible to prove
desirable bounds.

In this paper, we show that considering structured concurrency or relaxed concurrency models can enable establishing strong bounds, also for contention. To this end, we first
present a dynamic relaxed counter data structure that indicates the non-zero status of the counter. Our data structure
extends a recently proposed data structure, called SNZI, allowing our structure to grow dynamically in response to the
increasing degree of concurrency in the system.

Using the dynamic SNZI data structure, we then present a
concurrent data structure for series-parallel directed acyclic
graphs (sp-dags), a key data structure widely used in the
implementation of modern parallel programming languages.
The key component of sp-dags is an in-counter data structure that is an instance of our dynamic SNZI. We analyze
the efficiency of our concurrent sp-dags and in-counter data
structures under nested-parallel computing paradigm. This
paradigm offers a structured model for concurrency. Under
this model, we prove that our data structures require amortized O(1) shared memory steps, including contention. We
present an implementation and an experimental evaluation
that suggests that the sp-dags data structure is practical and
can perform well in practice.
",16.691746362846608,15.936020583190395,266
PPoPP_17_023.txt,14.473353887995614,12.74578804735416,PPoPP,11017,"
Record-and-replay systems are useful tools for debugging
non-deterministic parallel programs by first recording an
execution and then replaying that execution to produce
the same access pattern. Existing record-and-replay systems generally target thread-based execution models, and
record the behaviors and interleavings of individual threads.
Dynamic multithreaded languages and libraries, such as
the Cilk family, OpenMP, TBB, etc., do not have a notion
of threads. Instead, these languages provide a processoroblivious model of programming, where programs expose
task-parallelism using high-level constructs such as spawn/sync
without regard to the number of threads/cores available to
run the program. Thread-based record-and-replay would violate the processor-oblivious nature of these programs, as
they incorporate the number of threads into the recorded information, constraining the replayed execution to the same
number of threads.
In this paper, we present a processor-oblivious recordand-replay scheme for such languages where record and replay can use different number of processors and both are
scheduled using work stealing. We provide theoretical guarantees for our record and replay scheme — namely that
record is optimal for programs with one lock and replay is
near-optimal for all cases. In addition, we implemented this
scheme in the Cilk Plus runtime system and our evaluation
indicates that processor-obliviousness does not cause substantial overheads.
",17.553077303434723,16.918986486486492,223
PPoPP_17_024.txt,14.472564525433093,12.743782271944927,PPoPP,6276,"
Computed Tomography (CT) Image Reconstruction is an important technique used in a variety of domains, including medical imaging, electron microscopy, non-destructive testing and
transportation security. Model-based Iterative Reconstruction
(MBIR) using Iterative Coordinate Descent (ICD) is a CT algorithm that produces state-of-the-art results in terms of image
quality. However, MBIR is highly computationally intensive
and challenging to parallelize, and has traditionally been viewed
as impractical in applications where reconstruction time is critical. We present the first GPU-based algorithm for ICD-based
MBIR. The algorithm leverages the recently-proposed concept
of SuperVoxels [1], and efficiently exploits the three levels of
parallelism available in MBIR to better utilize the GPU hardware resources. We also explore data layout transformations to
obtain more coalesced accesses and several GPU-specific optimizations for MBIR that boost performance. Across a suite of
3200 test cases, our GPU implementation obtains a geometric
mean speedup of 4.43X over a state-of-the-art multi-core implementation on a 16-core iso-power CPU.
",18.548980349730343,17.495536770921387,171
PPoPP_17_025.txt,16.56147869317822,15.363743866170442,PPoPP,8953,"
The current design trend in large scale machine learning is to use distributed clusters of CPUs and GPUs with
MapReduce-style programming. Some have been led to believe that this type of horizontal scaling can reduce or even
eliminate the need for traditional algorithm development,
careful parallelization, and performance engineering. This
paper is a case study showing the contrary: that the benefits
of algorithms, parallelization, and performance engineering,
can sometimes be so vast that it is possible to solve “clusterscale” problems on a single commodity multicore machine.

Connectomics is an emerging area of neurobiology that
uses cutting edge machine learning and image processing to extract brain connectivity graphs from electron microscopy images. It has long been assumed that the processing of connectomics data will require mass storage, farms of
CPU/GPUs, and will take months (if not years) of processing time. We present a high-throughput connectomics-ondemand system that runs on a multicore machine with less
than 100 cores and extracts connectomes at the terabyte per
hour pace of modern electron microscopes.
",18.243605946275583,17.41338150289017,174
PPoPP_17_026.txt,15.046870037891164,13.662887487590144,PPoPP,9770,"
The four-index integral transform is a fundamental and computationally demanding calculation used in many computational chemistry suites such as NWChem. It transforms
a four-dimensional tensor from one basis to another. This
transformation is most efficiently implemented as a sequence of four tensor contractions that each contract a fourdimensional tensor with a two-dimensional transformation
matrix. Differing degrees of permutation symmetry in the
intermediate and final tensors in the sequence of contractions cause intermediate tensors to be much larger than the
final tensor and limit the number of electronic states in the
modeled systems.

Loop fusion, in conjunction with tiling, can be very effective in reducing the total space requirement, as well as data
movement. However, the large number of possible choices
for loop fusion and tiling, and data/computation distribution
across a parallel system, make it challenging to develop an
optimized parallel implementation for the four-index integral transform. We develop a novel approach to address this
problem, using lower bounds modeling of data movement
complexity. We establish relationships between available aggregate physical memory in a parallel computer system and
ineffective fusion configurations, enabling their pruning and
consequent identification of effective choices and a characterization of optimality criteria. This work has resulted in
the development of a significantly improved implementation
of the four-index transform that enables higher performance
and the ability to model larger electronic systems than the
current implementation in the NWChem quantum chemistry
software suite.
",18.83193753551143,18.803333333333338,241
PPoPP_17_027.txt,15.28255861320866,14.019029132579252,PPoPP,6534,"
Availability of large data sets like ImageNet and massively
parallel computation support in modern HPC devices like
NVIDIA GPUs have fueled a renewed interest in Deep
Learning (DL) algorithms. This has triggered the development of DL frameworks like Caffe, Torch, TensorFlow, and
CNTK. However, most DL frameworks have been limited to
a single node. In order to scale out DL frameworks and bring
HPC capabilities to the DL arena, we propose, S-Caffe; a
scalable and distributed Caffe adaptation for modern multiGPU clusters. With an in-depth analysis of new requirements
brought forward by the DL frameworks and limitations of
current communication runtimes, we present a co-design of
the Caffe framework and the MVAPICH2-GDR MPI runtime. Using the co-design methodology, we modify Caffe’s
workflow to maximize the overlap of computation and communication with multi-stage data propagation and gradient
aggregation schemes. We bring DL-Awareness to the MPI
runtime by proposing a hierarchical reduction design that
benefits from CUDA-Aware features and provides up to a
massive 133x speedup over OpenMPI and 2.6x speedup over
MVAPICH2 for 160 GPUs. S-Caffe successfully scales up to
160 K-80 GPUs for GoogLeNet (ImageNet) with a speedup
of 2.5x over 32 GPUs. To the best of our knowledge, this is
the first framework that scales up to 160 GPUs. Furthermore,
even for single node training, S-Caffe shows an improve
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
",15.438520173184436,13.580385626643295,332
PPoPP_17_028.txt,15.048221429886404,13.241912752804541,PPoPP,7935,"
Task-based programming offers an elegant way to express
units of computation and the dependencies among them,
making it easier to distribute the computational load evenly
across multiple cores. However, this separation of problem
decomposition and parallelism requires a sufficiently large
input problem to achieve satisfactory efficiency on a given
number of cores. Unfortunately, finding a good match between input size and core count usually requires significant
experimentation, which is expensive and sometimes even
impractical. In this paper, we propose an automated empirical method for finding the isoefficiency function of a taskbased program, binding efficiency, core count, and the input
size in one analytical expression. This allows the latter two
to be adjusted according to given (realistic) efficiency objectives. Moreover, we not only find (i) the actual isoefficiency
function but also (ii) the function one would yield if the program execution was free of resource contention and (iii) an
upper bound that could only be reached if the program was
able to maintain its average parallelism throughout its execution. The difference between the three helps to explain
low efficiency, and in particular, it helps to differentiate between resource contention and structural conflicts related to
task dependencies or scheduling. The insights gained can be
used to co-design programs and shared system resources.
",17.971250198000288,17.510094339622643,213
PPoPP_17_029.txt,14.530564147462918,12.712548495385498,PPoPP,7159,"
Semi-structured data emerge in many domains, especially in
web analytics and business intelligence. However, querying
such data is inherently sequential due to the nested structure
of input data. Existing solutions pessimistically enumerate
all execution paths to circumvent dependencies, yielding
sub-optimal performance and limited scalability.
This paper presents GAP, a parallelization scheme that,
for the first time, leverages the grammar of the input data
to boost the parallelization efficiency. GAP leverages static
analysis to infer feasible execution paths for specific contexts based on the grammar of the semi-structured data. It
can eliminate unnecessary paths without compromising the
correctness. In the absence of a pre-defined grammar, GAP
switches into a speculative execution mode and takes potentially incomplete grammar extracted from prior inputs.
Together, the dual-mode GAP reduces the execution paths
from all paths to a minimum, therefore maximizing the parallelization efficiency and scalability. The benefits of path
elimination go beyond reducing extra computation – it also
enables the use of more efficient data structures, which further improves the efficiency. An evaluation on a large set of
standard benchmarks with diverse queries shows that GAP
yields significant efficiency increase and boosts the speedup
of the state-of-the-art from 2.9X to 17.6X on a 20-core machine for a set of 200 queries.
",16.280829582073984,15.736395348837213,218
SC_17_001.txt,14.945682627815877,13.52123132093438,SC,6465,"
This paper presents the first, 15-PetaFLOP Deep Learning system for
solving scientific pattern classification problems on contemporary
HPC architectures. We develop supervised convolutional architectures for discriminating signals in high-energy physics data as well
as semi-supervised architectures for localizing and classifying extreme weather in climate data. Our Intelcaffe-based implementation
obtains ~2TFLOP/s on a single Cori Phase-II Xeon-Phi node. We use
a hybrid strategy employing synchronous node-groups, while using
asynchronous communication across groups. We use this strategy
to scale training of a single model to ~9600 Xeon-Phi nodes; obtaining peak performance of 11.73-15.07 PFLOP/s and sustained
performance of 11.41-13.27 PFLOP/s. At scale, our HEP architecture produces state-of-the-art classification accuracy on a dataset
with 10M images, exceeding that achieved by selections on highlevel physics-motivated features. Our semi-supervised architecture
successfully extracts weather patterns in a 15TB climate dataset.
Our results demonstrate that Deep Learning can be optimized and
scaled effectively on many-core, HPC systems.
",16.373557378465907,15.311162280701755,174
SC_17_002.txt,16.503458190208605,14.546142857142861,SC,9773,"
Achieving high performance on modern systems is challenging.
Even with a detailed profile from a performance tool, writing or
refactoring a program to remove its performance issues is still a
daunting task for application programmers: it demands lots of
program optimization expertise that is often system specific.

Vendors often provide some detailed optimization guides to assist
programmers in the process. However, these guides are frequently
hundreds of pages long, making it difficult for application programmers to master and memorize all the rules and guidelines and
properly apply them to a specific problem instance.

In this work, we develop a framework named Egeria to alleviate
the difficulty. Through Egeria, one can easily construct an advising
tool for a certain high performance computing (HPC) domain (e.g.,
GPU programming) by providing Egeria with a optimization guide
or other related documents for the target domain. An advising
tool produced by Egeria provides a concise list of essential rules
automatically extracted from the documents. At the same time, the
advising tool serves as a question-answer agent that can interactively offers suggestions for specific optimization questions. Egeria
is made possible through a distinctive multi-layered design that
leverages natural language processing techniques and extends them
with knowledge of HPC domains and how to extract information
relevant to code optimization Experiments on CUDA, OpenCL, and
Xeon Phi programming guides demonstrate, both qualitatively and
quantitatively, the usefulness of Egeria for HPC.
",18.243605946275583,16.788461538461537,236
SC_17_003.txt,14.50222179615799,13.041062741158509,SC,9265,"
Key-Value stores provide scalable metadata service for distributed
file systems. However, the metadata’s organization itself, which is
organized using a directory tree structure, does not fit the key-value
access pattern, thereby limiting the performance. To address this
issue, we propose a distributed file system with a loosely-coupled
metadata service, LocoFS, to bridge the performance gap between
file system metadata and key-value stores. LocoFS is designed to
decouple the dependencies between different kinds of metadata
with two techniques. First, LocoFS decouples the directory content
and structure, which organizes file and directory index nodes in a
flat space while reversely indexing the directory entries. Second,
it decouples the file metadata to further improve the key-value
access performance. Evaluations show that LocoFS with eight nodes
boosts the metadata throughput by 5 times, which approaches 93%
throughput of a single-node key-value store, compared to 18% in
the state-of-the-art IndexFS.
",15.151101081350806,14.361843317972351,157
SC_17_004.txt,14.792469616233468,13.235342494858223,SC,7641,"
Data races in multi-threaded parallel applications are notoriously
damaging while extremely difficult to detect. Many tools have been
developed to help programmers find data races. However, there is
no dedicated OpenMP benchmark suite to systematically evaluate
data race detection tools for their strengths and limitations.

In this paper, we present DataRaceBench, an open-source benchmark suite designed to systematically and quantitatively evaluate
the effectiveness of data race detection tools. We focus on data
race detection in programs written in OpenMP, the popular parallel
programming model for multi-threaded applications. In particular,
DataRaceBench includes a set of microbenchmark programs with
or without data races. These microbenchmarks are either manually
written, extracted from real scientific applications, or automatically
generated optimization variants.

We also define several metrics to represent effectiveness and
efficiency of data race detection tools. Using DataRaceBench and
its metrics, we evaluate four different data race detection tools: Helgrind, ThreadSanitizer, Archer, and Intel Inspector. The evaluation
results show that DataRaceBench is effective to provide comparable,
quantitative results and discover strengths and weaknesses of the
tools being evaluated.
",16.647925096878797,16.579666666666665,178
SC_17_005.txt,15.413430172789951,13.9774167020894,SC,10227,"
Data services such as search, discovery, and management in scalable
distributed environments have traditionally been decoupled from
the underlying file systems, and are often deployed using external
databases and indexing services. However, modern data production
rates, looming data movement costs, and the lack of metadata,
entail revisiting the decoupled file system-data services design
philosophy.

In this paper, we present Taglt, a scalable data management
service framework aimed at scientific datasets, which is tightly integrated into a shared-nothing distributed file system. A key feature
of Taglt is a scalable, distributed metadata indexing framework,
using which we implement a flexible tagging capability to support
data discovery. The tags can also be associated with an active operator, for pre-processing, filtering, or automatic metadata extraction,
which we seamlessly offload to file servers in a load-aware fashion.
Our evaluation shows that TagIt can expedite data search by up to
10x over the extant decoupled approach.
",18.7741,18.173246753246755,155
SC_17_006.txt,15.21477862913909,13.954027486408425,SC,9957,"
Deep learning neural networks (DNNs) have been successful in
solving a wide range of machine learning problems. Specialized
hardware accelerators have been proposed to accelerate the execution of DNN algorithms for high-performance and energy efficiency.
Recently, they have been deployed in datacenters (potentially for
business-critical or industrial applications) and safety-critical systems such as self-driving cars. Soft errors caused by high-energy
particles have been increasing in hardware systems, and these can
lead to catastrophic failures in DNN systems. Traditional methods
for building resilient systems, e.g., Triple Modular Redundancy
(TMR), are agnostic of the DNN algorithm and the DNN accelerator’s architecture. Hence, these traditional resilience approaches
incur high overheads, which makes them challenging to deploy. In
this paper, we experimentally evaluate the resilience characteristics of DNN systems (i.e, DNN software running on specialized
accelerators). We find that the error resilience of a DNN system
depends on the data types, values, data reuses, and types of layers
in the design. Based on our observations, we propose two efficient
protection techniques for DNN systems.
",15.616094167265928,14.514476190476191,179
SC_17_007.txt,13.523319448578402,12.00329548028661,SC,9154,"
HPC file systems today work in a best-effort manner where individual applications can flood the file system with requests, effectively
leading to a denial of service for all other tasks. This paper presents
a classful Token Bucket Filter (TBF) policy for the Lustre file system.
The TBF enforces Remote Procedure Call (RPC) rate limitations
based on (potentially complex) Quality of Service (QoS) rules. The
QoS rules are enforced in Lustre’s Object Storage Servers, where
each request is assigned to an automatically created QoS class.
The proposed QoS implementation for Lustre enables various
features for each class including the support for high-priority and
real-time requests even under heavy load and the utilization of spare
bandwidth by less important tasks under light load. The framework
also enables dependent rules to change a job’s RPC rate even at very
small timescales. Furthermore, we propose a Global Rate Limiting
(GRL) algorithm to enforce system-wide RPC rate limitations.
",14.138980108339055,13.450136487716108,160
SC_17_008.txt,14.149388064129422,12.53870998603713,SC,7810,"
Indexing technique has become an efficient tool to enable scientists
to directly access the most relevant data records. But, the time and
space requirements of building and storing indexes are expensive in
the traditional approaches, such as R-tree and bitmaps. Recently, we
started to address this issue by using the idea of ” block index”, and
our previous work has shown promising results from comparing
it against other well-known solutions, including ADIOS, SciDB,
and FastBit. In this work, we further improve the technique from
both theoretical and implementation perspectives. Driven by an
extensive effort in characterizing scientific datasets and modeling
I/O systems, we presented a theoretical model to analyze its query
performance with respect to a given block size configuration. We
also introduced three optimization techniques to achieve a 2.3x
query time reduction comparing to the original implementation.
",17.5058628484301,15.845714285714283,142
SC_17_009.txt,12.35906071565313,10.34298841236015,SC,7408,"
Training neural networks has become a big bottleneck. For example, training ImageNet dataset on one Nvidia K20 GPU needs 21
days. To speed up the training process, the current deep learning
systems heavily rely on the hardware accelerators. However, these
accelerators have limited on-chip memory compared with CPUs.

We use both self-host Intel Knights Landing (KNL) clusters and
multi-GPU clusters as our target platforms. From the algorithm
aspect, we focus on Elastic Averaging SGD (EASGD) to design
algorithms for HPC clusters.

We redesign four efficient algorithms for HPC systems to improve EASGD’s poor scaling on clusters. Async EASGD, Async
MEASGD, and Hogwild EASGD are faster than existing counterpart methods (Async SGD, Async MSGD, and Hogwild SGD) in all
comparisons. Sync EASGD achieves 5.3X speedup over original
EASGD on the same platform. We achieve 91.5% weak scaling efficiency on 4253 KNL cores, which is higher than the state-of-the-art
implementation.
",11.792908689023552,10.18483116883117,158
SC_17_010.txt,14.785772396130504,13.08024167030501,SC,8673,"
We present SIBIA (Scalable Integrated Biophysics-based Image
Analysis), a framework for coupling biophysical models with medical image analysis. It provides solvers for an image-driven inverse
brain tumor growth model and an image registration problem, the
combination of which can eventually help in diagnosis and prognosis of brain tumors. The two main computational kernels of SIBIA
are a Fast Fourier Transformation (FFT) implemented in the library
AccFFT to discretize differential operators, and a cubic interpolation
kernel for semi-Lagrangian based advection. We present efficiency
and scalability results for the computational kernels, the inverse
tumor solver and image registration on two x86 systems, Lonestar 5 at the Texas Advanced Computing Center and Hazel Hen at
the Stuttgart High Performance Computing Center. We showcase
results that demonstrate that our solver can be used to solve registration problems of unprecedented scale, 4096? resulting in ~ 200
billion unknowns—a problem size that is 64x larger than the stateof-the-art. For problem sizes of clinical interest, SIBIA is about 8x
faster than the state-of-the-art.
",16.26309291913925,16.27657142857143,175
SC_17_011.txt,14.982304801563018,13.627454668960155,SC,7859,"
Recently, various applications including data analytics and machine
learning have been developed for geo-distributed cloud data centers. For those applications, the ways to map parallel processes
to physical nodes (i.e., “process mapping”) could significantly impact the performance of the applications because of non-uniform
communication cost in such geo-distributed environments. While
process mapping has been widely studied in grid/cluster environments, few of the existing studies have considered the problem in
geo-distributed cloud environments. In this paper, we propose a
novel model to formulate the geo-distributed process mapping problem and develop a new method to efficiently find the near optimal
solution. Our algorithm considers both the network communication
performance of geo-distributed data centers as well as the communication matrix of the target application. Evaluation results with real
experiments on Amazon EC2 and simulations demonstrate that our
proposal achieves significant performance improvement (50% on
average) compared to the state-of-the-art algorithms.
",19.117987234576393,18.66595541401274,159
SC_17_012.txt,16.812767742650237,14.79061891294958,SC,8419,"
Sympiler is a domain-specific code generator that optimizes sparse
matrix computations by decoupling the symbolic analysis phase
from the numerical manipulation stage in sparse codes. The computation patterns in sparse numerical methods are guided by the
input sparsity structure and the sparse algorithm itself. In many
real-world simulations, the sparsity pattern changes little or not at
all. Sympiler takes advantage of these properties to symbolically
analyze sparse codes at compile time and to apply inspector-guided
transformations that enable applying low-level transformations to
sparse codes. As a result, the Sympiler-generated code outperforms
highly-optimized matrix factorization codes from commonly-used
specialized libraries, obtaining average speedups over Eigen and
CHOLMOD of 3.8x and 1.5x respectively.
",18.669449996058646,17.114,121
SC_17_013.txt,16.65492060839322,15.115198487712671,SC,7908,"
We present control replication, a technique for generating highperformance and scalable SPMD code from implicitly parallel programs. In contrast to traditional parallel programming models that
require the programmer to explicitly manage threads and the communication and synchronization between them, implicitly parallel
programs have sequential execution semantics and by their nature
avoid the pitfalls of explicitly parallel programming. However, without optimizations to distribute control overhead, scalability is often
poor.

Performance on distributed-memory machines is especially sensitive to communication and synchronization in the program, and
thus optimizations for these machines require an intimate understanding of a program’s memory accesses. Control replication
achieves particularly effective and predictable results by leveraging language support for first-class data partitioning in the source
programming model. We evaluate an implementation of control
replication for Regent and show that it achieves up to 99% parallel
efficiency at 1024 nodes with absolute performance comparable to
hand-written MPI(+X) codes.
",20.425298281703412,19.728856209150326,154
SC_17_014.txt,16.791371229503344,14.973457296949206,SC,7206,"
The nature of dark energy and the complete theory of gravity
are two central questions currently facing cosmology. A vital tool
for addressing them is the 3-point correlation function (3PCF),
which probes deviations from a spatially random distribution of
galaxies. However, the 3PCF’s formidable computational expense
has prevented its application to astronomical surveys comprising millions to billions of galaxies. We present Galactos, a highperformance implementation of a novel, O(N?) algorithm that uses
a load-balanced k-d tree and spherical harmonic expansions to
compute the anisotropic 3PCF. Our implementation is optimized
for the Intel Xeon Phi architecture, exploiting SIMD parallelism,
instruction and thread concurrency, and significant L1 and L2 cache
reuse, reaching 39% of peak performance on a single node. Galactos
scales to the full Cori system, achieving 9.8 PF (peak) and 5.06 PF
(sustained) across 9636 nodes, making the 3PCF easily computable
for all galaxies in the observable universe.
",18.243605946275583,16.880789473684214,156
SC_17_015.txt,15.788277327023067,14.396353796400188,SC,7760,"
Recent advances in hardware, such as systems with multiple GPUs
and their availability in the cloud, are enabling deep learning in
various domains including health care, autonomous vehicles, and Internet of Things. Multi-GPU systems exhibit complex connectivity
among GPUs and between GPUs and CPUs. Workload schedulers
must consider hardware topology and workload communication requirements in order to allocate CPU and GPU resources for optimal
execution time and improved utilization in shared cloud environments.

This paper presents a new topology-aware workload placement
strategy to schedule deep learning jobs on multi-GPU systems. The
placement strategy is evaluated with a prototype on a Power8 machine with Tesla P100 cards, showing speedups of up to ~1.30x
compared to state-of-the-art strategies; the proposed algorithm
achieves this result by allocating GPUs that satisfy workload requirements while preventing interference. Additionally, a largescale simulation shows that the proposed strategy provides higher
resource utilization and performance in cloud systems.
",18.062587368997235,17.53856687898089,158
SC_17_016.txt,15.621372964444134,13.642956780923999,SC,7105,"
We present a high-resolution simulation of the 2004 Sumatra-Andaman

earthquake, including non-linear frictional failure on a megathrustsplay fault system. Our method exploits unstructured meshes capturing the complicated geometries in subduction zones that are
crucial to understand large earthquakes and tsunami generation.
These up-to-date largest and longest dynamic rupture simulations
enable analysis of dynamic source effects on the seafloor displacements.

To tackle the extreme size of this scenario an end-to-end optimization of the simulation code SeisSol was necessary. We implemented a new cache-aware wave propagation scheme and optimized the dynamic rupture kernels using code generation. We
established a novel clustered local-time-stepping scheme for dynamic rupture. In total, we achieved a speed-up of 13.6 compared
to the previous implementation. For the Sumatra scenario with 221
million elements this reduced the time-to-solution to 13.9 hours on
86,016 Haswell cores. Furthermore, we used asynchronous output
to overlap I/O and compute time.
",15.470042427545799,13.5208281573499,165
SC_17_017.txt,15.300915558654445,14.234728217348735,SC,7207,"
The Geometric Multigrid (GMG) method is widely used in numerical analysis to accelerate the convergence of partial differential
equations solvers using a hierarchy of grid discretizations. Multiple grid sizes and recursive expression of multigrid cycles make
the task of program optimization tedious. A high-level language
that aids domain experts for GMG with effective optimization and
parallelization support is thus valuable.

We demonstrate how high performance can be achieved along
with enhanced programmability for GMG, with new language/optimization support in the PolyMage DSL framework. We compare
our approach with (a) hand-optimized code, (b) hand-optimized
code in conjunction with polyhedral optimization techniques, and
(c) the existing PolyMage optimizer adapted to multigrid. We use
benchmarks varying in multigrid cycle structure and smoothing
steps for evaluation. On a 24-core Intel Xeon Haswell multicore
system, our automatically optimized codes achieve a mean improvement of 3.2x over straightforward parallelization, and 1.31x over
the PolyMage optimizer.
",17.77360955136429,17.43285714285714,157
SC_17_018.txt,15.862463860305688,14.371287396111551,SC,9242,"
In this paper we propose DRAFTS — a methodology for implementing probabilistic guarantees of instance reliability in the Amazon
Spot tier. Amazon offers “unreliable” virtual machine instances
(ones that may be terminated at any time) at a potentially large discount relative to “reliable” On-demand and Reserved instances. Our
method predicts the “bid values” that users can specify to provision
Spot instances which ensure at least a fixed duration of execution
with a given probability. We illustrate the method and test its validity using Spot pricing data post facto, both randomly and using
real-world workload traces. We also test the efficacy of the method
experimentally by using it to launch Spot instances and then observing the instance termination rate. Our results indicate that it
is possible to obtain the same level of reliability from unreliable
instances that the Amazon service level agreement guarantees for
reliable instances with a greatly reduced cost.
",17.693802365651003,17.121688741721858,152
SC_17_019.txt,14.597251662043927,12.827994126650129,SC,8151,"
We present an in-depth analysis of transient faults effects on HPC
applications in Intel Xeon Phi processors based on radiation experiments and high-level fault injection. Besides measuring the realistic
error rates of Xeon Phi, we quantify Silent Data Corruption (SDCs)
by correlating the distribution of corrupted elements in the output to the application’s characteristics. We evaluate the benefits
of imprecise computing for reducing the programs’ error rate. For
example, for HotSpot a 0.5% tolerance in the output value reduces
the error rate by 85%.

We inject different fault models to analyze the sensitivity of
given applications. We show that portions of applications can be
graded by different criticalities. For example, faults occurring in
the middle of LUD execution, or in the Sort and Tree portions of
CLAMR, are more critical than the remaining portions. Mitigation
techniques can then be relaxed or hardened based on the criticality
of the particular portions.
",15.742502247213078,13.556842105263158,155
SC_17_020.txt,15.961680142587927,14.441305120855404,SC,9349,"
GPUs are widespread across clusters of compute nodes due to their
attractive performance for data parallel codes. However, communicating between GPUs across the cluster is cumbersome when
compared to CPU networking implementations. A number of recent
works have enabled GPUs to more naturally access the network,
but suffer from performance problems, require hidden CPU helper
threads, or restrict communications to kernel boundaries.
In this paper, we propose GPU Triggered Networking, a novel,
GPU-centric networking approach which leverages the best of
CPUs and GPUs. In this model, CPUs create and stage network
messages and GPUs trigger the network interface when data is
ready to send. GPU Triggered Networking decouples these two
operations, thereby removing the CPU from the critical path. We
illustrate how this approach can provide up to 25% speedup compared to standard GPU networking across microbenchmarks, a
Jacobi stencil, an important MPI collective operation, and machinelearning workloads.
",15.903189008614273,14.410757430488975,150
SC_17_021.txt,17.422791921872264,16.59435897435898,SC,8277,"
Memory accesses limit the performance and scalability of countless
applications. Many design and optimization efforts will benefit
from an in-depth understanding of memory access behavior, which
is not offered by extant access tracing and profiling methods.

In this paper, we adopt a holistic memory access profiling approach to enable a better understanding of program-system memory
interactions. We have developed a two-pass tool adopting fast online and slow offline profiling, with which we have profiled, at the
variable/object level, a collection of 38 representative applications
spanning major domains (HPC, personal computing, data analytics, AI, graph processing, and datacenter workloads), at varying
problem sizes. We have performed detailed result analysis and code
examination. Our findings provide new insights into application
memory behavior, including insights on per-object access patterns,
adoption of data structures, and memory-access changes at different problem sizes. We find that scientific computation applications
exhibit distinct behaviors compared to datacenter workloads, motivating separate memory system design/optimizations.
",18.3970566412798,18.201146384479717,163
SC_17_022.txt,14.402658529488736,12.541983525483893,SC,10260,"
High-bandwidth On-Package Memory (OPM) innovates the conventional memory hierarchy by augmenting a new on-package layer
between classic on-chip cache and off-chip DRAM. Due to its relative location and capacity, OPM is often used as a new type of LLC.
Despite the adaptation in modern processors, the performance and.
power impact of OPM on HPC applications, especially scientific
kernels, is still unknown. In this paper, we fill this gap by conducting
a comprehensive evaluation for a wide spectrum of scientific kernels
with a large amount of representative inputs, including dense, sparse
and medium, on two Intel OPMs: eDRAM on multicore Broadwell
and MCDRAM on manycore Knights Landing. Guided by our general optimization models, we demonstrate OPM’s effectiveness for
easing programmers’ tuning efforts to reach ideal throughput for
both compute-bound and memory-bound applications.
",17.58133193835471,16.636318840579708,140
SC_17_023.txt,14.80605745443436,12.643154245746071,SC,7156,"
Distributed systems incorporate GPUs because they provide
massive parallelism in an energy-efficient manner. Unfortunately,
existing programming models make it difficult to route a GPUinitiated network message. The traditional coprocessor model
forces programmers to manually route messages through the host
CPU. Other models allow GPU-initiated communication, but are
inefficient for small messages.

To enable fine-grain PGAS-style communication between threads
executing on different GPUs, we introduce Gravel. GPU-initiated
messages are offloaded through a GPU-efficient concurrent queue
to an aggregator (implemented with CPU threads), which
combines messages targeting to the same destination. Gravel
leverages diverged work-group-level semantics to amortize
synchronization across the GPU’s data-parallel lanes.

Using Gravel, we can distribute six applications, each with
frequent small messages, across a cluster of eight GPU-accelerated
nodes. Compared to one node, these applications run 5.3x faster,
on average. Furthermore, we show Gravel is more programmable
and usually performs better than prior GPU networking models.
",16.03029750255766,14.844151898734179,161
SC_17_024.txt,16.051442587075424,14.311408755622626,SC,5886,"
Compiler-based fault injection (FI) has become a popular technique
for resilience studies to understand the impact of soft errors in
supercomputing systems. Compiler-based FI frameworks inject
faults at a high intermediate-representation level. However, they
are less accurate than machine code, binary-level FI because they
lack access to all dynamic instructions, thus they fail to mimic
certain fault manifestations. In this paper, we study the limitations
of current practices in compiler-based FI and how they impact the
interpretation of results in resilience studies.

We propose REFINE, a novel framework that addresses these limitations, performing FI in a compiler backend. Our approach provides
the portability and efficiency of compiler-based FI, while keeping
accuracy comparable to binary-level FI methods. We demonstrate
our approach in 14 HPC programs and show that, due to our unique
design, its runtime overhead is significantly smaller than state-ofthe-art compiler-based FI frameworks, reducing the time for large
FI experiments.
",16.26309291913925,15.154285714285717,161
SC_17_025.txt,14.139479456599208,12.202499251818377,SC,8749,"
While many algorithm-based fault tolerance (ABFT) schemes have
been proposed to detect soft errors offline in the fast Fourier transform (FFT) after computation finishes, none of the existing ABFT
schemes detect soft errors online before the computation finishes.
This paper presents an online ABFT scheme for FFT so that soft
errors can be detected online and the corrupted computation can
be terminated in a much more timely manner. We also extend our
scheme to tolerate both arithmetic errors and memory errors, develop strategies to reduce its fault tolerance overhead and improve
its numerical stability and fault coverage, and finally incorporate
it into the widely used FFTW library - one of the today’s fastest
FFT software implementations. Experimental results demonstrate
that: (1) the proposed online ABFT scheme introduces much lower
overhead than the existing offline ABFT schemes; (2) it detects
errors in a much more timely manner; and (3) it also has higher
numerical stability and better fault coverage.
",19.78447435784618,21.025126582278485,160
SC_17_026.txt,14.540751850897909,12.913275878705736,SC,8598,"
Octree-based mesh adaptation has enabled simulations of complex
physical phenomena. Existing meshing algorithms were proposed
with the assumption that computer memory is volatile. Consequently,
for failure recovery, the in-core algorithms need to save memory
states as snapshots with slow file I/Os. The out-of-core algorithms
store octants on disks for persistence. However, neither of them
was designed to leverage unique characteristics of non-volatile byteaddressable memory (NVBM). In this paper, we propose a novel data
structure Persistent Merged octree (PM-octree) for both meshing
and in-memory storage of persistent octrees using NVBM. It is a
multi-version data structure and can recover from failures using its
earlier persistent version stored in NVBM. In addition, we design a
feature-directed sampling approach to help dynamically transform
the PM-octree layout for reducing NVBM-induced memory write
latency. PM-octree has been successfully integrated with Gerris
software for simulation of fluid dynamics. Our experimental results
with real-world scientific workloads show that PM-octree scales
up to 1.1 billion mesh elements with 1000 processors on the Titan
supercomputer.
",15.112257680678326,13.569552486187849,183
SC_17_027.txt,16.41033367825084,14.948950559456478,SC,9930,"
3D-stacked memory devices with processing logic can help alleviate
the memory bandwidth bottleneck in GPUs. However, in order
for such Near-Data Processing (NDP) memory stacks to be used
for different GPU architectures, it is desirable to standardize the
NDP architecture. Our proposal enables this standardization by
allowing data to be spread across multiple memory stacks as is
the norm in high-performance systems without an MMU on the
NDP stack. The keys to this architecture are the ability to move
data between memory stacks as required for computation, and a
partitioned execution mechanism that offloads memory-intensive
application segments onto the NDP stack and decouples address
translation from DRAM accesses. By enhancing this system with a
smart offload selection mechanism that is cognizant of the compute
capability of the NDP and cache locality on the host processor,
system performance and energy are improved by up to 66.8% and
37.6%, respectively.
",20.267338824336647,18.00284210526316,155
SC_17_028.txt,16.57935236566214,15.764198527623943,SC,7293,"
Tuning application parameters for optimal performance is a challenging combinatorial problem. Hence, techniques for modeling
the functional relationships between various input features in the
parameter space and application performance are important. We
show that simple statistical inference techniques are inadequate to
capture these relationships. Even with more complex ensembles
of models, the minimum coverage of the parameter space required
via experimental observations is still quite large. We propose a
deep learning based approach that can combine information from
exhaustive observations collected at a smaller scale with limited
observations collected at a larger target scale. The proposed approach is able to accurately predict performance in the regimes of
interest to performance analysts while outperforming many traditional techniques. In particular, our approach can identify the best
performing configurations even when trained using as few as 1%
of observations at the target scale.
",17.451712890111917,16.231428571428573,141
SC_17_029.txt,14.467831090283081,12.717820661534365,SC,6424,"
Supercomputing centers are seeing increasing demand for userdefined software stacks (UDSS), instead of or in addition to the stack
provided by the center. These UDSS support user needs such as
complex dependencies or build requirements, externally required
configurations, portability, and consistency. The challenge for centers is to provide these services in a usable manner while minimizing the risks: security, support burden, missing functionality, and
performance. We present Charliecloud, which uses the Linux user
and mount namespaces to run industry-standard Docker containers with no privileged operations or daemons on center resources.
Our simple approach avoids most security risks while maintaining access to the performance and functionality already on offer,
doing so in just 800 lines of code. Charliecloud promises to bring
an industry-standard UDSS user workflow to existing, minimally
altered HPC resources.
",17.122413403193683,16.21139097744361,134
SC_17_030.txt,18.092090500890176,16.504742355181786,SC,6242,"
Dynamic scheduling of tasks in large-scale HPC platforms is normally accomplished using ad-hoc heuristics, based on task characteristics, combined with some backfilling strategy. Defining heuristics that work efficiently in different scenarios is a difficult task,
specially when considering the large variety of task types and platform architectures. In this work, we present a methodology based
on simulation and machine learning to obtain dynamic scheduling
policies. Using simulations and a workload generation model, we
can determine the characteristics of tasks that lead to a reduction
in the mean slowdown of tasks in an execution queue. Modeling
these characteristics using a nonlinear function and applying this
function to select the next task to execute in a queue improved the
mean task slowdown in synthetic workloads. When applied to real
workload traces from highly different machines, these functions still
resulted in performance improvements, attesting the generalization
capability of the obtained heuristics.
",18.422482065455632,16.580000000000002,151
SC_17_031.txt,13.27553156708084,10.703974854932301,SC,9838,"
Representative paths analysis generalizes and improves MPI critical
path analysis. To improve diagnostic insight, we sample the distribution of program path costs and retain k representative paths. We
describe scalable algorithms to collect representative paths and path
profiles. To collect full paths efficiently, we introduce path pruning
that reduces permanent space requirements from a trace (proportional to ranks and MPI events) to path length (the minimum). To
make space requirements independent of ranks and events — even
a small constant in practice — we profile program paths. Avoiding
the limitations of prior path profiling approaches, we dynamically
discover tasks and attribute costs in high resolution. We evaluate
our algorithms on seven applications scaled up to 7000 MPI ranks.
Full program paths use as little as 0.01% the permanent space of
current methods; profiles require a nearly constant 100-1000 KB.
Execution overhead is under 5% when synchronization intervals
are sufficiently large (a few milliseconds).
",15.021129683784007,13.199298245614035,154
SC_17_032.txt,14.78341845414846,14.007120486372255,SC,5716,"
Near-term quantum computers will soon reach sizes that are challenging to directly simulate, even when employing the most powerful supercomputers. Yet, the ability to simulate these early devices
using classical computers is crucial for calibration, validation, and
benchmarking. In order to make use of the full potential of systems featuring multi- and many-core processors, we use automatic
code generation and optimization of compute kernels, which also
enables performance portability. We apply a scheduling algorithm
to quantum supremacy circuits in order to reduce the required
communication and simulate a 45-qubit circuit on the Cori II supercomputer using 8, 192 nodes and 0.5 petabytes of memory. To our
knowledge, this constitutes the largest quantum circuit simulation
to this date. Our highly-tuned kernels in combination with the
reduced communication requirements allow an improvement in
time-to-solution over state-of-the-art simulations by more than an
order of magnitude at every scale.
",18.422482065455632,17.177142857142858,156
SC_17_033.txt,17.559360892189947,16.095623501199043,SC,8314,"
Modern HPC centers comprise clusters, storage, networks, power
and cooling infrastructure, and more. Analyzing the efficiency
of these complex facilities is a daunting task. Increasingly, facilities deploy sensors and monitoring tools, but with millions of
instrumented components, analyzing collected data manually is
intractable. Data from an HPC center comprises different formats,
granularities, and semantics, and handwritten scripts no longer
suffice to transform the data into a digestible form.

We present ScrubJay, an intuitive, scalable framework for automatic analysis of disparate HPC data. ScrubJay decouples the task
of specifying data relationships from the task of analyzing data.
Domain experts can store reusable transformations that describe
relations between domains. ScrubJay also automates performance
analysis. Analysts provide a query over logical domains of interest, and ScrubJay automatically derives needed steps to transform
raw measurements. ScrubJay makes large-scale analysis tractable,
reproducible, and provides insights into HPC facilities.
",14.975302809339372,15.182944444444448,145
SC_17_034.txt,14.885188564557978,14.11093695948085,SC,8772,"
Multi-factor authentication (MFA) is rapidly becoming the de facto
standard for access to all computing, whether via web, phone, or
direct command-line access. HPC centers and other institutions
supporting hundreds or thousands of users face challenging cost,
licensing, user support, and infrastructure deployment decisions
when considering a transition to MFA at scale.

This paper describes our experiences and lessons learned throughout the assessment, planning, and phased deployment of MFA across
production systems supporting more than 10,000 accounts. It focuses on the ultimate curation, creation, and integration of a multitude of software components, some developed in-house and built to
be compatible within existing HPC environments, and all of which
are freely available for open source distribution. We motivate the
development of this customized infrastructure by highlighting some
of the particular needs of our research community. What follows
is an information resource for others when considering their own
MFA deployments.
",18.062587368997235,16.574668874172186,153
SC_17_035.txt,15.420025036015957,13.623773947561428,SC,8556,"
QMCPACK has enabled cutting-edge materials research on
supercomputers for over a decade. It scales nearly ideally
but has low single-node efficiency due to the physics-based
abstractions using array-of-structures objects, causing inefficient vectorization. We present a systematic approach
to transform QMCPACK to better exploit the new hardware features of modern CPUs in portable and maintainable
ways. We develop miniapps for fast prototyping and optimizations. We implement new containers in structure-of-arrays
data layout to facilitate vectorizations by the compilers. Further speedup and smaller memory-footprints are obtained by
computing data on the fly with the vectorized routines and
expanding single-precision use. All these are seamlessly incorporated in production QMCPACK. We demonstrate upto
4.5x speedups on recent Intel® processors and IBM Blue
Gene/Q for representative workloads. Energy consumption
is reduced significantly commensurate to the speedup factor.
Memory-footprints are reduced by up-to 3.8x, opening the
possibility to solve much larger problems of future.
",14.265292616868656,13.26291304347826,163
SC_17_036.txt,15.339488635297403,14.325307463900852,SC,7308,"
Resilience is one of the key challenges in maintaining high efficiency
of future extreme scale supercomputers. Researchers and system
practitioners rely on field-data studies to understand reliability
characteristics and plan for future HPC systems. In this work, we
compare and contrast the reliability characteristics of multiple largescale HPC production systems. Our study covers more than one
billion compute node hours across five different systems over a
period of 8 years. We confirm previous findings which continue to
be valid, discover new findings, and discuss their implications.
",14.836745963215662,14.389103448275865,88
SC_17_037.txt,15.653820939392837,13.828205846636184,SC,5570,"
There is a consensus that exascale systems should operate within a
power envelope of 20MW. Consequently, energy conservation is
still considered as the most crucial constraint if such systems are
to be realized.

So far, most research on this topic focused on strategies such as
power capping and dynamic power management. Although these
approaches can reduce power consumption, we believe that they
might not be sufficient to reach the exascale energy-efficiency goals.
Hence, we aim to adopt techniques from embedded systems, where
energy-efficiency has always been the fundamental objective.

A successful energy-saving technique used in embedded systems is to integrate fine-grained autotuning with dynamic voltage
and frequency scaling. In this paper, we apply a similar technique
to a real-world HPC application. Our experimental results on a
HPC cluster indicate that such an approach saves up to 20% of
energy compared to the baseline configuration, with negligible
performance loss.
",15.579741850924794,13.232148692810458,154
SC_17_038.txt,14.114002881033244,12.171318661192519,SC,6155,"
Modern OpenMP threading techniques are used to convert the
MPI-only Hartree-Fock code in the GAMESS program to a hybrid
MPI/OpenMP algorithm. Two separate implementations that differ
by the sharing or replication of key data structures among threads
are considered, density and Fock matrices. All implementations are
benchmarked on a super-computer of 3,000 Intel® Xeon Phi™ processors. With 64 cores per processor, scaling numbers are reported
on up to 192,000 cores. The hybrid MPI/OpenMP implementation
reduces the memory footprint by approximately 200 times compared to the legacy code. The MPI/OpenMP code was shown to
run up to six times faster than the original for a range of molecular
system sizes.
",13.5591,11.883376068376073,118
SC_17_039.txt,14.159673229024389,12.9752918241966,SC,7904,"
Parameter tuning is an important task of storage performance
optimization. Current practice usually involves numerous tweakbenchmark cycles that are slow and costly. To address this issue,
we developed CAPES, a model-less deep reinforcement learningbased unsupervised parameter tuning system driven by a deep
neural network (DNN). It is designed to find the optimal values
of tunable parameters in computer systems, from a simple clientserver system to a large data center, where human tuning can be
costly and often cannot achieve optimal performance. CAPES takes
periodic measurements of a target computer system’s state, and
trains a DNN which uses Q-learning to suggest changes to the
system’s current parameter values. CAPES is minimally intrusive,
and can be deployed into a production system to collect training
data and suggest tuning actions during the system’s daily operation.
Evaluation of a prototype on a Lustre file system demonstrates an
increase in I/O throughput up to 45% at saturation point.
",15.151101081350806,14.810402930402933,160
SC_17_040.txt,15.114245431820919,13.356199865879137,SC,9052,"
In this paper, we describe the GUIDE framework used to collect,
federate, and analyze log data from the Oak Ridge Leadership Computing Facility (OLCF), and how we use that data to derive insights
into facility operations. We collect system logs and extract monitoring data at every level of the various OLCF subsystems, and
have developed a suite of pre-processing tools to make the raw
data consumable. The cleansed logs are then ingested and federated
into a central, scalable data warehouse, Splunk, that offers storage,
indexing, querying, and visualization capabilities. We have further
developed and deployed a set of tools to analyze these multiple
disparate log streams in concert and derive operational insights. We
describe our experience from developing and deploying the GUIDE
infrastructure, and deriving valuable insights on the various subsystems, based on two years of operations in the production OLCF
environment.
",18.458006810337128,17.183580419580426,144
SC_17_041.txt,15.156340567797571,13.429475742463591,SC,5475,"
Complex band structures (CBSs) are useful to characterize the static
and dynamical electronic properties of materials. Despite the intensive developments, the first-principles calculation of CBS for
over several hundred atoms are still computationally demanding.
We here propose an efficient and scalable computational method to
calculate CBSs. The basic idea is to express the Kohn-Sham equation of the real-space grid scheme as a quadratic eigenvalue problem and compute only the solutions which are necessary to construct the CBS by Sakurai-Sugiura method. The serial performance
of the proposed method shows a significant advantage in both runtime and memory usage compared to the conventional method.
Furthermore, owing to the hierarchical parallelism in Sakurai-Sugiura
method and the domain-decomposition technique for real-space
grids, we can achieve an excellent scalability in the CBS calculation
of a boron and nitrogen doped carbon nanotube consisting of more
than 10,000 atoms using 2,048 nodes (139,264 cores) of OakforestPACS.
",19.117987234576393,17.69615384615385,160
SC_17_042.txt,15.101358944743886,13.932864336759668,SC,8165,"
The Oak Ridge Leadership Computing Facility (OLCF) runs the No.
4 supercomputer in the world, supported by a petascale file system, to facilitate scientific discovery. In this paper, using the daily
file system metadata snapshots collected over 500 days, we have
studied the behavioral trends of 1,362 active users and 380 projects
across 35 science domains. In particular, we have analyzed both individual and collective behavior of users and projects, highlighting
needs from individual communities and the overall requirements
to operate the file system. We have analyzed the metadata across
three dimensions, namely (i) the projects’ file generation and usage
trends, using quantitative file system-centric metrics, (ii) scientific
user behavior on the file system, and (iii) the data sharing trends of
users and projects. To the best of our knowledge, our work is the
first of its kind to provide comprehensive insights on user behavior from multiple science domains through metadata analysis of
a large-scale shared file system. We envision that this OLCF case
study will provide valuable insights for the design, operation, and
management of storage systems at scale, and also encourage other
HPC centers to undertake similar such efforts.
",18.243605946275583,18.353963730569948,195
SC_17_043.txt,14.505688804408877,12.428814867949551,SC,8573,"
Optimizing communication performance is imperative for largescale computing because communication overheads limit the strong
scalability of parallel applications. Today’s network cards contain
rather powerful processors optimized for data movement. However,
these devices are limited to fixed functions, such as remote direct
memory access. We develop sPIN, a portable programming model to
offload simple packet processing functions to the network card. To
demonstrate the potential of the model, we design a cycle-accurate
simulation environment by combining the network simulator LogGOPSim and the CPU simulator gem5. We implement offloaded
message matching, datatype processing, and collective communications and demonstrate transparent full-application speedups. Furthermore, we show how sPIN can be used to accelerate redundant
in-memory filesystems and several other use cases. Our work investigates a portable packet-processing network acceleration model
similar to compute acceleration with CUDA or OpenCL. We show
how such network acceleration enables an eco-system that can
significantly speed up applications and system services.
",17.631426480028413,16.917791932059448,159
SC_17_044.txt,16.16378077626718,14.911726805961639,SC,7357,"
The fat-tree topology is one of the most commonly used network
topologies in HPC systems. Vendors support several options that
can be configured when deploying fat-tree networks on production systems, such as link bandwidth, number of rails, number of
planes, and tapering. This paper showcases the use of simulations
to compare the impact of these design options on representative
production HPC applications, libraries, and multi-job workloads.
We present advances in the TraceR-CODES simulation framework
that enable this analysis and evaluate its prediction accuracy against
experiments on a production fat-tree network. In order to understand the impact of different network configurations on various
anticipated scenarios, we study workloads with different communication patterns, computation-to-communication ratios, and scaling
characteristics. Using multi-job workloads, we also study the impact of inter-job interference on performance and compare the
cost-performance tradeoffs.
",17.5058628484301,16.878333333333334,145
SC_17_045.txt,14.835408939747222,13.094564459063694,SC,8872,"
Betweenness centrality (BC) is a crucial graph problem that measures the significance of a vertex by the number of shortest paths
leading through it. We propose Maximal Frontier Betweenness Centrality (MFBC): a succinct BC algorithm based on novel sparse matrix multiplication routines that performs a factor of p/3 less communication on p processors than the best known alternatives, for
graphs with n vertices and average degree k = n/p?/>, We formulate,
implement, and prove the correctness of MFBC for weighted graphs
by leveraging monoids instead of semirings, which enables a surprisingly succinct formulation. MFBC scales well for both extremely
sparse and relatively dense graphs. It automatically searches a space
of distributed data decompositions and sparse matrix multiplication
algorithms for the most advantageous configuration. The MFBC implementation outperforms the well-known CombBLAS library by
up to 8x and shows more robust performance. Our design methodology is readily extensible to other graph problems.
",16.26309291913925,14.98090909090909,153
SC_17_046.txt,15.677081070569415,13.8012306127638,SC,7808,"
The OpenFlow-style Software Defined Networking (SDN) technology has shown promising performance in data centers and campus networks; and the HPC community is significantly interested
in adopting the SDN technology. However, while OpenFlow-style
SDN allows dynamic per-flow resource management using a global
network view, it does not support adaptive routing, which is widely
used in HPC systems. This gives rise to the question whether SDN
can achieve the performance that HPC systems expect with adaptive routing. In this work, we investigate possible methods to apply
the SDN technology on the current generation HPC interconnects
with the Dragonfly topology, and compare the performance of SDN
with that of adaptive routing. Our results indicate that adaptive
routing results in higher performance than SDN when both have
similar resource allocation for a given traffic condition. However,
SDN can use the global network view to compete with adaptive
routing by allocating network resources more effectively.
",18.7741,17.183758169934638,154
SC_17_047.txt,15.169918683476023,13.19001090201969,SC,5856,"
We present a new algorithm, the Distributed Southwell method,
as a competitor to Block Jacobi for preconditioning and multigrid
smoothing. It is based on the Southwell iterative method, which
is sequential, where only the equation with the largest residual is
relaxed per iteration. The Parallel Southwell method extends this
idea by relaxing equation i if it has the largest residual among all the
equations coupled to variable i. Since communication is required for
processes to exchange residuals, this method in distributed memory
can be expensive. Distributed Southwell uses a novel scheme to
reduce this communication of residuals while avoiding deadlock.
Using test problems from the SuiteSparse Matrix Collection, we
show that Distributed Southwell requires less communication to
reach the same accuracy when compared to Parallel Southwell. Additionally, we show that the convergence of Distributed Southwell
does not degrade like that of Block Jacobi when the number of
processes is increased.
",16.954822765917157,15.58047619047619,151
SC_17_048.txt,15.48562174937451,14.071147589188197,SC,6732,"
The increasing complexity of HPC systems has introduced new
sources of variability, which can contribute to significant differences
in run-to-run performance of applications. With components at
various levels of the system contributing variability, application
developers and system users are now faced with the difficult task of
running and tuning their applications in an environment where runto-run performance measurements can vary by as much as a factor
of two to three. In this study, we classify, quantify, and present ways
to mitigate the sources of run-to-run variability on Cray XC systems
with Intel Xeon Phi processors and a dragonfly interconnect. We
further demonstrate that the code-tuning performance observed ina
variability-mitigating environment correlates with the performance
observed in production running conditions.
",20.267338824336647,19.54579365079365,127
SC_17_049.txt,13.997981475154564,11.600076823844276,SC,6086,"
Stencil computations represent a very common class of nested loops
in scientific and engineering applications. The exhaustively studied
tiling is one of the most powerful transformation techniques to
explore the data locality and parallelism. Unlike previous work,
which mostly blocks the iteration space of a stencil directly, this
paper proposes a novel two-level tessellation scheme. A set of blocks
are designed to tessellate the spatial space in various ways. The
blocks can be processed in parallel without redundant computation.
This corresponds to extending them along the time dimension and
can form a tessellation of the iteration space. Experimental results
show that our code performs up to 12% better than the existing
highly concurrent schemes for the 3d27p stencil.
",14.554592549557764,12.65680672268908,120
SC_17_050.txt,13.316862282070833,10.969331835402524,SC,9108,"
We present GOFMM (geometry-oblivious FMM), a novel method that
creates a hierarchical low-rank approximation, or “compression,” of
an arbitrary dense symmetric positive definite (SPD) matrix. For
many applications, GOFMM enables an approximate matrix-vector
multiplication in N log N or even N time, where N is the matrix size.
Compression requires N log N storage and work. In general, our
scheme belongs to the family of hierarchical matrix approximation
methods. In particular, it generalizes the fast multipole method
(FMM) to a purely algebraic setting by only requiring the ability to
sample matrix entries. Neither geometric information (i.e., point
coordinates) nor knowledge of how the matrix entries have been
generated is required, thus the term “geometry-oblivious.” Also, we
introduce a shared-memory parallel scheme for hierarchical matrix
computations that reduces synchronization barriers. We present
results on the Intel Knights Landing and Haswell architectures, and
on the NVIDIA Pascal architecture for a variety of matrices.
",16.52667757954773,15.690641025641028,158
SC_17_051.txt,13.027443435978196,10.6004122988633,SC,8150,"
The byte-addressable non-volatile memory (NVM) is new promising storage medium. Compared to NAND flash memory, the nextgeneration NVM not only preserves the durability of stored data
but has much shorter access latencies. An architect can utilize the
fast and persistent NVM as an external disk cache. Regarding the
system’s crash consistency, a prevalent journaling file system needs
to run atop an NVM disk cache. However, the performance is severely impaired by redundant efforts in achieving crash consistency
in both file system and disk cache. Therefore, we propose a new
mechanism called transactional NVM disk cache (Tinca). In brief,
Tinca jointly guarantees consistency of file system and disk cache
and removes the performance penalty of file system journaling with
a lightweight transaction scheme. Evaluations confirm that Tinca
significantly outperforms state-of-the-art design by up to 2.5x in
local and cluster tests without causing any inconsistency issue.
",15.078166124597352,12.739521812080536,152
SC_17_052.txt,15.151824579144701,13.196018654962142,SC,5997,"
Communication-avoiding algorithms have been a subject of growing interest in the last decade due to the growth of distributed
memory systems and the disproportionate increase of computational throughput to communication bandwidth. For distributed
1D FFTs, communication costs quickly dominate execution time
as all industry-standard implementations perform three all-to-all
transpositions of the data. In this work, we reformulate an existing
algorithm that employs the Fast Multipole Method to reduce the
communication requirements to approximately a single all-to-all
transpose. We present a detailed and clear implementation strategy
that relies heavily on existing library primitives, demonstrate that
our strategy achieves consistent speed-ups between 1.3x and 2.2x
against cuFFTXT on up to eight NVIDIA Tesla P100 GPUs, and
develop an accurate compute model to analyze the performance
and dependencies of the algorithm.
",20.736966565827903,20.910277777777782,138
SC_17_053.txt,14.333158282955957,12.241900158295575,SC,9256,"
This paper introduces PapyrusKV, a parallel embedded key-value
store (KVS) for distributed high-performance computing (HPC)
architectures that offer potentially massive pools of nonvolatile
memory (NVM). PapyrusKV stores keys with their values in arbitrary byte arrays across multiple NVMs in a distributed system.
PapyrusKV provides standard KVS operations such as put, get, and
delete. More importantly, PapyrusKV provides advanced features
for HPC such as dynamic consistency control, zero-copy workflow, and asynchronous checkpoint/restart. Beyond filesystems,
PapyrusKV provides HPC programmers with a high-level interface to exploit distributed NVM in the system, and it transparently
organizes data to achieve high performance. Also, it allows HPC
applications to specialize PapyrusKV to meet their specific requirements. We empirically evaluate PapyrusKV on three HPC systems
with real NVM devices: OLCF’s Summitdev, TACC’s Stampede,
and NERSC’s Cori. Our results show that PapyrusKV can offer
high performance, scalability, and portability across these various
distributed NVM architectures.
",16.975882523387877,15.652894736842107,156
SC_17_054.txt,16.15250807761948,15.081668237102527,SC,6660,"
Many applications, such as PDE based simulations and machine
learning, apply BLAS/LAPACK routines to large groups of small matrices, While existing batched Bias APIs provide meaningful speedup
for this problem type, a non-canonical data layout enabling crossmatrix vectorization may provide further significant speedup. In
this paper, we propose a new compact data layout that interleaves
matrices in blocks according to the SIMD vector length. We combine
this compact data layout with a new interface to BLAS/LAPACK routines that can be used within a hierarchical parallel application. Our
layout provides up to 14x, 45x, and 27x speedup against OpenMP
loops around optimized DGEMM, DTRSM and DGETRF kernels, respectively, on the Intel Knights Landing architecture. We discuss the
compact batched BLAS/LAPACK implementations in two libraries,
KokkosKernels and Intel® Math Kernel Library. We demonstrate
the APIs in a line solver for coupled PDEs. Finally, we present
detailed performance analysis of our kernels.
",14.554592549557764,14.367922077922078,154
SC_17_055.txt,14.451902983825441,12.520099522816448,SC,8914,"
Non-volatile memory (NVM) provides a scalable and power-efficient
solution to replace DRAM as main memory. However, because of
relatively high latency and low bandwidth of NVM, NVM is often paired with DRAM to build a heterogeneous memory system
(HMS). As a result, data objects of the application must be carefully
placed to NVM and DRAM for best performance. In this paper,
we introduce a lightweight runtime solution that automatically
and transparently manage data placement on HMS without the
requirement of hardware modifications and disruptive change to
applications. Leveraging online profiling and performance models, the runtime characterizes memory access patterns associated
with data objects, and minimizes unnecessary data movement. Our
runtime solution effectively bridges the performance gap between
NVM and DRAM. We demonstrate that using NVM to replace the
majority of DRAM can be a feasible solution for future HPC systems
with the assistance of a software-based data management.
",17.122413403193683,15.328817407757807,152
SC_17_056.txt,16.454682910802205,15.558580823477424,SC,5732,"
Efficient implementations of HPC applications for parallel architectures generally rely on external software packages (e.g.,
BLAS, LAPACK, CUDNN). While these libraries provide
highly optimized routines for certain characteristics of inputs
(e.g., square matrices), they generally do not retain optimal
performance across the wide range of problems encountered
in practice. In this paper, we present an input-aware autotuning framework for matrix multiplications and convolutions,
ISAAC, which uses predictive modeling techniques to drive
highly parameterized PTX code templates towards not only
hardware-, but also application-specific kernels. Numerical
experiments on the NVIDIA Maxwell and Pascal architectures show up to 3x performance gains over both cuBLAS
and cuDNN after only a few hours of auto-tuning.

(auto-tuning). There, the performance-critical portions (kernels) of the application code are parameterized, and those
parameters optimized for the architecture – and inputs –
of interest [5, 21]. The wide adoption of this technique in
fields like Linear Algebra [15, 18, 19] and Machine Learning
[2, 20] has given rise to a plethora of hardware-oblivious
software libraries capable of efficiently adapting virtually
any underlying memory hierarchies and/or multi-threading
schemes.
",19.454632303725965,19.779864864864866,188
SC_17_057.txt,16.414561988579848,15.055686040432207,SC,8580,"
This paper provides an in-depth analysis of the software overheads
in the MPI performance-critical path and exposes mandatory performance overheads that are unavoidable based on the MPI-3.1
specification. We first present a highly optimized implementation
of the MPI-3.1 standard in which the communication stack—all the
way from the application to the low-level network communication
API—takes only a few tens of instructions. We carefully study these
instructions and analyze the root cause of the overheads based
on specific requirements from the MPI standard that are unavoidable under the current MPI standard. We recommend potential
changes to the MPI standard that can minimize these overheads.
Our experimental results on a variety of network architectures and
applications demonstrate significant benefits from our proposed
changes.
",18.243605946275583,16.703375000000005,131
SC_17_058.txt,14.213455907419831,12.311208483393361,SC,9377,"
With the increasing size of HPC systems, the system mean time to
interrupt will decrease. This requires checkpoints to be stored in
a smaller time when using checkpoint/restart (C/R) for mitigation.
Multilevel checkpointing improves C/R efficiency by saving most
checkpoints to fast compute-node local storage. But it incurs a high
cost for writing a few checkpoints to slow global-I/O. We show
that leveraging NDP to offload writing of checkpoints to globalI/O improves C/R efficiency. We explore additional opportunities
using NDP to further reduce C/R overhead and evaluate checkpoint
compression using NDP as a starting point.
We evaluate the performance of our novel application of NDP for
C/R and compare it to existing C/R optimizations. Our evaluation for
a projected exascale system using multilevel checkpointing shows
that with NDP, the host processor is able to increase its efficiency on
an average from 51% to 78% (i.e., a >50% speedup in performance).
",14.756829357015494,13.539417989417988,163
SC_17_059.txt,14.39878308515058,12.442347219327626,SC,9176,"
While program hangs on large parallel systems can be detected via
the widely used timeout mechanism, it is difficult for the users to
set the timeout — too small a timeout leads to high false alarm rates
and too large a timeout wastes a vast amount of valuable computing
resources. To address the above problems with hang detection, this
paper presents ParaStack, an extremely lightweight tool to detect
hangs in a timely manner with high accuracy, negligible overhead
with great scalability, and without requiring the user to select a
timeout value. For a detected hang, it provides direction for further analysis by telling users whether the hang is the result of an
error in the computation phase or the communication phase. For
a computation-error induced hang, our tool pinpoints the faulty
process by excluding hundreds and thousands of other processes.
We have adapted ParaStack to work with the Torque and Slurm
parallel batch schedulers and validated its functionality and performance on Tianhe-2 and Stampede that are respectively the world’s
current 2”4 and 12!” fastest supercomputers. Experimental results
demonstrate that ParaStack detects hangs in a timely manner at
negligible overhead with over 99% accuracy. No false alarm is observed in correct runs taking 66 hours at scale of 256 processes and
39.7 hours at scale of 1024 processes. ParaStack accurately reports
the faulty process for computation-error induced hangs.
",15.616094167265928,14.89840579710145,233
SC_17_060.txt,15.569571993171635,13.810098838579652,SC,8416,"
Global sensitivity analysis is an important step for analyzing and
validating numerical simulations. One classical approach consists
in computing statistics on the outputs from well-chosen multiple
simulation runs. Simulation results are stored to disk and statistics are computed postmortem. Even if supercomputers enable to
run large studies, scientists are constrained to run low resolution
simulations with a limited number of probes to keep the amount
of intermediate storage manageable. In this paper we propose a
file avoiding, adaptive, fault tolerant and elastic framework that
enables high resolution global sensitivity analysis at large scale.
Our approach combines iterative statistics and in transit processing to compute Sobol’ indices without any intermediate storage.
Statistics are updated on-the-fly as soon as the in transit parallel
server receives results from one of the running simulations. For
one experiment, we computed the Sobol’ indices on 10M hexahedra
and 100 timesteps, running 8000 parallel simulations executed in
1h27 on up to 28672 cores, avoiding 48TB of file storage.
",17.410965686947208,15.789146341463418,165
SC_17_061.txt,15.889592316087814,14.65599903009915,SC,6574,"
Existing designs for MPI_Allreduce do not take advantage of the vast
parallelism available in modern multi-/many-core processors like
Intel Xeon/Xeon Phis or the increases in communication throughput and recent advances in high-end features seen with modern
interconnects like InfiniBand and Omni-Path. In this paper, we
propose a high-performance and scalable Data Partitioning-based
Multi-Leader (DPML) solution for MPI_Allreduce that can take
advantage of the parallelism offered by multi-/many-core architectures in conjunction with the high throughput and high-end
features offered by InfiniBand and Omni-Path to significantly enhance the performance of MPI_Allreduce on modern HPC systems.
We also model DPML-based designs to analyze the communication costs theoretically. Microbenchmark level evaluations show
that the proposed DPML-based designs are able to deliver up to 3.5
times performance improvement for MPI_Allreduce for multiple
HPC systems at scale. At the application-level, up to 35% and 60%
improvement is seen in communication for HPCG and miniAMR
respectively.
",18.878054631974784,19.434926829268296,166
SIGCOMM_17_001.txt,14.639834134823055,12.912306128738738,SIGCOMM,12496,"
Software-based sequential service chains in Network Function
Virtualization (NFV) could introduce significant performance overhead. Current acceleration efforts for NFV mainly target on optimizing each component of the sequential service chain. However, based on the statistics from real world enterprise networks,
we observe that 53.8% network function (NF) pairs can work in
parallel. In particular, 41.5% NF pairs can be parallelized without
causing extra resource overhead. In this paper, we present NFP, a
high performance framework, that innovatively enables network
function parallelism to improve NFV performance. NFP consists
of three logical components. First, NFP provides a policy specification scheme for operators to intuitively describe sequential or
parallel NF chaining intents. Second, NFP orchestrator intelligently
identifies NF dependency and automatically compiles the policies
into high performance service graphs. Third, NFP infrastructure
performs light-weight packet copying, distributed parallel packet
delivery, and load-balanced merging of packet copies to support NF
parallelism. We implement an NFP prototype based on DPDK in
Linux containers. Our evaluation results show that NFP achieves
significant latency reduction for real world service chains.
",15.308715981407026,14.451136363636362,179
SIGCOMM_17_002.txt,14.537918205668806,13.02840005685297,SIGCOMM,11402,"

We present the design of Espresso, Google’s SDN-based Internet
peering edge routing infrastructure. This architecture grew out of a
need to exponentially scale the Internet edge cost-effectively and to
enable application-aware routing at Internet-peering scale. Espresso
utilizes commodity switches and host-based routing/packet processing to implement a novel fine-grained traffic engineering capability.
Overall, Espresso provides Google a scalable peering edge that is
programmable, reliable, and integrated with global traffic systems.
Espresso also greatly accelerated deployment of new networking
features at our peering edge. Espresso has been in production for
two years and serves over 22% of Google’s total traffic to the Internet.
",16.11434528070225,14.811481481481483,111
SIGCOMM_17_003.txt,13.643422850608179,11.742179710862178,SIGCOMM,14769,"

Small RTTs (~tens of microseconds), bursty flow arrivals, and a
large number of concurrent flows (thousands) in datacenters bring
fundamental challenges to congestion control as they either force
a flow to send at most one packet per RTT or induce a large queue
build-up. The widespread use of shallow buffered switches also
makes the problem more challenging with hosts generating many
flows in bursts. In addition, as link speeds increase, algorithms that
gradually probe for bandwidth take a long time to reach the fairshare. An ideal datacenter congestion control must provide 1) zero
data loss, 2) fast convergence, 3) low buffer occupancy, and 4) high
utilization. However, these requirements present conflicting goals.

This paper presents a new radical approach, called ExpressPass,
an end-to-end credit-scheduled, delay-bounded congestion control
for datacenters. ExpressPass uses credit packets to control congestion even before sending data packets, which enables us to achieve
bounded delay and fast convergence. It gracefully handles bursty
flow arrivals. We implement ExpressPass using commodity switches
and provide evaluations using testbed experiments and simulations.
ExpressPass converges up to 80 times faster than DCTCP in 10 Gbps
links, and the gap increases as link speeds become faster. It greatly
improves performance under heavy incast workloads and significantly reduces the flow completion times, especially, for small and
medium size flows compared to RCP, DCTCP, HULL, and DX under
realistic workloads.
",14.937675812826573,13.33939393939394,231
SIGCOMM_17_004.txt,15.449074987509924,14.230253031259636,SIGCOMM,13866,"

We present a Network Address Translator (NAT) written in C and
proven to be semantically correct according to RFC 3022, as well
as crash-free and memory-safe. There exists a lot of recent work
on network verification, but it mostly assumes models of network
functions and proves properties specific to network configuration,
such as reachability and absence of loops. Our proof applies directly
to the C code of a network function, and it demonstrates the absence
of implementation bugs. Prior work argued that this is not feasible
(ie., that verifying a real, stateful network function written in C
does not scale) but we demonstrate otherwise: NAT is one of the
most popular network functions and maintains per-flow state that
needs to be properly updated and expired, which is a typical source
of verification challenges. We tackle the scalability challenge with a
new combination of symbolic execution and proof checking using
separation logic; this combination matches well the typical structure
of a network function. We then demonstrate that formally proven

correctness in this case does not come at the cost of performance.

The NAT code, proof toolchain, and proofs are available at [58].
",15.903189008614273,14.986612665684834,195
SIGCOMM_17_005.txt,14.531120600583012,13.264627714119499,SIGCOMM,15086,"

Middleboxes are crucial for improving network security and performance, but only if the right traffic goes through the right middleboxes at the right time. Existing traffic-steering techniques rely
on a central controller to install fine-grained forwarding rules in
network elements—at the expense of a large number of rules, a central point of failure, challenges in ensuring all packets of a session
traverse the same middleboxes, and difficulties with middleboxes
that modify the “five tuple” We argue that a session-level protocol is
a fundamentally better approach to traffic steering, while naturally
supporting host mobility and multihoming in an integrated fashion.
In addition, a session-level protocol can enable new capabilities like
dynamic service chaining, where the sequence of middleboxes can
change during the life of a session, e.g., to remove a load-balancer
that is no longer needed, replace a middlebox undergoing maintenance, or add a packet scrubber when traffic looks suspicious.
Our Dysco protocol steers the packets of a TCP session through
a service chain, and can dynamically reconfigure the chain for
an ongoing session. Dysco requires no changes to end-host and
middlebox applications, host TCP stacks, or IP routing. Dysco’s
distributed reconfiguration protocol handles the removal of proxies
that terminate TCP connections, middleboxes that change the size
of a byte stream, and concurrent requests to reconfigure different
parts of a chain. Through formal verification using Spin and experiments with our Linux-based prototype, we show that Dysco is
provably correct, highly scalable, and able to reconfigure service
chains across a range of middleboxes.
",19.430816780756558,20.162007722007726,262
SIGCOMM_17_006.txt,14.500961127551758,13.057816296926681,SIGCOMM,14864,"

We present the design, implementation, validation, and deployment
of the Price $heriff, a highly distributed system for detecting various types of online price discrimination in e-commerce. The Price
$heriff uses a peer-to-peer architecture, sandboxing, and secure
multiparty computation to allow users to tunnel price check requests through the browsers of other peers without tainting their
local or server-side browsing history and state. Having operated
the Price $heriff for several months with approximately one thousand real users, we identify several instances of cross-border price
discrimination based on the country of origin. Even within national
borders, we identify several retailers that return different prices for
the same product to different users. We examine whether the observed differences are due to personal-data-induced discrimination
or A/B testing, and conclude that it is the latter.
",17.122413403193683,17.250553191489363,139
SIGCOMM_17_007.txt,14.409357943340332,12.519513693754451,SIGCOMM,11933,"

Network measurement remains a missing piece in today’s software
packet processing platforms. Sketches provide a promising building
block for filling this void by monitoring every packet with fixed-size
memory and bounded errors. However, our analysis shows that
existing sketch-based measurement solutions suffer from severe
performance drops under high traffic load. Although sketches
are efficiently designed, applying them in network measurement
inevitably incurs heavy computational overhead.

We present SketchVisor, a robust network measurement framework for software packet processing. It augments sketch-based
measurement in the data plane with a fast path, which is activated
under high traffic load to provide high-performance local measurement with slight accuracy degradations. It further recovers accurate
network-wide measurement results via compressive sensing. We
have built a SketchVisor prototype on top of Open vSwitch. Extensive testbed experiments show that SketchVisor achieves high
throughput and high accuracy for a wide range of network measurement tasks and microbenchmarks.
",15.021129683784007,13.560261437908498,155
SIGCOMM_17_008.txt,15.20497872421818,14.382292066205931,SIGCOMM,9757,"

Recent studies have observed that large data center networks often
have a few hotspots while most of the network is underutilized.
Consequently, numerous data center network designs have explored the approach of identifying these communication hotspots
in real-time and eliminating them by leveraging flexible optical or
wireless connections to dynamically alter the network topology.
These proposals are based on the premise that statically wired
network topologies, which lack the opportunity for such online
optimization, are fundamentally inefficient, and must be built at
uniform full capacity to handle unpredictably skewed traffic.

We show this assumption to be false. Our results establish that
state-of-the-art static networks can also achieve the performance
benefits claimed by dynamic, reconfigurable designs of the same
cost: for the skewed traffic workloads used to make the case for
dynamic networks, the evaluated static networks can achieve
performance matching full-bandwidth fat-trees at two-thirds of the
cost. Surprisingly, this can be accomplished even without relying
on any form of online optimization, including the optimization of
routing configuration in response to the traffic demands.

Our results substantially lower the barriers for improving upon
today’s data centers by showing that a static, cabling-friendly topology built using commodity equipment yields superior performance
when combined with well-understood routing methods.
",17.93193317476759,18.94033377837116,216
SIGCOMM_17_009.txt,15.631622263608634,15.111994844378788,SIGCOMM,13858,"

We propose and evaluate a new metric for understanding the dependence of the AS-level Internet on individual routers. Whereas prior
work uses large volumes of reachability probes to infer outages,
we design an efficient active probing technique that directly and
unambiguously reveals router restarts. We use our technique to
survey 149,560 routers across the Internet for 2.5 years. 59,175 of
the surveyed routers (40%) experience at least one reboot, and we
quantify the resulting impact of each router outage on global IPv4
and IPv6 BGP reachability.

Our technique complements existing data and control plane outage analysis methods by providing a causal link from BGP reachability failures to the responsible router(s) and multi-homing configurations. While we found the Internet core to be largely robust, we
identified specific routers that were single points of failure for the
prefixes they advertised. In total, 2,385 routers — 4.0% of the routers
that restarted over the course of 2.5 years of probing — were single
points of failure for 3,396 IPv6 prefixes announced by 1,708 ASes.
We inferred 59% of these routers were the customer-edge border
router. 2,374 (70%) of the withdrawn prefixes were not covered by
a less specific prefix, so 1,726 routers (2.9%) of those that restarted
were single points of failure for at least one network. However, a
covering route did not imply reachability during a router outage,
as no previously-responsive address in a withdrawn more specific
prefix responded during a one-week sample. We validate our reboot
and single point of failure inference techniques with four networks,
finding no false positive or false negative reboots, but find some
false negatives in our single point of failure inferences.
",16.04434344847333,16.716487455197136,291
SIGCOMM_17_010.txt,13.491674065169207,11.595001115864132,SIGCOMM,11277,"

This paper promotes convertible data center network architectures, which can dynamically change the network topology
to combine the benefits of multiple architectures. We propose
the flat-tree prototype architecture as the first step to realize
this concept. Flat-tree can be implemented as a Clos network
and later be converted to approximate random graphs of
different sizes, thus achieving both Clos-like implementation
simplicity and random-graph-like transmission performance.
We present the detailed design for the network architecture
and the control system. Simulations using real data center
traffic traces show that flat-tree is able to optimize various
workloads with different topology options. We implement an
example flat-tree network on a 20-switch 24-server testbed.
The traffic reaches the maximal throughput in 2.5s after a
topology change, proving the feasibility of converting topology at run time. The network core bandwidth is increased by
27.6% just by converting the topology from Clos to approximate random graph. This improvement can be translated
into acceleration of applications as we observe reduced communication time in Spark and Hadoop jobs.
",15.172626615295595,13.800861423220976,181
SIGCOMM_17_011.txt,13.305290678774117,10.764189360593075,SIGCOMM,8267,"
Monitoring tasks, such as anomaly and DDoS detection, require
identifying frequent flow aggregates based on common IP prefixes.
These are known as hierarchical heavy hitters (HHH), where the
hierarchy is determined based on the type of prefixes of interest
in a given application. The per packet complexity of existing HHH
algorithms is proportional to the size of the hierarchy, imposing
significant overheads.
In this paper, we propose a randomized constant time algorithm
for HHH. We prove probabilistic precision bounds backed by an
empirical evaluation. Using four real Internet packet traces, we
demonstrate that our algorithm indeed obtains comparable accuracy and recall as previous works, while running up to 62 times
faster. Finally, we extended Open vSwitch (OVS) with our algorithm
and showed it is able to handle 13.8 million packets per second. In
contrast, incorporating previous works in OVS only obtained 2.5
times lower throughput.
",15.579741850924794,13.858060344827589,148
SIGCOMM_17_012.txt,14.865881585117112,13.308331805196975,SIGCOMM,9404,"

Low-Power Wide Area Networks (LP-WANSs) are an attractive
emerging platform to connect the Internet-of-things. LP-WANs
enable low-cost devices with a 10-year battery to communicate
at few kbps to a base station, kilometers away. But deploying LPWANs in large urban environments is challenging, given the sheer
density of nodes that causes interference, coupled with attenuation
from buildings that limits signal range. Yet, state-of-the-art techniques to address these limitations demand inordinate hardware
complexity at the base stations or clients, increasing their size and
cost.

This paper presents Choir, a system that overcomes challenges
pertaining to density and range of urban LP-WANs despite the limited capabilities of base station and client hardware. First, Choir proposes a novel technique that aims to disentangle and decode large
numbers of interfering transmissions at a simple, single-antenna
LP-WAN base station. It does so, perhaps counter-intuitively, by
taking the hardware imperfections of low-cost LP-WAN clients
to its advantage. Second, Choir exploits the correlation of sensed
data collected by LP-WAN nodes to collaboratively reach a faraway base station, even if individual clients are beyond its range.
We implement and evaluate Choir on USRP N210 base stations
serving a 10 square kilometer area surrounding Carnegie Mellon
University campus. Our results reveal that Choir improves network
throughput of commodity LP-WAN clients by 6.84 x and expands
communication range by 2.65 x.
",15.514038796780547,15.110487394957985,241
SIGCOMM_17_013.txt,14.150494606560887,12.371396388977654,SIGCOMM,14038,"

Emails today are often encrypted, but only between mail servers—
the vast majority of emails are exposed in plaintext to the mail
servers that handle them. While better than no encryption, this
arrangement leaves open the possibility of attacks, privacy violations, and other disclosures. Publicly, email providers have stated
that default end-to-end encryption would conflict with essential
functions (spam filtering, etc.), because the latter requires analyzing
email text. The goal of this paper is to demonstrate that there is
no conflict. We do so by designing, implementing, and evaluating
Pretzel. Starting from a cryptographic protocol that enables two
parties to jointly perform a classification task without revealing
their inputs to each other, Pretzel refines and adapts this protocol
to the email context. Our experimental evaluation of a prototype
demonstrates that email can be encrypted end-to-end and providers
can compute over it, at tolerable cost: clients must devote some
storage and processing, and provider overhead is roughly 5x versus
the status quo.
",16.084390811093357,15.200432900432904,166
SIGCOMM_17_014.txt,14.942669712996029,14.092348335237748,SIGCOMM,11961,"
The ever-increasing bandwidth requirements of modern datacenters have led researchers to propose networks based upon optical
circuit switches, but these proposals face significant deployment
challenges. In particular, previous proposals dynamically configure circuit switches in response to changes in workload, requiring
network-wide demand estimation, centralized circuit assignment,
and tight time synchronization between various network elements—
resulting in a complex and unwieldy control plane. Moreover, limitations in the technologies underlying the individual circuit switches
restrict both the rate at which they can be reconfigured and the
scale of the network that can be constructed.

We propose RotorNet, a circuit-based network design that addresses these two challenges. While RotorNet dynamically reconfigures its constituent circuit switches, it decouples switch configuration from traffic patterns, obviating the need for demand collection
and admitting a fully decentralized control plane. At the physical
layer, RotorNet relaxes the requirements on the underlying circuit
switches—in particular by not requiring individual switches to implement a full crossbar—enabling them to scale to 1000s of ports.
We show that RotorNet outperforms comparably priced Fat Tree
topologies under a variety of workload conditions, including traces
taken from two commercial datacenters. We also demonstrate a
small-scale RotorNet operating in practice on an eight-node testbed.
",19.412932280823824,18.22434466019418,207
SIGCOMM_17_015.txt,14.222472711894252,12.443368916214823,SIGCOMM,12725,"

The trend towards simple datacenter network fabric strips most network functionality, including load balancing, out of the network core
and pushes it to the edge. This slows reaction to microbursts, the
main culprit of packet loss in datacenters. We investigate the opposite
direction: could slightly smarter fabric significantly improve load
balancing? This paper presents DRILL, a datacenter fabric for Clos
networks which performs micro load balancing to distribute load as
evenly as possible on microsecond timescales. DRILL employs perpacket decisions at each switch based on local queue occupancies
and randomized algorithms to distribute load. Our design addresses
the resulting key challenges of packet reordering and topological
asymmetry. In simulations with a detailed switch hardware model
and realistic workloads, DRILL outperforms recent edge-based load.
balancers, particularly under heavy load. Under 80% load, for example, it achieves 1.3-1.4x lower mean flow completion time than
recent proposals, primarily due to shorter upstream queues. To test
hardware feasibility, we implement DRILL in Verilog and estimate
its area overhead to be less than 1%. Finally, we analyze DRILL’s
stability and throughput-efficiency.
",15.903189008614273,14.002173184357542,183
SIGCOMM_17_016.txt,14.49671073251356,13.040842184123779,SIGCOMM,10675,"

Network performance monitoring today is restricted by existing
switch support for measurement, forcing operators to rely heavily on
endpoints with poor visibility into the network core. Switch vendors
have added progressively more monitoring features to switches, but
the current trajectory of adding specific features is unsustainable
given the ever-changing demands of network operators. Instead,
we ask what switch hardware primitives are required to support an
expressive language of network performance questions. We believe
that the resulting switch hardware design could address a wide
variety of current and future performance monitoring needs.

We present a performance query language, Marple, modeled on familiar functional constructs like map, filter, groupby, and zip. Marple
is backed by a new programmable key-value store primitive on
switch hardware. The key-value store performs flexible aggregations
at line rate (e.g., a moving average of queueing latencies per flow),
and scales to millions of keys. We present a Marple compiler that
targets a P4-programmable software switch and a simulator for highspeed programmable switches. Marple can express switch queries
that could previously run only on end hosts, while Marple queries
only occupy a modest fraction of a switch’s hardware resources.
",14.867677710551934,14.226975945017184,197
SIGCOMM_17_017.txt,15.317293512407424,13.815628783614795,SIGCOMM,11690,"
 KEYWORDS

Cellular traffic continues to grow rapidly making the scalability of
the cellular infrastructure a critical issue. However, there is mounting evidence that the current Evolved Packet Core (EPC) is ill-suited
to meet these scaling demands: EPC solutions based on specialized
appliances are expensive to scale and recent software EPCs perform
poorly, particularly with increasing numbers of devices or signaling
traffic.

In this paper, we design and evaluate a new system architecture
for a software EPC that achieves high and scalable performance.
We postulate that the poor scaling of existing EPC systems stems
from the manner in which the system is decomposed which leads
to device state being duplicated across multiple components which
in turn results in frequent interactions between the different components. We propose an alternate approach in which state for a
single device is consolidated in one location and EPC functions are
(re)organized for efficient access to this consolidated state. In effect,
our design “slices” the EPC by user.

We prototype and evaluate PEPC, a software EPC that implements the key components of our design. We show that PEPC
achieves 3-7 higher throughput than comparable software EPCs
that have been implemented in industry and over 10x higher throughput than a popular open-source implementation (OpenAirInterface).
Compared to the industrial EPC implementations, PEPC sustains
high data throughput for 10-100x more users devices per core, and
a 10x higher ratio of signaling-to-data traffic. In addition to high
performance, PEPC’s by-user organization enables efficient state
migration and customization of processing pipelines. We implement user migration in PEPC and show that state can be migrated
with little disruption, e.g., migration adds only up to 4ys of latency
to median per packet latencies.
",17.228024813938504,15.883015521064305,290
SIGCOMM_17_018.txt,14.52296170071308,13.019408849625698,SIGCOMM,12778,"

The Internet's inter-domain routing infrastructure, provided today by
BGB is extremely rigid and does not facilitate the introduction of new
inter-domain routing protocols. This rigidity has made it incredibly
difficult to widely deploy critical fixes to BGP. It has also depressed
ASes’ ability to sell value-added services or replace BGP entirely
with a more sophisticated protocol. Even if operators undertook the
significant effort needed to fix or replace BGP, it is likely the next
protocol will be just as difficult to change or evolve. To help, this
paper identifies two features needed in the routing infrastructure
(Le., within any inter-domain routing protocol) to facilitate evolution
to new protocols. To understand their utility, it presents D-BGP, a
version of BGP that incorporates them.
",15.903189008614273,14.778385826771657,129
SIGCOMM_17_019.txt,14.7795348233705,12.868679797343791,SIGCOMM,12465,"
 1 INTRODUCTION

Traffic shaping, including pacing and rate limiting, is fundamental
to the correct and efficient operation of both datacenter and wide
area networks. Sample use cases include policy-based bandwidth allocation to flow aggregates, rate-based congestion control algorithms,
and packet pacing to avoid bursty transmissions that can overwhelm
router buffers. Driven by the need to scale to millions of flows and
to apply complex policies, traffic shaping is moving from network
switches into the end hosts, typically implemented in software in the
kernel networking stack.

In this paper, we show that the performance overhead of end-host
traffic shaping is substantial limits overall system scalability as we
move to thousands of individual traffic classes per server. Measurements from production servers show that shaping at hosts consumes
considerable CPU and memory, unnecessarily drops packets, suffers
from head of line blocking and inaccuracy, and does not provide
backpressure up the stack. We present Carousel, a framework that
scales to tens of thousands of policies and flows per server, built from
the synthesis of three key ideas: i) a single queue shaper using time as
the basis for releasing packets, ii) fine-grained, just-in-time freeing
of resources in higher layers coupled to actual packet departures, and
iii) one shaper per CPU core, with lock-free coordination. Our production experience in serving video traffic at a Cloud service provider
shows that Carousel shapes traffic accurately while improving overall machine CPU utilization by 8% (an improvement of 20% in the
CPU utilization attributed to networking) relative to state-of-art deployments. It also conforms 10 times more accurately to target rates,
and consumes two orders of magnitude less memory than existing
approaches.
",19.412932280823824,19.525120106761566,282
SIGCOMM_17_020.txt,15.733930071373635,14.446944336429755,SIGCOMM,13880,"

Large content providers build points of presence around the world,
each connected to tens or hundreds of networks. Ideally, this connectivity lets providers better serve users, but providers cannot
obtain enough capacity on some preferred peering paths to handle peak traffic demands. These capacity constraints, coupled with
volatile traffic and performance and the limitations of the 20 year
old BGP protocol, make it difficult to best use this connectivity.
We present EpcE Fasric, an SDN-based system we built and
deployed to tackle these challenges for FacEBoox, which serves
over two billion users from dozens of points of presence on six
continents. We provide the first public details on the connectivity
of a provider of this scale, including opportunities and challenges.
We describe how EpcE Fasric operates in near real-time to avoid
congesting links at the edge of FAcEBooK’s network. Our evaluation on production traffic worldwide demonstrates that EpcE
Fasric efficiently uses interconnections without congesting them
and degrading performance. We also present real-time performance
measurements of available routes and investigate incorporating
them into routing decisions. We relate challenges, solutions, and
lessons from four years of operating and evolving EpcE Fasric.
",15.760457277294734,14.424791666666671,194
SIGCOMM_17_021.txt,14.461535928631545,12.741017373637892,SIGCOMM,12495,"

Battery-free sensors, such as RFIDs, are annually attached to billions
of items including pharmaceutical drugs, clothes, and manufacturing
parts. The fundamental challenge with battery-free sensors is that
they are only reliable at short distances of tens of centimeters to few
meters. As a result, today’s systems for communicating with and
localizing battery-free sensors are crippled by the limited range.

To overcome this challenge, this paper presents RFly, a system
that leverages drones as relays for battery-free networks. RFly delivers two key innovations. It introduces the first full-duplex relay
for battery-free networks. The relay can seamlessly integrate with
a deployed RFID infrastructure, and it preserves phase and timing
characteristics of the forwarded packets. RFly also develops the first
RF-localization algorithm that can operate through a mobile relay.

We built a hardware prototype of RFly’s relay into a custom PCB
circuit and mounted it on a Parrot Bebop drone. Our experimental
evaluation demonstrates that RFly enables communication with commercial RFIDs at over 50 m. Moreover, its through-relay localization
algorithm has a median accuracy of 19 centimeters. These results
demonstrate that RFly provides powerful primitives for communication and localization in battery-free networks.
",14.672994598444667,13.253080808080806,201
SIGCOMM_17_022.txt,14.233887917216016,12.55913243765061,SIGCOMM,12247,"

In this paper, we show that up to hundreds of software
load balancer (SLB) servers can be replaced by a single
modern switching ASIC, potentially reducing the cost of load
balancing by over two orders of magnitude. Today, large data
centers typically employ hundreds or thousands of servers to
load-balance incoming traffic over application servers. These
software load balancers (SLBs) map packets destined to a
service (with a virtual IP address, or VIP), to a pool of servers
tasked with providing the service (with multiple direct IP
addresses, or DIPs). An SLB is stateful, it must always map
a connection to the same server, even if the pool of servers
changes and/or if the load is spread differently across the
pool. This property is called per-connection consistency or
PCC. The challenge is that the load balancer must keep track
of millions of connections simultaneously.

Until recently, it was not possible to implement a load
balancer with PCC in a merchant switching ASIC, because
high-performance switching ASICs typically can not maintain
per-connection states with PCC. Newer switching ASICs
provide resources and primitives to enable PCC at a large
scale. In this paper, we explore how to use switching ASICs to
build much faster load balancers than have been built before.
Our system, called SilkRoad, is defined in a 400 line P4
program and when compiled to a state-of-the-art switching
ASIC, we show it can load-balance ten million connections
simultaneously at line rate.
",13.968273953766033,13.04,251
SIGCOMM_17_023.txt,14.252543680099606,11.830227102013197,SIGCOMM,11789,"

Network operators often face the problem of remote outages in
transit networks leading to significant (sometimes on the order of
minutes) downtimes. The issue is that BGP, the Internet routing
protocol, often converges slowly upon such outages, as large bursts
of messages have to be processed and propagated router by router.

In this paper, we present SWIFT, a fast-reroute framework which
enables routers to restore connectivity in few seconds upon remote
outages. SWIFT is based on two novel techniques. First, SWIFT
deals with slow outage notification by predicting the overall extent
of a remote failure out of few control-plane (BGP) messages. The
key insight is that significant inference speed can be gained at
the price of some accuracy. Second, SWIFT introduces a new dataplane encoding scheme, which enables quick and flexible update of
the affected forwarding entries. SWIFT is deployable on existing
devices, without modifying BGP.

We present a complete implementation of SWIFT and demonstrate that it is both fast and accurate. In our experiments with real
BGP traces, SWIFT predicts the extent of a remote outage in few
seconds with an accuracy of ~90% and can restore connectivity for
99% of the affected destinations.
",14.410869940926823,12.692606060606064,198
SIGCOMM_17_024.txt,16.327029604045222,15.423270895770898,SIGCOMM,13691,"

The existing slowness of the web on mobile devices frustrates users
and hurts the revenue of website providers. Prior studies have attributed high page load times to dependencies within the page load.
process: network latency in fetching a resource delays its processing,
which in turn delays when dependent resources can be discovered
and fetched.

To securely address the impact that these dependencies have on
page load times, we present VROOM, a rethink of how clients and
servers interact to facilitate web page loads. Unlike existing solutions, which require clients to either trust proxy servers or discover
all the resources on any page themselves, VROOM’s key characteristics are that clients fetch every resource directly from the domain
that hosts it but web servers aid clients in discovering resources.
Input from web servers decouples a client’s processing of resources
from its fetching of resources, thereby enabling independent use of
both the CPU and the network. As a result, VROOM reduces the
median page load time by more than 5 seconds across popular News
and Sports sites. To enable these benefits, our contributions lie in
making web servers capable of accurately aiding clients in resource
discovery and judiciously scheduling a client’s receipt of resources.
",17.451712890111917,16.214541577825162,205
SIGCOMM_17_025.txt,14.10369420222425,12.399457533429345,SIGCOMM,12003,"

Production datacenters operate under various uncertainties such
as traffic dynamics, topology asymmetry, and failures. Therefore,
datacenter load balancing schemes must be resilient to these uncertainties; i.e., they should accurately sense path conditions and
timely react to mitigate the fallouts. Despite significant efforts,
prior solutions have important drawbacks. On the one hand, solutions such as Presto and DRB are oblivious to path conditions and
blindly reroute at fixed granularity. On the other hand, solutions
such as CONGA and CLOVE can sense congestion, but they can
only reroute when flowlets emerge; thus, they cannot always react
timely to uncertainties. To make things worse, these solutions fail
to detect/handle failures such as blackholes and random packet
drops, which greatly degrades their performance.

In this paper, we introduce Hermes, a datacenter load balancer
that is resilient to the aforementioned uncertainties. At its heart,
Hermes leverages comprehensive sensing to detect path conditions
including failures unattended before, and it reacts using timely
yet cautious rerouting. Hermes is a practical edge-based solution
with no switch modification. We have implemented Hermes with
commodity switches and evaluated it through both testbed experiments and large-scale simulations. Our results show that Hermes
achieves comparable performance to CONGA and Presto in normal
cases, and well handles uncertainties: under asymmetries, Hermes
achieves up to 10% and 20% better flow completion time (FCT) than
CONGA and CLOVE; under switch failures, it outperforms all other
schemes by over 32%.
",15.786528217884907,14.620322209436136,239
SIGCOMM_17_026.txt,15.216855451890837,13.893885854416673,SIGCOMM,12975,"

Conventional operating systems used for video streaming employ
an in-memory disk buffer cache to mask the high latency and low
throughput of disks. However, data from Netflix servers show that
this cache has a low hit rate, so does little to improve throughput.
Latency is not the problem it once was either, due to PCle-attached.
flash storage. With memory bandwidth increasingly becoming a bottleneck for video servers, especially when end-to-end encryption is
considered, we revisit the interaction between storage and networking for video streaming servers in pursuit of higher performance.

We show how to build high-performance userspace network services that saturate existing hardware while serving data directly from
disks, with no need for a traditional disk buffer cache. Employing
netmap, and developing a new diskmap service, which provides safe
high-performance userspace direct I/O access to NVMe devices, we
amortize system overheads by utilizing efficient batching of outstanding I/O requests, process-to-completion, and zerocopy operation. We
demonstrate how a buffer-cache-free design is not only practical,
but required in order to achieve efficient use of memory bandwidth
on contemporary microarchitectures. Minimizing latency between
DMA and CPU access by integrating storage and TCP control loops
allows many operations to access only the last-level cache rather
than bottle-necking on memory bandwidth. We illustrate the power
of this design by building Atlas, a video streaming web server that
outperforms state-of-the-art configurations, and achieves ~72Gbps
of plaintext or encrypted network traffic using a fraction of the
available CPU cores on commodity hardware.
",18.001758247042904,17.25305449936629,263
SIGCOMM_17_027.txt,15.848082344803267,13.95345930196439,SIGCOMM,12700,"

Client-side video players employ adaptive bitrate (ABR) algorithms
to optimize user quality of experience (QoE). Despite the abundance
of recently proposed schemes, state-of-the-art ABR algorithms suffer
from a key limitation: they use fixed control rules based on simplified
or inaccurate models of the deployment environment. As a result,
existing schemes inevitably fail to achieve optimal performance
across a broad set of network conditions and QoE objectives.

We propose Pensieve, a system that generates ABR algorithms
using reinforcement learning (RL). Pensieve trains a neural network
model that selects bitrates for future video chunks based on observations collected by client video players. Pensieve does not rely
on pre-programmed models or assumptions about the environment.
Instead, it learns to make ABR decisions solely through observations
of the resulting performance of past decisions. As a result, Pensieve
automatically learns ABR algorithms that adapt to a wide range of
environments and QoE metrics. We compare Pensieve to state-of-theart ABR algorithms using trace-driven and real world experiments
spanning a wide variety of network conditions, QoE metrics, and
video properties. In all considered scenarios, Pensieve outperforms
the best state-of-the-art scheme, with improvements in average QoE
of 12%-25%. Pensieve also generalizes well, outperforming existing
schemes even on networks for which it was not explicitly trained.
",16.691746362846608,14.211988376919887,220
SIGCOMM_17_028.txt,14.08483056089392,12.492721564073118,SIGCOMM,10214,"

In network management today, dynamic updates are required for
traffic engineering and for timely response to security threats. Decisions for such updates are based on monitoring network traffic
to compute numerical quantities based on a variety of network
and application-level performance metrics. Today’s state-of-the-art
tools lack programming abstractions that capture application or
session-layer semantics, and thus require network operators to
specify and reason about complex state machines and interactions
across layers. To address this limitation, we present the design and
implementation of NetQRE, a high-level declarative toolkit that
aims to simplify the specification and implementation of such quantitative network policies. NetQRE integrates regular-expression-like
pattern matching at flow-level as well as application-level payloads
with aggregation operations such as sum and average counts. We
describe a compiler for NetQRE that automatically generates an efficient implementation with low memory footprint. Our evaluation
results demonstrate that NetQRE allows natural specification of a
wide range of quantitative network tasks ranging from detecting security attacks to enforcing application-layer network management
policies. NetQRE results in high performance that is comparable
with optimized manually-written low-level code and is significantly
more efficient than alternative solutions, and can provide timely
enforcement of network policies that require quantitative network
monitoring.
",19.412932280823824,19.17990566037736,214
SIGCOMM_17_029.txt,13.444888828654488,11.696247846857744,SIGCOMM,13040,"
Modern datacenter networks provide very high capacity via redundant Clos topologies and low switch latency, but transport protocols
rarely deliver matching performance. We present NDP, a novel datacenter transport architecture that achieves near-optimal completion
times for short transfers and high flow throughput in a wide range
of scenarios, including incast. NDP switch buffers are very shallow and when they fill the switches trim packets to headers and
priority forward the headers. This gives receivers a full view of
instantaneous demand from all senders, and is the basis for our
novel, high-performance, multipath-aware transport protocol that
can deal gracefully with massive incast events and prioritize traffic
from different senders on RTT timescales. We implemented NDP in
Linux hosts with DPDK, in a software switch, in a NetFPGA-based
hardware switch, and in P4. We evaluate NDP’s performance in
our implementations and in large-scale simulations, simultaneously
demonstrating support for very low-latency and high throughput.
",16.32212239822248,15.960222929936311,159
SIGCOMM_17_030.txt,15.255822794195996,13.970804871555973,SIGCOMM,7509,"
We present our experience with QUIC, an encrypted, multiplexed,
and low-latency transport protocol designed from the ground up to
improve transport performance for HTTPS traffic and to enable rapid
deployment and continued evolution of transport mechanisms. QUIC
has been globally deployed at Google on thousands of servers and
is used to serve traffic to a range of clients including a widely-used
web browser (Chrome) and a popular mobile video streaming app
(YouTube). We estimate that 7% of Internet traffic is now QUIC. We
describe our motivations for developing a new transport, the principles that guided our design, the Internet-scale process that we used
to perform iterative experiments on QUIC, performance improvements seen by our various services, and our experience deploying
QUIC globally. We also share lessons about transport design and the
Internet ecosystem that we learned from our deployment.
",17.80541091248751,16.676140845070424,143
SIGCOMM_17_031.txt,15.73250164688594,14.372180441400847,SIGCOMM,12658,"
 1 INTRODUCTION

Peering infrastructures, namely, colocation facilities and Internet
exchange points, are located in every major city, have hundreds
of network members, and support hundreds of thousands of interconnections around the globe. These infrastructures are well
provisioned and managed, but outages have to be expected, e.g.,
due to power failures, human errors, attacks, and natural disasters.
However, little is known about the frequency and impact of outages
at these critical infrastructures with high peering concentration.

In this paper, we develop a novel and lightweight methodology
for detecting peering infrastructure outages. Our methodology relies on the observation that BGP communities, announced with
routing updates, are an excellent and yet unexplored source of
information allowing us to pinpoint outage locations with high
accuracy. We build and operate a system that can locate the epicenter of infrastructure outages at the level of a building and track
the reaction of networks in near real-time. Our analysis unveils
four times as many outages as compared to those publicly reported
over the past five years. Moreover, we show that such outages have
significant impact on remote networks and peering infrastructures.
Our study provides a unique view of the Internet’s behavior under
stress that often goes unreported.
",16.594172100314452,15.369751243781096,204
SIGCOMM_17_032.txt,14.36487307206523,12.635637636548818,SIGCOMM,13772,"

We present Minesweeper, a tool to verify that a network satisfies a wide range of intended properties such as reachability or
isolation among nodes, waypointing, black holes, bounded path
length, load-balancing, functional equivalence of two routers, and
fault-tolerance. Minesweeper translates network configuration files
into a logical formula that captures the stable states to which the
network forwarding will converge as a result of interactions between routing protocols such as OSPF, BGP and static routes. It
then combines the formula with constraints that describe the intended property. If the combined formula is satisfiable, there exists
a stable state of the network in which the property does not hold.
Otherwise, no stable state (if any) violates the property. We used
Minesweeper to check four properties of 152 real networks from
a large cloud provider. We found 120 violations, some of which
are potentially serious security vulnerabilities. We also evaluated
Minesweeper on synthetic benchmarks, and found that it can verify
rich properties for networks with hundreds of routers in under five
minutes. This performance is due to a suite of model-slicing and
hoisting optimizations that we developed, which reduce runtime
by over 460x for large networks.
",15.903189008614273,13.553333333333335,197
SIGCOMM_17_033.txt,14.50695905882478,12.847738917435663,SIGCOMM,14479,"
We present dRMT (disaggregated Reconfigurable Match-Action
Table), a new architecture for programmable switches. dRMT overcomes two important restrictions of RMT, the predominant pipelinebased architecture for programmable switches: (1) table memory
is local to an RMT pipeline stage, implying that memory not used
by one stage cannot be reclaimed by another, and (2) RMT is hardwired to always sequentially execute matches followed by actions
as packets traverse pipeline stages. We show that these restrictions
make it difficult to execute programs efficiently on RMT.
dRMT resolves both issues by disaggregating the memory and
compute resources of a programmable switch. Specifically, dRMT
moves table memories out of pipeline stages and into a centralized
pool that is accessible through a crossbar. In addition, dRMT replaces
RMT’s pipeline stages with a cluster of processors that can execute
match and action operations in any order.
We show how to schedule a P4 program on dRMT at compile
time to guarantee deterministic throughput and latency. We also
present a hardware design for dRMT and analyze its feasibility and
chip area. Our results show that dRMT can run programs at line rate
with fewer processors compared to RMT, and avoids performance
cliffs when there are not enough processors to run a program at line
rate. dRMT’s hardware design incurs a modest increase in chip area
relative to RMT, mainly due to the crossbar.
",17.77360955136429,17.834171934260436,229
SIGCOMM_17_034.txt,14.297216789243468,13.120213759134046,SIGCOMM,12292,"

This paper presents the design and implementation of Wi-Fi Goes
to Town, the first Wi-Fi based roadside hotspot network designed to
operate at vehicular speeds with meter-sized picocells. Wi-Fi Goes
to Town APs make delivery decisions to the vehicular clients they
serve at millisecond-level granularities, exploiting path diversity
in roadside networks. In order to accomplish this, we introduce
new buffer management algorithms that allow participating APs
to manage each others’ queues, rapidly quenching each others’
transmissions and flushing each others’ queues. We furthermore
integrate our fine-grained AP selection and queue management into
802.11’s frame aggregation and block acknowledgement functions,
making the system effective at modern 802.11 bit rates that need
frame aggregation to maintain high spectral efficiency. We have
implemented our system in an eight-AP network alongside a nearby
road, and evaluate its performance with mobile clients moving at
up to 35 mph. Depending on the clients’ speed, Wi-Fi Goes to Town
achieves a 2.4—4.7x TCP throughput improvement over a baseline
fast handover protocol that captures the state of the art in Wi-Fi
roaming, including the recent IEEE 802.11k and 802.11r standards.
",17.5058628484301,17.436865284974093,200
SIGCOMM_17_035.txt,15.661700477543747,13.96876369321528,SIGCOMM,12944,"

Managing Network Function (NF) service chains requires careful
system resource management. We propose NFVnice, a user space
NF scheduling and service chain management framework to provide fair, efficient and dynamic resource scheduling capabilities on
Network Function Virtualization (NFV) platforms. The NFVnice
framework monitors load on a service chain at high frequency
(1000Hz) and employs backpressure to shed load early in the service chain, thereby preventing wasted work. Borrowing concepts
such as rate proportional scheduling from hardware packet schedulers, CPU shares are computed by accounting for heterogeneous
packet processing costs of NFs, I/O, and traffic arrival characteristics. By leveraging cgroups, a user space process scheduling abstraction exposed by the operating system, NFVnice is capable of
controlling when network functions should be scheduled. NFVnice
improves NF performance by complementing the capabilities of the
OS scheduler but without requiring changes to the OS’s scheduling mechanisms. Our controlled experiments show that NFVnice
provides the appropriate rate-cost proportional fair share of CPU
to NFs and significantly improves NF performance (throughput
and loss) by reducing wasted work across an NF chain, compared
to using the default OS scheduler. NFVnice achieves this even for
heterogeneous NFs with vastly different computational costs and
for heterogeneous workloads.
",17.410965686947208,16.816000000000006,202
SIGCOMM_17_036.txt,14.161308839620379,11.964199310744156,SIGCOMM,9925,"

We take a comprehensive look at packet corruption in data center
networks, which leads to packet losses and application performance
degradation. By studying 350K links across 15 production data
centers, we find that the extent of corruption losses is significant
and that its characteristics differ markedly from congestion losses.
Corruption impacts fewer links than congestion, but imposes a
heavier loss rate; and unlike congestion, corruption rate on a link
is stable over time and is not correlated with its utilization.

Based on these observations, we developed CorrOpt, a system to
mitigate corruption. To minimize corruption losses, it intelligently
selects which corrupting links can be safely disabled, while ensuring
that each top-of-rack switch has a minimum number of paths to
reach other switches. CorrOpt also recommends specific actions
(e.g., replace cables, clean connectors) to repair disabled links, based
on our analysis of common symptoms of different root causes of
corruption. Our recommendation engine has been deployed in over
seventy data centers of a large cloud provider. Our analysis shows
that, compared to current state of the art, CorrOpt can reduce
corruption losses by three to six orders of magnitude and improve
repair accuracy by 60%.
",16.678067442207542,15.277244897959182,198
SIGIR_17_001.txt,18.05187083559768,17.417496158496792,SIGIR,7159,"

Technology-assisted review (“TAR”) systems seek to achieve
“total recall”; that is, to approach, as nearly as possible, the
ideal of 100% recall and 100% precision, while minimizing
human review effort. The literature reports that TAR methods using relevance feedback can achieve considerably greater
than the 65% recall and 65% precision reported by Voorhees
as the “practical upper bound on retrieval performance . . .
since that is the level at. which humans agree with one another”
(Variations in Relevance Judgments and the Measurement
of Retrieval Effectiveness, 2000). This work argues that in
order to build—as well as to, evaluate—TAR systems that
approach 100% recall and 100% precision, it is necessary to
model human assessment, not as absolute ground truth, but
as an indirect indicator of the amorphous property known as
“relevance.” The choice of model impacts both the evaluation
of system effectiveness, as well as the simulation of relevance
feedback. Models are presented that better fit available data
than the infallible ground-truth model. These models suggest
ways to improve TAR-system effectiveness so that hybrid
human-computer systems can improve on both the accuracy
and efficiency of human review alone. This hypothesis is
tested by simulating TAR using two datasets: the TREC 4
AdHoc collection, and a dataset consisting of 401,960 email
messages that were manually reviewed and classified by a single individual, Roger, in his official capacity as Senior State
Records Archivist. The results using the TREC 4 data show
that TAR achieves higher recall and higher precision than the
assessments by either of two independent NIST assessors, and
blind adjudication of the email dataset, conducted by Roger,
more than two years after his original review, shows that
he could have achieved the same recall and better precision,
while reviewing substantially fewer than 401,960 emails, had
he employed TAR in place of exhaustive manual review.

",19.53771442962202,20.439442182410428,310
SIGIR_17_002.txt,16.666771927578612,15.928948343079924,SIGIR,8161,"

As in most information retrieval (IR) studies, evaluation plays an
essential part in Web search research. Both offline and online evaluation metrics are adopted in measuring the performance of search
engines. Offline metrics are usually based on relevance judgments
of query-document pairs from assessors while online metrics exploit the user behavior data, such as clicks, collected from search
engines to compare search algorithms. Although both types of IR
evaluation metrics have achieved success, to what extent can they
predict user satisfaction still remains under-investigated. To shed
light on this research question, we meta-evaluate a series of existing
online and offline metrics to study how well they infer actual search
user satisfaction in different search scenarios. We find that both
types of evaluation metrics significantly correlate with user satisfaction while they reflect satisfaction from different perspectives
for different search tasks. Offline metrics better align with user satisfaction in homogeneous search (ie. ten blue links) whereas online
metrics outperform when vertical results are federated. Finally, we
also propose to incorporate mouse hover information into existing
online evaluation metrics, and empirically show that they better
align with search user satisfaction than click-based online metrics.
",16.52667757954773,16.54804487179487,196
SIGIR_17_003.txt,16.704374695906044,15.774063318011056,SIGIR,8390,"

Using classical statistical significance tests, researchers can only
discuss P(D*|H), the probability of observing the data D at hand or
something more extreme, under the assumption that the hypothesis
H is true (i.e., the p-value). But what we usually want is P(H|D),
the probability that a hypothesis is true, given the data. If we use
Bayesian statistics with state-of-the-art Markov Chain Monte Carlo
(MCMC) methods for obtaining posterior distributions, this is no
longer a problem. That is, instead of the classical p-values and 95%
confidence intervals, which are often misinterpreted respectively
as “probability that the hypothesis is (in)correct” and “probability
that the true parameter value drops within the interval is 95%,’ we
can easily obtain P(H|D) and credible intervals which represent
exactly the above. Moreover, with Bayesian tests, we can easily
handle virtually any hypothesis, not just “equality of means,’ and
obtain an Expected A Posteriori (EAP) value of any statistic that
we are interested in. We provide simple tools to encourage the
IR community to take up paired and unpaired Bayesian tests for
comparing two systems. Using a variety of TREC and NTCIR data,
we compare P(H|D) with p-values, credible intervals with confidence intervals, and Bayesian EAP effect sizes with classical ones.
Our results show that (a) p-values and confidence intervals can
respectively be regarded as approximations of what we really want,
namely, P(H|D) and credible intervals; and (b) sample effect sizes
from classical significance tests can differ considerably from the
Bayesian EAP effect sizes, which suggests that the former can be
poor estimates of population effect sizes. For both paired and unpaired tests, we propose that the IR community report the EAP, the
credible interval, and the probability of hypothesis being true, not
only for the raw difference in means but also for the effect size in
terms of Glass’s A.
",19.287186520377343,19.27715746421268,324
SIGIR_17_004.txt,15.194893283901077,13.499025798467141,SIGIR,8411,"

Increasing test collection sizes and limited judgment budgets create
measurement challenges for IR batch evaluations, challenges that
are greater when using deep effectiveness metrics than when using
shallow metrics, because of the increased likelihood that unjudged
documents will be encountered. Here we study the problem of
metric score adjustment, with the goal of accurately estimating
system performance when using deep metrics and limited judgment
sets, assuming that dynamic score adjustment is required per topic
due to the variability in the number of relevant documents. We
seek to induce system orderings that are as close as is possible to
the orderings that would arise if full judgments were available.

Starting with depth-based pooling, and no prior knowledge of
sampling probabilities, the first phase of our two-stage process computes a background gain for each document based on rank-level
statistics. The second stage then accounts for the distributional variance of relevant documents. We also exploit the frequency statistics
of pooled relevant documents in order to determine a threshold for
dynamically determining the set of topics to be adjusted. Taken
together, our results show that: (i) better score estimates can be
achieved when compared to previous work; (ii) by setting a global
threshold, we are able to adapt our methods to different collections;
and (iii) the proposed estimation methods reliably approximate the
system orderings achieved when many more relevance judgments
are available. We also consider pools generated by a two-strata
sampling approach.
",18.243605946275583,17.61328512396695,243
SIGIR_17_005.txt,15.170605719800992,13.75083759238932,SIGIR,6645,"

Many learning-to-rank (LtR) algorithms focus on query-independent
model, in which query and document do not lie in the same feature

space, and the rankers rely on the feature ensemble about querydocument pair instead of the similarity between query instance

and documents. However, existing algorithms do not consider local structures in query-document feature space, and are fragile to

irrelevant noise features. In this paper, we propose a novel Riemannian metric learning algorithm to capture the local structures and
develop a robust LtR algorithm. First, we design a concept called
ideal candidate document to introduce metric learning algorithm

to query-independent model. Previous metric learning algorithms

aiming to find an optimal metric space are only suitable for querydependent model, in which query instance and documents belong

to the same feature space and the similarity is directly computed
from the metric space. Then we extend the new and extremely
fast global Geometric Mean Metric Learning (GMML) algorithm to

develop a localized GMML, namely L-GMML. Based on the combination of local learned metrics, we employ the popular Normalized
Discounted Cumulative Gain (NDCG) scorer and Weighted Approximate Rank Pairwise (WARP) loss to optimize the ideal candidate
document for each query candidate set. Finally, we can quickly evaluate all candidates via the similarity between the ideal candidate
document and other candidates. By leveraging the ability of metric

learning algorithms to describe the complex structural information,
our approach gives us a principled and efficient way to perform LtR
tasks. The experiments on real-world datasets demonstrate that

our proposed L-GMML algorithm outperforms the state-of-the-art

metric learning to rank methods and the stylish query-independent

LtR algorithms regarding accuracy and computational efficiency.
",18.458006810337128,18.046293286219086,284
SIGIR_17_006.txt,13.426837803060913,12.006227286662682,SIGIR,7392,"

This paper proposes K-NRM, a kernel based neural model for document ranking. Given a query and a set of documents, K-NRM uses
a translation matrix that models word-level similarities via word
embeddings, a new kernel-pooling technique that uses kernels
to extract multi-level soft match features, and a learning-to-rank
layer that combines those features into the final ranking score.
The whole model is trained end-to-end. The ranking layer learns
desired feature patterns from the pairwise ranking loss. The kernels transfer the feature patterns into soft-match targets at each
similarity level and enforce them on the translation matrix. The
word embeddings are tuned accordingly so that they can produce
the desired soft matches. Experiments on a commercial search engine’s query log demonstrate the improvements of K-NRM over prior
feature-based and neural-based states-of-the-art, and explain the
source of K-NRM’s advantage: Its kernel-guided embedding encodes
a similarity metric tailored for matching query words to document
words, and provides effective multi-level soft matches.
",13.484332010920856,13.804761904761907,180
SIGIR_17_007.txt,15.895421243220632,15.164439836771496,SIGIR,8434,"

Despite the impressive improvements achieved by unsupervised
deep neural networks in computer vision and NLP tasks, such improvements have not yet been observed in ranking for information
retrieval. The reason may be the complexity of the ranking problem,
as it is not obvious how to learn from queries and documents when
no supervised signal is available. Hence, in this paper, we propose
to train a neural ranking model using weak supervision, where labels
are obtained automatically without human annotators or any external resources (e.g., click data). To this aim, we use the output of an
unsupervised ranking model, such as BM25, as a weak supervision
signal. We further train a set of simple yet effective ranking models
based on feed-forward neural networks. We study their effectiveness
under various learning scenarios (point-wise and pair-wise models)
and using different input representations (i.e., from encoding querydocument pairs into dense/sparse vectors to using word embedding
representation). We train our networks using tens of millions of
training instances and evaluate it on two standard collections: a homogeneousnews collection (Robust) and a heterogeneous large-scale
web collection (ClueWeb). Our experiments indicate that employing
proper objective functions and letting the networks to learn the input
representation based on weakly supervised data leads to impressive
performance, with over 13% and 35% MAP improvements over the
BM25 model on the Robust and the ClueWeb collections. Our findings also suggest that supervised neural ranking models can greatly
benefit from pre-training on large amounts of weakly labeled data
that can be easily obtained from unsupervised IR models.
",16.594172100314452,16.923831417624523,264
SIGIR_17_008.txt,15.430626988549022,13.770642375447292,SIGIR,7115,"

As the amount of textual data has been rapidly increasing over
the past decade, efficient similarity search methods have become a
crucial component of large-scale information retrieval systems. A
popular strategy is to represent original data samples by compact
binary codes through hashing. A spectrum of machine learning
methods have been utilized, but they often lack expressiveness and
flexibility in modeling to learn effective representations. The recent advances of deep learning in a wide range of applications has
demonstrated its capability to learn robust and powerful feature
representations for complex data. Especially, deep generative models naturally combine the expressiveness of probabilistic generative
models with the high capacity of deep neural networks, which is
very suitable for text modeling. However, little work has leveraged
the recent progress in deep learning for text hashing.

In this paper, we propose a series of novel deep document generative models for text hashing. The first proposed model is unsupervised while the second one is supervised by utilizing document labels/tags for hashing. The third model further considers
document-specific factors that affect the generation of words. The
probabilistic generative formulation of the proposed models provides a principled framework for model extension, uncertainty
estimation, simulation, and interpretability. Based on variational
inference and reparameterization, the proposed models can be interpreted as encoder-decoder deep neural networks and thus they
are capable of learning complex nonlinear distributed representations of the original documents. We conduct a comprehensive set
of experiments on four public testbeds. The experimental results
have demonstrated the effectiveness of the proposed supervised
learning models for text hashing.
",16.666482175067898,16.275343511450384,263
SIGIR_17_009.txt,17.141516911615142,16.518561774853854,SIGIR,7214,"

This paper proposes an item concept embedding (ICE) framework
to model item concepts via textual information. Specifically, in
the proposed framework there are two stages: graph construction
and embedding learning. In the first stage, we propose a generalized network construction method to build a network involving
heterogeneous nodes and a mixture of both homogeneous and heterogeneous relations. The second stage leverages the concept of
neighborhood proximity to learn the embeddings of both items and
words. With the proposed carefully designed ICE networks, the
resulting embedding facilitates both homogeneous and heterogeneous retrieval, including item-to-item and word-to-item retrieval.
Moreover, as a distributed embedding approach, the proposed ICE
approach not only generates related retrieval results but also delivers more diverse results than traditional keyword-matching-based
approaches. As our experiments on two real-world datasets show,
ICE encodes useful textual information and thus outperforms traditional methods in various item classification and retrieval tasks.
",17.77360955136429,16.66662337662338,155
SIGIR_17_010.txt,15.267515132727429,13.609473453260645,SIGIR,6726,"

As a framework for extractive summarization, sentence regression
has achieved state-of-the-art performance in several widely-used
practical systems. The most challenging task within the sentence
regression framework is to identify discriminative features to encode a sentence into a feature vector. So far, sentence regression
approaches have neglected to use features that capture contextual
relations among sentences.

We propose a neural network model, Contextual Relation-based
Summarization (CRSum), to take advantage of contextual relations
among sentences so as to improve the performance of sentence
regression. Specifically, we first use sentence relations with a wordlevel attentive pooling convolutional neural network to construct
sentence representations. Then, we use contextual relations with a
sentence-level attentive pooling recurrent neural network to construct context representations. Finally, CRSum automatically learns
useful contextual features by jointly learning representations of
sentences and similarity scores between a sentence and sentences
in its context. Using a two-level attention mechanism, CRSum is
able to pay attention to important content, i.e., words and sentences,
in the surrounding context of a given sentence.

We carry out extensive experiments on six benchmark datasets.
CRSum alone can achieve comparable performance with state-ofthe-art approaches; when combined with a few basic surface features, it significantly outperforms the state-of-the-art in terms of
multiple ROUGE metrics.
",16.887214914478655,16.051592592592595,218
SIGIR_17_011.txt,17.286467230060552,16.280596663311417,SIGIR,8810,"
Random Forest (RF) is one of the most successful strategies for automated classification tasks. Motivated by the RF success, recently
proposed RF-based classification approaches leverage the central RF
idea of aggregating a large number of low-correlated trees, which
are inherently parallelizable and provide exceptional generalization
capabilities. In this context, this work brings several new contributions to this line of research. First, we propose a new RF-based
strategy (BERT) that applies the boosting technique in bags of extremely randomized trees. Second, we empirically demonstrate
that this new strategy, as well as the recently proposed BROOF and
LazyNN_REF classifiers do complement each other, motivating us
to stack them to produce an even more effective classifier. Up to
our knowledge, this is the first strategy to effectively combine the
three main ensemble strategies: stacking, bagging (the cornerstone
of RFs) and boosting. Finally, we exploit the efficient and unbiased
stacking strategy based on out-of-bag (OOB) samples to considerably speedup the very costly training process of the stacking
procedure. Our experiments in several datasets covering two highdimensional and noisy domains of topic and sentiment classification
provide strong evidence in favor of the benefits of our RF-based
solutions. We show that BERT is among the top performers in the
vast majority of analyzed cases, while retaining the unique benefits
of RF classifiers (explainability, parallelization, easiness of parameterization). We also show that stacking only the recently proposed
RF-based classifiers and BERT using our OOB-based strategy is not
only significantly faster than recently proposed stacking strategies
(up to six times) but also much more effective, with gains up to 21%
and 17% on MacroF, and MicroF;, respectively, over the best base method, and of 5% and 6% over a stacking of traditional methods, performing no worse than a complete stacking of methods at a much lower computational effort.

",18.669449996058646,18.22135064935065,309
SIGIR_17_012.txt,15.172626615295595,13.791635320105463,SIGIR,7378,"

Extreme multi-label text classification (XMTC) refers to the problem of assigning to each document its most relevant subset of class
labels from an extremely large label collection, where the number
of labels could reach hundreds of thousands or millions. The huge
label space raises research challenges such as data sparsity and
scalability. Significant progress has been made in recent years by
the development of new machine learning methods, such as tree
induction with large-margin partitions of the instance spaces and
label-vector embedding in the target space. However, deep learning
has not been explored for XMTC, despite its big successes in other
related areas. This paper presents the first attempt at applying deep
learning to XMTC, with a family of new Convolutional Neural Network (CNN) models which are tailored for multi-label classification
in particular. With a comparative evaluation of 7 state-of-the-art
methods on 6 benchmark datasets where the number of labels is up
to 670,000, we show that the proposed CNN approach successfully
scaled to the largest datasets, and consistently produced the best
or the second best results on all the datasets. On the Wikipedia
dataset with over 2 million documents and 500,000 labels in particular, it outperformed the second best method by 11.7% ~ 15.3% in
precision@K and by 11.5% ~ 11.7% in NDCG@K for K = 1,3,5.
",16.439396014739867,16.717269841269843,233
SIGIR_17_013.txt,16.91025759228278,15.358558706922846,SIGIR,8697,"

One of the primary ways researchers have characterized
engagement during information search is by increases in search
behaviors, such as queries and clicks. However, studies have
shown that frustration is also characterized by increases in these
same behaviors. This research examines the differences in the
search behaviors and physiologies of people who are engaged or
frustrated during search. A 2x2 within-subject laboratory
experiment was conducted with 40 participants. Engagement
was induced by manipulating task interest and frustration was
induced by manipulating the quality of the search results.
Participants’ interactions and physiological responses were
recorded, and after they searched, they evaluated their levels of
engagement, frustration and stress. Participants reported
significantly greater levels of engagement when completing
tasks that interested them and significantly less engagement
during searches with poor results quality. For all search
behaviors measured, only two significant differences were found
according to task interest: participants had more scrolls and
longer query intervals when searching for interesting tasks,
suggesting greater interaction with content. Significant
differences were found for nine behaviors according to results
quality, including queries issued, number of SERPs displayed and
number of SERP clicks, suggesting these are potentially better
indicators of frustration rather than engagement. When
presented with poor quality results, participants had
significantly higher heart rates than when presented with
normal quality results. Finally, participants had lower heart rates
and greater skin conductance responses when conducting
interesting tasks than when conducting uninteresting tasks. This
research provides insight into the differences in search behaviors
and physiologies of participants when they are engaged versus
frustrated and presents techniques that can be used by those
wishing to induce engagement and frustration during laboratory
IR studies.
",18.511140095513987,16.80898550724638,277
SIGIR_17_014.txt,16.233304066095823,14.82617192389273,SIGIR,8252,"

The design and presentation of a Search Engine Results Page (SERP)
has been subject to much research. With many contemporary
aspects of the SERP now under scrutiny, work still remains in investigating more traditional SERP components, such as the result
summary. Prior studies have examined a variety of different aspects
of result summaries, but in this paper we investigate the influence
of result summary length on search behaviour, performance and
user experience. To this end, we designed and conducted a withinsubjects experiment using the TREC AQUAINT news collection
with 53 participants. Using Kullback-Leibler distance as a measure
of information gain, we examined result summaries of different
lengths and selected four conditions where the change in information gain was the greatest: (i) title only; (ii) title plus one snippet;
(iii) title plus two snippets; and (iv) title plus four snippets. Findings
show that participants broadly preferred longer result summaries,
as they were perceived to be more informative. However, their performance in terms of correctly identifying relevant documents was
similar across all four conditions. Furthermore, while the participants felt that longer summaries were more informative, empirical
observations suggest otherwise; while participants were more likely
to click on relevant items given longer summaries, they also were
more likely to click on non-relevant items. This shows that longer is
not necessarily better, though participants perceived that to be the
case — and second, they reveal a positive relationship between the
length and informativeness of summaries and their attractiveness
(ie. clickthrough rates). These findings show that there are tensions between perception and performance when designing result
summaries that need to be taken into account.
",17.353723509956247,16.354814814814812,271
SIGIR_17_015.txt,15.878599786567797,14.534978862694256,SIGIR,7790,"

In information retrieval and information visualization, hierarchies
are a common tool to structure information into topics or facets,
and network visualizations such as knowledge graphs link related
concepts within a domain. In this paper, we explore a multi-layer
extension to knowledge graphs, hierarchical knowledge graphs
(HKGs), that combines hierarchical and network visualizations into
a unified data representation. Through interaction logs, we show
that HKGs preserve the benefits of single-layer knowledge graphs
at conveying domain knowledge while incorporating the sensemaking advantages of hierarchies for knowledge seeking tasks.
Specially, this paper describes our algorithm to construct these visualizations, analyzes interaction logs to quantitatively demonstrate
performance parity with networks and performance advantages
over hierarchies, and synthesizes data from interaction logs, interviews, and thinkalouds on a testbed data set to demonstrate the
utility of the unified hierarchy+network structure in our HKGs.
",21.41881206717044,22.678067375886524,141
SIGIR_17_016.txt,16.798477979227254,16.39300597931028,SIGIR,8988,"

Smart phones and tablets are rapidly becoming our main method of
accessing information and are frequently used to perform on-thego search tasks. Mobile devices are commonly used in situations
where attention must be divided, such as when walking down a
street. Research suggests that this increases cognitive load and,
therefore, may have an impact on performance. In this work we
conducted a laboratory experiment with both device types in which
we simulated everyday, common mobile situations that may cause
fragmented attention, impact search performance and affect user
perception.

Our results showed that the fragmented attention induced by
the simulated conditions significantly affected both participants’
objective and perceived search performance, as well as how hurried
they felt and how engaged they were in the tasks. Furthermore,
the type of device used also impacted how users felt about the
search tasks, how well they performed and the amount of time they
spent engaged in the tasks. These novel insights provide useful
information to inform the design of future interfaces for mobile
search and give us a greater understanding of how context and
device size affect search behaviour and user experience.
",16.613394197324528,15.168571428571433,190
SIGIR_17_017.txt,17.059237850737176,16.05585704500699,SIGIR,7472,"

Detecting and understanding implicit measures of user satisfaction
are essential for meaningful experimentation aimed at enhancing
web search quality. While most existing studies on satisfaction
prediction rely on users’ click activity and query reformulation
behavior, often such signals are not available for all search sessions
and as a result, not useful in predicting satisfaction. On the other
hand, user interaction data (such as mouse cursor movement) is
far richer than just click data and can provide useful signals for
predicting user satisfaction. In this work, we focus on considering holistic view of user interaction with the search engine result
page (SERP) and construct detailed universal interaction sequences
of their activity. We propose novel ways of leveraging the universal interaction sequences to automatically extract informative,
interpretable subsequences. In addition to extracting frequent, discriminatory and interleaved subsequences, we propose a Hawkes
process model to incorporate temporal aspects of user interaction.
Through extensive experimentation we show that encoding the
extracted subsequences as features enables us to achieve significant improvements in predicting user satisfaction. We additionally
present an analysis of the correlation between various subsequences
and user satisfaction. Finally, we demonstrate the usefulness of the
proposed approach in covering abandonment cases. Our findings
provide a valuable tool for fine-grained analysis of user interaction
behavior for metric development.
",18.243605946275583,17.7345046728972,215
SIGIR_17_018.txt,15.997312994008297,14.875320988734472,SIGIR,7559,"

As online video service continues to grow in popularity, video content providers compete hard for more eyeball engagement. Some
users visit multiple video sites to enjoy videos of their interest
while some visit exclusively one site. However, due to the isolation
of data, mining and exploiting user behaviors in multiple video
websites remain unexplored so far. In this work, we try to model
user preferences in six popular video websites with user viewing
records obtained from a large ISP in China. The empirical study
shows that users exhibit both consistent cross-site interests as well
as site-specific interests. To represent this dichotomous pattern
of user preferences, we propose a generative model of Multi-site
Probabilistic Factorization (MPF) to capture both the cross-site as
well as site-specific preferences. Besides, we discuss the design
principle of our model by analyzing the sources of the observed
site-specific user preferences, namely, site peculiarity and data sparsity. Through conducting extensive recommendation validation, we
show that our MPF model achieves the best results compared to
several other state-of-the-art factorization models with significant
improvements of F-measure by 12.96%, 8.24% and 6.88%, respectively. Our findings provide insights on the value of integrating user
data from multiple sites, which stimulates collaboration between
video service providers.
",17.122413403193683,16.290809968847352,218
SIGIR_17_019.txt,15.294937264931004,14.336336883361632,SIGIR,8935,"

Online platforms can be divided into information-oriented and
social-oriented domains. The former refers to forums or Ecommerce sites that emphasize user-item interactions, like Trip.com
and Amazon; whereas the latter refers to social networking services
(SNSs) that have rich user-user connections, such as Facebook
and Twitter. Despite their heterogeneity, these two domains
can be bridged by a few overlapping users, dubbed as bridge
users. In this work, we address the problem of cross-domain social
recommendation, i.e., recommending relevant items of information
domains to potential users of social networks. To our knowledge,
this is a new problem that has rarely been studied before.

Existing cross-domain recommender systems are unsuitable
for this task since they have either focused on homogeneous
information domains or assumed that users are fully overlapped.
Towards this end, we present a novel Neural Social Collaborative
Ranking (NSCR) approach, which seamlessly sews up the user-item
interactions in information domains and user-user connections
in SNSs. In the information domain part, the attributes of users
and items are leveraged to strengthen the embedding learning of
users and items. In the SNS part, the embeddings of bridge users
are propagated to learn the embeddings of other non-bridge users.
Extensive experiments on two real-world datasets demonstrate the
effectiveness and rationality of our NSCR method.
",14.836745963215662,14.65909090909091,223
SIGIR_17_020.txt,16.68488479703092,16.2667330415935,SIGIR,7019,"

Venue category recommendation is an essential application for
the tourism and advertisement industries, wherein it may suggest attractive localities within close proximity to users’ current
location. Considering that many adults use more than three social networks simultaneously, it is reasonable to leverage on this
rapidly growing multi-source social media data to boost venue recommendation performance. Another approach to achieve higher
recommendation results is to utilize group knowledge, which is
able to diversify recommendation output. Taking into account these
two aspects, we introduce a novel cross-network collaborative recommendation framework C?R, which utilizes both individual and
group knowledge, while being trained on data from multiple social
media sources. Group knowledge is derived based on new crosssource user community detection approach, which utilizes both
inter-source relationship and the ability of sources to complement
each other. To fully utilize multi-source multi-view data, we process user-generated content by employing state-of-the-art text, image, and location processing techniques. Our experimental results
demonstrate the superiority of our multi-source framework over
state-of-the-art baselines and different data source combinations.
In addition, we suggest a new approach for automatic construction of inter-network relationship graph based on the data, which
eliminates the necessity of having pre-defined domain knowledge.
",17.122413403193683,17.419308176100632,213
SIGIR_17_021.txt,15.282438462161828,13.33649506169511,SIGIR,7538,"

Displaying banner advertisements (in short, ads) on webpages has
usually been discussed as an Internet economics topic where a
publisher uses auction models to sell an online user’s page view
to advertisers and the one with the highest bid can have her ad
displayed to the user. This is also called real-time bidding (RTB)
and the ad displaying process ensures that the publisher’s benefit
is maximized or there is an equilibrium in ad auctions. However,
the benefits of the other two stakeholders — the advertiser and the
user — have been rarely discussed. In this paper, we propose a
two-stage computational framework that selects a banner ad based
on the optimized trade-offs among all stakeholders. The first stage
is still auction based and the second stage re-ranks ads by considering the benefits of all stakeholders. Our metric variables are:
the publisher’s revenue, the advertiser’s utility, the ad memorability, the ad click-through rate (CTR), the contextual relevance, and
the visual saliency. To the best of our knowledge, this is the first
work that optimizes trade-offs among all stakeholders in RTB by
incorporating multimedia metrics. An algorithm is also proposed
to determine the optimal weights of the metric variables. We use
both ad auction datasets and multimedia datasets to validate the
proposed framework. Our experimental results show that the publisher can significantly improve the other stakeholders’ benefits by
slightly reducing her revenue in the short-term. In the long run,
advertisers and users will be more engaged, the increased demand
of advertising and the increased supply of page views can then
boost the publisher’s revenue.
",16.018793980428352,14.338535921007836,273
SIGIR_17_022.txt,16.95162100831525,16.054572542287186,SIGIR,8396,"

We develop a probabilistic formulation giving rise to a formal version of heuristic k nearest-neighbor (kNN) collaborative filtering.
Different independence assumptions in our scheme lead to userbased, item-based, normalized and non-normalized variants that
match in structure the traditional formulations, while showing
equivalent empirical effectiveness. The probabilistic formulation
provides a principled explanation why KNN is an effective recommendation strategy, and identifies a key condition for this to be
the case. Moreover, a natural explanation arises for the bias of
kNN towards recommending popular items. Thereupon the kNN
variants are shown to fall into two groups with similar trends in
behavior, corresponding to two different notions of item popularity. We show experiments where the comparative performance of
the two groups of algorithms changes substantially, which suggests that the performance measurements and comparison may
heavily depend on statistical properties of the input data sample.
",19.78447435784618,18.189444444444444,145
SIGIR_17_023.txt,17.33195060933313,16.23441681807795,SIGIR,6045,"

Hashing has been a widely-adopted technique for nearest
neighbor search in large-scale image retrieval tasks. Recent research has shown that leveraging supervised information can
lead to high quality hashing. However, the cost of annotating
data is often an obstacle when applying supervised hashing
to a new domain. Moreover, the results can suffer from the
robustness problem as the data at training and test stage
may come from different distributions. This paper studies
the exploration of generating synthetic data through semisupervised generative adversarial networks (GANs), which
leverages largely unlabeled and limited labeled training data
to produce highly compelling data with intrinsic invariance
and global coherence, for better understanding statistical
structures of natural data. We demonstrate that the above
two limitations can be well mitigated by applying the synthetic data for hashing. Specifically, a novel deep semantic
hashing with GANs (DSH-GANs) is presented, which mainly
consists of four components: a deep convolution neural networks (CNN) for learning image representations, an adversary
stream to distinguish synthetic images from real ones, a hash
stream for encoding image representations to hash codes and
a Classification stream. The whole architecture is trained endto-end by jointly optimizing three losses, i.e., adversarial loss
to correct label of synthetic or real for each sample, triplet
ranking loss to preserve the relative similarity ordering in the
input real-synthetic triplets and classification loss to classify
each sample accurately. Extensive experiments conducted on
both CIFAR-10 and NUS-WIDE image benchmarks validate
the capability of exploiting synthetic images for hashing. Our
framework also achieves superior results when compared to
state-of-the-art deep hash models.
",18.243605946275583,17.579810408921933,271
SIGIR_17_024.txt,14.921278800779998,13.727868575752286,SIGIR,7459,"

Email is still among the most popular online activities. People
spend a significant amount of time sending, reading and responding to email in order to communicate with others, manage tasks
and archive personal information. Most previous research on email
is based on either relatively small data samples from user surveys
and interviews, or on consumer email accounts such as those from
Yahoo! Mail or Gmail. Much less has been published on how people
interact with enterprise email even though it contains less automatically generated commercial email and involves more organizational
behavior than is evident in personal accounts. In this paper, we extend previous work on predicting email reply behavior by looking
at enterprise settings and considering more than dyadic communications. We characterize the influence of various factors such
as email content and metadata, historical interaction features and
temporal features on email reply behavior. We also develop models to predict whether a recipient will reply to an email and how
long it will take to do so. Experiments with the publicly-available
Avocado email collection show that our methods outperform all
baselines with large gains. We also analyze the importance of different features on reply behavior predictions. Our findings provide
new insights about how people interact with enterprise email and
have implications for the design of the next generation of email
clients.
",15.429909175157395,14.35043192102016,222
SIGIR_17_025.txt,15.352847748395064,13.617640112564239,SIGIR,9162,"

Spatiotemporal activity modeling is an important task for applications like tour recommendation and place search. The recently developed geographical topic models have demonstrated compelling
results in using geo-tagged social media (GTSM) for spatiotemporal
activity modeling. Nevertheless, they all operate in batch and cannot dynamically accommodate the latest information in the GTSM
stream to reveal up-to-date spatiotemporal activities. We propose
ReEAcT, a method that processes continuous GTSM streams and
obtains recency-aware spatiotemporal activity models on the fly.
Distinguished from existing topic-based methods, REAcT embeds
all the regions, hours, and keywords into the same latent space to
capture their correlations. To generate high-quality embeddings, it
adopts a novel semi-supervised multimodal embedding paradigm
that leverages the activity category information to guide the embedding process. Furthermore, as new records arrive continuously,
it employs strategies to effectively incorporate the new information
while preserving the knowledge encoded in previous embeddings.
Our experiments on the geo-tagged tweet streams in two major
cities have shown that REAcT significantly outperforms existing
methods for location and activity retrieval tasks.

",18.643177196211187,17.77208333333333,178
SIGIR_17_026.txt,14.393612786675146,12.602536664422718,SIGIR,7952,"
Tables are among the most powerful and practical tools for organizing and working with data. Our motivation is to equip spreadsheet
programs with smart assistance capabilities. We concentrate on
one particular family of tables, namely, tables with an entity focus.
We introduce and focus on two specific tasks: populating rows
with additional instances (entities) and populating columns with
new headings. We develop generative probabilistic models for both
tasks. For estimating the components of these models, we consider
a knowledge base as well as a large table corpus. Our experimental
evaluation simulates the various stages of the user entering content
into an actual table. A detailed analysis of the results shows that
the models’ components are complimentary and that our methods
outperform existing approaches from the literature.
",15.247664890283005,13.871547619047622,127
SIGIR_17_027.txt,14.00931708512493,13.018744344056945,SIGIR,9460,"

Email has been a dominant form of communication for many years,
and email search is an important problem. In contrast to other
search setting, such as web search, there have been few studies
of user behavior and models of email search success. Research in
email search is challenging for many reasons including the personal
and private nature of the collection. Third party judges can not
look at email search queries or email message content requiring
new modeling techniques.

In this study, we built an opt-in client application which monitors
a user’s email search activity and then pops up an in-situ survey
when a search session is finished. We then merged the survey data
with server-side behavioral logs. This approach allows us to study
the relationship between session-level outcome and user behavior,
and then build a model to predict success for email search based
on behavioral interaction patterns.
Our results show that generative models (MarkovChain) of success can predict the session-level success of email search better
than baseline heuristics and discriminative models (RandomForest).
The success model makes use of email-specific log activities such
as reply, forward and move, as well as generic signals such as click
with long dwell time. The learned model is highly interpretable,
and reusable in that it can be applied to unlabeled interaction logs
in the future.
",13.023866798666859,12.855858407079648,228
SIGIR_17_028.txt,16.187335241121087,14.685375146091406,SIGIR,6945,"

Image search engines show results differently from general Web
search engines in three key ways: (1) most Web-based image search
engines adopt the two-dimensional result placement instead of the
linear result list; (2) image searches show snapshots instead of snippets (query-dependent abstracts of landing pages) on search engine
result pages (SERPs); and (3) pagination is usually not (explicitly)
supported on image search SERPs, and users can view results without having to click on the “next page” button. Compared with the
extensive study of user behavior in general Web search scenarios,
there exists no thorough investigation how the different interaction mechanism of image search engines affects users’ examination
behavior. To shed light on this research question, we conducted an
eye-tracking study to investigate users’ examination behavior in
image searches. We focus on the impacts of factors in examination
including position, visual saliency, edge density, the existence of
textual information, and human faces in result images. Three interesting findings indicate users’ behavior biases: (1) instead of the
traditional “Golden Triangle” phenomena in the user examination
patterns of general Web search, we observe a middle-position bias,
(2) besides the position factor, the content of image results (e.g.,
visual saliency) affects examination behavior, and (3) some popular
behavior assumptions in general Web search (e.g., examination hypothesis) do not hold in image search scenarios. We predict users’
examination behavior with different impact factors. Results show
that combining position and visual content features can improve
prediction in image searches.
",20.130776976110326,21.018271889400925,251
SIGIR_17_029.txt,15.899245779933008,14.720573437373002,SIGIR,8122,"

A significant amount of search queries originate from some real
world information need or tasks [13]. In order to improve the
search experience of the end users, it is important to have accurate
representations of tasks. As a result, significant amount of research
has been devoted to extracting proper representations of tasks in
order to enable search systems to help users complete their tasks, as
well as providing the end user with better query suggestions [9], for
better recommendations [41], for satisfaction prediction [36] and for
improved personalization in terms of tasks [24, 38]. Most existing
task extraction methodologies focus on representing tasks as flat
structures. However, tasks often tend to have multiple subtasks
associated with them and a more naturalistic representation of tasks
would be in terms of a hierarchy, where each task can be composed
of multiple (sub)tasks. To this end, we propose an efficient Bayesian
nonparametric model for extracting hierarchies of such tasks &
subtasks. We evaluate our method based on real world query log
data both through quantitative and crowdsourced experiments and
highlight the importance of considering task/subtask hierarchies.
",16.785175570968402,15.696211180124227,185
SIGIR_17_030.txt,14.224839584683235,12.099861695984742,SIGIR,8849,"

This paper investigates if Information Foraging Theory can be used
to understand differences in user behavior when searching on mobile and desktop web search systems. Two groups of thirty-six
participants were recruited to carry out six identical web search
tasks on desktop or on mobile. The search tasks were prepared
with a different number and distribution of relevant documents on
the first result page. Search behaviors on mobile and desktop were
measurably different. Desktop participants viewed and clicked on
more results but saved fewer as relevant, compared to mobile participants, when information scent level increased. Mobile participants
achieved higher search accuracy than desktop participants for tasks
with increasing numbers of relevant search results. Conversely,
desktop participants were more accurate than mobile participants
for tasks with an equal number of relevant results that were more
distributed across the results page. Overall, both an increased number and better positioning of relevant search results improved the
ability of participants to locate relevant results on both desktop
and mobile. Participants spent more time and issued more queries
on desktop, but abandoned less and saved more results for initial
queries on mobile.
",15.172626615295595,13.834326241134754,189
SIGIR_17_031.txt,15.259681318510665,13.912693857698724,SIGIR,8166,"

Predicting the click-through rate of an advertisement is a critical
component of online advertising platforms. In sponsored search, the
click-through rate estimates the probability that a displayed advertisement is clicked by a user after she submits a query to the search
engine. Commercial search engines typically rely on machine learning models trained with a large number of features to make such
predictions. This inevitably requires a lot of engineering efforts
to define, compute, and select the appropriate features. In this paper, we propose two novel approaches (one working at character
level and the other working at word level) that use deep convolutional neural networks to predict the click-through rate of a queryadvertisement pair. Specifically, the proposed architectures only
consider the textual content appearing in a query-advertisement
pair as input, and produce as output a click-through rate prediction.
By comparing the character-level model with the word-level model,
we show that language representation can be learnt from scratch at
character level when trained on enough data. Through extensive experiments using billions of query-advertisement pairs of a popular
commercial search engine, we demonstrate that both approaches
significantly outperform a baseline model built on well-selected
text features and a state-of-the-art word2vec-based approach. Finally, by combining the predictions of the deep models introduced
in this study with the prediction of the model in production of
the same commercial search engine, we significantly improve the
accuracy and the calibration of the click-through rate prediction of
the production system.
",16.860833078287435,16.199427083333337,257
SIGIR_17_032.txt,14.01227991581925,11.909682853409759,SIGIR,7845,"

Key frames are playing a very important role for many video applications, such as on-line movie preview and video information
retrieval. Although a number of key frame selection methods have
been proposed in the past, existing technologies mainly focus on
how to precisely summarize the video content, but seldom take
the user preferences into consideration. However, in real scenarios, people may cast diverse interests on the contents even for the
same video, and thus they may be attracted by quite different key
frames, which makes the selection of key frames an inherently
personalized process. In this paper, we propose and investigate
the problem of personalized key frame recommendation to bridge
the above gap. To do so, we make use of video images and user
time-synchronized comments to design a novel key frame recommender that can simultaneously model visual and textual features
in a unified framework. By user personalization based on her/his
previously reviewed frames and posted comments, we are able to
encode different user interests in a unified multi-modal space, and
can thus select key frames in a personalized manner, which, to
the best of our knowledge, is the first time in the research field of
video content analysis. Experimental results show that our method
performs better than its competitors on various measures.
",17.28802050969988,17.32672811059908,218
SIGIR_17_033.txt,17.534936210476534,16.57724588670253,SIGIR,7802,"

Personalized itinerary recommendation is a complex and timeconsuming problem, due to the need to recommend popular attractions that are aligned to the interest preferences of a tourist, and to
plan these attraction visits as an itinerary that has to be completed
within a specific time limit. Furthermore, many existing itinerary
recommendation systems do not automatically determine and consider queuing times at attractions in the recommended itinerary,
which varies based on the time of visit to the attraction, e.g., longer
queuing times at peak hours. To solve these challenges, we propose
the PersQ algorithm for recommending personalized itineraries
that take into consideration attraction popularity, user interests
and queuing times. We also implement a framework that utilizes
geo-tagged photos to derive attraction popularity, user interests
and queuing times, which PersQ uses to recommend personalized
and queue-aware itineraries. We demonstrate the effectiveness of
PersQ in the context of five major theme parks, based on a Flickr
dataset spanning nine years. Experimental results show that PersQ
outperforms various state-of-the-art baselines, in terms of various
queuing-time related metrics, itinerary popularity, user interest
alignment, recall, precision and F1-score.
",19.946969662950774,20.04526455026455,191
SIGIR_17_034.txt,15.51271677303004,14.226843744937508,SIGIR,6828,"
 CCS CONCEPTS

Multimedia content is dominating today’s Web information. The
nature of multimedia user-item interactions is 1/0 binary implicit
feedback (e.g., photo likes, video views, song downloads, etc.), which
can be collected at a larger scale with a much lower cost than
explicit feedback (e.g., product ratings). However, the majority of
existing collaborative filtering (CF) systems are not well-designed
for multimedia recommendation, since they ignore the implicitness
in users’ interactions with multimedia content. We argue that, in
multimedia recommendation, there exists item- and component-level
implicitness which blurs the underlying users’ preferences. The
item-level implicitness means that users’ preferences on items (e.g.,
photos, videos, songs, etc.) are unknown, while the componentlevel implicitness means that inside each item users’ preferences
on different components (e.g., regions in an image, frames of a
video, etc.) are unknown. For example, a “view” on a video does not
provide any specific information about how the user likes the video
(ie, item-level) and which parts of the video the user is interested
in (i.e, component-level). In this paper, we introduce a novel
attention mechanism in CF to address the challenging item- and
component-level implicit feedback in multimedia recommendation,
dubbed Attentive Collaborative Filtering (ACF). Specifically, our
attention model is a neural network that consists of two attention
modules: the component-level attention module, starting from any
content feature extraction network (e.g., CNN for images/videos),
which learns to select informative components of multimedia
items, and the item-level attention module, which learns to score
the item preferences. ACF can be seamlessly incorporated into
classic CF models with implicit feedback, such as BPR and SVD++,
and efficiently trained using SGD. Through extensive experiments
on two real-world multimedia Web services: Vine and Pinterest,
we show that ACF significantly outperforms state-of-the-art CF
methods.
",19.587362393096058,19.635503267973863,312
SIGIR_17_035.txt,13.724665902772724,12.410067054714784,SIGIR,6603,"

Recently, some E-commerce sites launch a new interaction box
called Tips on their mobile apps. Users can express their experience
and feelings or provide suggestions using short texts typically several words or one sentence. In essence, writing some tips and giving
a numerical rating are two facets of a user’s product assessment action, expressing the user experience and feelings. Jointly modeling
these two facets is helpful for designing a better recommendation
system. While some existing models integrate text information
such as item specifications or user reviews into user and item latent factors for improving the rating prediction, no existing works
consider tips for improving recommendation quality. We propose
a deep learning based framework named NRT which can simultaneously predict precise ratings and generate abstractive tips with
good linguistic quality simulating user experience and feelings. For
abstractive tips generation, gated recurrent neural networks are
employed to “translate” user and item latent representations into a
concise sentence. Extensive experiments on benchmark datasets
from different domains show that NRT achieves significant improvements over the state-of-the-art methods. Moreover, the generated
tips can vividly predict the user experience and feelings.
",15.903189008614273,15.325925925925926,191
SIGIR_17_036.txt,14.845528595173015,13.827993721357277,SIGIR,7706,"

Many predictive tasks of web applications need to model categorical variables, such as user IDs and demographics like genders and
occupations. To apply standard machine learning techniques, these
categorical predictors are always converted to a set of binary features via one-hot encoding, making the resultant feature vector
highly sparse. To learn from such sparse data effectively, it is crucial
to account for the interactions between features.

Factorization Machines (FMs) are a popular solution for efficiently
using the second-order feature interactions. However, FM models feature interactions in a linear way, which can be insufficient
for capturing the non-linear and complex inherent structure of
real-world data. While deep neural networks have recently been
applied to learn non-linear feature interactions in industry, such as
the Wide&Deep by Google and DeepCross by Microsoft, the deep
structure meanwhile makes them difficult to train.

In this paper, we propose a novel model Neural Factorization Machine (NEM) for prediction under sparse settings. NFM seamlessly
combines the linearity of FM in modelling second-order feature
interactions and the non-linearity of neural network in modelling
higher-order feature interactions. Conceptually, NFM is more expressive than FM since FM can be seen as a special case of NFM
without hidden layers. Empirical results on two regression tasks
show that with one hidden layer only, NFM significantly outperforms FM with a 7.3% relative improvement. Compared to the recent
deep learning methods Wide&Deep and DeepCross, our NFM uses
a shallower structure but offers better performance, being much
easier to train and tune in practice.
",15.549919911452193,15.0951048951049,262
SIGIR_17_037.txt,15.678281776860914,14.022830344801001,SIGIR,7904,"

One important way for people to make their voice heard is to
comment on the articles they have read online, such as news reports
and each other’s posts. The user-generated comments together with
the commented documents form a unique correspondence structure.
Properly modeling the dependency in such data is thus vital for
one to obtain accurate insight of people’s opinions and attention.

In this work, we develop a Commented Correspondence Topic
Model to model correspondence in commented text data. We focus on two levels of correspondence. First, to capture topic-level
correspondence, we treat the topic assignments in commented documents as the prior to their comments’ topic proportions. This
captures the thematic dependency between commented documents
and their comments. Second, to capture word-level correspondence, we utilize the Dirichlet compound multinomial distribution
to model topics. This captures the word repetition patterns within
the commented data. By integrating these two aspects, our model
demonstrated encouraging performance in capturing the correspondence structure, which provides improved results in modeling usergenerated content, spam comment detection, and sentence-based
comment retrieval compared with state-of-the-art topic model solutions for correspondence modeling.
",15.6451,14.194756613756613,192
SIGIR_17_038.txt,14.853863551355548,13.16494098416161,SIGIR,7991,"

Word embedding models such as Skip-gram learn a vector-space
representation for each word, based on the local word collocation
patterns that are observed in a text corpus. Latent topic models,
on the other hand, take a more global view, looking at the word
distributions across the corpus to assign a topic to each word occurrence. These two paradigms are complementary in how they
represent the meaning of word occurrences. While some previous
works have already looked at using word embeddings for improving
the quality of latent topics, and conversely, at using latent topics
for improving word embeddings, such “two-step” methods cannot
capture the mutual interaction between the two paradigms. In this
paper, we propose STE, a framework which can learn word embeddings and latent topics in a unified manner. STE naturally obtains
topic-specific word embeddings, and thus addresses the issue of
polysemy. At the same time, it also learns the term distributions
of the topics, and the topic distributions of the documents. Our
experimental results demonstrate that the STE model can indeed
generate useful topic-specific word embeddings and coherent latent
topics in an effective and efficient way.
",15.579741850924794,13.928333333333335,193
SIGIR_17_039.txt,14.498225690939385,12.357056019437312,SIGIR,5441,"
About eight decades ago, Zipf postulated that the word frequency
distribution of languages is a power law, i.e., it is a straight line on a
log-log plot. Over the years, this phenomenon has been documented
and studied extensively. For many corpora, however, the empirical
distribution barely resembles a power law: when plotted on a log-
log scale, the distribution is concave and appears to be composed
of two differently sloped straight lines joined by a smooth curve. A
simple generative model is proposed to capture this phenomenon.
The word frequency distributions produced by this model are shown
to match the observations both analytically and empirically.

",14.836745963215662,13.27233962264151,108
SIGIR_17_040.txt,16.491205143955288,15.903652174923106,SIGIR,10357,"

A search engine that can return the ideal results for a person’s
information need, independent of the specific query that is used to
express that need, would be preferable to one that is overly swayed
by the individual terms used; search engines should be consistent
in the presence of syntactic query variations responding to the
same information need. In this paper we examine the retrieval
consistency of a set of five systems responding to syntactic query
variations over one hundred topics, working with the UQV100
test collection, and using Rank-Biased Overlap (RBO) relative to a
centroid ranking over the query variations per topic as a measure
of consistency. We also introduce a new data fusion algorithm,
Rank-Biased Centroid (RBC), for constructing a centroid ranking
over a set of rankings from query variations for a topic. RBC is
compared with alternative data fusion algorithms.

Our results indicate that consistency is positively correlated to a
moderate degree with “deep” relevance measures. However, it is
only weakly correlated with “shallow” relevance measures, as well
as measures of topic complexity and variety in query expression.
These findings support the notion that consistency is an independent property of a search engine’s retrieval effectiveness.
",17.613555460941566,17.264857142857142,203
SIGIR_17_041.txt,13.956336147208233,12.594801027516546,SIGIR,8263,"

To address concerns of TREC-style relevance judgments, we explore two improvements. The first one seeks to make relevance
judgments contextual, collecting in situ feedback of users in an
interactive search session and embracing usefulness as the primary
judgment criterion. The second one collects multidimensional assessments to complement relevance or usefulness judgments, with
four distinct alternative aspects examined in this paper—novelty,
understandability, reliability, and effort.

We evaluate different types of judgments by correlating them
with six user experience measures collected from a lab user study.
Results show that switching from TREC-style relevance criteria
to usefulness is fruitful, but in situ judgments do not exhibit clear
benefits over the judgments collected without context. In contrast,
combining relevance or usefulness with the four alternative judgments consistently improves the correlation with user experience
measures, suggesting future IR systems should adopt multi-aspect
search result judgments in development and evaluation.

We further examine implicit feedback techniques for predicting
these judgments. We find that click dwell time, a popular indicator
of search result quality, is able to predict some but not all dimensions
of the judgments. We enrich the current implicit feedback methods
using post-click user interaction in a search session and achieve
better prediction for all six dimensions of judgments.
",16.860833078287435,16.106858054226475,210
SIGIR_17_042.txt,15.202697889610203,14.23243105806651,SIGIR,8615,"

Real-time push notification systems monitor continuous document
streams such as social media posts and alert users to relevant content directly on their mobile devices. We describe a user study of
such systems in the context of the TREC 2016 Real-Time Summarization Track, where system updates are immediately delivered
as push notifications to the mobile devices of a cohort of users.
Our study represents, to our knowledge, the first deployment of
an interleaved evaluation framework for prospective information
needs, and also provides an opportunity to examine user behavior
in a realistic setting. Results of our online in-situ evaluation are
correlated against the results a more traditional post-hoc batch
evaluation. We observe substantial correlations between many online and batch evaluation metrics, especially for those that share
the same basic design (.g., are utility-based). For some metrics,
we observe little correlation, but are able to identify the volume of
messages that a system pushes as one major source of differences.

",17.122413403193683,17.374567901234567,163
SIGIR_17_043.txt,14.640177959490746,13.26577844436591,SIGIR,8913,"
The design of a Web search evaluation metric is closely related with
how the user’s interaction process is modeled. Each behavioral
model results in a different metric used to evaluate search performance. In these models and the user behavior assumptions behind
them, when a user ends a search session is one of the prime concerns because it is highly related to both benefit and cost estimation.
Existing metric design usually adopts some simplified criteria to
decide the stopping time point: (1) upper limit for benefit (e.g. RR,
AP); (2) upper limit for cost (e.g. Precision@N, DCG@N). However, in many practical search sessions (e.g. exploratory search),
the stopping criterion is more complex than the simplified case.
Analyzing benefit and cost of actual users’ search sessions, we
find that the stopping criteria vary with search tasks and are usually combination effects of both benefit and cost factors. Inspired
by a popular computer game named Bejeweled, we propose a Bejeweled Player Model (BPM) to simulate users’ search interaction
processes and evaluate their search performances. In the BPM, a
user stops when he/she either has found sufficient useful information or has no more patience to continue. Given this assumption,
a new evaluation framework based on upper limits (either fixed
or changeable as search proceeds) for both benefit and cost is proposed. We show how to derive a new metric from the framework
and demonstrate that it can be adopted to revise traditional metrics
like Discounted Cumulative Gain (DCG), Expected Reciprocal Rank
(ERR) and Average Precision (AP). To show effectiveness of the
proposed framework, we compare it with a number of existing
metrics in terms of correlation between user satisfaction and the
metrics based on a dataset that collects users’ explicit satisfaction
feedbacks and assessors’ relevance judgements. Experiment results
show that the framework is better correlated with user satisfaction
feedbacks.
",15.719379583869454,14.143436041083099,313
SIGIR_17_044.txt,14.619481535951127,13.150787818140113,SIGIR,8922,"

Mobile search engine result pages (SERPs) are becoming highly
visual and heterogenous. Unlike the traditional ten-blue-link SERPs
for desktop search, different verticals and cards occupy different
amounts of space within the small screen. Hence, traditional retrieval measures that regard the SERP as a ranked list of homogeneous items are not adequate for evaluating the overall quality of
mobile SERPs. Specifically, we address the following new problems
in mobile search evaluation: (1) Different retrieved items have different heights within the scrollable SERP, unlike a ten-blue-link SERP
in which results have similar heights with each other. Therefore,
the traditional rank-based decaying functions are not adequate for
mobile search metrics. (2) For some types of verticals and cards, the
information that the user seeks is already embedded in the snippet,
which makes clicking on those items to access the landing page
unnecessary. (3) For some results with complex sub-components
(and usually a large height), the total gain of the results cannot
be obtained if users only read part of their contents. The benefit
brought by the result is affected by user’s reading behavior and
the internal gain distribution (over the height) should be modeled
to get a more accurate estimation. To tackle these problems, we
conduct a lab-based user study to construct suitable user behavior
model for mobile search evaluation. From the results, we find that
the geometric heights of user’s browsing trails can be adopted as a
good signal of user effort. Based on these findings, we propose a
new evaluation metric, Height-Biased Gain, which is calculated by
summing up the product of gain distribution and discount factors
that are both modeled in terms of result height. To evaluate the
effectiveness of the proposed metric, we compare the agreement
of evaluation metrics with side-by-side user preferences on a test
collection composed of four mobile search engines. Experimental
results show that HBG agrees with user preferences 85.33% of the
time, which is better than all existing metrics.
",14.985893962826673,14.320419161676647,338
SIGIR_17_045.txt,15.502284394894616,14.820848023718487,SIGIR,8454,"

Complex machine learning models are now an integral part of
modern, large-scale retrieval systems. However, collection size
growth continues to outpace advances in efficiency improvements
in the learning models which achieve the highest effectiveness. In
this paper, we re-examine the importance of tightly integrating
feature costs into multi-stage learning-to-rank (LTR) IR systems.
We present a novel approach to optimizing cascaded ranking models
which can directly leverage a variety of different state-of-the-art
LTR rankers such as LambdaMART and Gradient Boosted Decision
Trees. Using our cascade model, we conclusively show that feature
costs and the number of documents being re-ranked in each stage
of the cascade can be balanced to maximize both efficiency and
effectiveness. Finally, we also demonstrate that our cascade model
can easily be deployed on commonly used collections to achieve
state-of-the-art effectiveness results while only using a subset of
the features required by the full model.

",16.52667757954773,15.52487421383648,160
SIGIR_17_046.txt,14.88058325043307,13.50191993413043,SIGIR,6319,"

Many professional organizations produce regular reports of social
indicators to monitor social progress. Despite their reasonable
results and societal value, early efforts on social indicator
computing suffer from three problems: 1) labor-intensive data
gathering, 2) insufficient data, and 3) expert-relied data fusion.
Towards this end, we present a novel graph-based multi-channel
ranking scheme for social indicator computation by exploring
the rich multi-channel Web data. For each channel, this scheme
presents the semi-structured and unstructured data with simple
graphs and hypergraphs, respectively. It then groups the channels
into different clusters according to their correlations. After that, it
uses a unified model to learn the cluster-wise common spaces,
perform ranking separately upon each space, and fuse these
rankings to produce the final one. We take Chinese university
ranking as a case study and validate our scheme over a real-world
dataset. It is worth emphasizing that our scheme is applicable
to computation of other social indicators, such as Educational
attainment.
",14.554592549557764,14.623446969696968,166
SIGIR_17_047.txt,15.137095661596366,13.154327734334892,SIGIR,9892,"

In competitive search settings as the Web, there is an ongoing ranking competition between document authors (publishers) for certain
queries. The goal is to have documents highly ranked, and the
means is document manipulation applied in response to rankings.
Existing retrieval models, and their theoretical underpinnings (e.g.,
the probability ranking principle), do not account for post-ranking
corpus dynamics driven by this strategic behavior of publishers.
However, the dynamics has major effect on retrieval effectiveness
since it affects content availability in the corpus. Furthermore, while
manipulation strategies observed over the Web were reported in
past literature, they were not analyzed as ongoing, and changing,
post-ranking response strategies, nor were they connected to the
foundations of classical ad hoc retrieval models (e.g., content-based.
document-query surface level similarities and document relevance
priors). We present a novel theoretical and empirical analysis of the
strategic behavior of publishers using these foundations. Empirical analysis of controlled ranking competitions that we organized
reveals a key strategy of publishers: making their documents (gradually) become similar to documents ranked the highest in previous
rankings. Our theoretical analysis of the ranking competition as a
repeated game, and its minmax regret equilibrium, yields a result that
supports the merits of this publishing strategy. We further show
that it can be predicted with high accuracy, and without explicit
knowledge of the ranking function, whether documents will be
promoted to the highest rank in our competitions. The prediction
utilizes very few features which quantify changes of documents,
specifically with respect to those previously ranked the highest.
",18.878054631974784,17.75751361867704,260
SIGIR_17_048.txt,16.0529968788904,14.917943849613522,SIGIR,9394,"
E-Commerce (E-Com) search is an emerging important new application of information retrieval. Learning to Rank (LETOR) is a
general effective strategy for optimizing search engines, and is thus
also a key technology for E-Com search. While the use of LETOR
for web search has been well studied, its use for E-Com search has
not yet been well explored. In this paper, we discuss the practical
challenges in applying learning to rank methods to E-Com search,
including the challenges in feature representation, obtaining reliable relevance judgments, and optimally exploiting multiple user
feedback signals such as click rates, add-to-cart ratios, order rates,
and revenue. We study these new challenges using experiments
on industry data sets and report several interesting findings that
can provide guidance on how to optimally apply LETOR to E-Com
search: First, popularity-based features defined solely on product
items are very useful and LETOR methods were able to effectively
optimize their combination with relevance-based features. Second,
query attribute sparsity raises challenges for LETOR, and selecting
features to reduce/avoid sparsity is beneficial. Third, while crowdsourcing is often useful for obtaining relevance judgments for Web
search, it does not work as well for E-Com search due to difficulty
in eliciting sufficiently fine grained relevance judgments. Finally,
among the multiple feedback signals, the order rate is found to be
the most robust training objective, followed by click rate, while
add-to-cart ratio seems least robust, suggesting that an effective
practical strategy may be to initially use click rates for training and
gradually shift to using order rates as they become available.
",18.7741,18.216821955719556,272
SIGIR_17_049.txt,16.42335281682827,15.776978395469342,SIGIR,7159,"

Query understanding is a challenging task primarily due to the
inherent ambiguity of natural language. A common strategy for
improving the understanding of natural language queries is to
annotate them with semantic information mined from a knowledge
base. Nevertheless, queries with different intents may arguably
benefit from specialized annotation strategies. For instance, some
queries could be effectively annotated with a single entity or an
entity attribute, others could be better represented by a list of
entities of a single type or by entities of multiple distinct types,
and others may be simply ambiguous. In this paper, we propose a
framework for learning semantic query annotations suitable to the
target intent of each individual query. Thorough experiments on a
publicly available benchmark show that our proposed approach can
significantly improve state-of-the-art intent-agnostic approaches
based on Markov random fields and learning to rank. Our results
further demonstrate the consistent effectiveness of our approach
for queries of various target intents, lengths, and difficulty levels,
as well as its robustness to noise in intent detection.
",17.93193317476759,16.816000000000006,176
SIGIR_17_050.txt,16.871991928972903,15.806262456043505,SIGIR,9571,"

To enhance effectiveness, a user’s query can be rewritten internally
by the search engine in many ways, for example by applying proximity, or by expanding the query with related terms. However, approaches that benefit effectiveness often have a negative impact on
efficiency, which has impacts upon the user satisfaction, if the query
is excessively slow. In this paper, we propose a novel framework for
using the predicted execution time of various query rewritings to
select between alternatives on a per-query basis, in a manner that
ensures both effectiveness and efficiency. In particular, we propose
the prediction of the execution time of ephemeral (e.g., proximity)
posting lists generated from uni-gram inverted index posting lists,
which are used in establishing the permissible query rewriting alternatives that may execute in the allowed time. Experiments examining both the effectiveness and efficiency of the proposed approach
demonstrate that a 49% decrease in mean response time (and 62%
decrease in 95th-percentile response time) can be attained without
significantly hindering the effectiveness of the search engine.
",20.075844112070616,20.191116279069764,175
SIGIR_17_051.txt,15.68387504310526,14.079223322087643,SIGIR,8612,"

Learning a high-dimensional dense representation for vocabulary
terms, also known as a word embedding, has recently attracted
much attention in natural language processing and information
retrieval tasks. The embedding vectors are typically learned based
on term proximity in a large corpus. This means that the objective
in well-known word embedding algorithms, e.g., word2vec, is to
accurately predict adjacent word(s) for a given word or context.
However, this objective is not necessarily equivalent to the goal
of many information retrieval (IR) tasks. The primary objective in
various IR tasks is to capture relevance instead of term proximity,
syntactic, or even semantic similarity. This is the motivation for
developing unsupervised relevance-based word embedding models
that learn word representations based on query-document relevance information. In this paper, we propose two learning models
with different objective functions; one learns a relevance distribution over the vocabulary set for each query, and the other classifies
each term as belonging to the relevant or non-relevant class for
each query. To train our models, we used over six million unique
queries and the top ranked documents retrieved in response to
each query, which are assumed to be relevant to the query. We
extrinsically evaluate our learned word representation models using two IR tasks: query expansion and query classification. Both
query expansion experiments on four TREC collections and query
classification experiments on the KDD Cup 2005 dataset suggest
that the relevance-based word embedding models significantly outperform state-of-the-art proximity-based embedding models, such
as word2vec and GloVe.
",18.243605946275583,16.563739299610898,259
SIGIR_17_052.txt,17.28896667699752,16.315225250278086,SIGIR,9697,"

This paper provides a unified account of two schools of thinking in
information retrieval modelling: the generative retrieval focusing
on predicting relevant documents given a query, and the discriminative retrieval focusing on predicting relevancy given a querydocument pair. We propose a game theoretical minimax game to
iteratively optimise both models. On one hand, the discriminative
model, aiming to mine signals from labelled and unlabelled data,
provides guidance to train the generative model towards fitting the
underlying relevance distribution over documents given the query.
On the other hand, the generative model, acting as an attacker
to the current discriminative model, generates difficult examples
for the discriminative model in an adversarial way by minimising
its discrimination objective. With the competition between these
two models, we show that the unified framework takes advantage
of both schools of thinking: (i) the generative model learns to fit
the relevance distribution over documents via the signals from
the discriminative model, and (ii) the discriminative model is able
to exploit the unlabelled data selected by the generative model
to achieve a better estimation for document ranking. Our experimental results have demonstrated significant performance gains as
much as 23.96% on Precision@5 and 15.50% on MAP over strong
baselines in a variety of applications including web search, item
recommendation, and question answering.

",21.493009986710348,22.353591549295775,217
SIGIR_17_053.txt,16.706161934352025,15.790929621848743,SIGIR,5113,"
Measures of node ranking, such as personalized PageRank, are
utilized in many web and social-network based prediction and
recommendation applications. Despite their effectiveness when
the underlying graph is certain, however, these measures become
difficult to apply in the presence of uncertainties, as they are not designed for graphs that include uncertain information, such as edges
that mutually exclude each other. While there are several ways to
naively extend existing techniques (such as trying to encode uncertainties as edge weights or computing all possible scenarios), as we
discuss in this paper, these either lead to large degrees of errors or
are very expensive to compute, as the number of possible worlds
can grow exponentially with the amount of uncertainty. To tackle
with this challenge, in this paper, we propose an efficient Uncertain
Personalized PageRank (UPPR) algorithm to approximately compute
personalized PageRank values on an uncertain graph with edge
uncertainties. UPPR avoids enumeration of all possible worlds, yet
it is able to achieve comparable accuracy by carefully encoding edge
uncertainties in a data structure that leads to fast approximations.
Experimental results show that UPPR is very efficient in terms of
execution time and its accuracy is comparable or better than more
costly alternatives.
",19.454632303725965,19.445940594059405,203
SIGIR_17_054.txt,15.980843760397917,14.824640235673609,SIGIR,8655,"

In this paper we address the issue of learning diverse ranking models for search result diversification. Typical methods treat the
problem of constructing a diverse ranking as a process of sequential document selection. At each ranking position, the document
that can provide the largest amount of additional information to the
users is selected, because the search users usually browse the documents in a top-down manner. Thus, to select an optimal document
for a position, it is critical for a diverse ranking model to capture the
utility of information the user have perceived from the preceding
documents. Existing methods usually calculate the ranking scores
(e.g., the marginal relevance) directly based on the query and the
selected documents, with heuristic rules or handcrafted features.
The utility the user perceived at each of the ranks, however, is not
explicitly modeled. In this paper, we present a novel diverse ranking
model on the basis of continuous state Markov decision process
(MDP) in which the user perceived utility is modeled as a part of the
MDP state. Our model, referred to as MDP-DIV, sequentially takes
the actions of selecting one document according to current state,
and then updates the state for the chosen of the next action. The
transition of the states are modeled in a recurrent manner and the
model parameters are learned with policy gradient. Experimental
results based on the TREC benchmarks showed that MDP-DIV can
significantly outperform the state-of-the-art baselines.
",15.774802946060372,14.334196721311475,246
SIGIR_17_055.txt,15.698790396611471,13.397058531746033,SIGIR,7371,"

Search result diversification aims to retrieve diverse results to satisfy as many different information needs as possible. Supervised
methods have been proposed recently to learn ranking functions
and they have been shown to produce superior results to unsupervised methods. However, these methods use implicit approaches
based on the principle of Maximal Marginal Relevance (MMR). In
this paper, we propose a learning framework for explicit result diversification where subtopics are explicitly modeled. Based on the
information contained in the sequence of selected documents, we
use attention mechanism to capture the subtopics to be focused on
while selecting the next document, which naturally fits our task
of document selection for diversification. The framework is implemented using recurrent neural networks and max-pooling which
combine distributed representations and traditional relevance features. Our experiments show that the proposed method significantly outperforms all the existing methods.
",17.122413403193683,16.03309017223911,142
SIGIR_17_056.txt,15.768836288018708,14.398049501387892,SIGIR,10029,"

While search technology is widely used for learning-oriented information needs, the results provided by popular services such as Web
search engines are optimized primarily for generic relevance, not effective learning outcomes. As a result, the typical information trail
that a user must follow while searching to achieve a learning goal
may be an inefficient one involving unnecessarily easy or difficult
content, or material that is irrelevant to actual learning progress
relative to a user’s existing knowledge. We address this problem
by introducing a novel theoretical framework, algorithms, and empirical analysis of an information retrieval model that is optimized
for learning outcomes instead of generic relevance. We do this by
formulating an optimization problem that incorporates a cognitive
learning model into a retrieval objective, and then give an algorithm
for an efficient approximate solution to find the search results that
represent the best ‘training set’ for a human learner. Our model can
personalize results for an individual user’s learning goals, as well as
account for the effort required to achieve those goals for a given set
of retrieval results. We investigate the effectiveness and efficiency
of our retrieval framework relative to a commercial search engine
baseline (‘Google’) through a crowdsourced user study involving
a vocabulary learning task, and demonstrate the effectiveness of
personalized results from our model on word learning outcomes.
",21.043213290922328,21.841063348416295,224
SIGIR_17_057.txt,16.06858904228957,14.836322197943584,SIGIR,9712,"

How do we create content that will become viral in a whole network
after we share it with friends or followers? Significant research
activity has been dedicated to the problem of strategically selecting
a seed set of initial adopters so as to maximize a meme’s spread
in a network. This line of work assumes that the success of such
a campaign depends solely on the choice of a tunable seed set
of adopters, while the way users perceive the propagated meme
is fixed. Yet, in many real-world settings, the opposite holds: a
meme’s propagation depends on users’ perceptions of its tunable
characteristics, while the set of initiators is fixed.

In this paper, we address the natural problem that arises in such
circumstances: Suggest content, expressed as a limited set of attributes, for a creative promotion campaign that starts out from a
given seed set of initiators, so as to maximize its expected spread
over a social network. To our knowledge, no previous work addresses this problem. Wefi nd that the problem is NP-hard and
inapproximable. As a tight approximation guarantee is not admissible, we design an efficient heuristic, Explore-Update, as well as a
conventional Greedy solution. Our experimental evaluation demonstrates that Explore-Update selects near-optimal attribute sets with
real data, achieves 30% higher spread than baselines, and runs an
order of magnitude faster than the Greedy solution.
",16.04434344847333,14.801818181818188,234
SIGIR_17_058.txt,16.195118080010197,14.595002481812731,SIGIR,9540,"

By incorporating healthiness into the food recommendation / ranking process we have the potential to improve the eating habits of a
growing number of people who use the Internet as a source of food
inspiration. In this paper, using insights gained from various data
sources, we explore the feasibility of substituting meals that would
typically be recommended to users with similar, healthier dishes.
First, by analysing a recipe collection sourced from Allrecipes.com,
we quantify the potential for finding replacement recipes, which are
comparable but have different nutritional characteristics and are
nevertheless highly rated by users. Building on this, we present two
controlled user studies (n=107, n=111) investigating how people
perceive and select recipes. We show participants are unable to
reliably identify which recipe contains most fat due to their answers
being biased by lack of information, misleading cues and limited
nutritional knowledge on their part. By applying machine learning
techniques to predict the preferred recipes, good performance can
be achieved using low-level image features and recipe meta-data as
predictors. Despite not being able to consciously determine which
of two recipes contains most fat, on average, participants select
the recipe with the most fat as their preference. The importance of
image features reveals that recipe choices are often visually driven.
A final user study (n=138) investigates to what extent the predictive
models can be used to select recipe replacements such that users
can be “nudged” towards choosing healthier recipes. Our findings
have important implications for online food systems.
",17.122413403193683,16.197102362204728,253
SIGIR_17_059.txt,14.954485181923449,13.858510101010104,SIGIR,7676,"

Existing recommender algorithms mainly focused on recommending individual items by utilizing user-item interactions. However,
little attention has been paid to recommend user generated lists
(e.g., playlists and booklists). On one hand, user generated lists
contain rich signal about item co-occurrence, as items within a list
are usually gathered based on a specific theme. On the other hand,
a user’s preference over a list also indicate her preference over
items within the list. We believe that 1) if the rich relevance signal
within user generated lists can be properly leveraged, an enhanced
recommendation for individual items can be provided, and 2) if
user-item and user-list interactions are properly utilized, and the
relationship between a list and its contained items is discovered,
the performance of user-item and user-list recommendations can
be mutually reinforced.

Towards this end, we devise embedding factorization models,
which extend traditional factorization method by incorporating
item-item (item-item-list) co-occurrence with embedding-based
algorithms. Specifically, we employ factorization model to capture
users’ preferences over items and lists, and utilize embeddingbased models to discover the co-occurrence information among
items and lists. The gap between the two types of models is
bridged by sharing items’ latent factors. Remarkably, our proposed
framework is capable of solving the new-item cold-start problem,
where items have never been consumed by users but exist in user
generated lists. Overall performance comparisons and micro-level
analyses demonstrate the promising performance of our proposed
approaches.
",17.238542476582836,17.30135483870968,251
SIGIR_17_060.txt,15.353505894508093,13.723248456182358,SIGIR,6514,"

This paper proposes a generic formulation that significantly expedites the training and deployment of image classification models,
particularly under the scenarios of many image categories and high
feature dimensions. As the core idea, our method represents both
the images and learned classifiers using binary hash codes, which
are simultaneously learned from the training data. Classifying an
image thereby reduces to retrieving its nearest class codes in the
Hamming space.

Specifically, we formulate multiclass image classification as an
optimization problem over binary variables. The optimization alternatingly proceeds over the binary classifiers and image hash
codes. Profiting from the special property of binary codes, we show
that the sub-problems can be efficiently solved through either a
binary quadratic program (BQP) or a linear program. In particular,
for attacking the BQP problem, we propose a novel bit-flipping
procedure which enjoys high efficacy and a local optimality guarantee. Our formulation supports a large family of empirical loss
functions and is, in specific, instantiated by exponential and linear
losses. Comprehensive evaluations are conducted on several representative image benchmarks. The experiments consistently exhibit
reduced computational and memory complexities of model training
and deployment, without sacrificing classification accuracy.
",17.80541091248751,17.43233678756477,194
SIGIR_17_061.txt,14.114610024938369,11.749420119901078,SIGIR,7831,"

Since the mid-90s there has been a widely-held belief that signature
files are inferior to inverted files for text indexing. In recent years
the Bing search engine has developed and deployed an index based
on bit-sliced signatures. This index, known as BitFunnel, replaced
an existing production system based on an inverted index. The
driving factor behind the shift away from the inverted index was
operational cost savings. This paper describes algorithmic innovations and changes in the cloud computing landscape that led us
to reconsider and eventually field a technology that was once considered unusable. The BitFunnel algorithm directly addresses four
fundamental limitations in bit-sliced block signatures. At the same
time, our mapping of the algorithm onto a cluster offers opportunities to avoid other costs associated with signatures. We show
these innovations yield a significant efficiency gain versus classic
bit-sliced signatures and then compare BitFunnel with Partitioned
Elias-Fano Indexes, MG4J, and Lucene.
",15.903189008614273,13.859928343949047,158
SIGIR_17_062.txt,14.051016940499611,11.960109390057887,SIGIR,10761,"

The efficient indexing of large and sparse N-gram datasets is crucial
in several applications in Information Retrieval, Natural Language
Processing and Machine Learning. Because of the stringent efficiency requirements, dealing with billions of N-grams poses the
challenge of introducing a compressed representation that preserves the query processing speed.

In this paper we study the problem of reducing the space required
by the representation of such datasets, maintaining the capability
of looking up for a given N-gram within micro seconds. For this
purpose we describe compressed, exact and lossless data structures
that achieve, at the same time, high space reductions and no time
degradation with respect to state-of-the-art software packages. In
particular, we present a trie data structure in which each word
following a context of fixed length k, i.e., its preceding k words, is
encoded as an integer whose value is proportional to the number of
words that follow such context. Since the number of words following a given context is typically very small in natural languages, we
are able to lower the space of representation to compression levels
that were never achieved before. Despite the significant savings in
space, we show that our technique introduces a negligible penalty
at query time.
",16.613394197324528,16.36491758241758,210
SIGIR_17_063.txt,14.285890691893147,11.966837670187406,SIGIR,8344,"

Query processing is one of the main bottlenecks in large-scale
search engines. Retrieving the top k most relevant documents for
a given query can be extremely expensive, as it involves scoring
large amounts of documents. Several dynamic pruning techniques
have been introduced in the literature to tackle this problem, such
as BlockMaxWAND, which splits the inverted index into constantsized blocks and stores the maximum document-term scores per
block; this information can be used during query execution to
safely skip low-score documents, producing many-fold speedups
over exhaustive methods.

We introduce a refinement for BlockMaxWAND that uses variablesized blocks, rather than constant-sized. We set up the problem of
deciding the block partitioning as an optimization problem which
maximizes how accurately the block upper bounds represent the
underlying scores, and describe an efficient algorithm to find an
approximate solution, with provable approximation guarantees.
Through an extensive experimental analysis we show that our
method significantly outperforms the state of the art roughly by a
factor 2x. We also introduce a compressed data structure to represent the additional block information, providing a compression ratio
of roughly 50%, while incurring only a small speed degradation, no
more than 10% with respect to its uncompressed counterpart.

",19.287186520377343,17.698263305322133,205
SIGIR_17_064.txt,15.036012000823256,14.05351869488694,SIGIR,5132,"

Locality Sensitive Hashing (LSH) algorithms are widely adopted
to index similar items in high dimensional space for approximate
nearest neighbor search. As the volume of real-world datasets keeps
growing, it has become necessary to develop distributed LSH solutions. Implementing a distributed LSH algorithm from scratch
requires high development costs, thus most existing solutions are
developed on general-purpose platforms such as Hadoop and Spark.
However, we argue that these platforms are both hard to use for
programming LSH algorithms and inefficient for LSH computation.
We propose LoSHa, a distributed computing framework that reduces the development cost by designing a tailor-made, general
programming interface and achieves high efficiency by exploring
LSH-specific system implementation and optimizations. We show
that many LSH algorithms can be easily expressed in LoSHa’s API.
We evaluate LoSHa and also compare with general-purpose platforms on the same LSH algorithms. Our results show that LoSHa’s
performance can be an order of magnitude faster, while the implementations on LoSHa are even more intuitive and require few lines
of code.

",17.410965686947208,15.62525289017341,176
SIGIR_17_065.txt,15.131580172691724,14.372398443212425,SIGIR,8177,"

Product search is an important part of online shopping. In contrast to many search tasks, the objectives of product search are
not confined to retrieving relevant products. Instead, it focuses
on finding items that satisfy the needs of individuals and lead to a
user purchase. The unique characteristics of product search make
search personalization essential for both customers and e-shopping
companies. Purchase behavior is highly personal in online shopping and users often provide rich feedback about their decisions
(e.g. product reviews). However, the severe mismatch found in the
language of queries, products and users make traditional retrieval
models based on bag-of-words assumptions less suitable for personalization in product search. In this paper, we propose a hierarchical
embedding model to learn semantic representations for entities (i.e.
words, products, users and queries) from different levels with their
associated language data. Our contributions are three-fold: (1) our
work is one of the initial studies on personalized product search;
(2) our hierarchical embedding model is the first latent space model
that jointly learns distributed representations for queries, products
and users with a deep neural network; (3) each component of our
network is designed as a generative model so that the whole structure is explainable and extendable. Following the methodology
of previous studies, we constructed personalized product search
benchmarks with Amazon product data. Experiments show that our
hierarchical embedding model significantly outperforms existing
product search baselines on multiple benchmark datasets.
",16.280829582073984,15.997945606694564,242
SIGIR_17_066.txt,15.126655163417432,13.632152065958511,SIGIR,9105,"

With the advancement of mobile computing technology and
cloud-based streaming music service, user-centered music
retrieval has become increasingly important. User-specific
information has a fundamental impact on personal music
preferences and interests. However, existing research pays little attention to the modeling and integration of user-specific
information in music retrieval algorithms/models to facilitate music search. In this paper, we propose a novel model,
named User-Information-Aware Music Interest Topic (UIAMIT) model. The model is able to effectively capture the
influence of user-specific information on music preferences,
and further associate users’ music preferences and search
terms under the same latent space. Based on this model, a
user information aware retrieval system is developed, which
can search and re-rank the results based on age- and/or
gender-specific music preferences. A comprehensive experimental study demonstrates that our methods can significantly
improve the search accuracy over existing text-based music
retrieval methods.
",17.122413403193683,16.950230414746546,156
SIGIR_17_067.txt,15.329967456635476,13.838233001213137,SIGIR,10754,"

Recommenders are becoming one of the main ways to navigate
the Internet. They recommend appropriate items to users based on
their clicks, i.e., likes, ratings, purchases, etc. These clicks are key
to providing relevant recommendations and, in this sense, have
a significant utility. Since clicks reflect the preferences of users,
they also raise privacy concerns. At first glance, there seems to
be an inherent trade-off between the utility and privacy effects
of a click. Nevertheless, a closer look reveals that the situation is
more subtle: some clicks do improve utility without compromising
privacy, whereas others decrease utility while hampering privacy.

In this paper, for the first time, we propose a way to quantify the
exact utility and privacy effects of each user click. More specifically,
we show how to compute the privacy effect (disclosure risk) of a
click using an information-theoretic approach, as well as its utility,
using a commonality-based approach. We determine precisely
when utility and privacy are antagonist and when they are not. To
illustrate our metrics, we apply them to recommendation traces
from Movielens and Jester datasets. We show, for instance, that,
considering the Movielens dataset, 5.94% of the clicks improve the
recommender utility without loss of privacy, whereas 16.43% of
the clicks induce a high privacy risk without any utility gain.

An appealing application of our metrics is what we call a
click-advisor, a visual user-aware clicking platform that helps users
decide whether it is actually worth clicking on an item or not
(after evaluating its potential utility and privacy effects using our
techniques). Using a game-theoretic approach, we evaluate several
user clicking strategies. We highlight in particular what we define
as a smart strategy, leading to a Nash equilibrium, where every
user reaches the maximum possible privacy while preserving the
average overall recommender utility for all users (with respect
to the case where user clicks are based solely on their genuine
preferences, i.e., without consulting the click-advisor).

",17.03887048696347,15.309954407294835,334
SIGIR_17_068.txt,14.510450108083678,12.992677229930148,SIGIR,7739,"

Online service providers gather vast amounts of data to build user
profiles. Such profiles improve service quality through personalization, but may also intrude on user privacy and incur discrimination
risks. In this work, we propose a framework which leverages solidarity in a large community to scramble user interaction histories.
While this is beneficial for anti-profiling, the potential downside is
that individual user utility, in terms of the quality of search results
or recommendations, may severely degrade. To reconcile privacy
and user utility and control their trade-off, we develop quantitative
models for these dimensions and effective strategies for assigning
user interactions to Mediator Accounts. We demonstrate the viability of our framework by experiments in two different application
areas (search and recommender systems), using two large datasets.
",17.315433740611066,16.636653543307087,128
SIGIR_17_069.txt,13.927170147695357,11.941712013199627,SIGIR,7845,"

Conversation systems are of growing importance since they
enable an easy interaction interface between humans and
computers: using natural languages. To build a conversation system with adequate intelligence is challenging, and
requires abundant resources including an acquisition of big
data and interdisciplinary techniques, such as information retrieval and natural language processing. Along with the prosperity of Web 2.0, the massive data available greatly facilitate data-driven methods such as deep learning for humancomputer conversation systems. Owing to the diversity of
Web resources, a retrieval-based conversation system will
come up with at least some results from the immense repository for any user inputs. Given a human issued message,
ie., query, a traditional conversation system would provide
a response after adequate training and learning of how to
respond. In this paper, we propose a new task for conversation systems: joint learning of response ranking featured
with next utterance suggestion. We assume that the new
conversation mode is more proactive and keeps user engaging. We examine the assumption in experiments. Besides,
to address the joint learning task, we propose a novel DualLSTM Chain Model to couple response ranking and next
utterance suggestion simultaneously. From the experimental results, we demonstrate the usefulness of the proposed
task and the effectiveness of the proposed model.
",16.03029750255766,15.244761904761909,212
SIGIR_17_070.txt,14.34289768550639,12.782232711877839,SIGIR,7925,"

We describe a new deep learning architecture for learning to rank
question answer pairs. Our approach extends the long short-term
memory (LSTM) network with holographic composition to model
the relationship between question and answer representations. As
opposed to the neural tensor layer that has been adopted recently,
the holographic composition provides the benefits of scalable and
rich representational learning approach without incurring huge
parameter costs. Overall, we present Holographic Dual LSTM (HDLSTM), a unified architecture for both deep sentence modeling and
semantic matching. Essentially, our model is trained end-to-end
whereby the parameters of the LSTM are optimized in a way that
best explains the correlation between question and answer representations. In addition, our proposed deep learning architecture
requires no extensive feature engineering. Via extensive experiments, we show that HD-LSTM outperforms many other neural
architectures on two popular benchmark QA datasets. Empirical
studies confirm the effectiveness of holographic composition over
the neural tensor layer.
",16.061879428645646,15.663750000000004,158
SIGIR_17_071.txt,12.634836204644603,10.505025851521935,SIGIR,7616,"

Intelligent personal assistants (IPAs) and interactive question answering (IQA) systems frequently encounter incomplete follow-up
questions. The incomplete follow-up questions only make sense
when seen in conjunction with the conversation context: the previous question and answer. Thus, IQA and IPA systems need to
utilize the conversation context in order to handle the incomplete
follow-up questions and generate an appropriate response. In this
work, we present a retrieval based sequence to sequence learning
system that can generate the complete (or intended) question for
an incomplete follow-up question (given the conversation context).
We can train our system using only a small labeled dataset (with
only a few thousand conversations), by decomposing the original
problem into two simpler and independent problems. The first problem focuses solely on selecting the candidate complete questions
from a library of question templates (built offline using the small
labeled conversations dataset). In the second problem, we re-rank
the selected candidate questions using a neural language model
(trained on millions of unlabelled questions independently). Our
system can achieve a BLEU score of 42.91, as compared to 29.11
using an existing generation based approach. We further demonstrate the utility of our system as a plug-in module to an existing
QA pipeline. Our system when added as a plug-in module, enables
Siri to achieve an improvement of 131.57% in answering incomplete
follow-up questions.
",15.6451,14.93139301310044,233
SIGIR_17_072.txt,14.901005092103293,12.93275603598131,SIGIR,7351,"

The increase of voice-based interaction has changed the way people
seek information, making search more conversational. Development
of effective conversational approaches to search requires better
understanding of how people express information needs in dialogue.
This paper describes the creation and examination of over 32K
spoken utterances collected during 34 hours of collaborative search
tasks. The contribution of this work is three-fold. First, we propose
a model of conversational information needs (CINs) based on a
synthesis of relevant theories in Information Seeking and Retrieval.
Second, we show several behavioural patterns of CINs based on the
proposed model. Third, we identify effective feature groups that
may be useful for detecting CINs categories from conversations.
This paper concludes with a discussion of how these findings can
facilitate advance of conversational search applications.
",14.90622815163357,13.495486641221376,132
SIGIR_17_073.txt,14.40427592998046,12.806385266360248,SIGIR,7600,"
Every day, social media users send millions of microblogs on every
imaginable topics. If we could predict which topics a user will
join in the future, it would be easy to determine what topics will
become popular and what kinds of users a topic may attract. It
also can be of great interest for many applications. In this study,
we investigate the problem of predicting whether a user will join
a topic based on his posting history. We introduce a novel deep
convolutional neural network with external neural memory and
attention mechanism to perform this problem. User’s posting
history and topics were modeled with an external neural memory
architecture. The convolutional neural network based matching
methods were used to construct the relations between users and
topics. Final decisions were made based on these matching results.
To train and evaluate the proposed method, we collected a largescale dataset from Twitter. The experimental results demonstrated
that the proposed method could perform significantly better than
other methods. Comparing to the state-of-the-art deep neural
networks, our approach achieves a relative improvement of 18.2%
in F1-score and 28.9% in MAP@10.
",13.319297830178854,12.026253006253011,194
SIGIR_17_074.txt,14.429033874752122,12.947807213475745,SIGIR,6958,"

User interests and expertise are valuable but often hidden resources
on social media. For example, Twitter Lists and LinkedIn’s Skill
Tags provide a partial perspective on what users are known for
(by aggregating crowd tagging knowledge), but the vast majority
of users are untagged; their interests and expertise are essentially
hidden from important applications such as personalized recommendation, community detection, and expert mining. A natural
approach to overcome these limitations is to intelligently learn user
topical profiles by exploiting information from multiple, heterogeneous footprints: for instance, Twitter users who post similar
hashtags may have similar interests, and YouTube users who upvote
the same videos may have similar preferences. And yet identifying
“similar” users by exploiting similarity in such a footprint space
often provides conflicting evidence, leading to poor-quality user
profiles. In this paper, we propose a unified model for learning user
topical profiles that simultaneously considers multiple footprints.
We show how these footprints can be embedded in a generalized
optimization framework that takes into account pairwise relations
among all footprints for robustly learning user profiles. Through
extensive experiments, we find the proposed model is capable of
learning high-quality user topical profiles, and leads to a 10-15%
improvement in precision and mean average error versus a crosstriadic factorization state-of-the-art baseline.
",19.99310930976973,19.604385382059807,217
SIGIR_17_075.txt,15.885797456630378,14.679632741985685,SIGIR,6218,"
Recently, online social networks are becoming increasingly popular
platforms for social interactions. Understanding how information
propagates in such networks is important for personalization and
recommendation in social search.
In this paper, we propose a Hierarchical Community-level Information Diffusion (HCID) model to capture the information diffusion
process in social networks. We introduce the notion of users’ topic
popularity as to enable our model to depict the information diffusion
process which is both topic-aware (which topic the information is
concerned with) and source-aware (where the information comes
from). Instead of assuming homogeneity of social communities,
we propose the notion of community hierarchy, where information
diffusion across inter-level communities is uni-directional from the
higher levels to the lower ones.
We design a Gibbs sampling algorithm to infer model parameters and propose prediction methods for two information diffusion
prediction tasks, the retweet prediction and the cascade prediction. Comparison experiments are conducted on two real datasets.
Results show that our model achieves substantial improvement
compared with the existing work.
",17.122413403193683,17.01679733727811,170
SIGIR_17_076.txt,14.652551262637218,12.430200465742267,SIGIR,8241,"

This paper presents a word-entity duet framework for utilizing
knowledge bases in ad-hoc retrieval. In this work, the query and
documents are modeled by word-based representations and entitybased representations. Ranking features are generated by the interactions between the two representations, incorporating information from the word space, the entity space, and the cross-space
connections through the knowledge graph. To handle the uncertainties from the automatically constructed entity representations,
an attention-based ranking model AttR-Duet is developed. With
back-propagation from ranking labels, the model learns simultaneously how to demote noisy entities and how to rank documents
with the word-entity duet. Evaluation results on TREC Web Track
ad-hoc task demonstrate that all of the four-way interactions in
the duet are useful, the attention mechanism successfully steers
the model away from noisy entities, and together they significantly
outperform both word-based and entity-based learning to rank
systems.
",17.315433740611066,16.48964052287582,154
SIGIR_17_077.txt,15.892598356793386,13.800902923905898,SIGIR,8352,"
Entity cards are being used frequently in modern web search engines to offer a concise overview of an entity directly on the results
page. These cards are composed of various elements, one of them
being the entity summary: a selection of facts describing the entity from an underlying knowledge base. These summaries, while
presenting a synopsis of the entity, can also directly address users’
information needs. In this paper, we make the first effort towards
generating and evaluating such factual summaries. We introduce
and address the novel problem of dynamic entity summarization
for entity cards, and break it down to two specific subtasks: fact
ranking and summary generation. We perform an extensive evaluation of our method using crowdsourcing. Our results show the
effectiveness of our fact ranking approach and validate that users
prefer dynamic summaries over static ones.
",16.439396014739867,14.15944099378882,139
SIGIR_17_078.txt,15.899579999075197,13.913857088707427,SIGIR,8270,"

We propose a new class of methods for learning vector space embeddings of entities. While most existing methods focus on modelling
similarity, our primary aim is to learn embeddings that are interpretable, in the sense that query terms have a direct geometric
representation in the vector space. Intuitively, we want all entities
that have some property (i.e. for which a given term is relevant) to
be located in some well-defined region of the space. This is achieved
by imposing max-margin constraints that are derived from a bagof-words representation of the entities. The resulting vector spaces
provide us with a natural vehicle for identifying entities that have
a given property (or ranking them according to how much they
have the property), and conversely, to describe what a given set
of entities have in common. As we show in our experiments, our
models lead to a substantially better performance in a range of
entity-oriented search tasks, such as list completion and entity
ranking.
",16.92669308720184,15.37378787878788,167
SIGMETRICS_17_001.txt,15.36319243001731,13.533880730965933,SIGMETRICS,7517,,NA,NA,0
SIGMETRICS_17_002.txt,14.827026660316093,13.51533360966977,SIGMETRICS,8909,"
A fundamental challenge in large-scale cloud networks and data centers is to achieve highly efficient server
utilization and limit energy consumption, while providing excellent user-perceived performance in the presence
of uncertain and time-varying demand patterns. Auto-scaling provides a popular paradigm for automatically
adjusting service capacity in response to demand while meeting performance targets, and queue-driven autoscaling techniques have been widely investigated in the literature. In typical data center architectures and cloud
environments however, no centralized queue is maintained, and load balancing algorithms immediately distribute
incoming tasks among parallel queues. In these distributed settings with vast numbers of servers, centralized
queue-driven auto-scaling techniques involve a substantial communication overhead and major implementation
burden, or may not even be viable at all.

Motivated by the above issues, we propose a joint auto-scaling and load balancing scheme which does not
require any global queue length information or explicit knowledge of system parameters, and yet provides provably
near-optimal service elasticity. We establish the fluid-level dynamics for the proposed scheme in a regime where
the total traffic volume and nominal service capacity grow large in proportion. The fluid-limit results show that
the proposed scheme achieves asymptotic optimality in terms of user-perceived delay performance as well as
energy consumption. Specifically, we prove that both the waiting time of tasks and the relative energy portion
consumed by idle servers vanish in the limit. At the same time, the proposed scheme operates in a distributed
fashion and involves only constant communication overhead per task, thus ensuring scalability in massive data
center operations. Extensive simulation experiments corroborate the fluid-limit results, and demonstrate that the
proposed scheme can match the user performance and energy consumption of state-of-the-art approaches that do
take full advantage of a centralized queue.
",18.9813275721742,18.88932450331126,303
SIGMETRICS_17_003.txt,14.763740340898767,13.521136007558574,SIGMETRICS,9801,"
Most present day switching systems, in Internet routers and
data-center switches, employ a single input-queued crossbar to interconnect input ports with output ports. Such
switches need to compute a matching, between input and
output ports, for each switching cycle (time slot). The main
challenge in designing such matching algorithms is to deal
with the unfortunate tradeoff between the quality of the
computed matching and the computational complexity of
the algorithm. In this paper, we propose a general approach
that can significantly boost the performance of both Serena
and iSLIP, yet incurs only O(1) additional computational
complexity at each input/output port. Our approach is a
novel proposing strategy, called Queue-Proportional Sampling (QPS), that generates an excellent starter matching.
We show, through rigorous simulations, that when starting
with this starter matching, iSLIP and Serena can output
much better final matching decisions, as measured by the
resulting throughput and delay performance, than they otherwise can.


",15.688483145680365,15.734745222929934,158
SIGMETRICS_17_004.txt,15.829525602120583,14.148431682611143,SIGMETRICS,7096,"
Despite their widespread adoption, large-scale graph processing
systems do not fully decouple computation and communication, of
ten yielding suboptimal performance. Locally-sufficient computation—

computation that relies only on the graph state local to a computing
host—can mitigate the effects of this coupling. In this paper, we
present Compute-Sync-Merge (CSM), a new programming abstraction that achieves efficient locally-sufficient computation. CSM enforces local sufficiency at the programming abstraction level and
enables the activation of vertex-centric computation on all vertex
replicas, thus supporting vertex-cut partitioning. We demonstrate
the simplicity of expressing several fundamental graph algorithms
in CSM. Hieroglyph—our implementation of a graph processing
system with CSM support—outperforms state of the art by up to
53x, with a median speedup of 3.5x and an average speedup of 6x
across a wide range of datasets.

",18.243605946275583,15.941402877697843,141
SIGMETRICS_17_005.txt,13.898416070759637,11.190267395156948,SIGMETRICS,5405,"
Graph edge partition models have recently become an appealing alternative to graph vertex partition models for distributed
computing due to their flexibility in balancing loads and their performance in reducing communication cost [6, 16]. In this
paper, we propose a simple yet effective graph edge partitioning algorithm. In practice, our algorithm provides good partition
quality (and better than similar state-of-the-art edge partition approaches, at least for power-law graphs) while maintaining
low partition overhead. In theory, previous work [6] showed that an approximation guarantee of O(dmax./log nlog k) apply
to the graphs with m = Q(k?) edges (k is the number of partitions). We further rigorously proved that this approximation
guarantee hold for all graphs.

We show how our edge partition model can be applied to parallel computing. We draw our example from GPU program
locality enhancement and demonstrate that the graph edge partition model does not only apply to distributed computing
with many computer nodes, but also to parallel computing in a single computer node with a many-core processor.
",16.827784334635936,13.972083333333334,177
SIGMETRICS_17_006.txt,15.058792579926205,13.472218645055104,SIGMETRICS,13023,"
This paper considers a traditional problem of resource allocation, scheduling jobs on machines. One such
recent application is cloud computing, where jobs arrive in an online fashion with capacity requirements
and need to be immediately scheduled on physical machines in data centers. It is often observed that the
requested capacities are not fully utilized, hence offering an opportunity to employ an overcommitment
policy, i.e., selling resources beyond capacity. Setting the right overcommitment level can induce a significant
cost reduction for the cloud provider, while only inducing a very low risk of violating capacity constraints.
We introduce and study a model that quantifies the value of overcommitment by modeling the problem as
a bin packing with chance constraints. We then propose an alternative formulation that transforms each
chance constraint into a submodular function. We show that our model captures the risk pooling effect and
can guide scheduling and overcommitment decisions. We also develop a family of online algorithms that are
intuitive, easy to implement and provide a constant factor guarantee from optimal. Finally, we calibrate
our model using realistic workload data, and test our approach in a practical setting. Our analysis and
experiments illustrate the benefit of overcommitment in cloud services, and suggest a cost reduction of 1.5%

to 17% depending on the provider’s risk tolerance.
",16.76809479433877,15.681511627906978,219
SIGMETRICS_17_007.txt,14.700252132417681,13.505869751141699,SIGMETRICS,8092,"
To combat blind in-window attacks against TCP, changes
proposed in RFC 5961 have been implemented by Linux
since late 2012. While successfully eliminating the old vulnerabilities, the new TCP implementation was reported in
August 2016 to have introduced a subtle yet serious security
flaw. Assigned CVE-2016-5696, the flaw exploits the challenge ACK rate limiting feature that could allow an off-path
attacker to infer the presence/absence of a TCP connection
between two arbitrary hosts, terminate such a connection,
and even inject payload into an unsecured TCP connection.

In this work, we perform a comprehensive measurement
of the impact of the new vulnerability. This includes (1)
tracking the vulnerable Internet servers, (2) monitoring the
patch behavior over time, (3) picturing the overall security
status of TCP stacks at scale. Towards this goal, we design a
scalable measurement methodology to scan the Alexa top 1
million websites for almost 6 months. We also present how
notifications impact the patching behavior, and compare the
result with the Heartbleed and the Debian PRNG vulnerability. The measurement represents a valuable data point in
understanding how Internet servers react to serious security
flaws in the operating system kernel.

",16.373557378465907,15.397653061224492,197
SIGMETRICS_17_008.txt,14.82723914397577,13.307846445828797,SIGMETRICS,9564,"
Industrial Control Systems (ICS) are widely deployed in mission critical infrastructures such as manufacturing, energy, and
transportation. The mission critical nature of ICS devices poses important security challenges for ICS vendors and asset
owners. In particular, the patching of ICS devices is usually deferred to scheduled production outages so as to prevent potential
operational disruption of critical systems. Unfortunately, anecdotal evidence suggests that ICS devices are riddled with
security vulnerabilities that are not patched in a timely manner, which leaves them vulnerable to exploitation by hackers,
nation states, and hacktivist organizations.

In this paper, we present the results from our longitudinal measurement and characterization study of ICS patching behavior.
Our study is based on IP scan data collected from Shodan over the duration of three years for more than 500 known industrial
ICS protocols and products. Our longitudinal measurements reveal the impact of vulnerability disclosures on ICS patching.
Our analysis of more than 100 thousand Internet-exposed ICS devices reveals that about 50% upgrade to newer patched
versions within 60 days of a vulnerability disclosure. Based on our measurement and analysis, we further propose a variation
of the Bass model to forecast the patching behavior of ICS devices. The evaluation shows that our proposed models have
comparable prediction accuracy when contrasted against traditional ARIMA timeseries forecasting models, while requiring
less parameters and being amenable to direct physical interpretation.
",17.80541091248751,17.574807017543865,229
SIGMETRICS_17_009.txt,15.962249474662265,14.070547211996118,SIGMETRICS,9046,"
There has been significant interest in studying security games for modeling the interplay of attacks and defenses on various
systems involving critical infrastructure, financial system security, political campaigns, and civil safeguarding. However,
existing security game models typically either assume additive utility functions, or that the attacker can attack only one target.
Such assumptions lead to tractable analysis, but miss key inherent dependencies that exist among different targets in current
complex networks. In this paper, we generalize the classical security game models to allow for non-additive utility functions.
We also allow attackers to be able to attack multiple targets. We examine such a general security game from a theoretical
perspective and provide a unified view. In particular, we show that each security game is equivalent to a combinatorial
optimization problem over a set system ¢, which consists of defender’s pure strategy space. The key technique we use is based
on the transformation, projection of a polytope, and the ellipsoid method. This work settles several open questions in security
game domain and significantly extends the state-of-the-art of both the polynomial solvable and NP-hard class of the security
game.
",17.631426480028413,15.915986038394419,192
SIGMETRICS_17_010.txt,13.97012096305843,12.427397869534747,SIGMETRICS,3553,"
Mean-field analysis is an analytical method for understanding large-scale stochastic systems such as large-scale data centers
and communication networks. The idea is to approximate the stationary distribution of a large-scale stochastic system using
the equilibrium point (called the mean-field limit) of a dynamical system (called the mean-field model). This approximation is
often justified by proving the weak convergence of stationary distributions to its mean-field limit. Most existing mean-field
models concerned the light-traffic regime where the load of the system, denote by p, is strictly less than one and is independent
of the size of the system. This is because a traditional mean-field model represents the limit of the corresponding stochastic
system. Therefore, the load of the mean-field model is p = limy_4o. p\%), where p) is the load of the stochastic system of
size N. Now if p(N) > 1as N — oo (ie., in the heavy-traffic regime), then p = 1. For most systems, the mean-field limits when
p = 1 are trivial and meaningless. To overcome this difficulty of traditional mean-field models, this paper takes a different
point of view on mean-field models. Instead of regarding a mean-field model as the limiting system of large-scale stochastic
system, it views the equilibrium point of the mean-field model, called a mean-field solution, simply as an approximation
of the stationary distribution of the finite-size system. Therefore both mean-field models and solutions can be functions
of N. This paper first outlines an analytical method to bound the approximation error based on Stein’s method and the
perturbation theory. We further present two examples: the M/M/N queueing system and the supermarket model under the
power-of-two-choices algorithm. For both applications, the method enables us to characterize the system performance under
a broad range of traffic loads. For the supermarket model, this is the first paper that rigorously quantifies the steady-state
performance of the-power-of-two-choices in the heavy-traffic regime. These results in the heavy-traffic regime cannot be
obtained using the traditional mean-field analysis and the interchange of the limits.
",13.911672136322576,12.675512295081969,364
SIGMETRICS_17_011.txt,13.089613480739835,10.834052452972312,SIGMETRICS,6738,"
Mean-field approximation is a powerful tool to study large-scale stochastic systems such as data-centers — one example being
the famous power of two-choice paradigm. It is shown in the literature that under quite general conditions, the empirical
measure of a system of N interacting objects converges at rate O(1/VN) to a deterministic dynamical system, called its
mean-field approximation.

In this paper, we revisit the accuracy of mean-field approximation by focusing on expected values. We show that, under
almost the same general conditions, the expectation of any performance functional converges at rate O(1/N) to its mean-field
approximation. Our result applies for finite and infinite-dimensional mean-field models. We also develop a new perturbation
theory argument that shows that the result holds for the stationary regime if the dynamical system is asymptotically
exponentially stable. We provide numerical experiments that demonstrate that this rate of convergence is tight and that
illustrate the necessity of our conditions. As an example, we apply our result to the classical two-choice model. By combining
our theory with numerical experiments, we claim that, as the load p goes to 1, the average queue length of a two-choice

system with N servers is log, =e + INCI=p) + O( x2).
",16.32212239822248,14.58568356374808,214
SIGMETRICS_17_012.txt,13.811254107808907,11.671864592969474,SIGMETRICS,5807,"
Distributed storage systems such as Hadoop File System or Google File System (GFS) ensure data availability and durability
using replication. Persistence is achieved by replicating the same data block on several nodes, and ensuring that a minimum
number of copies are available on the system at any time. Whenever the contents of a node are lost, for instance due to a hard
disk crash, the system regenerates the data blocks stored before the failure by transferring them from the remaining replicas.
This paper is focused on the analysis of the efficiency of replication mechanism that determines the location of the copies of a
given file at some server. The variability of the loads of the nodes of the network is investigated for several policies. Three
replication mechanisms are tested against simulations in the context of a real implementation of a such a system: Random,
Least Loaded and Power of Choice.

The simulations show that some of these policies may lead to quite unbalanced situations: if B is the average number of
copies per node it turns out that, at equilibrium, the load of the nodes may exhibit a high variability. It is shown in this paper
that a simple variant of a power of choice type algorithm has a striking effect on the loads of the nodes: at equilibrium, the
distribution of the load of a node has a bounded support, most of nodes have a load less than 28 which is an interesting
property for the design of the storage space of these systems. Stochastic models are introduced and investigated to explain
this interesting phenomenon.
",16.32212239822248,15.943433583959902,267
SIGMETRICS_17_013.txt,15.35462693682275,14.012576226325681,SIGMETRICS,7887,"
The energy consumption of DRAM is a critical concern in modern computing systems. Improvements in manufacturing
process technology have allowed DRAM vendors to lower the DRAM supply voltage conservatively, which reduces some of
the DRAM energy consumption. We would like to reduce the DRAM supply voltage more aggressively, to further reduce
energy. Aggressive supply voltage reduction requires a thorough understanding of the effect voltage scaling has on DRAM
access latency and DRAM reliability.

In this paper, we take a comprehensive approach to understanding and exploiting the latency and reliability characteristics
of modern DRAM when the supply voltage is lowered below the nominal voltage level specified by DRAM standards. Using an
FPGA-based testing platform, we perform an experimental study of 124 real DDR3L (low-voltage) DRAM chips manufactured
recently by three major DRAM vendors. We find that reducing the supply voltage below a certain point introduces bit errors
in the data, and we comprehensively characterize the behavior of these errors. We discover that these errors can be avoided
by increasing the latency of three major DRAM operations (activation, restoration, and precharge). We perform detailed
DRAM circuit simulations to validate and explain our experimental findings. We also characterize the various relationships
between reduced supply voltage and error locations, stored data patterns, DRAM temperature, and data retention.

Based on our observations, we propose a new DRAM energy reduction mechanism, called Voltron. The key idea of Voltron
is to use a performance model to determine by how much we can reduce the supply voltage without introducing errors and
without exceeding a user-specified threshold for performance loss. Our evaluations show that Voltron reduces the average
DRAM and system energy consumption by 10.5% and 7.3%, respectively, while limiting the average system performance loss
to only 1.8%, for a variety of memory-intensive quad-core workloads. We also show that Voltron significantly outperforms
prior dynamic voltage and frequency scaling mechanisms for DRAM.
",16.785175570968402,15.798442992338892,321
SIGMETRICS_17_014.txt,13.86112154670187,11.930020252944718,SIGMETRICS,11250,"
Storage-class memory (SCM) combines the benefits of a solid-state memory, such as high-performance and robustness, with
the archival capabilities and low cost of conventional hard-disk magnetic storage. Among candidate solid-state nonvolatile
memory technologies that could potentially be used to construct SCM, flash memory is a well-established technology and
have been widely used in commercially available SCM incarnations. Flash-based SCM enables much better tradeoffs between
performance, space and power than disk-based systems. However, write endurance is a significant challenge for a flash-based
SCM (each act of writing a bit may slightly damage a cell, so one flash cell can be written 104-10° times, depending on the
flash technology, before it becomes unusable). This is a well-documented problem and has received a lot of attention by
manufactures that are using some combination of write reduction and wear-leveling techniques for achieving longer lifetime.
In an effort to improve flash lifetime, first, by quantifying data longevity in an SCM, we show that a majority of the data stored
in a solid-state SCM do not require long retention times provided by flash memory (i.e., up to 10 years in modern devices);
second, by exploiting retention time relaxation, we propose a novel mechanism, called Dense-SLC (D-SLC), which enables us
perform multiple writes into a cell during each erase cycle for lifetime extension; and finally, we discuss the required changes
in the flash management software (FTL) in order to use D-SLC mechanism for extending the lifetime of the solid-state part of
an SCM. Using an extensive simulation-based analysis of an SLC flash-based SCM, we demonstrate that D-SLC is able to
significantly improve device lifetime (between 5.1Xx and 8.6x) with no performance overhead and also very small changes at
the FTL software.
",20.130776976110326,21.28875878220141,308
SIGMETRICS_17_015.txt,15.567027239435465,13.96736912807756,SIGMETRICS,13048,"
Variation has been shown to exist across the cells within a modern DRAM chip. Prior work has studied and exploited
several forms of variation, such as manufacturing-process- or temperature-induced variation. We empirically demonstrate a
new form of variation that exists within a real DRAM chip, induced by the design and placement of different components in
the DRAM chip: different regions in DRAM, based on their relative distances from the peripheral structures, require different
minimum access latencies for reliable operation. In particular, we show that in most real DRAM chips, cells closer to the
peripheral structures can be accessed much faster than cells that are farther. We call this phenomenon design-induced variation
in DRAM. Our goals are to i) understand design-induced variation that exists in real, state-of-the-art DRAM chips, ii) exploit
it to develop low-cost mechanisms that can dynamically find and use the lowest latency at which to operate a DRAM chip
reliably, and, thus, iii) improve overall system performance while ensuring reliable system operation.

To this end, we first experimentally demonstrate and analyze designed-induced variation in modern DRAM devices by
testing and characterizing 96 DIMMs (768 DRAM chips). Our characterization identifies DRAM regions that are vulnerable to
errors, if operated at lower latency, and finds consistency in their locations across a given DRAM chip generation, due to
design-induced variation. Based on our extensive experimental analysis, we develop two mechanisms that reliably reduce
DRAM latency. First, DIVA Profiling uses runtime profiling to dynamically identify the lowest DRAM latency that does not
introduce failures. DIVA Profiling exploits design-induced variation and periodically profiles only the vulnerable regions to
determine the lowest DRAM latency at low cost. It is the first mechanism to dynamically determine the lowest latency that
can be used to operate DRAM reliably. DIVA Profiling reduces the latency of read/write requests by 35.1%/57.8%, respectively,
at 55°C. Our second mechanism, DIVA Shuffling, shuffles data such that values stored in vulnerable regions are mapped to
multiple error-correcting code (ECC) codewords. As a result, DIVA Shuffling can correct 26% more multi-bit errors than
conventional ECC. Combined together, our two mechanisms reduce read/write latency by 40.0%/60.5%, which translates to an
overall system performance improvement of 14.7%/13.7%/13.8% (in 2-/4-/8-core systems) across a variety of workloads, while
ensuring reliable operation.
",17.410965686947208,16.081837216624688,404
SIGMETRICS_17_016.txt,13.832421418164127,12.417099339399229,SIGMETRICS,7103,"
The Named Data Networking (NDN) architecture retrieves content by names rather than connecting to specific
hosts. It provides benefits such as highly efficient and resilient content distribution, which fit well to data-intensive
distributed computing. This paper presents and discusses our experience in modifying Apache Hadoop, a popular
MapReduce framework, to operate on an NDN network. Through this first-of-its-kind implementation process, we
demonstrate the feasibility of running an existing, large, and complex piece of distributed software commonly
seen in data centers over NDN. We show advantages such as simplified network code and reduced network traffic
which are beneficial in a data center environment. There are also challenges faced by NDN, that are being
addressed by the community, which can be magnified under data center traffic. Through detailed evaluation, we
show a reduction of 16% for overall data transmission between Hadoop nodes while writing data with default
replication settings. Preliminary results also show promise for in-network caching of repeated reads in distributed
applications. We also show that overall performance is currently slower under NDN, and we identify challenges
and opportunities for further NDN improvements.
",16.18397175987059,15.040606060606063,188
SIGMETRICS_17_017.txt,16.09708585658045,14.26557871862121,SIGMETRICS,9208,"
Amazon EC2 and Google Compute Engine (GCE) have recently introduced a new class of virtual machines called “burstable”
instances that are cheaper than even the smallest traditional/regular instances. These lower prices come with reduced average
capacity and increased variance. Using measurements from both EC2 and GCE, we identify key idiosyncrasies of resource
capacity dynamism for burstable instances that set them apart from other instance types. Most importantly, certain resources
for these instances appear to be regulated by deterministic token bucket like mechanisms. We find widely different types of
disclosures by providers of the parameters governing these regulation mechanisms: full disclosure (e.g., CPU capacity for
EC2 t2 instances), partial disclosure (e.g., CPU capacity and remote disk IO bandwidth for GCE shared-core instances), or no
disclosure (network bandwidth for EC2 t2 instances). A tenant modeling these variations as random phenomena (as some
recent work suggests) might make sub-optimal procurement and operation decisions. We present modeling techniques for a
tenant to infer the properties of these regulation mechanisms via simple offline measurements. We also present two case studies
of how certain memcached workloads might benefit from our modeling when operating on EC2 by: (i) augmenting cheap but
low availability in-memory storage offered by spot instances with backup of popular content on burstable instances, and (ii)
temporal multiplexing of multiple burstable instances to achieve the CPU or network bandwidth (and thereby throughput)
equivalent of a more expensive regular EC2 instance.
",20.503739492662863,19.267500000000002,243
SIGMETRICS_17_018.txt,14.802287400136773,12.942855677813156,SIGMETRICS,10322,"
Bitcoin and other cryptocurrencies have surged in popularity over the last decade. Although Bitcoin does not claim to provide
anonymity for its users, it enjoys a public perception of being a privacy-preserving financial system. In reality, cryptocurrencies
publish users’ entire transaction histories in plaintext, albeit under a pseudonym; this is required for transaction validation.
Therefore, if a user’s pseudonym can be linked to their human identity, the privacy fallout can be significant. Recently,
researchers have demonstrated deanonymization attacks that exploit weaknesses in the Bitcoin network’s peer-to-peer (P2P)
networking protocols. In particular, the P2P network currently forwards content in a structured way that allows observers
to deanonymize users. In this work, we redesign the P2P network from first principles with the goal of providing strong,
provable anonymity guarantees. We propose a simple networking policy called DANDELION which provides quasi-optimal,
network-wide anonymity, with minimal cost to the network’s utility. We also discuss practical implementation challenges
and propose heuristic solutions.
",16.594172100314452,15.394197530864197,166
SIGMETRICS_17_019.txt,16.194164152787604,14.749038441843126,SIGMETRICS,8431,"
Cloud providers have begun to offer their surplus capacity in the form of low-cost transient servers, which can be revoked
unilaterally at any time. While the low cost of transient servers makes them attractive for a wide range of applications, such
as data processing and scientific computing, failures due to server revocation can severely degrade application performance.
Since different transient server types offer different cost and availability tradeoffs, we present the notion of server portfolios
that is based on financial portfolio modeling. Server portfolios enable construction of an “optimal” mix of severs to meet an
application’s sensitivity to cost and revocation risk. We implement model-driven portfolios in a system called ExoSphere, and
show how diverse applications can use portfolios and application-specific policies to gracefully handle transient servers. We
show that ExoSphere enables widely-used parallel applications such as Spark, MPI, and BOINC to be made transiency-aware
with modest effort. Our experiments show that allowing the applications to use suitable transiency-aware policies, ExoSphere
is able to achieve 80% cost savings when compared to on-demand servers and greatly reduces revocation risk compared to
existing approaches.
",18.548980349730343,17.603492063492066,191
SIGMETRICS_17_020.txt,14.345538437461958,12.790887490854878,SIGMETRICS,8438,"
We study online resource allocation in a cloud computing platform through posted pricing: The cloud provider publishes a unit
price for each resource type, which may vary over time; upon arrival at the cloud system, a cloud user either takes the current
prices, renting resources to execute its job, or refuses the prices without running its job there. We design pricing functions
based on current resource utilization ratios, in a wide array of demand-supply relationships and resource occupation durations,
and prove worst-case competitive ratios in social welfare. In the basic case of a single-type, non-recycled resource (allocated
resources are not later released for reuse), we prove that our pricing function design is optimal, in that it achieves the smallest
competitive ratio among all possible pricing functions. Insights obtained from the basic case are then used to generalize the
pricing functions to more realistic cloud systems with multiple types of resources, where a job occupies allocated resources
for a number of time slots till completion, upon which time the resources are returned to the cloud resource pool.
",19.032712561301913,22.216666666666672,181
SIGMETRICS_17_021.txt,15.314804267706485,14.73621592666225,SIGMETRICS,8779,"
Traditionally, Internet Access Providers (APs) only charge end-users for Internet access services; however, to recoup infrastructure costs and increase revenues, some APs have recently adopted two-sided pricing schemes under which both
end-users and content providers are charged. Meanwhile, with the rapid growth of traffic, network congestion could seriously
degrade user experiences and influence providers’ utility. To optimize profit and social welfare, APs and regulators need to
design appropriate pricing strategies and regulatory policies that take the effects of network congestion into consideration.
In this paper, we model two-sided networks under which users’ traffic demands are influenced by exogenous pricing and
endogenous congestion parameters and derive the system congestion under an equilibrium. We characterize the structures
and sensitivities of profit- and welfare-optimal two-sided pricing schemes and reveal that 1) the elasticity of system throughput plays a crucial role in determining the structures of optimal pricing, 2) the changes of optimal pricing under varying
AP’s capacity and users’ congestion sensitivity are largely driven by the type of data traffic, e.g., text or video, and 3) APs
and regulators will be incentivized to shift from one-sided to two-sided pricing when APs’ capacities and user demand for
video traffic grow. Our results can help APs design optimal two-sided pricing and guide regulators to legislate desirable policies.
",20.10790988173199,21.574095022624437,224
SIGMETRICS_17_022.txt,14.26930638258445,12.132762071965935,SIGMETRICS,6916,"
Estimating cascade size and nodes’ influence is a fundamental task in social, technological, and biological networks. Yet
this task is extremely challenging due to the sheer size and the structural heterogeneity of networks. We investigate a new
influence measure, termed outward influence (OJ), defined as the (expected) number of nodes that a subset of nodes S will
activate, excluding the nodes in S. Thus, OI equals, the de facto standard measure, influence spread of S minus |S|. OI is not
only more informative for nodes with small influence, but also, critical in designing new effective sampling and statistical
estimation methods,

Based on OI, we propose SIEA/SOIEA, novel methods to estimate influence spread/outward influence at scale and with
rigorous theoretical guarantees. The proposed methods are built on two novel components 1) IICP an important sampling
method for outward influence; and 2) RSA, a robust mean estimation method that minimize the number of samples through
analyzing variance and range of random variables. Compared to the state-of-the art for influence estimation, SIEA is Q(log* n)
times faster in theory and up to several orders of magnitude faster in practice. For the first time, influence of nodes in the
networks of billions of edges can be estimated with high accuracy within a few minutes. Our comprehensive experiments on
real-world networks also give evidence against the popular practice of using a fixed number, e.g. 10K or 20K, of samples to
compute the “ground truth” for influence spread.
",15.774802946060372,14.267200000000003,250
SIGMETRICS_17_023.txt,14.196978122149726,12.18359258938245,SIGMETRICS,7922,"
Recent years have seen a rapid growth of interest in exploiting monitoring data collected from enterprise
applications for automated management and performance analysis. In spite of this trend, even simple performance
inference problems involving queueing theoretic formulas often incur computational bottlenecks, for example upon
computing likelihoods in models of batch systems. Motivated by this issue, we revisit the solution of multiclass
closed queueing networks, which are popular models used to describe batch and distributed applications with
parallelism constraints. We first prove that the normalizing constant of the equilibrium state probabilities of
a closed model can be reformulated exactly as a multidimensional integral over the unit simplex. This gives
as a by-product novel explicit expressions for the multiclass normalizing constant. We then derive a method
based on cubature rules to efficiently evaluate the proposed integral form in small and medium-sized models.
For large models, we propose novel asymptotic expansions and Monte Carlo sampling methods to efficiently
and accurately approximate normalizing constants and likelihoods. We illustrate the resulting accuracy gains in
problems involving optimization-based inference.
",18.377959752453624,17.260454545454547,177
SIGMETRICS_17_024.txt,14.48010148756233,12.685941621828963,SIGMETRICS,6077,"
Networks are integral parts of modern safety-critical systems and certification demands the provision of guarantees for
data transmissions. Deterministic Network Calculus (DNC) can compute a worst-case bound on a data flow’s end-to-end
delay. Accuracy of DNC results has been improved steadily, resulting in two DNC branches: the classical algebraic analysis
and the more recent optimization-based analysis. The optimization-based branch provides a theoretical solution for tight
bounds. Its computational cost grows, however, (possibly super-)exponentially with the network size. Consequently, a
heuristic optimization formulation trading accuracy against computational costs was proposed. In this article, we challenge
optimization-based DNC with a new algebraic DNC algorithm.
We show that:

(1) no current optimization formulation scales well with the network size and
(2) algebraic DNC can be considerably improved in both aspects, accuracy and computational cost.

To that end, we contribute a novel DNC algorithm that transfers the optimization’s search for best attainable delay bounds to
algebraic DNC. It achieves a high degree of accuracy and our novel efficiency improvements reduce the cost of the analysis
dramatically. In extensive numerical experiments, we observe that our delay bounds deviate from the optimization-based
ones by only 1.142% on average while computation times simultaneously decrease by several orders of magnitude.
",17.228024813938504,16.441619991383032,215
SIGMETRICS_17_025.txt,17.986318963064914,17.42676707703797,SIGMETRICS,7218,"
The modern world is becoming increasingly dependent on computing and communication technology to function, but
unfortunately its application and impact on areas such as critical infrastructure and industrial control system (ICS) networks
remains to be thoroughly studied. Significant research has been conducted to address the myriad security concerns in these
areas, but they are virtually all based on artificial testbeds or simulations designed on assumptions about their behavior either
from knowledge of traditional IT networking or from basic principles of ICS operation. In this work, we provide the most
detailed characterization of an example ICS to date in order to determine if these common assumptions hold true. A live
power distribution substation is observed over the course of two and a half years to measure its behavior and evolution over
time. Then, a horizontal study is conducted that compared this behavior with three other substations from the same company.
Although most predictions were found to be correct, some unexpected behavior was observed that highlights the fundamental
differences between ICS and IT networks including round trip times dominated by processing speed as opposed to network
delay, several well known TCP features being largely irrelevant, and surprisingly large jitter from devices running real-time
operating systems. The impact of these observations is discussed in terms of generality to other embedded networks, network
security applications, and the suitability of the TCP protocol for this environment.

",19.99310930976973,19.715024630541873,233
SIGMETRICS_17_026.txt,15.624303158789953,13.18892564782356,SIGMETRICS,8425,"
Persistent spread measurement is to count the number of distinct elements that persist in each network flow
for predefined time periods. It has many practical applications, including detecting long-term stealthy network
activities in the background of normal-user activities, such as stealthy DDoS attack, stealthy network scan, or
faked network trend, which cannot be detected by traditional flow cardinality measurement. With big network
data, one challenge is to measure the persistent spreads of a massive number of flows without incurring too much
memory overhead as such measurement may be performed at the line speed by network processors with fast but
small on-chip memory. We propose a highly compact Virtual Intersection HyperLogLog (VI-HLL) architecture for
this purpose. It achieves far better memory efficiency than the best prior work of V-Bitmap, and in the meantime
drastically extends the measurement range. Theoretical analysis and extensive experiments demonstrate that
VI-HLL provides good measurement accuracy even in very tight memory space of less than 1 bit per flow.
",17.5058628484301,16.26095238095238,169
SIGMETRICS_17_027.txt,15.503899672038209,13.735803484438172,SIGMETRICS,10573,"
Several optimizations have been proposed to improve the performance of the mobile Web. However, these optimizations often
overlook an important factor, energy. Given the importance of battery life for mobile users, we argue that such optimizations
should be evaluated for their impact on energy consumption. Unfortunately, teasing out the effect of an optimization on
the energy consumption of the page load process is difficult, even with the help of power monitors. This is because of the
complexity of the page loads, the high variance in page load times, and the relatively short-lived page load process.

In this paper, we present RECON, a modeling approach that predicts the energy consumption of any Web page load. Our
key intuition is to model page load times using low-level information about the page load process. RECON combines the
low-level page load information with coarse-grained resource monitoring to build a power model. Experiments across 50 Web
pages and under four different optimizations show that RECON can predict energy consumption for a Web page load with an
error of less than 5%. By leveraging low-level page load information, RECON can not only identify when a given optimization
negatively affects energy, but can also reveal why. Importantly, RECON allows us to quickly evaluate various optimization
choices with respect to both performance and energy.
",14.937675812826573,13.496133278486223,222
SIGMOD_17_001.txt,15.08512580025564,12.717625040922147,SIGMOD,12262,"

The optimistic variants of Multi-Version Concurrency Control
(MVCC) avoid blocking concurrent transactions at the cost of having a validation phase. Upon failure in the validation phase, the
transaction is usually aborted and restarted from scratch. The “abort
and restart” approach becomes a performance bottleneck for use
cases with high contention objects or long running transactions. In
addition, restarting from scratch creates a negative feedback loop
in the system, because the system incurs additional overhead that
may create even more conflicts.

In this paper, we propose a novel approach for conflict resolution
in MVCC for in-memory databases. This low overhead approach
summarizes the transaction programs in the form of a dependency
graph. The dependency graph also contains the constructs used in
the validation phase of the MVCC algorithm. Then, when encountering conflicts among transactions, our mechanism quickly detects
the conflict locations in the program and partially re-executes the
conflicting transactions. This approach maximizes the reuse of the
computations done in the initial execution round, and increases the
transaction processing throughput.

",15.903189008614273,14.09124031007752,173
SIGMOD_17_002.txt,13.165072584727056,11.094809128932447,SIGMOD,11724,"

As one of the most well known graph computation problems, Personalized PageRank is an effective approach for computing the
similarity score between two nodes, and it has been widely used in
various applications, such as link prediction and recommendation.
Due to the high computational cost and space cost of computing the
exact Personalized PageRank Vector (PPV), most existing studies
compute PPV approximately. In this paper, we propose novel and
efficient distributed algorithms that compute PPV exactly based
on graph partitioning on a general coordinator-based share-nothing
distributed computing platform. Our algorithms takes three aspects
into account: the load balance, the communication cost, and the
computation cost of each machine. The proposed algorithms only
require one time of communication between each machine and the
coordinator at query time. The communication cost is bounded, and
the work load on each machine is balanced. Comprehensive experiments conducted on five real datasets demonstrate the efficiency
and the scalability of our proposed methods.
",16.785175570968402,15.829577717879605,160
SIGMOD_17_003.txt,13.604596399156613,11.931470330381465,SIGMOD,11672,"

Join size estimation is a critical step in query optimization,
and has been extensively studied in the literature. Among
the many techniques, sampling based approaches are particularly appealing, due to their ability to handle arbitrary selection predicates. In this paper, we propose a new sampling
algorithm for join size estimation, called two-level sampling,
which combines the advantages of three previous sampling
methods while making further improvements. Both analytical and empirical comparisons show that the new algorithm outperforms all the previous algorithms on a variety
of joins, including primary key-foreign key joins, many-tomany joins, and multi-table joins. The new sampling algorithm is also very easy to implement, requiring just one pass
over the data. It only relies on some basic statistical information about the data, such as the £,-norms and the heavy
hitters.
",16.52667757954773,15.709160583941607,137
SIGMOD_17_004.txt,12.830914104802147,10.673506664508121,SIGMOD,12352,"

OLAP tools have been extensively used by enterprises to make better and faster decisions. Nevertheless, they require users to specify
group-by attributes and know precisely what they are looking for.
This paper takes the first attempt towards automatically extracting
top-k insights from multi-dimensional data. This is useful not only
for non-expert users, but also reduces the manual effort of data
analysts. In particular, we propose the concept of insight which
captures interesting observation derived from aggregation results
in multiple steps (e.g., rank by a dimension, compute the percentage of measure by a dimension). An example insight is: “Brand B’s
rank (across brands) falls along the year, in terms of the increase in
sales”. Our problem is to compute the top-k insights by a score
function. It poses challenges on (i) the effectiveness of the result
and (ii) the efficiency of computation. We propose a meaningful
scoring function for insights to address (i). Then, we contribute a
computation framework for top-k insights, together with a suite of
optimization techniques (i.e., pruning, ordering, specialized cube,
and computation sharing) to address (ii). Our experimental study
on both real data and synthetic data verifies the effectiveness and
efficiency of our proposed solution.

",14.158211354625415,12.581942959001786,208
SIGMOD_17_005.txt,14.680247321616527,12.504189622259407,SIGMOD,15884,"

Learning novel relations from relational databases is an important
problem with many applications. Relational learning algorithms
learn the definition of a new relation in terms of existing relations in
the database. Nevertheless, the same database may be represented
under different schemas for various reasons, such as data quality,
efficiency and usability. The output of current relational learning algorithms tends to vary quite substantially over the choice of
schema. This variation complicates their off-the-shelf application.
We introduce and formalize the property of schema independence
of relational learning algorithms, and study both the theoretical and
empirical dependence of existing algorithms on the common class
of (de) composition schema transformations. We show that current
algorithms are not schema independent. We propose Castor, a relational learning algorithm that achieves schema independence by
leveraging data dependencies.

",16.827784334635936,16.09074248120301,134
SIGMOD_17_006.txt,14.015710390267195,11.773008363976142,SIGMOD,10497,"

Influence maximization (IM) on social networks is one of the most
active areas of research in computer science. While various IM
techniques proposed over the last decade have definitely enriched
the field, unfortunately, experimental reports on existing techniques
fall short in validity and integrity since many comparisons are not
based on a common platform or merely discussed in theory. In this
paper, we perform an in-depth benchmarking study of IM techniques on social networks. Specifically, we design a benchmarking
platform, which enables us to evaluate and compare the existing
techniques systematically and thoroughly under identical experimental conditions. Our benchmarking results analyze and diagnose
the inherent deficiencies of the existing approaches and surface the
open challenges in IM even after a decade of research. More fundamentally, we unearth and debunk a series of myths and establish
that there is no single state-of-the-art technique in IM. At best, a
technique is the state of the art in only one aspect.

",15.532846611407376,14.777689594356264,163
SIGMOD_17_007.txt,14.470707427351051,13.053381156730104,SIGMOD,12406,"

Data-driven applications rely on the correctness of their data
to function properly and effectively. Errors in data can be
incredibly costly and disruptive, leading to loss of revenue,
incorrect conclusions, and misguided policy decisions. While
data cleaning tools can purge datasets of many errors before
the data is used, applications and users interacting with the
data can introduce new errors. Subsequent valid updates can
obscure these errors and propagate them through the dataset
causing more discrepancies. Even when some of these discrepancies are discovered, they are often corrected superficially,
on a case-by-case basis, further obscuring the true underlying
cause, and making detection of the remaining errors harder.

In this paper, we propose QFix, a framework that derives
explanations and repairs for discrepancies in relational data,
by analyzing the effect of queries that operated on the data
and identifying potential mistakes in those queries. QFix
is flexible, handling scenarios where only a subset of the
true discrepancies is known, and robust to different types of
update workloads. We make four important contributions:
(a) we formalize the problem of diagnosing the causes of data
errors based on the queries that operated on and introduced
errors to a dataset; (b) we develop exact methods for deriving
diagnoses and fixes for identified errors using state-of-the-art
tools; (c) we present several optimization techniques that
improve our basic approach without compromising accuracy, and (d) we leverage a tradeoff between accuracy and
performance to scale diagnosis to large datasets and query
logs, while achieving near-optimal results. We demonstrate
the effectiveness of QF ix through extensive evaluation over
benchmark and synthetic data.

",18.599290044081553,18.27289052890529,272
SIGMOD_17_008.txt,15.264697315962238,13.472314944844218,SIGMOD,10380,"

Network provenance, which records the execution history
of network events as meta-data, is becoming increasingly
important for network accountability and failure diagnosis.
For example, network provenance may be used to trace the
path that a message traversed in a network, or to reveal
how a particular routing entry was derived and the parties
involved in its derivation. A challenge when storing the
provenance of a live network is that the large number of arriving messages may incur substantial storage overhead. In
this paper, we explore techniques to dynamically compress
distributed provenance stored at scale. Logically, compression is achieved by grouping equivalent provenance trees
and maintaining only one concrete copy for each equivalence class. To efficiently identify the equivalent provenance,
we (1) introduce distributed event-based linear programs
(DELPs) to specify distributed network applications, and
(2) statically analyze DELPs to allow for quick detection
of provenance equivalence at runtime. Our experimental
results demonstrate that our approach leads to significant
storage reduction and query latency improvement over alternative approaches.

",18.3970566412798,17.37,169
SIGMOD_17_009.txt,13.007775867450576,11.408252119075392,SIGMOD,16926,"

Answering reachability queries is one of the fundamental graph operations. The existing approaches build indexes and answer reachability
queries on a directed acyclic graph (DAG) G,, which is constructed by
coalescing each strongly connected component of the given directed
graph G into a node of G. Considering that G can still be large to be
processed efficiently, there are studies to further reduce G' to a smaller
graph. However, these approaches suffer from either inefficiency in
answering reachability queries, or cannot scale to large graphs.

In this paper, we study DAG reduction to accelerate reachability
query processing, which reduces the size of G by computing transitive
reduction (TR) followed by computing equivalence reduction (ER).
For TR, we propose a bottom-up algorithm, namely buTR, which
removes from G all redundant edges to get the unique smallest DAG
G"" satisfying that G* has the same transitive closure as that of G. For
ER, we propose a divide-and-conquer algorithm, namely linear-ER.
Given the result G* of TR, linear-ER gets a smaller DAG G* in linear
time based on equivalence relationship between nodes in G.. Our DAG
reduction approaches (TR and ER) significantly improve the cost of
time and space, and can be scaled to large graphs. We confirm the
efficiency of our approaches by extensive experimental studies for TR,
ER, and reachability query processing using 20 real datasets.

",17.833180683606166,16.605978260869566,231
SIGMOD_17_010.txt,17.156858534453185,15.897471314695974,SIGMOD,11923,"

As data volumes continue to rise, manual inspection is becoming
increasingly untenable. In response, we present MacroBase, a data
analytics engine that prioritizes end-user attention in high-volume
fast data streams. MacroBase enables efficient, accurate, and modular analyses that highlight and aggregate important and unusual
behavior, acting as a search engine for fast data. MacroBase is
able to deliver order-of-magnitude speedups over alternatives by
optimizing the combination of explanation and classification tasks
and by leveraging a new reservoir sampler and heavy-hitters sketch
specialized for fast data streams. As a result, MacroBase delivers
accurate results at speeds of up to 2M events per second per query
on a single core. The system has delivered meaningful results in
production, including at a telematics company monitoring hundreds
of thousands of vehicles.

",18.243605946275583,16.76878787878788,133
SIGMOD_17_011.txt,15.183810509712497,14.297438157361857,SIGMOD,15345,"

In many real applications such as bioinformatics and biological
network analysis, it has always been an important, yet challenging, topic to accurately infer/reconstruct gene regulatory networks
(GRNs) from microarray data, and efficiently identify those matching GRNs with similar interaction structures for potential disease
analysis and treatment tasks. Motivated by this, in this paper, we
formalize the problem of ad-hoc inference and matching over gene
regulatory networks (IM-GRN), which deciphers ad-hoc GRN graph
structures online from gene feature databases (without full GRN
materializations), and retrieves the inferred GRNs that are subgraphisomorphic to a query GRN graph with high confidences. Specifically, we propose a novel probabilistic score to measure the possible interaction between any two genes (inferred from gene feature vectors), and thus model GRNs by probabilistic graphs, containing edge existence probabilities. In order to efficiently process
IM-GRN queries, we propose effective reduction, pruning, and embedding strategies to significantly reduce the search space of GRN
inference and matching, without materializing all GRNs. We also
present an effective indexing mechanism and an efficient IM-GRN
query processing algorithm by the index traversal. Finally, extensive experiments have been conducted to verify the efficiency and
effectiveness of our proposed IM-GRN query answering approaches over real/synthetic GRN data sets.
",20.890749979661237,21.491428571428575,211
SIGMOD_17_012.txt,14.014288883387692,12.561252714965878,SIGMOD,14105,"

Modern data analytical tasks often witness very wide tables, from
a few hundred columns to a few thousand. While it is commonly
agreed that column stores are an appropriate data format for wide
tables and analytical workloads, the physical order of columns has
not been investigated. Column ordering plays a critical role in /O
performance, because in wide tables accessing the columns in a
single horizontal partition may involve multiple disk seeks. An
optimal column ordering will incur minimal cumulative disk seek
costs for the set of queries applied to the data. In this paper, we aim
to find such an optimal column layout to maximize I/O performance.
Specifically, we study two problems for column stores on HDFS:
column ordering and column duplication. Column ordering seeks an
approximately optimal order of columns; column duplication complements column ordering in that some columns may be duplicated
multiple times to reduce contention among the queries’ diverse requirements on the column order. We consider an actual fine-grained
cost model for column accesses and propose algorithms that take
a query workload as input and output a column ordering strategy
with or without storage redundancy that significantly improves the
overall I/O performance. Experimental results over real-life data
and production query workloads confirm the effectiveness of the
proposed algorithms in diverse settings.

",16.18397175987059,15.668036529680368,220
SIGMOD_17_013.txt,14.719612218680385,13.532310367709314,SIGMOD,11835,"
In Entity Resolution, the objective is to find which records
of a dataset refer to the same real-world entity. Crowd Entity Resolution uses humans, in addition to machine algorithms, to improve the quality of the outcome. We study an
approach that combines two common interfaces for human
tasks in Crowd Entity Resolution, taking into account some
key observations about the advantages and disadvantages
of the two interfaces. We give a formal definition to the
problem of human tasks’ selection and we derive algorithms
with strong optimality guarantees. Our experiments with
three real-world datasets show that our approach gives an
improvement of 50% to 300% in the crowd cost to resolve a
dataset, compared to using only tasks of the same interface.
",16.404322709996244,15.013756097560975,124
SIGMOD_17_014.txt,15.97287904571138,14.590959896011757,SIGMOD,11605,"

Finding the maxima of a database based on a user preference, especially when the ranking function is a linear combination of the
attributes, has been the subject of recent research. A critical observation is that the convex hull is the subset of tuples that can be
used to find the maxima of any linear function. However, in real
world applications the convex hull can be a significant portion of
the database, and thus its performance is greatly reduced. Thus,
computing a subset limited to r tuples that minimizes the regret ratio (a measure of the user’s dissatisfaction with the result from the
limited set versus the one from the entire database) is of interest.

In this paper, we make several fundamental theoretical as well
as practical advances in developing such a compact set. In the case
of two dimensional databases, we develop an optimal linearithmic
time algorithm by leveraging the ordering of skyline tuples. In
the case of higher dimensions, the problem is known to be NPcomplete. As one of our main results of this paper, we develop an
approximation algorithm that runs in linearithmic time and guarantees a regret ratio, within any arbitrarily small user-controllable
distance from the optimal regret ratio. The comprehensive set of
experiments on both synthetic and publicly available real datasets
confirm the efficiency, quality of output, and scalability of our proposed algorithms.

",18.001758247042904,16.078596491228073,230
SIGMOD_17_015.txt,13.960201583289564,12.29097291309937,SIGMOD,11454,"
We study distributed machine learning in heterogeneous environments in this work. We first conduct a systematic study of existing systems running distributed stochastic gradient descent; we
find that, although these systems work well in homogeneous environments, they can suffer performance degradation, sometimes up
to 10x, in heterogeneous environments where stragglers are common because their synchronization protocols cannot fit a heterogeneous setting. Our first contribution is a heterogeneity-aware algorithm that uses a constant learning rate schedule for updates before
adding them to the global parameter. This allows us to suppress
stragglers’ harm on robust convergence. As a further improvement,
our second contribution is a more sophisticated learning rate schedule that takes into consideration the delayed information of each
update. We theoretically prove the valid convergence of both approaches and implement a prototype system in the production cluster of our industrial partner Tencent Inc. We validate the performance of this prototype using a range of machine-learning workloads. Our prototype is 2-12 faster than other state-of-the-art systems, such as Spark, Petuum, and TensorFlow; and our proposed
algorithm takes up to 6x fewer iterations to converge.

",16.061879428645646,16.421808510638296,189
SIGMOD_17_016.txt,13.284119617715426,11.176773269723256,SIGMOD,12767,"

Recently, massive data management plays an increasingly important role in data analytics because data access is a major bottleneck. Data skipping is a promising technique to reduce the number of data accesses. Data skipping partitions data into pages and
accesses only pages that contain data to be retrieved by a query.
Therefore, effective data partitioning is required to minimize the
number of page accesses. However, it is an NP-hard problem to
obtain optimal data partitioning given query pattern and data distribution.

We propose a framework that involves a multidimensional indexing technique based on a space-filling curve. A space-filling
curve is a way to define which portion of data can be stored in the
same page. Therefore, the problem can be interpreted as selecting a
curve that distributes data to be accessed by a query to minimize the
number of page accesses. To solve this problem, we analyzed how
different space-filling curves affect the number of page accesses.
We found that it is critical for a curve to fit a query pattern and be
robust against any data distribution. We propose a cost model for
measuring how well a space-filling curve fits a given query pattern
and tolerates data skew. Also we propose a method for designing a
query-aware and skew-tolerant curve for a given query pattern.

We prototyped our framework using the defined query-aware
and skew-tolerant curve. We conducted experiments using a skew
data set, and confirmed that our framework can reduce the number of page accesses by an order of magnitude for data warehousing (DWH) and geographic information systems (GIS) applications
with real-world data.

",13.371156678226303,12.280896103896104,276
SIGMOD_17_017.txt,15.117634911229718,14.070284723899771,SIGMOD,11476,"

Internet of Things applications analyze the data coming from large
networks of sensor devices using relational and signal processing
operations and running the same query logic over groups of sensor
signals. To support such increasingly important scenarios, many
data management systems integrate with numerical frameworks like
R. Such solutions, however, incur significant performance penalties as relational data processing engines and numerical tools operate on fundamentally different data models with expensive intercommunication mechanisms. In addition, none of these solutions
supports efficient real-time and incremental analysis.

In this paper, we advocate a deep integration of signal processing
operations and general-purpose query processors. We aim to reconcile the disparate data models and provide a common query language that allows users to seamlessly interleave tempo-relational
and signal operations for both online and offline processing. Our
approach is extensible and offers frameworks for quick and easy
integration of user-defined operations while supporting incremental computation. Our system that deeply integrates relational and
signal operations, called TRILLDSP, achieves up to two orders of
magnitude better performance than popular loosely-coupled data
management systems on grouped signal processing workflows.

",18.643177196211187,18.36820945945946,186
SIGMOD_17_018.txt,14.110713360522215,12.290185114508386,SIGMOD,11357,"

In recent years, the popularity of graph databases has grown
rapidly. This paper focuses on single-graph as an effective
model to represent information and its related graph mining techniques. In frequent pattern mining in a single-graph
setting, there are two main problems: support measure and
search scheme. In this paper, we propose a novel framework
for constructing support measures that brings together existing minimum-image-based and overlap-graph-based support
measures. Our framework is built on the concept of occurrence / instance hypergraphs. Based on that, we present
two new support measures: minimum instance (MI) measure
and minimum vertex cover (MVC) measure, that combine
the advantages of existing measures. In particular, we show
that the existing minimum-image-based support measure is
an upper bound of the MI measure, which is also linear-time
computable and results in counts that are close to number of
instances of a pattern. Although the MVC measure is NPhard, it can be approximated to a constant factor in polynomial time. We also provide polynomial-time relaxations for
both measures and bounding theorems for all presented support measures in the hypergraph setting. We further show
that the hypergraph-based framework can unify all support
measures studied in this paper. This framework is also flexible in that more variants of support measures can be defined
and profiled in it.
",13.319297830178854,12.263717171717172,226
SIGMOD_17_019.txt,15.549919911452193,14.167159380649945,SIGMOD,13783,"

We describe BUDS, a declarative language for succinctly and simply specifying the implementation of large-scale machine learning
algorithms on a distributed computing platform. The types supported in BUDS—vectors, arrays, etc.—are simply logical abstractions useful for programming, and do not correspond to the actual
implementation. In fact, BUDS automatically chooses the physical realization of these abstractions in a distributed system, by taking into account the characteristics of the data. Likewise, there
are many available implementations of the abstract operations offered by BUDS (matrix multiplies, transposes, Hadamard products,
etc.). These are tightly coupled with the physical representation.
In BUDS, these implementations are co-optimized along with the
representation. All of this allows for the BUDS compiler to automatically perform deep optimizations of the user’s program, and
automatically generate efficient implementations.

",16.785175570968402,16.569640130861504,133
SIGMOD_17_020.txt,15.697502452512943,14.77765546985938,SIGMOD,14504,"

dbDedup is a similarity-based deduplication scheme for on-line
database management systems (DBMSs). Beyond block-level
compression of individual database pages or operation log (oplog)
messages, as used in today’s DBMSs, dbDedup uses byte-level
delta encoding of individual records within the database to achieve
greater savings. dbDedup’s single-pass encoding method can be
integrated into the storage and logging components of a DBMS
to provide two benefits: (1) reduced size of data stored on disk
beyond what traditional compression schemes provide, and (2) reduced amount of data transmitted over the network for replication
services. To evaluate our work, we implemented dbDedup in a distributed NoSQL DBMS and analyzed its properties using four real
datasets. Our results show that dbDedup achieves up to 37× reduction in the storage size and replication traffic of the database on its
own and up to 61× reduction when paired with the DBMS’s blocklevel compression. dbDedup provides both benefits with negligible
effect on DBMS throughput or client latency (average and tail).
",20.503739492662863,22.253180473372783,171
SIGMOD_17_021.txt,15.930571867494336,14.118385385568295,SIGMOD,11663,"

This paper studies the problem of efficiently computing a maximum
independent set from a large graph, a fundamental problem in graph
analysis. Due to the hardness results of computing an exact maximum independent set or an approximate maximum independent set
with accuracy guarantee, the existing algorithms resort to heuristic
techniques for approximately computing a maximum independent
set with good performance in practice but no accuracy guarantee
theoretically. Observing that the existing techniques have various
limits, in this paper, we aim to develop efficient algorithms (with
linear or near-linear time complexity) that can generate a highquality (large-size) independent set from a graph in practice. In particular, firstly we develop a Reducing-Peeling framework which
iteratively reduces the graph size by applying reduction rules on
vertices with very low degrees (Reducing) and temporarily removing the vertex with the highest degree (Peeling) if the reduction
rules cannot be applied. Secondly, based on our framework we design two baseline algorithms, BDOne and BDTwo, by utilizing the
existing reduction rules for handling degree-one and degree-two
vertices, respectively. Both algorithms can generate higher-quality
(larger-size) independent sets than the existing algorithms. Thirdly,
we propose a linear-time algorithm, LinearTime, and a near-linear
time algorithm, NearLinear, by designing new reduction rules and
developing techniques for efficiently and incrementally applying
reduction rules. In practice, LinearTime takes similar time and
space to BDOne but computes a higher quality independent set,
similar in size to that of an independent set generated by BDTwo.
Moreover, in practice NearLinear has a good chance to generate
a maximum independent set and it often generates near-maximum
independent sets. Fourthly, we extend our techniques to accelerate
the existing iterated local search algorithms. Extensive empirical
studies show that all our algorithms output much larger independent sets than the existing linear-time algorithms while having a
similar running time, as well as achieve significant speedup against
the existing iterated local search algorithms.

Graph model has been widely used to represent the relationships
among entities in a wide spectrum of applications such as social
networks, collaboration networks, communication networks and
biological networks. Significant research e↵orts have been devoted
towards many fundamental problems in managing and analysing
graph data. In this paper, we study the problem of efficiently computing an approximate maximum independent set of a graph. A
subset I of vertices in a graph G is an independent set if there is no
edge between any two vertices in I, and its size is measured by the
number of vertices in it. The independent set with the largest size
among all independent sets of G is called the maximum independent set of G, which is not unique. Consider the graph in Figure 1,
{v2 , v5 , v7 , v9 } is an independent set of size 4, while {v1 , v4 , v6 , v8 , v10 }
is a maximum independent set of size 5.
",19.227672187327304,17.791725408326176,479
SIGMOD_17_022.txt,15.412132840885441,13.33654264741405,SIGMOD,14484,"

National statistical agencies around the world publish tabular summaries based on combined employer-employee (ER-EE) data. The
privacy of both individuals and business establishments that feature
in these data are protected by law in most countries. These data are
currently released using a variety of statistical disclosure limitation (SDL) techniques that do not reveal the exact characteristics
of particular employers and employees, but lack provable privacy
guarantees limiting inferential disclosures.

In this work, we present novel algorithms for releasing tabular
summaries of linked ER-EE data with formal, provable guarantees
of privacy. We show that state-of-the-art differentially private algorithms add too much noise for the output to be useful. Instead,
we identify the privacy requirements mandated by current interpretations of the relevant laws, and formalize them using the Pufferfish framework. We then develop new privacy definitions that are
customized to ER-EE data and satisfy the statutory privacy requirements. We implement the experiments in this paper on production
data gathered by the U.S. Census Bureau. An empirical evaluation of utility for these data shows that for reasonable values of the
privacy-loss parameter « > 1, the additive error introduced by our
provably private algorithms is comparable, and in some cases better, than the error introduced by existing SDL techniques that have
no provable privacy guarantees. For some complex queries currently published, however, our algorithms do not have utility comparable to the existing traditional SDL algorithms. Those queries
are fodder for future research.
",17.693802365651003,15.669227642276425,247
SIGMOD_17_023.txt,14.447285120300485,13.365946955440194,SIGMOD,12613,"

In domains such as the Web, sensor networks and social
media, sources often provide conflicting information for the
same data item. Several data fusion techniques have been
proposed recently to resolve conflicts and identify correct
data. The performance of these fusion systems, while quite
accurate, is far from perfect. In this paper, we propose
to leverage user feedback for validating data conflicts and
rapidly improving the performance of fusion. To present the
most beneficial data items for the user to validate, we take
advantage of the level of consensus among sources, and the
output of fusion to generate an effective ordering of items.
We first evaluate data items individually, and then define a
novel decision-theoretic framework based on the concept of
value of perfect information (VPI) to order items by their
ability to boost the performance of fusion. We further derive approximate formulae to scale up the decision-theoretic
framework to large-scale data. We empirically evaluate our
algorithms on three real-world datasets with different characteristics, and show that the accuracy of fusion can be
significantly improved even while requesting feedback on a
few data items. We also show that the performance of the
proposed methods depends on the characteristics of data,
and assess the trade-off between the amount of feedback acquired, and the effectiveness and efficiency of the methods.

",15.903189008614273,15.556741405082214,224
SIGMOD_17_024.txt,13.419432488523341,10.973399852921514,SIGMOD,14603,"

This paper presents GRAPE, a parallel system for graph
computations. GRAPE differs from prior systems in its ability to parallelize existing sequential graph algorithms as a
whole. Underlying GRAPE are a simple programming model
and a principled approach, based on partial evaluation and
incremental computation. We show that sequential graph algorithms can be “plugged into” GRAPE with minor changes,
and get parallelized. As long as the sequential algorithms
are correct, their GRAPE parallelization guarantees to terminate with correct answers under a monotonic condition.
Moreover, we show that algorithms in MapReduce, BSP and
PRAM can be optimally simulated on GRAPE. In addition
to the ease of programming, we experimentally verify that
GRAPE achieves comparable performance to the state-ofthe-art graph systems, using real-life and synthetic graphs.
",15.903189008614273,14.063667041619798,128
SIGMOD_17_025.txt,13.645346300153271,10.788575031110515,SIGMOD,14900,"

To support analytics on massive graphs such as online social networks, RDF, Semantic Web, etc. many new graph algorithms are
designed to query graphs for a specific problem, and many distributed graph processing systems are developed to support graph
querying by programming. In this paper, we focus on RDBMS,
which has been well studied over decades to manage large datasets,
and we revisit the issue how RDBMS can support graph processing at the SQL level. Our work is motivated by the fact that there
are many relations stored in RDBMS that are closely related to a
graph in real applications and need to be used together to query the
graph, and RDBMS is a system that can query and manage data
while data may be updated over time. To support graph processing,
in this work, we propose 4 new relational algebra operations, MMjoin, MV-join, anti-join, and union-by-update. Here, MM-join and
MV-join are join operations between two matrices and between a
matrix and a vector, respectively, followed by aggregation computing over groups, given a matrix/vector can be represented by
a relation. Both deal with the semiring by which many graph algorithms can be supported. The anti-join removes nodes/edges in
a graph when they are unnecessary for the following computing.
The union-by-update addresses value updates to compute PageRank, for example. The 4 new relational algebra operations can be
defined by the 6 basic relational algebra operations with group-by
& aggregation. We revisit SQL recursive queries and show that
the 4 operations with others are ensured to have a fixpoint, following the techniques studied in DATALOG, and enhance the recursive
with clause in SQL’99. We conduct extensive performance studies to test 10 graph algorithms using 9 large real graphs in 3 major
RDBMSs. We show that RDBMSs are capable of dealing with
graph processing in reasonable time. The focus of this work is at
SQL level. There is high potential to improve the efficiency by
main-memory RDBMSs, efficient join processing in parallel, and
new storage management.

",15.719379583869454,13.794814203137904,347
SIGMOD_17_026.txt,13.277294942139097,10.854058029323848,SIGMOD,14916,"

The incremental problem for a class QO of graph queries aims
to compute, given a query Q € Q, graph G, output Q(G)
and updates AG to G as input, changes AO to Q(G) such
that Q(G@AG) = Q(G)@AO. It is called bounded if its cost
can be expressed as a polynomial function in the sizes of Q,
AG and AO. It is to reduce computations on possibly big
G to small AG and AO. No matter how desirable, however,
our first results are negative: for common graph queries such
as graph traversal, connectivity, keyword search and pattern
matching, their incremental problems are unbounded.

In light of the negative results, we propose two characterizations for the effectiveness of incremental computation: (a)
localizable, if its cost is decided by small neighbors of nodes
in AG instead of the entire G; and (b) bounded relative to
a batch algorithm 7, if the cost is determined by the sizes
of AG and changes to the affected area that is necessarily
checked by 7. We show that the incremental computations
above are either localizable or relatively bounded, by providing corresponding incremental algorithms. That is, we can
either reduce the incremental computations on big graphs to
small data, or incrementalize batch algorithms by minimizing unnecessary recomputation. Using real-life graphs, we
experimentally verify the effectiveness of our algorithms.
",20.075844112070616,22.740491228070173,228
SIGMOD_17_027.txt,14.915229550225412,12.95774032492217,SIGMOD,11268,"

Differential privacy has emerged as a preferred standard for
ensuring privacy in analysis tasks on sensitive datasets. Recent algorithms have allowed for significantly lower error by
adapting to properties of the input data. These so-called
data-dependent algorithms have different error rates for dif
ferent inputs. There is now a complex and growing landscape of algorithms without a clear winner that can offer
low error over all datasets. As a result, the best possible
error rates are not attainable in practice, because the data
curator cannot know which algorithm to select prior to actually running the algorithm.

We address this challenge by proposing a novel metaalgorithm designed to relieve the data curator of the burden of algorithm selection. It works by learning (from nonsensitive data} the association between dataset properties
and the best-performing algorithm. The meta-algorithm is
deployed by first testing the input for low-sensitivity properties and then using the results to select a good algorithm.
The result is an end-to-end differentially private system:
Pythia, which we show offers improvements over using any
single algorithm alone. We empirically demonstrate the benefit of Pythia for the tasks of releasing histograms, answering
1- and 2-dimensional range queries, as well as for constructing private Naive Bayes classifiers.

",16.03029750255766,15.008668246445499,212
SIGMOD_17_028.txt,15.033515458719219,13.337691754808592,SIGMOD,9588,"

Implementing parallel operators in multicore machines often involves a data partitioning step that divides the data
into cache-size blocks and arranges them so to allow concurrent threads to process them in parallel. Data partitioning
is expensive, in some cases up to 90% of the cost of, e.g.,
a parallel hash join. In this paper we explore the use of an
FPGA to accelerate data partitioning. We do so in the context of new hybrid architectures where the FPGA is located
as a co-processor residing on a socket and with coherent access to the same memory as the CPU residing on the other
socket. Such an architecture reduces data transfer overheads
between the CPU and the FPGA, enabling hybrid operator
execution where the partitioning happens on the FPGA and
the build and probe phases of a join happen on the CPU.
Our experiments demonstrate that FPGA based partitioning is significantly faster and more robust than CPU based
partitioning. The results open interesting options as FPGAs
are gradually integrated tighter with the CPU.

",16.785175570968402,15.056663914120563,175
SIGMOD_17_029.txt,13.517008189904551,11.348423132544156,SIGMOD,9432,"
Event processing applications from financial fraud detection
to health care analytics continuously execute event queries
with Kleene closure to extract event sequences of arbitrary,
statically unknown length, called Complete Event Trends
(CETs). Due to common event sub-sequences in CETs, either the responsiveness is delayed by repeated computations
or an exorbitant amount of memory is required to store
partial results. To overcome these limitations, we define
the CET graph to compactly encode all CETs matched by
a query. Based on the graph, we define the spectrum of
CET detection algorithms from CPU-optimal to memoryoptimal. We find the middle ground between these two extremes by partitioning the graph into time-centric graphlets
and caching partial CETs per graphlet to enable effective
reuse of these intermediate results. We reveal cost monotonicity properties of the search space of graph partitioning plans. Our CET optimizer leverages these properties to
prune significant portions of the search to produce a partitioning plan with minimal CPU costs yet within the given
memory limit. Our experimental study demonstrates that
our CET detection solution achieves up to 42–fold speedup even under rigid memory constraints compared to the
state-of-the-art techniques in diverse scenarios.
",16.827784334635936,16.05340909090909,199
SIGMOD_17_030.txt,12.773864536357713,10.436031162351934,SIGMOD,11420,"

Comparable Encryption proposed by Furukawa (ESORICS 2013,
CANS 2014) is a variant of order-preserving encryption (OPE) and
order-revealing encryption (ORE); we cannot compare a ciphertext
of v and another ciphertext of v’, but we can compare a ciphertext
of v and a token of b and compare a token of b and another token of
b’. Comparable encryption allows us to implement range and point
queries while keeping the order of v’s as secret as possible.

Recently, Karras, Malhotra, Bhatt, Nikitin, Antyukhov, and Idreos
independently re-define comparable encryption and propose two
schemes, a basic one and an “ambiguous” one, based on linear
algebra (SIGMOD 2016). The basic scheme is just comparable encryption. To hide the order revealed by tokens, they also proposed
an ambiguous scheme where each ciphertext has two interpretations
v and Vdummy- In the context of an indexed database, this means
that every encryption has two places in the database corresponding
to the two interpretations, masking the correct placement in the
database unless the dummy value is detectable. They assessed that
their basic scheme (and ambiguous scheme upon the basic scheme)
is secure against known-plaintext attacks; the adversary will require
O(é) plaintext-ciphertext pairs to recover secret key, where ¢ is a
security parameter.

This paper cryptanalyzes their comparable encryption schemes
by using simple linear algebra. We show that a few tokens and a few
plaintext-ciphertext pairs instead of O(£) pairs allow us to mount
several attacks efficiently. Our attacks are summarized as follows:

e Attacks against the basic scheme:

- A ciphertext-only attack using two tokens orders the
ciphertexts correctly.

- A known-plaintext attack using two tokens and two
plaintexts reveals exact value of v.

e Attacks against the ambiguous scheme:

- A ciphertext-only attack using two tokens orders the
ciphertexts with a constant probability.

- A known-plaintext attack using three tokens and three
plaintexts reveals exact value of v.
",18.36309006607702,18.926666666666666,320
SIGMOD_17_031.txt,16.566335024687767,15.37364644507252,SIGMOD,12708,"

Analyzing interconnection structures among underlying entities or
objects in a dataset through the use of graph analytics can provide
tremendous value in many application domains. However, graphs
are not the primary representation choice for storing most data today, and in order to have access to these analyses, users are forced
to manually extract data from their data stores, construct the requisite graphs, and then Joad them into some graph engine in order to
execute their graph analysis task. Moreover, in many cases (especially when the graphs are dense), these graphs can be significantly
larger than the initial input stored in the database, making it infeasible to construct or analyze such graphs in memory. In this paper we address both of these challenges by building a system that
enables users to declaratively specify graph extraction tasks over
a relational database schema and then execute graph algorithms on
the extracted graphs. We propose a declarative domain specific language for this purpose, and pair it up with a novel condensed, inmemory representation that significantly reduces the memory footprint of these graphs, permitting analysis of larger-than-memory
graphs. We present a general algorithm for creating such a condensed representation for a large class of graph extraction queries
against arbitrary schemas. We observe that the condensed representation suffers from a duplication issue, that results in inaccuracies
for most graph algorithms. We then present a suite of in-memory
representations that handle this duplication in different ways and
allow trading off the memory required and the computational cost
for executing different graph algorithms. We also introduce several
novel deduplication algorithms for removing this duplication in the
graph, which are of independent interest for graph compression,
and provide a comprehensive experimental evaluation over several
real-world and synthetic datasets illustrating these trade-offs.
",20.161218678152366,19.966195286195283,298
SIGMOD_17_032.txt,14.114626925450015,12.309150482075658,SIGMOD,9728,"

How can we measure similarity between nodes quickly and

accurately on large graphs? Random walk with restart (RWR)

provides a good measure, and has been used in various data
mining applications including ranking, recommendation, link
prediction and community detection. However, existing meth
ods for computing RWR do not scale to large graphs containing billions of edges; iterative methods are slow in query
time, and preprocessing methods require too much memory.

In this paper, we propose BEPI, a fast, memory-efficient,
and scalable method for computing RWR on billion-scale
graphs. BEPI exploits the best properties from both preprocessing methods and iterative methods. BEPI uses a block
elimination approach, which is a preprocessing method, to
enable fast query time. Also, BEPI uses a preconditioned
iterative method to decrease memory requirement. The performance of BEPI is further improved by decreasing nonzeros of the matrix for the iterative method. Through extensive experiments, we show that BEPI processes 100x larger
graphs, and requires up to 130x less memory space than
other preprocessing methods. In the query phase, BEPI
computes RWR scores up to 9x faster than existing methods.
",14.836745963215662,13.133826086956521,185
SIGMOD_17_033.txt,14.718781785741005,13.322241528790592,SIGMOD,13285,"
We focus on data fusion, i.e., the problem of unifying conflicting
data from data sources into a single representation by estimating
the source accuracies. We propose SLiMFast, a framework that expresses data fusion as a statistical learning problem over discriminative probabilistic models, which in many cases correspond to logistic regression. In contrast to previous approaches that use complex generative models, discriminative models make fewer distributional assumptions over data sources and allow us to obtain rigorous theoretical guarantees. Furthermore, we show how SLiMFast
enables incorporating domain knowledge into data fusion, yielding
accuracy improvements of up to 50% over state-of-the-art baselines. Building upon our theoretical results, we design an optimizer
that obviates the need for users to manually select an algorithm
for learning SLiMFast’s parameters. We validate our optimizer on
multiple real-world datasets and show that it can accurately predict
the learning algorithm that yields the best data fusion results.
",17.5058628484301,18.511052631578952,155
SIGMOD_17_034.txt,14.832692302999554,13.14383338132792,SIGMOD,13832,"

Social influence has attracted significant attention owing to
the prevalence of social networks (SNs). In this paper, we
study a new social influence problem, called personalized social influential tags exploration (PITEX), to help any user
in the SN explore how she influences the network. Given
a target user, it finds a size-k tag set that maximizes this
user’s social influence. We prove the problem is NP-hard
to be approximated within any constant ratio. To solve
it, we introduce a sampling-based framework, which has an
approximation ratio of = with high probabilistic guarantee. To speedup the computation, we devise more efficient
sampling techniques and propose best-effort exploration to
quickly prune tag sets with small influence. To further
enable instant exploration, we devise a novel index structure and develop effective pruning and materialization techniques. Experimental results on real large-scale datasets validate our theoretical findings and show high performances of
our proposed methods.

",14.731742533061166,14.043669354838709,156
SIGMOD_17_035.txt,14.321023875543972,12.16193234398585,SIGMOD,11221,"

Crowdsourced query processing is an emerging processing
technique that tackles computationally challenging problems
by human intelligence. The basic idea is to decompose a
computationally challenging problem into a set of human
friendly microtasks (e.g., pairwise comparisons) that are distributed to and answered by the crowd. The solution of the
problem is then computed (e.g., by aggregation) based on
the crowdsourced answers to the microtasks. In this work,
we attempt to revisit the crowdsourced processing of the topk queries, aiming at (1) securing the quality of crowdsourced
comparisons by a certain confidence level and (2) minimizing
the total monetary cost. To secure the quality of each paired
comparison, we employ two statistical tools, Student’s tdistribution estimation and Stein’s estimation, to estimate
the confidence interval of the underlying mean value, which
is then used to draw a conclusion to the comparison. Based
on the pairwise comparison process, we attempt to minimize
the monetary cost of the top-k processing within a SelectPartition-Rank framework. Our experiments, conducted on
four real datasets, demonstrate that our stochastic method
outperforms other existing top-k processing techniques by a
visible difference.

",19.5731925562951,17.551737451737456,190
SIGMOD_17_036.txt,15.40182539892388,13.87062770364565,SIGMOD,10821,"
Taking advantage of recently released hybrid multicore architectures, such as the Intel Xeon+FPGA machine, where the FPGA
has coherent access to the main memory through the QPI bus, we
explore the benefits of specializing operators to hardware. We focus on two commonly used SQL operators for strings: LIKE, and
REGEXP_LIKE, and provide a novel and efficient implementation of these operators in reconfigurable hardware. We integrate
the hardware accelerator into MonetDB, a main-memory column
store, and demonstrate a significant improvement in response time
and throughput. Our Hardware User Defined Function (HUDF)
can speed up complex pattern matching by an order of magnitude
in comparison to the database running on a 10-core CPU. The insights gained from integrating hardware based string operators into
MonetDB should also be useful for future designs combining hardware specialization and databases.
",17.58133193835471,17.40588405797101,138
SIGMOD_17_037.txt,11.883460183568548,9.15282847137378,SIGMOD,13179,"

Consider a directed edge-labeled graph, such as a social network or a citation network. A fundamental query on such
data is to determine if there is a path in the graph from
a given source vertex to a given target vertex, using only
edges with labels in a restricted subset of the edge labels
in the graph. Such label-constrained reachability (LCR)
queries play an important role in graph analytics, for example, as a core fragment of the so-called regular path queries
which are supported in practical graph query languages such
as the W3C’s SPARQL 1.1, Neo4j’s Cypher, and Oracle’s
PGQL. Current solutions for LCR evaluation, however, do
not scale to large graphs which are increasingly common
in a broad range of application domains. In this paper we
present the first practical solution for efficient LCR evaluation, leveraging landmark-based indexes for large graphs.
We show through extensive experiments that our indexes
are significantly smaller than state-of-the-art LCR indexing techniques, while supporting up to orders of magnitude
faster query evaluation times. Our complete C++ codebase
is available as open source for further research.
",16.084390811093357,14.869398496240603,193
SIGMOD_17_038.txt,12.488179546239053,10.585241761233139,SIGMOD,11021,"

Tree-structured data formats, such as JSON and Protocol Buffers,
are capable of expressing sophisticated data types, including nested,
repeated, and missing values. While such expressing power contributes to their popularity in real-world applications, it presents
a significant challenge for systems supporting tree-structured data.
Existing systems have focused on general-purpose solutions either
extending RDBMSs or designing native systems. However, the
general-purpose approach often results in sophisticated data structures and algorithms, which may not reflect and optimize for the
actual structure patterns in the real world.

In this paper, we aim to better understand tree-structured data
types in real uses and optimize for the common patterns. We present
an in-depth study of five types of real-world use cases of treestructured data. We find that a majority of the root-to-leaf paths in
the tree structures are simple, containing up to one repeated node.
Given this insight, we design and implement Steed, a native analytical database system for tree-structured data. Steed implements the
baseline general-purpose support for storing and querying data in
both row and column layouts. Then we enhance the baseline design
with a set of optimizations to simplify and improve the processing
of simple paths. Experimental evaluation shows that our optimization improves the baseline by a factor of up to 1.74x. Compared
to three representative state-of-the-art systems (ie. PostgreSQL,
MongoDB, and Hive+Parquet), Steed achieves orders of magnitude
better performance in both cold cache and hot cache scenarios.

",13.639024540459548,12.479523809523815,253
SIGMOD_17_039.txt,15.62049331983054,14.568931183301007,SIGMOD,15140,"

Reusing intermediates in databases to speed-up analytical
query processing was studied in prior work. Existing solutions require intermediate results of individual operators
to be materialized using materialization operators. However, inserting such materialization operations into a query
plan not only incurs additional execution costs but also often
eliminates important cache- and register-locality opportunities, resulting in even higher performance penalties. This
paper studies a novel reuse model for intermediates, which
caches internal physical data structures materialized during
query processing (due to pipeline breakers) and externalizes
them so that they become reusable for upcoming operations.
We focus on hash tables, the most commonly used internal
data structure in main memory databases to perform join
and aggregation operations. As queries arrive, our reuseaware optimizer reasons about the reuse opportunities for
hash tables, employing cost models that take into account
hash table statistics together with the CPU and data movement costs within the cache hierarchy. Experimental results,
based on our prototype implementation, demonstrate performance gains of 2x for typical analytical workloads with
no additional overhead for materializing intermediates.

",20.130776976110326,21.19885714285714,176
SIGMOD_17_040.txt,14.192571824249843,12.564353151187586,SIGMOD,10893,"

Under update intensive workloads (TPC, LinkBench) small
updates dominate the write behavior, e.g. 70% of all updates
change less than 10 bytes across all TPC OLTP workloads.
These are typically performed as in-place updates and result
in random writes in page-granularity, causing major writeoverhead on Flash storage, a write amplification of several
hundred times and lower device longevity.

In this paper we propose an approach that transforms
those small in-place updates into small update deltas that
are appended to the original page. We utilize the commonly
ignored fact that modern Flash memories (SLC, MLC, 3D
NAND) can handle appends to already programmed physical pages by using various low-level techniques such as ISPP
to avoid expensive erases and page migrations. Furthermore, we extend the traditional NSM page-layout with a
delta-record area that can absorb those small updates. We
propose a scheme to control the write behavior as well as
the space allocation and sizing of database pages.

The proposed approach has been implemented under ShoreMT and evaluated on real Flash hardware (OpenSSD) and
a Flash emulator. Compared to In-Page Logging [21] it performs up to 62% less reads and writes and up to 74% less
erases on a range of workloads. The experimental evaluation
indicates: (i) significant reduction of erase operations resulting in twice the longevity of Flash devices under updateintensive workloads; (ii) 15%-60% lower read/write I/O latencies; (iii) up to 45% higher transactional throughput; (iv)
2x to 3x reduction in overall write amplification.
",15.903189008614273,15.52194993412385,255
SIGMOD_17_041.txt,16.29588184310981,15.161046686937826,SIGMOD,12974,"

In this paper we present BatchDB, an in-memory database engine
designed for hybrid OLTP and OLAP workloads. BatchDB achieves good performance, provides a high level of data freshness, and
minimizes load interaction between the transactional and analytical
engines, thus enabling real time analysis over fresh data under tight
SLAs for both OLTP and OLAP workloads.

BatchDB relies on primary-secondary replication with dedicated
replicas, each optimized for a particular workload type (OLTP,
OLAP), and a light-weight propagation of transactional updates.
The evaluation shows that for standard TPC-C and TPC-H benchmarks, BatchDB can achieve competitive performance to specialized engines for the corresponding transactional and analytical workloads, while providing a level of performance isolation and predictable runtime for hybrid workload mixes (OLTP+OLAP) otherwise unmet by existing solutions.

",19.53771442962202,20.061889312977105,131
SIGMOD_17_042.txt,13.772818027056637,11.864104281207581,SIGMOD,13083,"

Many works have applied crowdsourcing to entity matching (EM). While promising, these approaches are limited in
that they often require a developer to be in the loop. As
such, it is difficult for an organization to deploy multiple
crowdsourced EM solutions, because there are simply not
enough developers. To address this problem, a recent work
has proposed Corleone, a solution that crowdsources the entire EM workflow, requiring no developers. While promising, Corleone is severely limited in that it does not scale to
large tables. We propose Falcon, a solution that scales up
the hands-off crowdsourced EM approach of Corleone, using RDBMS-style query execution and optimization over a
Hadoop cluster. Specifically, we define a set of operators and
develop efficient implementations. We translate a hands-off
crowdsourced EM workflow into a plan consisting of these
operators, optimize, then execute the plan. These plans involve both machine and crowd activities, giving rise to novel
optimization techniques such as using crowd time to mask
machine time. Extensive experiments show that Falcon can
scale up to tables of millions of tuples, thus providing a
practical solution for hands-off crowdsourced EM, to build
cloud-based EM services.
",15.514038796780547,12.831410256410258,196
SIGMOD_17_043.txt,14.890527722567132,12.935121667141953,SIGMOD,12291,"

Algebras based on combinators, i.e., variable-free, have been
proposed as a better representation for query compilation
and optimization. A key benefit of combinators is that they
avoid the need to handle variable shadowing or accidental
capture during rewrites. This simplifies both the optimizer
specification and its correctness analysis, but the environment from the source language has to be reified as records,
which can lead to more complex query plans.

This paper proposes NRA®, an extension of a combinatorsbased nested relational algebra (NRA) with built-in support
for environments. We show that it can naturally encode an
equivalent NRA with lambda terms and that all optimizations on NRA carry over to NRA®. This extension provides
an elegant way to represent views in query plans, and can
radically simplify compilation and optimization for source
languages with rich environment manipulations.

We have specified a query compiler using the Coq proof
assistant with NRA® at its heart. Most of the compiler, including the query optimizer, is accompanied by a (machinechecked) correctness proof. The implementation is automatically extracted from the specification, resulting in a query
compiler with a verified core.
",16.860833078287435,15.076190476190476,188
SIGMOD_17_044.txt,15.364432837618391,13.905212823846494,SIGMOD,11233,"

Multicore CPUs and cheap co-processors such as GPUs create opportunities for vastly accelerating database queries.
However, given the differences in their threading models, expected granularities of parallelism, and memory subsystems,
effectively utilising all cores with all co-processors for an intensive query is very difficult. This paper introduces a novel
templating methodology to create portable, yet architectureaware, algorithms. We apply this methodology on the very
compute-intensive task of calculating the skycube, a materialisation of exponentially many skyline query results, which
finds applications in data exploration and multi-criteria decision making. We define three parallel templates, two that
leverage insights from previous skycube research and a third
that exploits a novel point-based paradigm to expose more
data parallelism. An experimental study shows that, relative
to the state-of-the-art that does not parallelise well due to
its memory and cache requirements, our algorithms provide
an order of magnitude improvement on either architecture
and proportionately improve as more GPUs are added.

",19.454632303725965,19.47371165644172,164
SIGMOD_17_045.txt,13.139031821181153,11.753556357744124,SIGMOD,11501,"

Approximate Membership Query (AMQ) data structures, such as
the Bloom filter, quotient filter, and cuckoo filter, have found numerous applications in databases, storage systems, networks, computational biology, and other domains. However, many applications must work around limitations in the capabilities or performance of current AMQs, making these applications more complex
and less performant. For example, many current AMQs cannot
delete or count the number of occurrences of each input item, take
up large amounts of space, are slow, cannot be resized or merged,
or have poor locality of reference and hence perform poorly when
stored on SSD or disk.

This paper proposes a new general-purpose AMQ, the counting
quotient filter (CQF). The CQF supports approximate membership
testing and counting the occurrences of items in a data set. This
general-purpose AMQ is small and fast, has good locality of reference, scales out of RAM to SSD, and supports deletions, counting
(even on skewed data sets), resizing, merging, and highly concurrent access. The paper reports on the structure’s performance on
both manufactured and applications-generated data sets.

In our experiments, the CQF performs in-memory inserts and
queries up to an order-of-magnitude faster than the original quotient filter, several times faster than a Bloom filter, and similarly to
the cuckoo filter, even though none of these other data structures
support counting. On SSD, the CQF outperforms all structures by
a factor of at least 2 because the CQF has good data locality.

The CQF achieves these performance gains by restructuring the
metadata bits of the quotient filter to obtain fast lookups at high load
factors (i.e., even when the data structure is almost full). As a result,
the CQF offers good lookup performance even up to a load factor
of 95%. Counting is essentially free in the CQF in the sense that
the structure is comparable or more space efficient even than noncounting data structures (e.g., Bloom, quotient, and cuckoo filters).

The paper also shows how to speed up CQF operations by using
new x86 bit-manipulation instructions introduced in Intel’s Haswell
line of processors. The restructured metadata transforms many
quotient filter metadata operations into rank-and-select bit-vector
operations. Thus, our efficient implementations of rank and select
may be useful for other rank-and-select-based data structures.

",14.743456460863424,14.636764397905761,387
SIGMOD_17_046.txt,12.567030095431509,9.445123130900242,SIGMOD,9718,"
Bitmap compression has been studied extensively in the
database area and many efficient compression schemes were
proposed, e.g., BBC, WAH, EWAH, and Roaring. Inverted
list compression is also a well-studied topic in the information retrieval community and many inverted list compression algorithms were developed as well, e.g., VB, PforDelta,
GroupVB, Simple8b, and SIMDPforDelta. We observe that
they essentially solve the same problem, i.e., how to store a
collection of sorted integers with as few as possible bits and
support query processing as fast as possible. Due to historical reasons, bitmap compression and inverted list compression were developed as two separated lines of research
in the database area and information retrieval area. Thus,
a natural question is: Which one is better between bitmap
compression and inverted list compression?
To answer the question, we present the first comprehensive
experimental study to compare a series of 9 bitmap compression methods and 12 inverted list compression methods. We
compare these 21 algorithms on synthetic datasets with different distributions (uniform, zipf, and markov) as well as
8 real-life datasets in terms of the space overhead, decompression time, intersection time, and union time. Based on
the results, we provide many lessons and guidelines that can
be used for practitioners to decide which technique to adopt
in future systems and also for researchers to develop new
algorithms.
",18.511140095513987,16.65322072072072,226
SIGMOD_17_047.txt,14.953368279302069,13.096852742934093,SIGMOD,12214,"

Ridesharing enables drivers to share any empty seats in their vehicles with riders to improve the efficiency of transportation for the
benefit of both drivers and riders. Different from existing studies in ridesharing that focus on minimizing the travel costs of vehicles, we consider that the satisfaction of riders (the utility values) is more important nowadays. Thus, we formulate the problem of utility-aware ridesharing on road networks (URR) with the
goal of providing the optimal rider schedules for vehicles to maximize the overall utility, subject to spatial-temporal and capacity
constraints. To assign a new rider to a given vehicle, we propose
an efficient algorithm with a minimum increase in travel cost without reordering the existing schedule of the vehicle. We prove that
the URR problem is NP-hard by reducing it from the 0-1 Knapsack
problem and it is unlikely to be approximated within any constant
factor in polynomial time through a reduction from the DENS kSUBGRAPH problem. Therefore, we propose three efficient approximate algorithms, including a bilateral arrangement algorithm,
an efficient greedy algorithm and a grouping-based scheduling algorithm, to assign riders to suitable vehicles with a high overall
utility. Through extensive experiments, we demonstrate the efficiency and effectiveness of our URR approaches on both real and
synthetic data sets.
",19.99310930976973,18.830053404539388,215
SIGMOD_17_048.txt,14.42224023962822,13.05004352267176,SIGMOD,9571,"

Recommender systems have many successful applications in ecommerce and social media, including Amazon, Netflix, and Yelp.
Matrix Factorization (MF) is one of the most popular recommendation approaches; the original user-product rating matrix R. with
millions of rows and columns is decomposed into a user matrix Q
and an item matrix P, such that the product Q’P approximates
R. Each column q (p) of Q (P) holds the latent factors of the
corresponding user (item), and q’'p is a prediction of the rating
to item p by user q. Recommender systems based on MF suggest to a user in q the items with the top-k scores in q’ P. For
this problem, we propose a Fast and EXact Inner PROduct retrieval
(FEXIPRO) framework, based on sequential scan, which includes
three elements. First, FEXIPRO applies an SVD transformation to
P, after which the first several dimensions capture a large percentage of the inner products. This enables us to prune item vectors
by only computing their partial inner products with q. Second, we
construct an integer approximation version of P, which can be used
to compute fast upper bounds for the inner products that can prune
item vectors. Finally, we apply a lossless transformation to P, such
that the resulting matrix has only positive values, allowing for the
inner products to be monotonically increasing with dimensionality.
Experiments on real data demonstrate that our framework outperforms alternative approaches typically by an order of magnitude.

",15.247664890283005,14.427740740740745,245
SIGMOD_17_049.txt,14.506463667286859,12.679025935185045,SIGMOD,11231,"

Mapping relationships, such as (country, country-code) or
(company, stock-ticker), are versatile data assets for an
array of applications in data cleaning and data integration
like auto-correction and auto-join. However, today there are
no good repositories of mapping tables that can enable these
intelligent applications.
Given a corpus of tables such as web tables or spreadsheet tables, we observe that values of these mappings often exist in pairs of columns in same tables. Motivated by
their broad applicability, we study the problem of synthesizing mapping relationships using a large table corpus. Our
synthesis process leverages compatibility of tables based on
co-occurrence statistics, as well as constraints such as functional dependency. Experiment results using web tables and
enterprise spreadsheets suggest that the proposed approach
can produce high quality mappings.
",14.790194502661404,15.15969696969697,133
SIGMOD_17_050.txt,16.740867994394677,14.893587804406938,SIGMOD,14514,"

Main-memory database management systems (DBMS) can achieve
excellent performance when processing massive volume of on-line
transactions on modern multi-core machines. But existing durability schemes, namely, tuple-level and transaction-level logging-andrecovery mechanisms, either degrade the performance of transaction processing or slow down the process of failure recovery. In
this paper, we show that, by exploiting application semantics, it is
possible to achieve speedy failure recovery without introducing any
costly logging overhead to the execution of concurrent transactions.
We propose PACMAN, a parallel database recovery mechanism that
is specifically designed for lightweight, coarse-grained transactionlevel logging. PACMAN leverages a combination of static and dynamic analyses to parallelize the log recovery: at compile time,
PACMAN decomposes stored procedures by carefully analyzing dependencies within and across programs; at recovery time, PACMAN
exploits the availability of the runtime parameter values to attain
an execution schedule with a high degree of parallelism. As such,
recovery performance is remarkably increased. We evaluated PACMAN ina fully-fledged main-memory DBMS running on a 40-core
machine. Compared to several state-of-the-art database recovery
mechanisms, PACMAN can significantly reduce recovery time without compromising the efficiency of transaction processing.

",19.287186520377343,18.70887755102041,197
SIGMOD_17_051.txt,14.798937046313029,13.463601730618063,SIGMOD,10116,"
We present ZipG, a distributed memory-efficient graph store for
serving interactive graph queries. ZipG achieves memory efficiency by storing the input graph data using a compressed representation. What differentiates ZipG from other graph stores is
its ability to execute a wide range of graph queries directly on
this compressed representation. ZipG can thus execute a larger
fraction of queries in main memory, achieving query interactivity.
ZipG exposes a minimal API that is functionally rich enough
to implement published functionalities from several industrial
graph stores. We demonstrate this by implementing and evaluating graph queries from Facebook TAO, LinkBench, Graph Search
and several other workloads on top of ZipG. On a single server
with 244GB memory, ZipG executes tens of thousands of queries
from these workloads for raw graph data over half a TB; this
leads to an order of magnitude (sometimes as much as 23×)
higher throughput than Neo4j and Titan. We get similar gains in
distributed settings compared to Titan.
",15.532846611407376,14.413492063492061,162
SIGMOD_17_052.txt,15.190965637987485,14.085782507059413,SIGMOD,13421,"

In today’s databases, previous query answers rarely benefit answering future queries. For the first time, to the best of our knowledge,
we change this paradigm in an approximate query processing (AQP)
context. We make the following observation: the answer to each
query reveals some degree of knowledge about the answer to another
query because their answers stem from the same underlying distribution that has produced the entire dataset. Exploiting and refining
this knowledge should allow us to answer queries more analytically,
rather than by reading enormous amounts of raw data. Also, processing more queries should continuously enhance our knowledge
of the underlying distribution, and hence lead to increasingly faster
response times for future queries.

We call this novel idea—learning from past query answers—
Database Learning. We exploit the principle of maximum entropy
to produce answers, which are in expectation guaranteed to be more
accurate than existing sample-based approximations. Empowered
by this idea, we build a query engine on top of Spark SQL, called
Verdict. We conduct extensive experiments on real-world query
traces from a large customer of a major database vendor. Our results
demonstrate that Verdict supports 73.7% of these queries, speeding
them up by up to 23.0x for the same accuracy level compared to
existing AQP systems.

",14.975302809339372,13.778336492890997,215
SIGMOD_17_053.txt,13.8476931383871,11.589089896198946,SIGMOD,13400,"

As many applications encounter exponential growth in graph sizes,
a fast and scalable graph generator has become more important than
ever before due to lack of large-scale realistic graphs for evaluating
the performance of graph processing methods. Although there have
been proposed a number of methods to generate synthetic graphs,
they are not very efficient in terms of space and time complexities,
and so, cannot generate even trillion-scale graphs using a moderate
size cluster of commodity machines. Here, we propose an efficient and scalable disk-based graph generator, TrillionG that can
generate massive graphs in a short time only using a small amount
of memory. It can generate a graph of a trillion edges following
the RMAT or Kronecker models within two hours only using 10
PCs. We first generalize existing graph generation models to the
scope-based generation model, where RMAT and Kronecker correspond to two extremes. Then, we propose a new graph generation model called the recursive vector model, which compromises
two extremes, and so, solves the space and time complexity problems existing in RMAT and Kronecker. We also extend the recursive vector model so as to generate a semantically richer graph
database. Through extensive experiments, we have demonstrated
that TrillionG outperforms the state-of-the-art graph generators by
up to orders of magnitude.

",16.975882523387877,15.561135844748858,220
SIGMOD_17_054.txt,15.442787500085,13.654499621849375,SIGMOD,12390,"

Multi-core in-memory databases promise high-speed online transaction processing. However, the performance of individual designs
suffers when the workload characteristics miss their small sweet
spot of a desired contention level, read-write ratio, record size, processing rate, and so forth.

Cicada is a single-node multi-core in-memory transactional database with serializability. To provide high performance under diverse
workloads, Cicada reduces overhead and contention at several levels
of the system by leveraging optimistic and multi-version concurrency control schemes and multiple loosely synchronized clocks
while mitigating their drawbacks. On the TPC-C and YCSB benchmarks, Cicada outperforms Silo, TicToc, FOEDUS, MOCC, twophase locking, Hekaton, and ERMIA in most scenarios, achieving
up to 3X higher throughput than the next fastest design. It handles
up to 2.07 M TPC-C transactions per second and 56.5 M YCSB
transactions per second, and scans up to 356 M records per second
on a single 28-core machine.

",17.315433740611066,15.880769230769232,159
SIGMOD_17_055.txt,15.985162613520004,13.639232665983084,SIGMOD,10094,"

While much of the research on transaction processing has focused on improving overall performance in terms of throughput and mean latency, surprisingly less attention has been
given to performance predictability: how often individual
transactions exhibit execution latency far from the mean.
Performance predictability is increasingly important when
transactions lie on the critical path of latency-sensitive applications, enterprise software, or interactive web services.
In this paper, we focus on understanding and mitigating
the sources of performance unpredictability in today’s transactional databases. We conduct the first quantitative study
of major sources of variance in MySQL, Postgres (two of
the largest and most popular open-source products on the
market), and VoltDB (a non-conventional database). We
carry out our study with a tool called TProfiler that, given
the source code of a database system and programmer annotations indicating the start and end of a transaction, is
able to identify the dominant sources of variance in transaction latency. Based on our findings, we investigate alternative algorithms, implementations, and tuning strategies to reduce latency variance without compromising mean
latency or throughput. Most notably, we propose a new
lock scheduling algorithm, called Variance-Aware Transaction Scheduling (VATS), and a lazy buffer pool replacement
policy. In particular, our modified MySQL exhibits signif
icantly lower variance and 99th percentile latencies by up
to 5.6x and 6.3x, respectively. Our proposal has been welcomed by the open-source community, and our VATS algorithm has already been adopted as of MySQL’s 5.7.17 release
(and been made the default scheduling policy in MariaDB).

",20.267338824336647,18.60021080368907,260
SIGMOD_17_056.txt,14.098400750573013,12.508135265700485,SIGMOD,12966,"

Users are increasingly engaging in buying and selling data
over the web. Facilitated by the proliferation of online marketplaces that bring such users together, data brokers need
to serve requests where they provide results for user queries
over the underlying datasets, and price them fairly according
to the information disclosed by the query. In this work, we
present a novel pricing system, called QIRANA, that performs
query-based data pricing for a large class of SQL queries (including aggregation) in real time. QIRANA provides prices
with formal guarantees: for example, it avoids prices that
create arbitrage opportunities. Our framework also allows
flexible pricing, by allowing the data seller to choose from a
variety of pricing functions, as well as specify relation and
attribute-level parameters that control the price of queries
and assign different value to different portions of the data.
We test QIRANA on a variety of real-world datasets and
query workloads, and we show that it can efficiently compute
the prices for queries over large-scale data.
",16.32212239822248,16.630588235294116,171
SIGMOD_17_057.txt,14.1013710001696,12.45013471949737,SIGMOD,13756,"

Systems for processing big data—e.g., Hadoop, Spark, and massively parallel databases—need to run workloads on behalf of multiple tenants simultaneously. The abundant disk-based storage in
these systems is usually complemented by a smaller, but much
faster, cache. Cache is a precious resource: Tenants who get to use
the cache can see two orders of magnitude performance improvement. Cache is also a limited and hence shared resource: Unlike a
resource like a CPU core which can be used by only one tenant at a
time, a cached data item can be accessed by multiple tenants at the
same time. Cache, therefore, has to be shared by a multi-tenancyaware policy across tenants, each having a unique set of priorities
and workload characteristics.

In this paper, we develop cache allocation strategies that speed
up the overall workload while being fair to each tenant. We build a
novel fairness model targeted at the shared resource setting that incorporates not only the more standard concepts of Pareto-efficiency
and sharing incentive, but we also define envy freeness via the notion of core from cooperative game theory. Our cache management
platform, ROBUS, uses randomization over small time batches,
and we develop a proportionally fair allocation mechanism that
satisfies the core property in expectation. We show that this algorithm and related fair algorithms can be approximated to arbitrary
precision in polynomial time. We evaluate these algorithms on a
ROBUS prototype implemented on Spark with RDD store used as
cache. Our evaluation on an industry-standard workload shows that
our algorithms score high on both performance and fairness metrics
across a wide variety of practical multi-tenant setups.

",15.903189008614273,15.183669542136698,276
SIGMOD_17_058.txt,16.40630512211959,14.957993983201103,SIGMOD,15773,"

Recent tools for interactive data exploration significantly increase
the chance that users make false discoveries. They allow users
to (visually) examine many hypotheses and make inference with
simple interactions, and thus incur the issue commonly known in
Statistics as the “multiple hypothesis testing error.” In this work, we
propose a solution to integrate the control of multiple hypothesis
testing into interactive data exploration systems. A key insight is
that existing methods for controlling the false discovery rate (such
as FDR) are not directly applicable to interactive data exploration.
We therefore discuss a set of new control procedures that are better
suited for this task and integrate them in our system, QUDE. Via
extensive experiments on both real-world and synthetic data sets we
demonstrate how QUDE can help experts and novice users alike to
efficiently control false discoveries.

",16.728156217252725,15.697391304347828,139
SIGMOD_17_059.txt,14.635088772132292,13.505679677276316,SIGMOD,10276,"

Online event-based social networks (EBSNs) and studies on
global event-participant arrangement strategies for EBSNs
are becoming popular recently. Existing works measure satisfaction of an arrangement by a linear combination of few
factors, weights of which are predefined and fixed, and do
not allow users to provide feedbacks on whether accepting
the arrangement or not. Besides, most of them only consider
offline scenarios, where full information of users is known in
advance. However, on real-world EBSN platforms, users can
dynamically log in the platform and register for events on
a first come, first served basis. In other words, online scenarios of event-participant arrangement strategies should be
considered. In this work, we study a new event-participant
arrangement strategy for online scenarios, the FeedbackAware Social Event-participant Arrangement (FASEA) problem, where satisfaction scores of an arrangement are learned
adaptively and users can choose to accept or reject the arranged events. Particularly, we model the problem as a contextual combinatorial bandit setting and use efficient and
effective algorithms to solve the problem. The effectiveness and efficiency of the solutions are evaluated with extensive experimental studies and our findings indicate that
the state-of-the-art Thompson Sampling that is reported to
work well under basic multi-armed bandit does not perform
well under FASEA.

",17.267425705330172,16.73497093023256,216
SIGMOD_17_060.txt,14.68264587895202,13.280763415410679,SIGMOD,13308,"

Locally Linear Embedding (LLE) is a popular approach to
dimensionality reduction as it can effectively represent nonlinear structures of high-dimensional data. For dimensionality reduction, it computes a nearest neighbor graph from
a given dataset where edge weights are obtained by applying the Lagrange multiplier method, and it then computes
eigenvectors of the LLE kernel where the edge weights are
used to obtain the kernel. Although LLE is used in many
applications, its computation cost is significantly high. This
is because, in obtaining edge weights, its computation cost
is cubic in the number of edges to each data point. In addition, the computation cost in obtaining the eigenvectors of
the LLE kernel is cubic in the number of data points. Our
approach, Ripple, is based on two ideas: (1) it incrementally updates the edge weights by exploiting the Woodbury
formula and (2) it efficiently computes eigenvectors of the
LLE kernel by exploiting the LU decomposition-based inverse power method. Experiments show that Ripple is significantly faster than the original approach of LLE by guaranteeing the same results of dimensionality reduction.

",16.785175570968402,16.071904761904765,181
SIGMOD_17_061.txt,13.379182412507237,10.743473700511665,SIGMOD,10483,"

Optimization of disjunctive predicates is a very challenging
task which has been vastly neglected by the research community and commercial databases. In this work, we focus
on the complex problem of optimizing disjunctive predicates
by means of the bypass processing technique. In bypass processing, selection operators split the input tuple stream into
two disjoint output streams: the true-stream with tuples
that satisfy the selection predicate and the false-stream with
tuples that do not. Bypass processing is crucial in avoiding expensive predicates whenever the outcome of the query
predicate can be determined by evaluating the less expensive
ones.

In main memory databases, CPU architectural characteristics, such as the branch misprediction penalty, become a
prominent cost factor which cannot be ignored. Our algorithm takes into account the branch misprediction penalty,
and, in addition, it eliminates common subexpressions.

The current literature relies on two assumptions: (1) predicate costs are assumed to be constant, (2) predicate selectivities are assumed to be independent. Since both assumptions do not hold in practice, our approach is not based on
any of them.
",17.122413403193683,15.30541666666667,178
SIGMOD_17_062.txt,16.850672712058472,15.303318371460715,SIGMOD,10555,"

The ever-growing data storage and I/O demands of modern
large-scale data analytics are challenging the current distributed storage systems. A promising trend is to exploit the
recent improvements in memory, storage media, and networks for sustaining high performance and low cost. While
past work explores using memory or SSDs as local storage or
combine local with network-attached storage in cluster computing, this work focuses on managing multiple storage tiers
in a distributed setting. We present OctopusFS, a novel distributed file system that is aware of heterogeneous storage
media (e.g., memory, SSDs, HDDs, NAS) with different capacities and performance characteristics. The system offers
a variety of pluggable policies for automating data management across the storage tiers and cluster nodes. The policies
employ multi-objective optimization techniques for making
intelligent data management decisions based on the requirements of fault tolerance, data and load balancing, and
throughput maximization. At the same time, the storage
media are explicitly exposed to users and applications, allowing them to choose the distribution and placement of replicas in the cluster based on their own performance and fault
tolerance requirements. Our extensive evaluation shows the
immediate benefits of using OctopusF'S with data-intensive
processing systems, such as Hadoop and Spark, in terms of
both increased performance and better cluster utilization.
",18.377959752453624,17.39357558139535,218
SIGMOD_17_063.txt,14.254565827463082,11.935609532918573,SIGMOD,13600,"

Error detection is the process of identifying problematic data
cells that are different from their ground truth. Functional
dependencies (FDs)} have been widely studied in support of
this process. Oftentimes, it is assumed that FDs are given
by experts. Unfortunately, it is usually hard and expensive
for the experts to define such FDs. In addition, automatic
data profiling over dirty data in order to find correct FDs
is known to be a hard problem. In this paper, we propose
an end-to-end solution to detect FD-detectable errors from
dirty data. The broad intuition is that given a dirty dataset,
it is feasible to automatically find approximate FDs, as well
as data that is possibly erroneous. Arguably, at this point,
only experts can confirm true FDs or true errors. However,
in practice, experts never have enough budget to find all
errors. Hence, our problem is, given a limited budget of
expert’s time, which questions we should ask, either FDs,
cells, or tuples, such that we can find as many data errors as
possible. We present efficient algorithms to interact with the
user. Extensive experiments demonstrate that our proposed
framework is effective in detecting errors from dirty data.

",12.745084868956482,11.107626262626265,200
SIGMOD_17_064.txt,14.779759516676474,12.737312908498222,SIGMOD,12038,"

Many modern databases include personal and sensitive correlated data, such as private information on users connected
together in a social network, and measurements of physical
activity of single subjects across time. However, differential
privacy, the current gold standard in data privacy, does not
adequately address privacy issues in this kind of data.

This work looks at a recent generalization of differential
privacy, called Pufferfish, that can be used to address privacy
in correlated data. The main challenge in applying Pufferfish
is a lack of suitable mechanisms. We provide the first mechanism — the Wasserstein Mechanism — which applies to any
general Pufferfish framework. Since this mechanism may be
computationally inefficient, we provide an additional mechanism that applies to some practical cases such as physical
activity measurements across time, and is computationally
efficient. Our experimental evaluations indicate that this
mechanism provides privacy and utility for synthetic as well
as real data in two separate domains.
",18.3970566412798,17.410150375939853,153
SIGMOD_17_065.txt,15.4855363977751,13.651712880882013,SIGMOD,9661,"
Tuple-independent probabilistic databases (TI-PDBs) handle uncertainty by annotating each tuple with a probability
parameter; when the user submits a query, the database derives the marginal probabilities of each output-tuple, assuming input-tuples are statistically independent. While query
processing in TI-PDBs has been studied extensively, limited
research has been dedicated to the problems of updating or
deriving the parameters from observations of query results.
Addressing this problem is the main focus of this paper. We
introduce Beta Probabilistic Databases (B-PDBs), a generalization of TI-PDBs designed to support both (i) belief updating and (ii) parameter learning in a principled and scalable
way. The key idea of B-PDBs is to treat each parameter as
a latent, Beta-distributed random variable. We show how
this simple expedient enables both belief updating and parameter learning in a principled way, without imposing any
burden on regular query processing. We use this model to
provide the following key contributions: (i) we show how to
scalably compute the posterior densities of the parameters
given new evidence; (ii) we study the complexity of performing Bayesian belief updates, devising efficient algorithms for
tractable classes of queries; (iii) we propose a soft-EM algorithm for computing maximum-likelihood estimates of the
parameters; (iv) we show how to embed the proposed algorithms into a standard relational engine; (v) we support our
conclusions with extensive experimental results.
",20.670646682091633,20.624588744588753,232
SIGMOD_17_066.txt,13.085650929994213,10.772289565003764,SIGMOD,11026,"

Modern computing tasks such as real-time analytics require
refresh of query results under high update rates. Incremental View Maintenance (IVM) approaches this problem by
materializing results in order to avoid recomputation. IVM
naturally induces a trade-off between the space needed to
maintain the materialized results and the time used to process updates. In this paper, we show that the full materialization of results is a barrier for more general optimization
strategies. In particular, we present a new approach for
evaluating queries under updates. Instead of the materialization of results, we require a data structure that allows: (1)
linear time maintenance under updates, (2) constant-delay
enumeration of the output, (3) constant-time lookups in the
output, while (4) using only linear space in the size of the
database. We call such a structure a Dynamic Constantdelay Linear Representation (DCLR) for the query. We
show that DyN, a dynamic version of the Yannakakis algorithm, yields DCLRs for the class of free-connex acyclic
CQs. We show that this is optimal in the sense that no
DCLR can exist for CQs that are not free-connex acyclic.
Moreover, we identify a sub-class of queries for which DYN
features constant-time update per tuple and show that this
class is maximal. Finally, using the TPC-H and TPC-DS
benchmarks, we experimentally compare DYN and a higherorder IVM (HIVM) engine. Our approach is not only more
efficient in terms of memory consumption (as expected), but
is also consistently faster in processing updates.
",14.434950587195996,13.156349206349208,253
SIGMOD_17_067.txt,15.293453861257014,13.785777241382508,SIGMOD,12770,"

While schema mapping specification is a cumbersome task
for data curation specialists, it becomes unfeasible for nonexpert users, who are unacquainted with the semantics and
languages of the involved transformations.

In this paper, we present an interactive framework for
schema mapping specification suited for non-expert users.
The underlying key intuition is to leverage a few exemplar
tuples to infer the underlying mappings and iterate the inference process via simple user interactions under the form
of boolean queries on the validity of the initial exemplar
tuples. The approaches available so far are mainly assuming pairs of complete universal data examples, which can be
solely provided by data curation experts, or are limited to
poorly expressive mappings.

We present several exploration strategies of the space of
all possible mappings that satisfy arbitrary user exemplar
tuples. Along the exploration, we challenge the user to retain the mappings that fit the user’s requirements at best
and to dynamically prune the exploration space, thus reducing the number of user interactions. We prove that after the
refinement process, the obtained mappings are correct. We
present an extensive experimental analysis devoted to measure the feasibility of our interactive mapping strategies and
the inherent quality of the obtained mappings.

",18.377959752453624,17.50700495049505,204
SIGMOD_17_068.txt,14.601170372319505,12.989844003899904,SIGMOD,13055,"

Database management system (DBMS) configuration tuning is an
essential aspect of any data-intensive application effort. But this
is historically a difficult task because DBMSs have hundreds of
configuration “knobs” that control everything in the system, such
as the amount of memory to use for caches and how often data
is written to storage. The problem with these knobs is that they
are not standardized (i.e., two DBMSs use a different name for the
same knob), not independent (i.e., changing one knob can impact
others), and not universal (i.e., what works for one application may
be sub-optimal for another). Worse, information about the effects
of the knobs typically comes only from (expensive) experience.

To overcome these challenges, we present an automated approach
that leverages past experience and collects new information to tune
DBMS configurations: we use a combination of supervised and unsupervised machine learning methods to (1) select the most impactful knobs, (2) map unseen database workloads to previous workloads from which we can transfer experience, and (3) recommend
knob settings. We implemented our techniques in a new tool called
OtterTune and tested it on three DBMSs. Our evaluation shows that
OtterTune recommends configurations that are as good as or better
than ones generated by existing tools or a human expert.

",17.613555460941566,17.149730458221025,216
SIGMOD_17_069.txt,15.117758327137715,13.339378935590478,SIGMOD,16616,"

Verifying the integrity of outsourced data is a classic, well-studied
problem. However current techniques have fundamental performance and concurrency limitations for update-heavy workloads. In
this paper, we investigate the potential advantages of deferred and
batched verification rather than the per-operation verification used in
prior work. We present Concerto, a comprehensive key-value store
designed around this idea. Using Concerto, we argue that deferred
verification preserves the utility of online verification and improves
concurrency resulting in orders-of-magnitude performance improvement. On standard benchmarks, the performance of Concerto is
within a factor of two when compared to state-of-the-art key-value
stores without integrity.

",15.470042427545799,15.139259259259262,109
SIGMOD_17_070.txt,16.189746277689565,14.391921509574313,SIGMOD,11831,"

This paper presents Tebaldi, a distributed key-value store that explores
new ways to harness the performance opportunity of combining
different specialized concurrency control mechanisms (CCs) within
the same database. Tebaldi partitions conflicts at a fine granularity and
matches them to specialized CCs within a hierarchical framework
that is modular, extensible, and able to support a wide variety of
concurrency control techniques, from single-version to multiversion
and from lock-based to timestamp-based. When running the TPC-C
benchmark, Tebaldi yields more than 20 x the throughput of the basic
two-phase locking protocol, and over 3.7 x the throughput of Callas,
arecent system that, like Tebaldi, aims to combine different CCs.

",20.890749979661237,21.029203539823012,115
SIGMOD_17_071.txt,13.323128011795099,11.59228754380425,SIGMOD,11513,"

The wide availability of tracking devices has drastically increased
the role of geolocation in social networks, resulting in new commercial applications; for example, marketers can identify current
trending topics within a region of interest and focus their products
accordingly. In this paper we study a basic analytics query on geotagged data, namely: given a spatiotemporal region, find the most
frequent terms among the social posts in that region. While there
has been prior work on keyword search on spatial data (find the objects nearest to the query point that contain the query keywords),
and on group keyword search on spatial data (retrieving groups of
objects), our problem is different in that it returns keywords and
aggregated frequencies as output, instead of having the keyword
as input. Moreover, we differ from works addressing the streamed
version of this query in that we operate on large, disk resident data
and we provide exact answers. We propose an index structure and
algorithms to efficiently answer such top-k spatiotemporal range
queries, which we refer as Top-k Frequent Spatiotemporal Terms
(KFST) queries. Our index structure employs an R-tree augmented
by top-k sorted term lists (STLs), where a key challenge is to balance the size of the index to achieve faster execution and smaller
space requirements. We theoretically study and experimentally validate the ideal length of the stored term lists, and perform detailed
experiments to evaluate the performance of the proposed methods
compared to baselines on real datasets.
",16.26309291913925,18.549860627177697,247
SIGMOD_17_072.txt,14.420956460546307,12.586073532080515,SIGMOD,12012,"

Blockchain technologies are taking the world by storm. Public blockchains, such as Bitcoin and Ethereum, enable secure
peer-to-peer applications like crypto-currency or smart contracts. Their security and performance are well studied.
This paper concerns recent private blockchain systems designed with stronger security (trust) assumption and performance requirement. These systems target and aim to disrupt applications which have so far been implemented on top
of database systems, for example banking, finance and trading applications. Multiple platforms for private blockchains
are being actively developed and fine tuned. However, there
is a clear lack of a systematic framework with which different
systems can be analyzed and compared against each other.
Such a framework can be used to assess blockchains’ viability as another distributed data processing platform, while
helping developers to identify bottlenecks and accordingly
improve their platforms.

In this paper, we first describe BLOCKBENCH, the first
evaluation framework for analyzing private blockchains. It
serves as a fair means of comparison for different platforms
and enables deeper understanding of different system design choices. Any private blockchain can be integrated to
BLOCKBENCH via simple APIs and benchmarked against
workloads that are based on real and synthetic smart contracts. BLOCKBENCH measures overall and componentwise performance in terms of throughput, latency, scalability and fault-tolerance. Next, we use BLOCKBENCH
to conduct comprehensive evaluation of three major private
blockchains: Ethereum, Parity and Hyperledger Fabric. The
results demonstrate that these systems are still far from displacing current database systems in traditional data processing workloads. Furthermore, there are gaps in performance
among the three systems which are attributed to the design
choices at different layers of the blockchain’s software stack.
We have released BLOCKBENCH for public use.
",14.283593685282177,13.107142857142858,282
SIGMOD_17_073.txt,15.341764976784933,14.299784910709189,SIGMOD,10801,"

Data transformation is a critical first step in modern data
analysis: before any analysis can be done, data from a variety of sources must be wrangled into a uniform format
that is amenable to the intended analysis and analytical
software package. This data transformation task is tedious,
time-consuming, and often requires programming skills beyond the expertise of data analysts. In this paper, we develop
a technique to synthesize data transformation programs by
example, reducing this burden by allowing the analyst to describe the transformation with a small input-output example
pair, without being concerned with the transformation steps
required to get there. We implemented our technique in a
system, FOOFAH, that efficiently searches the space of possible data transformation operations to generate a program
that will perform the desired transformation. We experimentally show that data transformation programs can be created
quickly with FooFAH for a wide variety of cases, with 60%
less user effort than the well-known WRANGLER system.
",19.48791578843652,19.541913043478264,162
SIGMOD_17_074.txt,15.303244002704318,13.543862268655435,SIGMOD,9275,"
Density estimation forms a critical component of many analytics
tasks including outlier detection, visualization, and statistical testing.
These tasks often seek to classify data into high and low-density
regions of a probability distribution. Kernel Density Estimation
(KDE) is a powerful technique for computing these densities, offer-
ing excellent statistical accuracy but quadratic total runtime. In this
paper, we introduce a simple technique for improving the perfor-
mance of using a KDE to classify points by their density (density
classification). Our technique, thresholded kernel density classi-
fication (tKDC), applies threshold-based pruning to spatial index
traversal to achieve asymptotic speedups over naïve KDE, while
maintaining accuracy guarantees. Instead of exactly computing each
point’s exact density for use in classification, tKDC iteratively com-
putes density bounds and short-circuits density computation as soon
as bounds are either higher or lower than the target classification
threshold. On a wide range of dataset sizes and dimensions, tKDC
demonstrates empirical speedups of up to 1000x over alternatives.
",18.699421769314853,17.453391761612625,165
SIGMOD_17_075.txt,13.319063785280967,11.24975826490127,SIGMOD,11781,"

Crowdsourcing database systems have been proposed to leverage crowd-powered operations to encapsulate the complexities of interacting with the crowd. Existing systems suffer
from two major limitations. Firstly, in order to optimize
a query, they often adopt the traditional tree model to select an optimized table-level join order. However, the tree
model provides a coarse-grained optimization, which generates the same order for different joined tuples and limits the
optimization potential that different joined tuples can be optimized by different orders. Secondly, they mainly focus on
optimizing the monetary cost. In fact, there are three optimization goals (i.e., smaller monetary cost, lower latency,
and higher quality) in crowdsourcing, and it calls for a system to enable multi-goal optimization.

To address the limitations, we develop a crowd-powered
database system CDB that supports crowd-based query optimizations, with focus on join and selection. CDB has fundamental differences from existing systems. First, CDB employs
a graph-based query model that provides more fine-grained
query optimization. Second, CDB adopts a unified framework
to perform the multi-goal optimization based on the graph
model. We have implemented our system and deployed it
on AMT, CrowdFlower and ChinaCrowd. We have also created a benchmark for evaluating crowd-powered databases.
We have conducted both simulated and real experiments,
and the experimental results demonstrate the performance
superiority of CDB on cost, latency and quality.
",15.091289759661642,13.83260869565218,232
SIGMOD_17_076.txt,17.097580529315717,15.70973718816899,SIGMOD,12181,"

In theory, database transactions protect application data from corruption and integrity violations. In practice, database transactions
frequently execute under weak isolation that exposes programs to
a range of concurrency anomalies, and programmers may fail to
correctly employ transactions. While low transaction volumes mask
many potential concurrency-related errors under normal operation,
determined adversaries can exploit them programmatically for fun
and profit. In this paper, we formalize a new kind of attack on
database-backed applications called an ACIDRain attack, in which
an adversary systematically exploits concurrency-related vulnerabilities via programmatically accessible APIs. These attacks are not
theoretical: ACTDRain attacks have already occurred in a handful
of applications in the wild, including one attack which bankrupted
a popular Bitcoin exchange. To proactively detect the potential for
ACIDRain attacks, we extend the theory of weak isolation to analyze
latent potential for non-serializable behavior under concurrent web
API calls. We introduce a language-agnostic method for detecting
potential isolation anomalies in web applications, called Abstract
Anomaly Detection (2AD), that uses dynamic traces of database
accesses to efficiently reason about the space of possible concurrent
interleavings. We apply a prototype 2AD analysis tool to 12 popular
self-hosted eCommerce applications written in four languages and.
deployed on over 2M websites. We identify and verify 22 critical
ACIDRain attacks that allow attackers to corrupt store inventory,
over-spend gift cards, and steal inventory.

",21.30694733542451,20.565016375545856,230
SIGMOD_17_077.txt,13.376199232130535,11.576884812001207,SIGMOD,16775,"

Due to the advance of the geo-spatial positioning and the computer graphics technology, digital terrain data become more and
more popular nowadays. Query processing on terrain data has
attracted considerable attention from both the academic community and the industry community. One fundamental and important
query is the shortest distance query and many other applications
such as proximity queries (including nearest neighbor queries and
range queries), 3D object feature vector construction and 3D object
data mining are built based on the result of the shortest distance
query. In this paper, we study the shortest distance query which
is to find the shortest distance between a point-of-interest and another point-of-interest on the surface of the terrain due to a variety
of applications. As observed by existing studies, computing the
exact shortest distance is very expensive. Some existing studies
proposed e€-approximate distance oracles where ¢ is a non-negative
real number and is an error parameter. However, the best-known algorithm has a large oracle construction time, a large oracle size and
a large distance query time. Motivated by this, we propose a novel
€-approximate distance oracle called the Space Efficient distance
oracle (SE) which has a small oracle construction time, a small oracle size and a small distance query time due to its compactness
storing concise information about pairwise distances between any
two points-of-interest. Our experimental results show that the oracle construction time, the oracle size and the distance query time
of SE are up to two orders of magnitude, up to 3 orders of magnitude and up to 5 orders of magnitude faster than the best-known
algorithm.

",17.755912252390015,17.329578820697957,275
SIGMOD_17_078.txt,16.039401018334157,14.510937772151902,SIGMOD,13754,"

We transform join ordering into a mixed integer linear program (MILP). This allows to address query optimization by
mature MILP solver implementations that have evolved over
decades and steadily improved their performance. They offer features such as anytime optimization and parallel search
that are highly relevant for query optimization.

We present a MILP formulation for searching left-deep
query plans. We use sets of binary variables to represent
join operands and intermediate results, operator implementation choices or the presence of interesting orders. Linear
constraints restrict value assignments to the ones representing valid query plans. We approximate the cost of scan
and join operations via linear functions, allowing to increase
approximation precision up to arbitrary degrees. We integrated a prototypical implementation of our approach into
the Postgres optimizer and compare against the original optimizer and several variants. Our experimental results are
encouraging: we are able to optimize queries joining 40 tables within less than one minute of optimization time. Such
query sizes are far beyond the capabilities of traditional
query optimization algorithms with worst case guarantees
on plan quality. Furthermore, as we use an existing solver,
our optimizer implementation is small and can be integrated
with low overhead.
",15.903189008614273,15.59353022611906,198
SIGMOD_17_079.txt,15.520181273623123,14.349393200034317,SIGMOD,9784,"

Parametric query optimization (PQO) deals with the problem of finding and reusing a relatively small number of plans
that can achieve good plan quality across multiple instances
of a parameterized query. An ideal solution to PQO would
process query instances online and ensure (a) tight, bounded
cost sub-optimality for each instance, (b) low optimization
overheads, and (c) only a small number of plans need to be
stored. Existing solutions to online PQO however, fall short
on at least one of the above metrics. We propose a plan recosting based approach that enables us to perform well on
all three metrics. We empirically show the effectiveness of
our technique on industry benchmark and real-world query
workloads with our modified version of the Microsoft SQL
Server query optimizer.
",15.903189008614273,15.136187500000002,129
SIGMOD_17_080.txt,14.330281370596136,12.190927806020643,SIGMOD,11453,"

Science applications are producing an ever-increasing volume of
multi-dimensional data that are mainly processed with distributed
array databases. These raw arrays are “cooked” into derived data
products using complex pipelines that are time-consuming. As a
result, derived data products are released infrequently and become
stale soon thereafter. In this paper, we introduce materialized array
views as a database construct for scientific data products. We model
the “cooking” process as incremental view maintenance with batch
updates and give a three-stage heuristic that finds effective update
plans. Moreover, the heuristic repartitions the array and the view
continuously based on a window of past updates as a side-effect
of view maintenance without overhead. We design an analytical
cost model for integrating materialized array views in queries. A
thorough experimental evaluation confirms that the proposed techniques are able to incrementally maintain a real astronomical data
product in a production environment.

",14.554592549557764,14.43350165562914,152
SIGMOD_17_081.txt,14.256024040467313,12.416871801342158,SIGMOD,12548,"
As the use of machine learning (ML) permeates into diverse
application domains, there is an urgent need to support a
declarative framework for ML. Ideally, a user will specify an
ML task in a high-level and easy-to-use language and the
framework will invoke the appropriate algorithms and system configurations to execute it. An important observation
towards designing such a framework is that many ML tasks
can be expressed as mathematical optimization problems,
which take a specific form. Furthermore, these optimization problems can be efficiently solved using variations of
the gradient descent (GD) algorithm. Thus, to decouple a
user specification of an ML task from its execution, a key
component is a GD optimizer. We propose a cost-based GD
optimizer that selects the best GD plan for a given ML task.
To build our optimizer, we introduce a set of abstract operators for expressing GD algorithms and propose a novel approach to estimate the number of iterations a GD algorithm
requires to converge. Extensive experiments on real and synthetic datasets show that our optimizer not only chooses the
best GD plan but also allows for optimizations that achieve
orders of magnitude performance speed-up.
",16.52667757954773,15.397506345177664,198
SIGMOD_17_082.txt,15.498314955843124,14.868869484308714,SIGMOD,12482,"

Sorting is at the core of many database operations, such as
index creation, sort-merge joins, and user-requested output
sorting. As GPUs are emerging as a promising platform
to accelerate various operations, sorting on GPUs becomes
a viable endeavour. Over the past few years, several improvements have been proposed for sorting on GPUs, leading to the first radix sort implementations that achieve a
sorting rate of over one billion 32-bit keys per second. Yet,
state-of-the-art approaches are heavily memory bandwidthbound, as they require substantially more memory transfers
than their CPU-based counterparts. Our work proposes a
novel approach that almost halves the amount of memory
transfers and, therefore, considerably lifts the memory bandwidth limitation. Being able to sort two gigabytes of eightbyte records in as little as 50 milliseconds, our approach
achieves a 2.32-fold improvement over the state-of-the-art
GPU-based radix sort for uniform distributions, sustaining
a minimum speed-up of no less than a factor of 1.66 for
skewed distributions. To address inputs that either do not
reside on the GPU or exceed the available device memory, we
build on our efficient GPU sorting approach with a pipelined
heterogeneous sorting algorithm that mitigates the overhead
associated with PCle data transfers. Comparing the end-toend sorting performance to the state-of-the-art CPU-based
radix sort running 16 threads, our heterogeneous approach
achieves a 2.06-fold and a 1.53-fold improvement for sorting
64 GB key-value pairs with a skewed and a uniform distribution, respectively.

",17.971250198000288,17.401718750000004,261
SIGMOD_17_083.txt,14.102927240302265,12.179470294255026,SIGMOD,12601,"

In rank-aware processing, user preferences are typically represented by a numeric weight per data attribute, collectively forming
a weight vector. The score of an option (data record) is defined as
the weighted sum of its individual attributes. The highest-scoring
options across a set of alternatives (dataset) are shortlisted for the
user as the recommended ones. In that setting, the user input is a
vector (equivalently, a point) in a d-dimensional preference space,
where d is the number of data attributes. In this paper we study the
problem of determining in which regions of the preference space
the weight vector should lie so that a given option (focal record) is
among the top-k score-wise. In effect, these regions capture all possible user profiles for which the focal record is highly preferable,
and are therefore essential in market impact analysis, potential customer identification, profile-based marketing, targeted advertising,
etc. We refer to our problem as k-Shortlist Preference Region identification (KSPR), and exploit its computational geometric nature to
develop a framework for its efficient (and exact) processing. Using
real and synthetic benchmarks, we show that our most optimized
algorithm outperforms by three orders of magnitude a competitor
we constructed from previous work on a different problem.

",16.827784334635936,16.335592105263157,210
SIGMOD_17_084.txt,14.164436512514897,11.659805142906787,SIGMOD,9772,"

Fueled by the increasing popularity of online social networks,
social influence analysis has attracted a great deal of research attention in the past decade. The diffusion process
is often modeled using influence graphs, and there has been
a line of research that involves algorithmic problems in influence graphs. However, the vast size of today’s real-world
networks raises a serious issue with regard to computational
efficiency.

In this paper, we propose a new algorithm for reducing
influence graphs. Given an input influence graph, the proposed algorithm produces a vertex-weighted influence graph,
which is compact and approximates the diffusion properties of the input graph. The central strategy of influence
graph reduction is coarsening, which has the potential to
greatly reduce the number of edges by merging a vertex
set into a single weighted vertex. We provide two implementations; a speed-oriented implementation which runs in
linear time with linear space and a scalability-oriented implementation which runs in practically linear time with sublinear space. Further, we present general frameworks using
our compact graphs that accelerate existing algorithms for
influence maximization and influence estimation problems,
which are motivated by practical applications, such as viral
marketing. Using these frameworks, we can quickly obtain
solutions that have accuracy guarantees under a reasonable
assumption. Experiments with real-world networks demonstrate that the proposed algorithm can scale to billion-edge
graphs and reduce the graph size to up to 4%. In addition,
our influence maximization framework achieves four times
speed-up of a state-of-the-art D-SSA algorithm, and our influence estimation framework cuts down the computation
time of a simulation-based method to 3.5%.

",17.332850977054683,16.17863636363636,275
SIGMOD_17_085.txt,15.159231857132738,13.816317169754857,SIGMOD,12741,"

We introduce end-to-end support of co-existing schema versions within one database. While it is state of the art to
run multiple versions of a continuously developed application
concurrently, it is hard to do the same for databases. In order
to keep multiple co-existing schema versions alive—which are
all accessing the same data set—developers usually employ
handwritten delta code (e.g. views and triggers in SQL).
This delta code is hard to write and hard to maintain: if a
database administrator decides to adapt the physical table
schema, all handwritten delta code needs to be adapted as
well, which is expensive and error-prone in practice. In this
paper, we present INVERDA: developers use the simple bidirectional database evolution language BIDEL, which carries
enough information to generate all delta code automatically.
Without additional effort, new schema versions become immediately accessible and data changes in any version are
visible in all schema versions at the same time. INVERDA
also allows for easily changing the physical table design without affecting the availability of co-existing schema versions.
This greatly increases robustness (orders of magnitude less
lines of code) and allows for significant performance optimization. A main contribution is the formal evaluation that
each schema version acts like a common full-fledged database
schema independently of the chosen physical table design.
",16.728156217252725,16.088558558558557,224
SIGMOD_17_086.txt,13.513832475257953,11.32868677307582,SIGMOD,12321,"

While significant progress has been made separately on analytics
systems for scalable stochastic gradient descent (SGD) and private
SGD, none of the major scalable analytics frameworks have incorporated differentially private SGD. There are two inter-related
issues for this disconnect between research and practice: (1) low
model accuracy due to added noise to guarantee privacy, and (2)
high development and runtime overhead of the private algorithms.
This paper takes a first step to remedy this disconnect and proposes
a private SGD algorithm to address both issues in an integrated
manner. In contrast to the white-box approach adopted by previous
work, we revisit and use the classical technique of output perturbation to devise a novel “bolt-on” approach to private SGD. While
our approach trivially addresses (2), it makes (1) even more challenging. We address this challenge by providing a novel analysis
of the L2-sensitivity of SGD, which allows, under the same privacy guarantees, better convergence of SGD when only a constant
number of passes can be made over the data. We integrate our algorithm, as well as other state-of-the-art differentially private SGD,
into Bismarck, a popular scalable SGD-based analytics system on
top of an RDBMS. Extensive experiments show that our algorithm
can be easily integrated, incurs virtually no overhead, scales well,
and most importantly, yields substantially better (up to 4X) test accuracy than the state-of-the-art algorithms on many real datasets.

",18.243605946275583,17.439492677824273,240
SIGMOD_17_087.txt,15.045827591351852,13.344110945261349,SIGMOD,11973,"

This paper presents a joint effort between a group of computer scientists and bioinformaticians to take an important
step towards a general big data platform for genome analysis pipelines. The key goals of this study are to develop a
thorough understanding of the strengths and limitations of
big data technology for genomic data analysis, and to identify the key questions that the research community could address to realize the vision of personalized genomic medicine.
Our platform, called GESALL, is based on the new “Wrapper
Technology” that supports existing genomic data analysis
programs in their native forms, without having to rewrite
them. To do so, our system provides several layers of software, including a new Genome Data Parallel Toolkit (GDPT)
which can be used to “wrap” existing data analysis programs. This platform offers a concrete context for evaluating
big data technology for genomics: we report on super-linear
speedup and sublinear speedup for various tasks, as well as
the reasons why a parallel program could produce different
results from those of a serial program. These results lead to
key research questions that require a synergy between genomics scientists and computer scientists to find solutions.

",18.422482065455632,18.415103626943004,194
SIGMOD_17_088.txt,14.570628336466125,12.94422474113961,SIGMOD,12977,"

The advent of columnar data analytics engines fueled a series of
optimizations on the scan operator. New designs include columngroup storage, vectorized execution, shared scans, working directly
over compressed data, and operating using SIMD and multi-core
execution. Larger main memories and deeper cache hierarchies increase the efficiency of modern scans, prompting a revisit of the
question of access path selection.

In this paper, we compare modern sequential scans and secondary
index scans. Through detailed analytical modeling and experimentation we show that while scans have become useful in more cases
than before, both access paths are still useful, and so, access path
selection (APS) is still required to achieve the best performance
when considering variable workloads. We show how to perform
access path selection. In particular, contrary to the way traditional
systems choose between scans and secondary indexes, we find that
in addition to the query selectivity, the underlying hardware, and
the system design, modern optimizers also need to take into account query concurrency. We further discuss the implications of
integrating access path selection in a modern analytical data system. We demonstrate, both theoretically and experimentally, that
using the proposed model a system can quickly perform access
path selection, outperforming solutions that rely on a single access
path or traditional access path models. We outline a light-weight
mechanism to integrate APS into main-memory analytical systems
that does not interfere with low latency queries. We also use the
APS model to explain how the division between sequential scan
and secondary index scan has historically changed due to hardware
and workload changes, which allows for future projections based
on hardware advancements.

",16.35954948731386,16.355451190875545,272
SIGMOD_17_089.txt,12.85143281909519,10.614603953343938,SIGMOD,12480,"

Dynamic clustering—how to efficiently maintain data clusters along
with updates in the underlying dataset—is a difficult topic. This
is especially true for density-based clustering, where objects are
aggregated based on transitivity of proximity, under which deciding
the cluster(s) of an object may require the inspection of numerous
other objects. The phenomenon is unfortunate, given the popular
usage of this clustering approach in many applications demanding
data updates.

Motivated by the above, we investigate the algorithmic principles for dynamic clustering by DBSCAN, a successful representative of density-based clustering, and p-approximate DBSCAN,
proposed to bring down the computational hardness of the former
on static data. Surprisingly, we prove that the p-approximate version suffers from the very same hardness when the dataset is fully
dynamic, namely, when both insertions and deletions are allowed.
We also show that this issue goes away as soon as tiny further relaxation is applied, yet still ensuring the same quality—known as the
“sandwich guarantee”—of p-approximate DBSCAN. Our algorithms
guarantee near-constant update processing, and outperform existing
approaches by a factor over two orders of magnitude.
",18.699421769314853,17.41894576012223,188
SIGMOD_17_090.txt,14.833178190479089,13.033067098307757,SIGMOD,11225,"

During exploratory statistical analysis, data scientists repeatedly
compute statistics on data sets to infer knowledge. Moreover, statistics form the building blocks of core machine learning classification and filtering algorithms. Modern data systems, software
libraries, and domain-specific tools provide support to compute
statistics but lack a cohesive framework for storing, organizing, and
reusing them. This creates a significant problem for exploratory
Statistical analysis as data grows: Despite existing overlap in exploratory workloads (which are repetitive in nature), statistics are
always computed from scratch. This leads to repeated data movement and recomputation, hindering interactive data exploration.

We address this challenge in Data Canopy, where descriptive and
dependence statistics are synthesized from a library of basic aggregates. These basic aggregates are stored within an in-memory data
structure, and are reused for overlapping data parts and for various statistical measures. What this means for exploratory statistical analysis is that repeated requests to compute different statistics do not trigger a full pass over the data. We discuss in detail
the basic design elements in Data Canopy, which address multiple
challenges: (1) How to decompose statistics into basic aggregates
for maximal reuse? (2) How to represent, store, maintain, and access these basic aggregates? (3) Under different scenarios, which
basic aggregates to maintain? (4) How to tune Data Canopy in
a hardware conscious way for maximum performance and how to
maintain good performance as data grows and memory pressure
increases?

We demonstrate experimentally that Data Canopy results in an
average speed-up of at least 10x after just 100 exploratory queries
when compared with state-of-the-art systems used for exploratory
statistical analysis.

",17.03242331605538,15.816802973977698,270
SIGMOD_17_091.txt,14.378582433914893,12.634045717407044,SIGMOD,12335,"

A key feature of database systems is to provide transparent access
to stored data. In distributed database systems, this includes data
allocation and fragmentation. Transparent access introduces data
dependencies and increases system complexity and inter-process
communication. Therefore, many developers are exchanging transparency for better scalability using sharding and similar techniques.
However, explicitly managing data distribution and data flow requires a deep understanding of the distributed system and the data
access, and it reduces the possibilities for optimizations.

To address this problem, we present an approach for efficient data
allocation that features good scalability while keeping the data distribution transparent. We propose a workload-aware, query-centric,
heterogeneity-aware analytical model. We formalize our approach
and present an efficient allocation algorithm. The algorithm optimizes the partitioning and data layout for local query execution and
balances the workload on homogeneous and heterogeneous systems
according to the query history. In the evaluation, we demonstrate
that our approach scales well in performance for OLTP- and OLAPstyle workloads and reduces storage requirements significantly over
replicated systems while guaranteeing configurable availability.

",16.887214914478655,18.025136363636367,177
SIGMOD_17_092.txt,14.74188656428807,13.346731756778713,SIGMOD,12085,"

Iceberg queries, commonly used for decision support, find groups
whose aggregate values are above or below a threshold. In practice,
iceberg queries are often posed over complex joins that are expensive to evaluate. This paper proposes a framework for combining
a number of techniques—a-priori, memoization, and pruning—to
optimize iceberg queries with complex joins. A-priori pushes partial GROUP BY and HAVING condition before a join to reduce its input size. Memoization caches and reuses join computation results.
Pruning uses cached results to infer that certain tuples cannot contribute to the final query result, and short-circuits join computation.
We formally derive conditions for correctly applying these techniques. Our practical rewrite algorithm produces highly efficient
SQL that can exploit combinations of optimization opportunities in
ways previously not possible. We evaluate our PostgreSQL-based
implementation experimentally and show that it outperforms both
baseline PostgreSQL and a commercial database system.

",14.554592549557764,13.88066666666667,151
SIGMOD_17_093.txt,13.786774290618766,12.396688451653045,SIGMOD,12322,"

In this paper, we show that key-value stores backed by an LSM-tree
exhibit an intrinsic trade-off between lookup cost, update cost, and
main memory footprint, yet all existing designs expose a suboptimal and difficult to tune trade-off among these metrics. We pinpoint the problem to the fact that all modern key-value stores suboptimally co-tune the merge policy, the buffer size, and the Bloom
filters’ false positive rates in each level.

We present Monkey, an LSM-based key-value store that strikes
the optimal balance between the costs of updates and lookups with
any given main memory budget. The insight is that worst-case
lookup cost is proportional to the sum of the false positive rates
of the Bloom filters across all levels of the LSM-tree. Contrary
to state-of-the-art key-value stores that assign a fixed number of
bits-per-element to all Bloom filters, Monkey allocates memory
to filters across different levels so as to minimize this sum. We
show analytically that Monkey reduces the asymptotic complexity of the worst-case lookup I/O cost, and we verify empirically
using an implementation on top of LevelDB that Monkey reduces
lookup latency by an increasing margin as the data volume grows
(50% — 80% for the data sizes we experimented with). Furthermore, we map the LSM-tree design space onto a closed-form model
that enables co-tuning the merge policy, the buffer size and the filters’ false positive rates to trade among lookup cost, update cost
and/or main memory, depending on the workload (proportion of
lookups and updates), the dataset (number and size of entries), and
the underlying hardware (main memory available, disk vs. flash).
We show how to use this model to answer what-if design questions about how changes in environmental parameters impact performance and how to adapt the various LSM-tree design elements
accordingly.

",16.975882523387877,18.73630520504732,318
SIGMOD_17_094.txt,15.561237780640933,14.661238423947928,SIGMOD,11288,"

In this paper, we present CrowdDQ§, a system that uses
the most recent set of crowdsourced voting evidence to dynamically issue questions to workers on Amazon Mechanical
Turk (AMT). CrowdDQS posts all questions to AMT in a
single batch, but delays the decision of the exact question to
issue a worker until the last moment, concentrating votes on
uncertain questions to maximize accuracy. Unlike previous
works, CrowdDQS also (1) optionally can decide when it is
more beneficial to issue gold standard questions with known
answers than to solicit new votes (both can help us estimate worker accuracy, but gold standard questions provide
a less noisy estimate of worker accuracy at the expense of
not obtaining new votes), (2) estimates worker accuracies in
real-time even with limited evidence (with or without gold
standard questions), and (3) infers the distribution of worker
skill levels to actively block poor workers. We deploy our
system live on AMT to over 1000 crowdworkers, and find
that CrowdDQ$S can accurately answer questions using up
to 6x fewer votes than standard approaches. We also find
there are many non-obvious practical challenges involved in
deploying such a system seamlessly to crowdworkers, and
discuss techniques to overcome these challenges.

",18.458006810337128,20.220039215686274,204
SIGMOD_17_095.txt,14.721396884518768,13.425474959515402,SIGMOD,13648,"

The increasing reliance on robust data-driven decision-making across
many domains has made it necessary for data management systems
to manage many thousands to millions of versions of datasets,
acquired or constructed at various stages of analysis pipelines over
time. Delta encoding is an effective and widely-used solution to
compactly store a large number of datasets, that simultaneously
exploits redundancies across them and keeps the average retrieval
cost of reconstructing any dataset low. However, supporting any
kind of rich retrieval or querying functionality, beyond single dataset
checkout, is challenging in such storage engines. In this paper, we
initiate a systematic study of this problem, and present DEX, a novel
stand-alone delta-oriented execution engine, whose goal is to take
advantage of the already computed deltas between the datasets for
efficient query processing. In this work, we study how to execute
checkout, intersection, union and t-threshold queries over recordbased files; we show that processing of even these basic queries leads
to many new and unexplored challenges and trade-offs. Starting from
a query plan that confines query execution to a small set of deltas, we
introduce new transformation rules based on the algebraic properties
of the deltas, that allow us to explore the search space of alternative
plans. For the case of checkout, we present a dynamic programming
algorithm to efficiently select the optimal query plan under our cost
model, while we design efficient heuristics to select effective plans
that vastly outperform the base checkout-then-query approach for
other queries. A key characteristic of our query execution methods
is that the computational cost is primarily dependent on the size
and the number of deltas in the expression (typically small), and
not the input dataset versions (which can be very large). We have
implemented DEX prototype on top of git, a widely used version
control system. We present an extensive experimental evaluation
on synthetic data with diverse characteristics, that shows that our
methods perform exceedingly well compared to the baseline.
",18.669449996058646,19.01261561561562,334
SIGMOD_17_096.txt,14.191785591663535,12.385539847673197,SIGMOD,10237,"

Searchable encryption (SE) allows a client to outsource a
dataset to an untrusted server while enabling the server to
answer keyword queries in a private manner. SE can be
used as a building block to support more expressive private
queries such as range/point and boolean queries, while providing formal security guarantees. To scale SE to big data
using external memory, new schemes with small locality have
been proposed, where locality is defined as the number of
non-continuous reads that the server makes for each query.
Previous space-efficient SE schemes achieve optimal locality
by increasing the read efficiency—the number of additional
memory locations (false positives) that the server reads per
result item. This can hurt practical performance.

In this work, we design, formally prove secure, and evaluate the first SE scheme with tunable locality and linear
space. Our first scheme has optimal locality and outperforms existing approaches (that have a slightly different leakage profile) by up to 2.5 orders of magnitude in terms of read
efficiency, for all practical database sizes. Another version
of our construction with the same leakage as previous works
can be tuned to have bounded locality, optimal read efficiency and up to 60x more efficient end-to-end search time.
We demonstrate that our schemes work fast in in-memory as
well, leading to search time savings of up to 1 order of magnitude when compared to the most practical in-memory SE
schemes. Finally, our construction can be tuned to achieve
trade-offs between space, read efficiency, locality, parallelism
and communication overhead.

",17.238542476582836,14.973076923076924,262
SLE_17_001.txt,15.295138732927285,14.227974845103741,SLE,9114,"
Domain-Specific Languages (DSL’s) offer language-level abstractions that General-Purpose Languages do not offer, thus
speeding up the implementation of the solution of problems
within a specific domain. Developers have the choice of developing a DSL by building an interpreter/compiler for it,
which is a hard and time-consuming task, or embedding it in
a host language, thus speeding up the development process
but losing several advantages that having a dedicated compiler might bring. In this work we present a meta-compiler
called Metacasanova, whose meta-language is based on operational semantics. Then, we propose a language extension
with functors and modules that allows to embed the type
system of a language definition inside the meta-type system
of Metacasanova and improves the performance of manipulating data structures at run-time. Our results show that
Metacasanova dramatically reduces the code lines required
to develop a compiler, and that the running time of the Metaprogram is improved by embedding the host language type
system in the meta-type system with the use of functors in
the meta-language.
",18.458006810337128,19.466972067039105,181
SLE_17_002.txt,15.287203543098563,13.797913104565122,SLE,7953,"

Over the years a lot of effort has been put on solving extensibility problems, while retaining important software engineering properties such as modular type-safety and separate
compilation. Most previous work focused on operations that
traverse and process extensible Abstract Syntax Tree (AST)
structures. However, there is almost no work on operations
that build such extensible ASTs, including parsing.

This paper investigates solutions for the problem of modular parsing. We focus on semantic modularity and not just
syntactic modularity. That is, the solutions should not only
allow complete parsers to be built out of modular parsing
components, but also enable the parsing components to be
modularly type-checked and separately compiled. We present
a technique based on parser combinators that enables modular parsing. We show that Packrat parsing techniques, provide solutions for such modularity problems, and enable
reasonable performance in a modular setting. Extensibility
is achieved using multiple inheritance and Object Algebras.
To evaluate the approach we conduct a case study based
on the “Types and Programming Languages” interpreters.
The case study shows the effectiveness at reusing parsing
code from existing interpreters, and the total parsing code is
69% shorter than an existing code base using a non-modular
parsing approach.
",15.549919911452193,14.0209407507915,202
SLE_17_003.txt,14.621517582036233,12.952687883526213,SLE,7792,"

Domain-Specific Modelling Languages (DSLs) allow domain
experts to create models using abstractions they are most
familiar with. A DSL’s syntax is specified in two parts: the syntax defines the language’s concepts and their
allowed combinations, and the concrete syntax defines how
those concepts are presented to the user (typically using
a graphical or textual notation). However important concrete syntax is for the usability of the language, current
modelling tools offer limited possibilities for defining the
mapping between abstract and concrete syntax. Often, the
language designer is restricted to defining a single icon representation of each concept, which is then rendered to the
user in a (fixed) graphical interface. This paper presents a
framework that explicitly models the bi-directional mapping
between the abstract and concrete syntax, thereby making
these restrictions easy to overcome. It is more flexible and
allows, amongst others, for a model to be represented in
multiple front-ends, using multiple representation formats,
and multiple mappings. Our approach is evaluated with an
implementation in our prototype tool, the Modelverse, and
by applying it on an example language.
",17.122413403193683,16.66190476190477,183
SLE_17_004.txt,18.04816837410028,18.056867372639605,SLE,4099,"
Modern enterprises operate in an unprecedented
regulatory environment where increasing regulation and
heavy penalties on non-compliance have placed
regulatory compliance among the topmost concerns of
enterprises worldwide. Previous research in the field of
compliance has established that the manual specification
of the regulations used by GRC frameworks not only fails
to ensure their proper coverage but also negatively
affects the turnaround time both in proving and
maintaining the compliance. Our key contribution in this
paper is an implementation of a controlled natural
English like (domain-specific) language that can be used
by domain experts to specify regulations for automated
compliance checking. We demonstrate this language
using examples from industry regulations in financial
services domain.
",20.503739492662863,19.73554347826087,116
SLE_17_005.txt,15.019513369854181,14.676072962227526,SLE,6849,"
Extensible languages enable the convenient construction of
many kinds of domain-specific languages (DSLs) by mapping
domain-specific surface syntax into the host language’s core
forms in a layered and composable way. The host language’s
debugger, however, reports evaluation and data details in
ways that reflect the host language, instead of the DSL in its
own terms, and closing the gap may require more than correlating host evaluation steps to the original DSL source. In
this paper, we describe an approach to DSL construction with
macros that pairs the mapping of DSL terms to host terms
with a mapping to convert primitive events back to domainspecific concepts. Domain-specific events are then suitable
for presenting to a user or wiring into a domain-specific visualization. We present a core model of evaluation and events,
and we present a language designÐanalogous to patternbased notations for macros, but in the other directionÐfor
describing how events in a DSL’s expansion are mapped to
events at the DSL’s level.
",17.80541091248751,18.01939393939394,170
SLE_17_006.txt,14.816443482419619,13.602247684643256,SLE,8506,"

Context-free grammars are suitable for formalizing the syntax of programming languages concisely and declaratively.
Thus, such grammars are often found in reference manuals of
programming languages, and used in language workbenches
for language prototyping. However, the natural and concise
way of writing a context-free grammar is often ambiguous.

Safe and complete declarative disambiguation of operator
precedence and associativity conflicts guarantees that all
ambiguities arising from combining the operators of the
language are resolved. Ambiguities can occur due to shallow
conflicts, which can be captured by one-level tree patterns,
and deep conflicts, which require more elaborate techniques.
Approaches to solve deep priority conflicts include grammar
transformations, which may result in large unambiguous
grammars, or may require adapted parser technologies to
include data-dependency tracking at parse time.

In this paper we study deep priority conflicts “in the wild”.
We investigate the efficiency of grammar transformations
to solve deep priority conflicts by using a lazy parse table
generation technique. On top of lazily-generated parse tables,
we define metrics, aiming to answer how often deep priority
conflicts occur in real-world programs and to what extent
programmers explicitly disambiguate programs themselves.
By applying our metrics to a small corpus of popular opensource repositories we found that in OCaml, up to 17% of
the source files contain deep priority conflicts.
",16.280829582073984,15.83909090909091,221
SLE_17_007.txt,15.18929698978555,13.776336746418732,SLE,8116,"

A critical step in model-driven engineering (MDE) is the
automatic synthesis of a textual artifact from models. This is
a very useful model transformation to generate application
code, to serialize the model in persistent storage, generate
documentation or reports. Among the various model-totext (M2T) paradigms, template-based code generation is the
most popular in MDE. This is supported by over 70 different
tools, whether they are model-based (e.g., Acceleo, EGL) or
code-based (JET, Velocity). To help developers in their difficult choice of selecting the M2T tool, we compare the expressiveness power and performance of the nine most popular
tools spanning the different technological approaches. We
evaluate the expressiveness based on common metamodel
patterns and evaluate the performance on a range of models
that conform to a metamodel composed by the combination
of these patterns. The results show that MDE-based tools
are more expressive, but that code-based tools are more performant. Xtend2 offers the best compromise between the
expressiveness and the performance.
",16.218646115125612,13.890172155688624,169
SLE_17_008.txt,16.189880278733796,15.81153231047442,SLE,7467,"
Packrat parsing is a popular technique for implementing
top-down, unlimited-lookahead parsers that operate in guaranteed linear time. In this paper, we describe a method for
turning a standard packrat parser into an incremental parser
through a simple modification to its memoization strategy.
By łincremental”, we mean that the parser can perform syntax analysis without completely reparsing the input after
each edit operation. This makes packrat parsing suitable for
interactive use in code editors and IDEs — even with large
inputs. Our experiments show that with our technique, an
incremental packrat parser for JavaScript can outperform
even a hand-optimized, non-incremental parser.
",15.903189008614273,15.3566213592233,104
SLE_17_009.txt,15.797031303778109,13.298120282677989,SLE,8943,"

Extensible language frameworks aim to allow independentlydeveloped language extensions to be easily added to a host
programming language. It should not require being a compiler expert, and the resulting compiler should “just work” as
expected. Previous work has shown how specifications for
parsing (based on context free grammars) and for semantic
analysis (based on attribute grammars) can be automatically
and reliably composed, ensuring that the resulting compiler
does not terminate abnormally.

However, this work does not ensure that a property proven
to hold for a language (or extended language) still holds when
another extension is added, a problem we call interference. We
present a solution to this problem using of a logical notion of
coherence. We show that a useful class of language extensions,
implemented as attribute grammars, preserve all coherent
properties. If we also restrict extensions to only making use
of coherent properties in establishing their correctness, then
the correctness properties of each extension will hold when
composed with other extensions. As a result, there can be
no interference: each extension behaves as specified.
",17.122413403193683,14.788107142857143,176
SLE_17_010.txt,15.357058515151898,13.468031752026267,SLE,9342,"
The similarities and differences between attribute grammar
systems are obscured by their implementations. A formalism that captures the essence of such systems would allow
for equivalence, correctness, and other analyses to be formally framed and proven. We present Saiga, a core language
and small-step operational semantics that precisely captures
the fundamental concepts of the specification and execution
of parameterised reference attribute grammars. We demonstrate the utility of Saiga by a) proving a meta-theoretic
property about attribute caching, and b) by specifying two
attribute grammars for a realistic name analysis problem and
proving that they are equivalent. The language, semantics
and associated tests have been mechanised in Coq; we are
currently mechanising the proofs.
",17.80541091248751,17.185217391304352,116
SLE_17_011.txt,13.659800441615683,11.459029551240455,SLE,7064,"

Parsing expression grammars (PEGs) are a powerful and
popular foundation for describing syntax. Despite PEGs’ expressiveness, they cannot recognize many syntax patterns of
popular programming languages. Typical examples include
typedef-defined names in C/C++ and here documents appearing in many scripting languages. We use a single unified
state representation, called a symbol table, to capture various
context-sensitive patterns. Over the symbol table, we design
a small set of restricted semantic predicates and actions. The
extended PEGs are called SPEGs, and are designed to be safe
in contexts of backtracking and the linear time guarantee
of packrat parsing. This paper will show that SPEGs have
improved the expressive power in such ways that they recognize practical context-sensitive grammars, including back
referencing, indentation-based code layout, and contextual
keywords.
",15.903189008614273,13.487012987012989,131
SLE_17_012.txt,17.310640250445292,16.30895337420515,SLE,9183,"
In model-driven engineering (MDE), models abstract the relevant features of software artefacts and model management
operations, including model transformations, act on them
automating large tasks of the development process. Flexible
reuse of such operations is an important factor to improve
productivity when developing and maintaining MDE solutions. In this work, we revisit the traditional notion of object
subtyping based on subsumption, discarded by other approaches to model subtyping. We refine a type system for
object-oriented programming, with multiple inheritance, to
support model types in order to analyse its advantages and
limitations with respect to reuse in MDE. Specifically, we extend type expressions with referential constraints and with
OCL constraints. Our approach has been validated with a tool
that extracts model types from (EMF) metamodels, paired
with their OCL constraints, automatically and that exploits
the extended subtyping relation to reuse model management
operations. We show that structural model subtyping is expressive enough to support variants of model subtyping,
including multiple, partial and dynamic model subtyping.
The tool has received the ACM badge ""Artifacts Evaluated −
Functional"".
",17.971250198000288,16.438750000000002,177
SLE_17_013.txt,16.91943222446761,15.46735352678434,SLE,8581,"
There is currently a lack of Requirements Engineering (RE)
approaches applied to, or supporting, the development of
a Domain-Specific Language (DSL) taking into account the
environment in which it is to be used. We present a modelbased RE approach to support DSL development with a focus
on usability concerns. RDAL is a RE fragment language that
can be complemented with other languages to support RE
and design. USE-ME is a model driven approach for DSLs
usability evaluation which is integrable with a DSL development approach. We combine RDAL and a new DSL, named
DSSL, that we created for the specification of DSL-based
systems. Integrated with this combination we add USE-ME
to support usability evaluation. This combination of existing
languages and tools provides a comprehensive RE approach
for DSL development. We illustrate the approach with the
development of the Gyro DSL for programming robots.
",14.37465228746014,12.687814625850343,148
SLE_17_014.txt,17.971250198000288,17.412396755162245,SLE,1684,"

Alf is an action language designed as a textual notation for
specifying detailed behaviors within an executable UML
model. The Alf implementation in MagicDraw, a leading
commercial tool for modeling using the Unified Modeling
Language (UML) from No Magic, Inc., aims to support the
practical application of Alf in real-world uses of
executable UML modeling. It includes syntax-aware
editing and checking of Alf code, with valid code
automatically and transparently compiled into UML
activity models. The resulting models are fully integrated
within the wider UML modeling context, and they can
then be executed as part of full system simulation
scenarios. The Alf compiler also tracks the dependencies
of all Alf text on other UML model elements, allowing for
automatic re-checking and re-building the Alf code as
necessitated by changes in referenced elements. The goal
is to provide an IDE-level experience for the easy entry
and maintenance of Alf code within an overall executable
UML model.
",17.315433740611066,16.56625,161
SLE_17_015.txt,14.291551793972971,12.236159171659768,SLE,7564,"
We present FLowSrec, a declarative specification language
for the domain of dataflow analysis. FLowSpec has declarative support for the specification of control flow graphs
of programming languages, and dataflow analyses on these
control flow graphs. We define the formal semantics of
FLtowSpec, which is rooted in Monotone Frameworks. We
also discuss implementation techniques for the language,
partly used in the prototype implementation built in the
Spoorax Language Workbench. Finally, we evaluate the expressiveness and conciseness of the language with two case
studies. These case studies are analyses for GREEN-MARL, an
industrial, domain-specific language for graph processing.
The first case study is a classical dataflow analysis, scaled to
this full language. The second case study is a domain-specific
analysis of GREEN-MARL.
",13.81666964889586,12.627580645161292,125
SLE_17_016.txt,15.209611147864539,14.496718729730244,SLE,9204,"
We present a new language construct, filtered iterators, for
robust input processing. Filtered iterators are designed to
eliminate many common input processing errors while enabling robust continued execution. The design is inspired
by (1) observed common input processing errors and (2) successful strategies implemented by human developers fixing
input processing errors. Filtered iterators decompose inputs
into input units and atomically and automatically discard
units that trigger errors. Statistically significant results from
a developer study demonstrate the effectiveness of filtered
iterators in enabling developers to produce robust input processing code without common input processing defects.
",16.887214914478655,18.480297872340426,95
SLE_17_017.txt,17.356301134967886,15.340820171324836,SLE,8692,"

This paper presents a study of the runtime, memory usage
and energy consumption of twenty seven well-known software languages. We monitor the performance of such languages using ten different programming problems, expressed
in each of the languages. Our results show interesting findings, such as, slower/faster languages consuming less/more
energy, and how memory usage influences energy consumption. We show how to use our results to provide software
engineers support to decide which language to use when
energy efficiency is a concern.
",16.52667757954773,14.396475903614459,84
SLE_17_018.txt,16.359313933844536,14.326089934168618,SLE,9665,"
Reference Attribute Grammars (RAGs) is a declarative executable formalism used for constructing compilers and related tools. Existing implementations support concurrent
evaluation only with global evaluation locks. This may lead
to long latencies in interactive tools, where interactive and
background threads query attributes concurrently.
We present lock-free algorithms for concurrent attribute
evaluation, enabling low latency in interactive tools. Our algorithms support important extensions to RAGs like circular
(fixed-point) attributes and higher-order attributes.
We have implemented our algorithms in Java, for the JastAdd metacompiler. We evaluate the implementation on a
JastAdd-specified compiler for the Java language, demonstrating very low latencies for interactive attribute queries,
on the order of milliseconds. Furthermore, initial experiments show a speedup of about a factor 2 when using four
parallel compilation threads.
",17.122413403193683,16.462500000000002,129
SLE_17_019.txt,15.581280011638665,13.792207236765016,SLE,4814,"

Crowdsourcing has emerged as a novel paradigm where humans are employed to perform computational tasks. In the
context of Domain-Specific Modeling Language (DSML) development, where the involvement of end-users is crucial
to assure that the resulting language satisfies their needs,
crowdsourcing tasks could be defined to assist in the language definition process. By relying on the crowd, it is possible to show an early version of the language to a wider
spectrum of users, thus increasing the validation scope and
eventually promoting its acceptance and adoption. We propose a systematic method for creating crowdsourcing campaigns aimed at refining the graphical notation of DSMLs.
The method defines a set of steps to identify, create and
order the questions for the crowd. As a result, developers
are provided with a set of notation choices that best fit endusers’ needs. We also report on an experiment validating the
approach.
",15.343465313023842,13.305714285714288,149
SLE_17_020.txt,17.887005091858285,17.92463575642328,SLE,11651,"
While contemporary projectional editors make sure that the
edited programs conform to the programming language’s
metamodel, they do not enforce that they are also wellformed, that is, that they obey the well-formedness rules defined for the language. We show how, based on a constraintbased capture of well-formedness, projectional editors can
be empowered to enforce well-formedness in much the same
way they enforce conformance with the metamodel. The
resulting robust edits may be more complex than ordinary,
well-formedness breaking edits, and hence may require more
user involvement; yet, maintaining well-formedness at all
times ensures that necessary corrections of a program are
linked to the edit that necessitated them, and that the projectional editor’s services are never compromised by inconsistent programs. Robust projectional editing is not a straitjacket, however: If a programmer prefers to work without
it, its constraint-based capture of well-formedness will still
catch all introduced errors Ð unlike many other editor services, well-formedness checking and robust editing are based
on the same implementation, and are hence guaranteed to
behave consistently.
",22.501965172709998,22.978483146067415,181
SLE_17_021.txt,16.668867091567368,15.398030614301316,SLE,8478,"

Any grammar engineer can tell a good grammar from a bad
one, but there is no commonly accepted taxonomy of indicators of required grammar refactorings. One of the consequences of this lack of general smell taxonomy is the scarcity
of tools to assess and improve the quality of grammars. By
combining two lines of research—on smell detection and on
grammar transformation—we have assembled a taxonomy of
smells in grammars. As a pilot case, the detectors for identified smells were implemented for grammars in a broad sense
and applied to the 641 grammars of the Grammar Zoo.
",15.903189008614273,13.471122448979592,99
SLE_17_022.txt,14.005025920528372,11.689768801374655,SLE,4097,"

Red Shift is a new design pattern for implementing parsers.
The pattern draws ideas from traditional shift-reduce parsing
as well as procedural PEG parsers. Red Shift parsers behave
like shift-reduce parsers, but eliminate ambiguity by always
prioritizing reductions over shifts. To compensate the resulting lack of expressivity, reducers are not simple reduction
rules but full-blown procedures written in a general-purpose
host language. I found many advantages to this style of parsing. In particular, we can generate high-quality error messages more easily; and compose different style of parsers.
I also speculate about how Red Shift parsers may improve
partial compilation in the context of an IDE.
",13.707050652182112,11.885844155844158,111
SLE_17_023.txt,16.700946416226166,15.853147496502313,SLE,7716,"
The maintenance of modern systems often requires developers to perform complex and error-prone cognitive tasks,
which are caused by the obscurity, redundancy, and irrelevancy of code, distracting from essential maintenance tasks.
Typical maintenance scenarios include multiple branches of
code in repositories, which involves dealing with branchinterdependent changes, and aspects in aspect-oriented development, which requires in-depth knowledge of behaviorinterdependent changes. Thus, merging branched files as
well as validating the behavior of statically composed code
requires developers to conduct exhaustive individual introspection.
In this work we present VirtualEdit for associative,
commutative, and invertible model composition. It allows
simultaneous editing of multiple model versions or variants through dynamically derived virtual models. We implemented the approach in terms of an open-source framework
that enables multi-version editing and aspect-orientation
by selectively focusing on specific parts of code, which are
significant for a particular engineering task. The VirtualEdit
framework is evaluated based on its application to the most
popular publicly available Xtext-based languages. Our results indicate that VirtualEdit can be applied to existing
languages with reasonably low effort.
",18.903936251131103,18.45021648044693,180
SLE_17_024.txt,17.122413403193683,15.991324743341767,SLE,6663,"
Selecting and properly using approaches for DSL implementation can be challenging, given their variety and complexity.
To support developers, we present the software chrestomathy
MetaLib, a well-organized and well-documented collection
of DSL implementations useful for learning. We focus on
basic metaprogramming techniques for implementing DSL
syntax and semantics. The DSL implementations are organized and enhanced by feature modeling, semantic annotation, and model-based documentation. The chrestomathy
enables side-by-side exploration of different implementation
approaches for DSLs. Source code, feature model, feature
configurations, semantic annotations, and documentation
are publicly available online, explorable through a web application, and maintained by a collaborative process
",17.5058628484301,17.379615384615388,105
SOCC_17_001.txt,15.4796725050374,13.601547761646021,SOCC,11046,"

Myriad of parameter estimation algorithms can be performed by an
Expectation-Maximization (EM) approach. Traditional synchronous
frameworks can parallelize these EM algorithms on the cloud to
accelerate computation while guaranteeing the convergence. However, expensive synchronization costs pose great challenges for efficiency. Asynchronous solutions have been recently designed to
bypass high-cost synchronous barriers but at expense of potentially
losing convergence guarantee.

This paper first proposes a flexible synchronous parallel framework (FSP) that provides the capability of synchronous EM algorithms implementations, as well as significantly reduces the barrier
cost. Under FSP, every distributed worker can immediately suspend.
local computation when necessary, to quickly synchronize with each
other. That maximizes the time fast workers spend doing useful work,
instead of waiting for slow, straggling workers. We then formally
prove the algorithm convergence. Further, we analyze how to automatically identify a proper barrier interval to strike a nice balance
between reduced synchronization costs and the convergence speed.
Empirical results demonstrate that on a broad spectrum of real-world
and synthetic datasets, FSP achieves as much as 3x speedup over the
up-to-date synchronous solution.
",17.238542476582836,15.821186813186817,183
SOCC_17_002.txt,15.044708065449075,13.404844055020895,SOCC,10604,"

In an Infrastructure As A Service (IaaS) cloud, the scheduler deploys VMs to servers according to service level objectives (SLOs).
Clients and service providers must both trust the infrastructure. In
particular they must be sure that the VM scheduler takes decisions
that are consistent with its advertised behaviour. The difficulties to
master every theoretical and practical aspects of a VM scheduler
implementation leads however to faulty behaviours that break SLOs
and reduce the provider revenues.

We present SafePlace, a specification and testing framework that
exhibits inconsistencies in VM schedulers. SafePlace mixes a DSL to
formalise scheduling decisions with fuzz testing to generate a large
spectrum of test cases and automatically report implementation
faults.

We evaluate SafePlace on the VM scheduler BtrPlace. Without
any code modification, SafePlace allows to write test campaigns
that are 3.83 times smaller than BtrPlace unit tests. SafePlace performs 200 tests per second, exhibited new non-trivial bugs, and
outperforms the BtrPlace runtime assertion system.
",15.903189008614273,14.035147679324897,160
SOCC_17_003.txt,13.802836591133463,12.122166714357157,SOCC,10591,"

With the ever increasing DRAM capacity in commodity computers,
applications tend to store large amount of data in main memory
for fast access. Accordingly, efficient traversal of index structures
to locate requested data becomes crucial to their performance. The
index data structures grow so large that only a fraction of them can
be cached in the CPU cache. The CPU cache can leverage access
locality to keep the most frequently used part of an index in it for
fast access. However, the traversal on the index to a target data during a search for a data item can result in significant false temporal
and spatial localities, which make CPU cache space substantially
underutilized. In this paper we show that even for highly skewed
accesses the index traversal incurs excessive cache misses leading
to suboptimal data access performance. To address the issue, we
introduce Search Lookaside Buffer (SLB) to selectively cache only
the search results, instead of the index itself. SLB can be easily integrated with any index data structure to increase utilization of
the limited CPU cache resource and improve throughput of search
requests on a large data set. We integrate SLB with various index
data structures and applications. Experiments show that SLB can
improve throughput of the index data structures by up to an order
of magnitude. Experiments with real-world key-value traces also
show up to 73% throughput improvement on a hash table.
",14.554592549557764,13.277272727272727,237
SOCC_17_004.txt,13.585028240637119,11.289936881857749,SOCC,11204,"

Graph analytics systems have gained significant popularity due to
the prevalence of graph data. Many of these systems are designed to
run in a shared-nothing architecture whereby a cluster of machines
can process a large graph in parallel. In more recent proposals,
others have argued that a single-machine system can achieve better
performance and/or is more cost-effective. There is however no
clear consensus which approach is better. In this paper, we classify
existing graph analytics systems into four categories based on the
architectural differences, i.e., processing infrastructure (centralized
vs distributed), and memory consumption (in-memory vs out-ofcore). We select eight open-source systems to cover all categories,
and perform a comparative measurement study to compare their
performance and cost characteristics across a spectrum of input
data, applications, and hardware settings. Our results show that
the best performing configuration can depend on the type of applications and input graphs, and there is no dominant winner across
all categories. Based on our findings, we summarize the trends in
performance and cost, and provide several insights that help to illuminate the performance and resource cost tradeoffs across different
graph analytics systems and categories.
",16.52667757954773,15.700865384615387,197
SOCC_17_005.txt,14.872322054041486,12.914584648628061,SOCC,12498,"

Cloud Infrastructure as a Service (IaaS) providers continually seek
higher resource utilization to better amortize capital costs. Higher
utilization not only can enable higher profit for IaaS providers but
also provides a mechanism to raise energy efficiency; therefore
creating greener cloud services. Unfortunately, achieving high utilization is difficult mainly due to infrastructure providers needing
to maintain spare capacity to service demand fluctuations.
Graceful degradation is a self-adaptation technique originally
designed for constructing robust services that survive resource
shortages. Previous work has shown that graceful degradation can
also be used to improve resource utilization in the cloud by absorbing demand fluctuations and reducing spare capacity. In this work,
we build a system and pricing model that enables infrastructure
providers to incentivize their tenants to use graceful degradation.
By using graceful degradation with an appropriate pricing model,
the infrastructure provider can realize higher resource utilization
while simultaneously, its tenants can increase their profit. Our proposed solution is based on a hybrid model which guarantees both
reserved and peak on-demand capacities over flexible periods. It
also includes a global dynamic price pair for capacity which remains
uniform during each tenant’s Service Level Agreement (SLA) term.
We evaluate our scheme using simulations based on real-world
traces and also implement a prototype using RUBiS on the Xen
hypervisor as an end-to-end demonstration. Our analysis shows
that the proposed scheme never hurts a tenant’s net profit, but can
improve it by as much as 93%. Simultaneously, it can also improve
the effective utilization of contracts from 42% to as high as 99%.
",16.52667757954773,16.130814176245213,264
SOCC_17_006.txt,14.72577051111638,12.661287499870344,SOCC,10802,"

The growing pressure on cloud application scalability has
accentuated storage performance as a critical bottleneck. Although cache replacement algorithms have been extensively
studied, cache prefetching — reducing latency by retrieving
items before they are actually requested — remains an underexplored area. Existing approaches to history-based prefetching,
in particular, provide too few benefits for real systems for the
resources they cost.

We propose MITHRIL, a prefetching layer that efficiently

Reza Karimi
Emory University
rkarimi @emory.edu

exploits historical patterns in cache request associations. MITHRIL

is inspired by sporadic association rule mining and only relies on the timestamps of requests. Through evaluation of
135 block-storage traces, we show that MITHRIL is effective,
giving an average of a 55% hit ratio increase over LRU and
PROBABILITY GRAPH, and a 36% hit ratio gain over AMP
at reasonable cost. Finally, we demonstrate the improvement
comes from MITHRIL being able to capture mid-frequency
blocks.
",18.08858127442927,16.78659635666347,151
SOCC_17_007.txt,13.344112289762553,10.984990700173856,SOCC,12680,"

Graphics processing units (GPUs) have become an attractive platform for general-purpose computing (GPGPU) in various domains.
Making GPUs a time-multiplexing resource is a key to consolidating GPGPU applications (apps) in multi-tenant cloud platforms.
However, advanced GPGPU apps pose a new challenge for consolidation. Such highly functional GPGPU apps, referred to as GPU
eaters, can easily monopolize a shared GPU and starve collocated
GPGPU apps. This paper presents GLoop, which is a software runtime that enables us to consolidate GPGPU apps including GPU
eaters. GLoop offers an event-driven programming model, which
allows GLoop-based apps to inherit the GPU eaters’ high functionality while proportionally scheduling them on a shared GPU in
an isolated manner. We implemented a prototype of GLoop and
ported eight GPU eaters on it. The experimental results demonstrate that our prototype successfully schedules the consolidated
GPGPU apps on the basis of its scheduling policy and isolates resources among them.
",15.078166124597352,13.20625,156
SOCC_17_008.txt,14.15137909133077,12.5220796180438,SOCC,10290,"
 1 INTRODUCTION

State machine replication (SMR) uses Paxos to enforce the same
inputs for a program (e.g., Redis) replicated on a number of hosts,
tolerating various types of failures. Unfortunately, traditional Paxos
protocols incur prohibitive performance overhead on server programs due to their high consensus latency on TCP/IP. Worse, the
consensus latency of extant Paxos protocols increases drastically
when more concurrent client connections or hosts are added. This
paper presents APUS, the first RDMA-based Paxos protocol that
aims to be fast and scalable to client connections and hosts. APUS
intercepts inbound socket calls of an unmodified server program,
assigns a total order for all input requests, and uses fast ROMA
primitives to replicate these requests concurrently.

We evaluated APUS on nine widely-used server programs (e.g.,
Redis and MySQL). APUS incurred a mean overhead of 4.3% in
response time and 4.2% in throughput. We integrated APUS with an
SMR system Calvin. Our Calvin-APUS integration was 8.2X faster
than the extant Calvin-ZooKeeper integration. The consensus
latency of APUS outperformed an RDMA-based consensus protocol
by 4.9X. APUS source code and raw results are released on github.
com/hku-systems/apus.
",14.410869940926823,13.213683937823834,200
SOCC_17_009.txt,15.881969577006299,14.541547496392052,SOCC,10965,"

The typical enterprise data architecture consists of several actively
updated data sources (e.g., NoSQL systems, data warehouses), and a
central data lake such as HDFS, in which all the data is periodically
loaded through ETL processes. To simplify query processing, stateof-the-art data analysis approaches solely operate on top of the
local, historical data in the data lake, and ignore the fresh tail
end of data that resides in the original remote sources. However,
as many business operations depend on real-time analytics, this
approach is no longer viable. The alternative is hand-crafting the
analysis task to explicitly consider the characteristics of the various
data sources and identify optimization opportunities, rendering the
overall analysis non-declarative and convoluted.

Based on our experiences operating in data lake environments,
we design System-PV, a real-time analytics system that masks the
complexity of dealing with multiple data sources while offering
minimal response times. System-PV extends Spark with a sophisticated data virtualization module that supports multiple applications
— from SQL queries to machine learning. The module features a
location-aware compiler that considers source complexity, and a
two-phase optimizer that produces and refines the query plans, not
only for SQL queries but for all other types of analysis as well.
The experiments show that System-PV is often faster than Spark
by more than an order of magnitude. In addition, the experiments
show that the approach of accessing both the historical and the
remote fresh data is viable, as it performs comparably to solely
operating on top of the local, historical data.
",18.71604785175511,18.372493638676847,264
SOCC_17_010.txt,15.247664890283005,14.016943863697659,SOCC,12489,"

We present CAPNET, a capability-based network architecture designed to enable least authority and secure collaboration in the
cloud. CapNeET allows fine-grained management of rights, recursive delegation, hierarchical policies, and least privilege. To enable
secure collaboration, CAPNET extends a classical capability model
with support for decentralized authority. We implement CAPNET
in the substrate of a software-defined network, integrate it with
the OpenStack cloud, and develop protocols enabling secure multiparty collaboration.
",17.410965686947208,17.32444444444445,73
SOCC_17_011.txt,14.661270833634301,13.459880187235271,SOCC,15255,"
 KEYWORDS

Key-value stores are increasingly adopting LSM-trees as their
enabling data structure in the backend storage, and persisting
their clustered data through a file system. A file system
is expected to not only provide file/directory abstraction
to organize data but also retain the key benefits of LSMtrees, namely, sequential and aggregated I/O patterns on the
physical device. Unfortunately, our in-depth experimental
analysis reveals that some of these benefits of LSM-trees can
be completely negated by the underlying file level indexes
from the perspectives of both data layout and I/O processing.
As a result, the write performance of LSM-trees is kept at a
level far below that promised by the sequential bandwidth
offered by the storage devices. In this paper, we address
this problem and propose LDS, an LSM-tree based Direct
Storage system that manages the storage space and provides
simplified consistency control by exploiting the copy-on-write
nature of the LSM-tree structure, so as to fully reap the
benefits of LSM-trees.

Running LSM-trees on LDS as a baseline for comparison,
we evaluate LSM-trees on three representative file systems
(EXT4, F2F8, BTRFS) with HDDs and SSDs respectively, to
study the performance potentials of LSM-trees. Evaluation
results show that the write throughputs of LSM-trees can
be improved by from 1.8x to 3x on HDDs, and from 1.3x
to 2.5x on SSDs, by employing the LSM-tree friendly data
layout of LDS.
",16.439396014739867,16.988163265306124,249
SOCC_17_012.txt,14.454680015459129,13.120593858851745,SOCC,9661,"

Network Function Virtualization has been touted as the silver bullet
for tackling a number of operator problems, including vendor lockin, fast deployment of new functionality, converged management,
and lower expenditure since packet processing runs on inexpensive
commodity servers. The reality, however, is that, in practice, it has
proved hard to achieve the stable, predictable performance provided by hardware middleboxes, and so operators have essentially
resorted to throwing money at the problem, deploying highly underutilized servers (e.g., one NF per CPU core) in order to guarantee
high performance during peak periods and meet SLAs.

In this work we introduce HyperNF, a high performance NFV
framework aimed at maximizing server performance when concurrently running large numbers of NFs. To achieve this, HyperNF
implements hypercall-based virtual I/O, placing packet forwarding
logic inside the hypervisor to significantly reduce I/O synchronization overheads. HyperNF improves throughput by 10%-73%
depending on the NF, is able to closely match resource allocation
specifications (with deviations of only 3.5%), and to efficiently cope
with changing traffic loads.
",19.882160675589997,20.519929824561405,174
SOCC_17_013.txt,14.612346039103866,12.845247176738017,SOCC,11084,"

Search advertising depends on accurate predictions of user behavior and interest, accomplished today using complex and computationally expensive machine learning algorithms that estimate the
potential revenue gain of thousands of candidate advertisements
per search query. The accuracy of this estimation is important for
revenue, but the cost of these computations represents a substantial
expense, e.g., 10% to 30% of the total gross revenue. Caching the
results of previous computations is a potential path to reducing
this expense, but traditional domain-agnostic and revenue-agnostic
approaches to do so result in substantial revenue loss. This paper
presents three domain-specific caching mechanisms that successfully optimize for both factors. Simulations on a trace from the Bing
advertising system show that a traditional cache can reduce cost
by up to 27.7% but has negative revenue impact as bad as —14.1%.
On the other hand, the proposed mechanisms can reduce cost by
up to 20.6% while capping revenue impact between —1.3% and 0%.
Based on Microsoft’s earnings release for FY16 Q4, the traditional
cache would reduce the net profit of Bing Ads by $84.9 to $166.1
million in the quarter, while our proposed cache could increase the
net profit by $11.1 to $71.5 million.
",17.77360955136429,16.310028011204484,211
SOCC_17_014.txt,15.885836830793984,14.307057520112036,SOCC,9654,"

Optimizing the performance of big-data streaming applications has
become a daunting and time-consuming task: parameters may be
tuned from a space of hundreds or even thousands of possible configurations. In this paper, we present a framework for automating
parameter tuning for stream-processing systems. Our framework
supports standard black-box optimization algorithms as well as a
novel gray-box optimization algorithm. We demonstrate the multiple benefits of automated parameter tuning in optimizing three
benchmark applications in Apache Storm. Our results show that
a hill-climbing algorithm that uses a new heuristic sampling approach based on Latin Hypercube provides the best results. Our
gray-box algorithm provides comparable results while being two
to five times faster.
",15.247664890283005,14.606452991452993,118
SOCC_17_015.txt,16.3367083865337,15.31552619187472,SOCC,9101,"

Protecting the customer’s SSL private key is the paramount issue
to persuade the website owners to migrate their contents onto the
cloud infrastructure, besides the advantages of cloud infrastructure in terms of flexibility, efficiency, scalability and elasticity. The
emerging Keyless SSL solution retains on-premise custody of customers’ SSL private keys on their own servers. However, it suffers
from significant performance degradation and limited scalability,
caused by the long distance connection to Key Server for each new
coming end-user request. The performance improvements using
persistent session and key caching onto cloud will degrade the key
invulnerability and discourage the website owners because of the
cloud’s security bugs.

In this paper, the challenges of secured key protection and distribution are addressed in philosophy of “Storing the trusted DATA on
untrusted platform and transmitting through untrusted channel”.
To this end, a three-phase hierarchical key management scheme,
called STYX! is proposed to provide the secured key protection together with hardware assisted service acceleration for cloud-based
content delivery network (CCDN) applications. The STYX is implemented based on Intel Software Guard Extensions (SGX), Intel
QuickAssist Technology (QAT) and SIGMA (SIGn-and-MAc) protocol. STYX can provide the tight key security guarantee by SGX
based key distribution with a light overhead, and it can further
significantly enhance the system performance with QAT based acceleration. The comprehensive evaluations show that the STYX
not only guarantees the absolute security but also outperforms
the direct HTTPS server deployed CDN without QAT by up to 5x
throughput with significant latency reduction at the same time.
",17.467979349516824,16.47122480620155,261
SOCC_17_016.txt,14.490101954201798,12.67470355560825,SOCC,11200,"

This paper introduces QFrag, a distributed system for graph
search on top of bulk synchronous processing (BSP) systems
such as MapReduce and Spark. Searching for patterns in
graphs is an important and computationally complex problem.
Most current distributed search systems scale to graphs that
do not fit in main memory by partitioning the input graph.
For analytical queries, however, this approach entails running
expensive distributed joins on large intermediate data.

In this paper we explore an alternative approach: replicating the input graph and running independent parallel
instances of a sequential graph search algorithm. In principle, this approach leads us to an embarrassingly parallel
problem, since workers can complete their tasks in parallel
without coordination. However, the skew present in natural
graphs makes this problem a deceitfully parallel one, i.e., an
embarrassingly parallel problem with poor load balancing.
We therefore introduce a task fragmentation technique that
avoids stragglers but at the same time minimizes coordination.
Our evaluation shows that QFrag outperforms BSP-based
systems by orders of magnitude, and performs similar to
asynchronous MPI-based systems on simple queries. Furthermore, it is able to run computationally complex analytical
queries that other systems are unable to handle.
",15.514038796780547,14.465256410256412,197
SOCC_17_017.txt,14.797233039293026,12.754281525822737,SOCC,10836,"
Data partitioning is crucial to improving query performance and
several workload-based partitioning techniques have been proposed
in database literature. However, many modern analytic applications
involve ad-hoc or exploratory analysis where users do not have a
representative query workload a priori. Static workload-based data
partitioning techniques are therefore not suitable for such settings.
In this paper, we propose Amoeba, a distributed storage system
that uses adaptive multi-attribute data partitioning to efficiently
support ad-hoc as well as recurring queries. Amoeba requires zero
set-up and tuning effort, allowing analysts to get the benefits of
partitioning without requiring an upfront query workload. The key
idea is to build and maintain a partitioning tree on top of the dataset.
The partitioning tree allows us to answer queries with predicates by
reading a subset of the data. The initial partitioning tree is created
without requiring an upfront query workload and Amoeba adapts
it over time by incrementally modifying subtrees based on user
queries using repartitioning. A prototype of Amoeba running on
top of Apache Spark improves query performance by up to 7x over
full scans and up to 2x over range-based partitioning techniques
on TPC-H as well as a real-world workload.
",16.32212239822248,15.461764705882356,205
SOCC_17_018.txt,14.581677911666212,12.459201151787152,SOCC,9118,"
Cloud computing needs to process and analyze massive highdimensional data in a real-time manner. Approximate queries
in cloud computing systems can provide timely queried results with acceptable accuracy, thus alleviating the consumption of a large amount of resources. Locality Sensitive Hashing (LSH) is able to maintain the data locality and support
approximate queries. However, due to randomly choosing
hash functions, LSH has to use too many functions to guarantee the query accuracy. The extra computation and storage overheads exacerbate the real performance of LSH. In
order to reduce the overheads and deliver high performance,
we propose a distribution-aware scheme, called DLSH, to offer cost-effective approximate nearest neighbor query service
for cloud computing. The idea of DLSH is to leverage the
principal components of the data distribution as the projection vectors of hash functions in LSH, further quantify the
weight of each hash function and adjust the interval value in
each hash table. We then refine the queried result set based
on the hit frequency to significantly decrease the time overhead of distance computation. Extensive experiments in a
large-scale cloud computing testbed demonstrate significant
improvements in terms of multiple system performance mettics. We have released the source code of DLSH for public
use.
",15.774802946060372,13.981864077669908,207
SOCC_17_019.txt,15.758477285546967,14.192297822566768,SOCC,9634,"

In a cloud data center, a single physical machine simultaneously
executes dozens of highly heterogeneous tasks. Such colocation
results in more efficient utilization of machines, but, when tasks’
requirements exceed available resources, some of the tasks might
be throttled down or preempted. We analyze version 2.1 of the
Google cluster trace that shows short-term (1 second) task CPU
usage. Contrary to the assumptions taken by many theoretical
studies, we demonstrate that the empirical distributions do not
follow any single distribution. However, high percentiles of the total
processor usage (summed over at least 10 tasks) can be reasonably
estimated by the Gaussian distribution. We use this result for a
probabilistic fit test, called the Gaussian Percentile Approximation
(GPA), for standard bin-packing algorithms. To check whether a
new task will fit into a machine, GPA checks whether the resulting
distribution’s percentile corresponding to the requested service
level objective, SLO is still below the machine’s capacity. In our
simulation experiments, GPA resulted in colocations exceeding the
machines’ capacity with a frequency similar to the requested SLO.
",17.122413403193683,15.664678571428574,179
SOCC_17_020.txt,14.80510775649789,12.638354357072604,SOCC,11280,"

While virtualization helps to enable multi-tenancy in data centers,
it introduces new challenges to the resource management in traditional OSes. We find that one important design in an OS, prioritizing
interactive and I/O-bound workloads, can become ineffective in a
virtualized OS. Resource multiplexing between multiple tenants
breaks the assumption of continuous CPU availability in physical
systems and causes two types of priority inversions in virtualized
OSes. In this paper, we present xBALLoon , a lightweight approach
to preserving I/O prioritization. It uses a balloon process in the
virtualized OS to avoid priority inversion in both short-term and
long-term scheduling. Experiments in a local Xen environment and
Amazon EC2 show that xBatLoon improves I/O performance in a
recent Linux kernel by as much as 136% on network throughput,
95% on disk throughput, and 125x on network tail latency.
",16.52667757954773,15.489615384615387,144
SOCC_17_021.txt,15.757906260955018,13.776385980411956,SOCC,12765,"

To maximize the effectiveness of modern virtualization systems,
resources must be allocated fairly and efficiently amongst virtual
machines (VMs). However, current policies for allocating memory
are relatively static. As a result, system-wide memory utilization
is often sub-optimal, leading to unnecessary paging and performance degradation. To better utilize the large-scale memory resources of modern machines, the virtualization system must allow
virtual machines to expand beyond their initial memory reservations, while still fairly supporting concurrent virtual machines.
This paper presents a system for dynamically allocating memory
amongst virtual machines at runtime, as well as an evaluation of
six allocation policies implemented within the system. The system
allows guest VMs to expand and contract according to their changing demands by uniquely improving and integrating mechanisms
such as memory ballooning, memory hotplug, and hypervisor paging. Furthermore, the system provides fairness by guaranteeing
each guest a minimum reservation, charging for rentals beyond
this minimum, and enforcing timely reclamation of memory.
",19.142268018852484,18.53057866184449,159
SOCC_17_022.txt,13.335643704675352,11.254181291273746,SOCC,13941,"
We reveal loopholes of Speculative Execution (SE) implementations
under a unique fault model: node-level network throughput degradation. This problem appears in many data-parallel frameworks
such as Hadoop MapReduce and Spark. To address this, we present
PBSE, a robust, path-based speculative execution that employs three
key ingredients: path progress, path diversity, and path-straggler
detection and speculation. We show how PBSE is superior to other
approaches such as cloning and aggressive speculation under the
aforementioned fault model. PBSE is a general solution, applicable
to many data-parallel frameworks such as Hadoop/HDFS+QF%S,
Spark and Flume.
",14.836745963215662,14.04,100
SOCC_17_023.txt,16.986088102837435,16.419849085447144,SOCC,10995,"
 accountability, such as determining the bandwidth of each point
Network usage accountability is critical in helping operators and
customers of multi-tenant data centers deal with concerns such
as capacity planning, resource allocation, hotspot detection, link
failure detection, and troubleshooting. However, the cost of measurements and instrumentation to achieve flow-level accountability
is non-trivial. We propose Polygravity to determine tenant traffic
usage via lightweight measurements in multi-tenant data centers.
We adopt a tomogravity model widely used in ISP networks, and
adapt it to a multi-tenant data center environment. By integrating datacenter-specific domain knowledge, sampling-based partial
estimation and gravity-based internal sinks/sources estimation,
Polygravity addresses two key challenges for adapting tomogravity
to a data center environment: sparse traffic matrices and internal
traffic sinks/sources. We conducted extensive evaluation of our approach using realistic data center workloads. Our results show that
Polygravity can determine tenant IP flow usage with less than 1%
average relative error for tenants with fine-grained domain knowledge. In addition, for tenants with coarse-grained domain knowledge and with partial host-based sampling, Polygravity reduces the
relative error of sampling-based estimation by 5
",17.267425705330172,17.984583333333337,193
SOCC_17_024.txt,14.498579404303552,12.794953411931647,SOCC,12705,"
Next-generation non-volatile memories (NVMs) will provide byte
addressability, persistence, high density, and DRAM-like performance. They have the potential to benefit many datacenter applications. However, most previous research on NVMs has focused on
using them in a single machine environment. It is still unclear how
to best utilize them in distributed, datacenter environments.

We introduce Distributed Shared Persistent Memory (DSPM), a
new framework for using persistent memories in distributed datacenter environments. DSPM provides a new abstraction that allows
applications to both perform traditional memory load and store
instructions and to name, share, and persist their data.

We built Hotpot, a kernel-level DSPM system that provides lowlatency, transparent memory accesses, data persistence, data reliability, and high availability. The key ideas of Hotpot are to integrate
distributed memory caching and data replication techniques and
to exploit application hints. We implemented Hotpot in the Linux
kernel and demonstrated its benefits by building a distributed graph
engine on Hotpot and porting a NoSQL database to Hotpot. Our
evaluation shows that Hotpot outperforms a recent distributed
shared memory system by 1.3x to 3.2x and a recent distributed
PM-based file system by 1.5x to 3.0x.
",16.52667757954773,15.047880829015543,198
SOCC_17_025.txt,14.582157107229023,13.09196976932191,SOCC,12255,"

An ever increasing number of configuration parameters are provided to system users. But many users have used one configuration setting across different workloads, leaving untapped the performance potential of systems. A good configuration setting can
greatly improve the performance of a deployed system under certain workloads. But with tens or hundreds of parameters, it becomes a highly costly task to decide which configuration setting
leads to the best performance. While such task requires the strong
expertise in both the system and the application, users commonly
lack such expertise.

To help users tap the performance potential of systems, we present
BestConfig, a system for automatically finding a best configuration setting within a resource limit for a deployed system under a
given application workload. BestConfig is designed with an extensible architecture to automate the configuration tuning for general
systems. To tune system configurations within a resource limit,
we propose the divide-and-diverge sampling method and the recursive bound-and-search algorithm. BestConfig can improve the
throughput of Tomcat by 75%, that of Cassandra by 63%, that of
MySQL by 430%, and reduce the running time of Hive join job by
about 50% and that of Spark join job by about 80%, solely by configuration adjustment.
",14.867677710551934,14.420588235294119,205
SOCC_17_026.txt,15.596390811138221,13.545886545135968,SOCC,10044,"

Live migration is one of the key technologies to improve data center
utilization, power efficiency, and maintenance. Various live migration algorithms have been proposed; each exhibiting distinct characteristics in terms of completion time, amount of data transferred,
virtual machine (VM) downtime, and VM performance degradation. To make matters worse, not only the migration algorithm but
also the applications running inside the migrated VM affect the
different performance metrics. With service-level agreements and
operational constraints in place, choosing the optimal live migration technique has so far been an open question. In this work, we
propose an adaptive machine learning-based model that is able to
predict with high accuracy the key characteristics of live migration
in dependence of the migration algorithm and the workload running inside the VM. We discuss the important input parameters for
accurately modeling the target metrics, and describe how to profile
them with little overhead. Compared to existing work, we are not
only able to model all commonly used migration algorithms but also
predict important metrics that have not been considered so far such
as the performance degradation of the VM. In a comparison with
the state-of-the-art, we show that the proposed model outperforms
existing work by a factor 2 to 5.
",16.975882523387877,15.827458133971295,210
SOCC_17_027.txt,15.709720756529936,14.319782712715966,SOCC,8515,"

Erasure coding has been used in storage systems to enhance data durability at a lower storage overhead. However, these systems suffer from long access latency tails
due to a lack of flexible load balancing mechanisms and
passively launched degraded reads when the original
storage node of the requested data becomes a hotspot.
We provide a new perspective to load balancing in coded
storage systems by proactively and intelligently launching degraded reads and propose a variety of schemes
to make optimal decisions either per request or across
requests statistically. Experiments on a 98-machine cluster based on the request traces of 12 million objects collected from Windows Azure Storage (WAS) show that
our schemes can reduce the median latency by 44.7%
and the 95th-percentile tail latency by 77.8% in coded
storage systems.
",17.971250198000288,18.3769696969697,135
SOCC_17_028.txt,14.390340891164694,12.6534324473253,SOCC,9188,"

The level of demand for bare-metal cloud services has increased rapidly because such services are cost-effective for
several types of workloads, and some cloud clients prefer a
single-tenant environment due to the lower security vulnerability of such enviornments. However, as the bare-metal
cloud does not utilize a virtualization layer, it cannot use
live migration. Thus, there is a lack of manageability with
the bare-metal cloud. Live migration support can improve
the manageability of bare-metal cloud services significantly.

This paper suggests an on-demand virtualization technique
to improve the manageability of bare-metal cloud services.
A thin virtualization layer is inserted into the bare-metal
cloud when live migration is requested. After the completion
of the live migration process, the thin virtualization layer is
removed from the host. We modified BitVisor [19] to implement on-demand virtualization and live migration on the x86
architecture.

The elapsed time of on-demand virtualization was negligible. It takes about 20 ms to insert the virtualization layer and
30 ms to remove the one. After removing the virtualization
layer, the host machine works with bare-metal performance.
",14.02287970970757,12.666943907156675,189
SOCC_17_029.txt,15.752675714854778,13.945422188440443,SOCC,11691,"

Training machine learning (ML) models with large
datasets can incur significant resource contention on
shared clusters. This training typically involves many
iterations that continually improve the quality of the
model. Yet in exploratory settings, better models can be
obtained faster by directing resources to jobs with the
most potential for improvement. We describe SLAQ, a
cluster scheduling system for approximate ML training
jobs that aims to maximize the overall job quality.

When allocating cluster resources, SLAQ explores the
quality-runtime trade-offs across multiple jobs to maximize system-wide quality improvement. To do so,
SLAQ leverages the iterative nature of ML training algorithms, by collecting quality and resource usage information from concurrent jobs, and then generating highlytailored quality-improvement predictions for future iterations. Experiments show that SLAQ achieves an average
quality improvement of up to 73% and an average delay
reduction of up to 44% on a large set of ML training jobs,
compared to resource fairness schedulers.
",17.613555460941566,15.69260397830018,159
SOCC_17_030.txt,15.291393592414309,13.590882871807867,SOCC,11714,"

Data provenance describes how data came to be in its present form. It
includes data sources and the transformations that have been applied
to them. Data provenance has many uses, from forensics and security
to aiding the reproducibility of scientific experiments. We present
CamFlow, a whole-system provenance capture mechanism that integrates easily into a PaaS offering. While there have been several prior
whole-system provenance systems that captured a comprehensive,
systemic and ubiquitous record of a system’s behavior, none have
been widely adopted. They either A) impose too much overhead, B)
are designed for long-outdated kernel releases and are hard to port
to current systems, C) generate too much data, or D) are designed
for a single system. CamFlow addresses these shortcoming by: 1)
leveraging the latest kernel design advances to achieve efficiency; 2)
using a self-contained, easily maintainable implementation relying
on a Linux Security Module, NetFilter, and other existing kernel facilities; 3) providing a mechanism to tailor the captured provenance
data to the needs of the application; and 4) making it easy to integrate
provenance across distributed systems. The provenance we capture
is streamed and consumed by tenant-built auditor applications. We
illustrate the usability of our implementation by describing three
such applications: demonstrating compliance with data regulations;
performing fault/intrusion detection; and implementing data loss
prevention. We also show how CamFlow can be leveraged to capture
meaningful provenance without modifying existing applications.
",17.80541091248751,16.65553164556962,239
SOCC_17_031.txt,14.7444220306752,13.48464068727321,SOCC,11328,"

Many popular big data analytics systems today make liberal use of
user-defined functions (UDFs) in their programming interface and
are written in languages based on the Java Virtual Machine (JVM).
This combination creates a barrier when we want to integrate
processing engines written in a language that compiles down to
machine code with a JVM-based big data analytics ecosystem.

In this paper, we investigate efficient ways of executing UDFs
written in Java inside a data processing engine written in C++.
While it is possible to call Java code from machine code via the Java
Native Interface (JNI), a naive implementation that applies the UDF
one row at a time incurs a significant overhead, up to an order of
magnitude.

Instead, we can significantly reduce the costs of JNI calls and
data copies between Java and machine code, if we execute UDFs
on batches of rows, and reuse input/output buffers when possible.
Our evaluation of these techniques using different scalar UDFs,
in a prototype system that combines Spark and a columnar data
processing engine written in C++, shows that such a combination
does not slow down the execution of SparkSQL queries containing
such UDFs. In fact, we find that the execution of Java UDFs inside
an embedded JVM in our C++ engine is 1.12x to 1.53x faster than
executing in Spark alone. Our analysis also shows that compiling
Java UDFs directly into machine code is not always beneficial over
strided execution in the JVM.
",16.218646115125612,16.595793650793656,249
SOCC_17_032.txt,14.390377446454615,13.006798228703623,SOCC,9425,"

Many cloud-based data management and analytics systems
support complex objects. Dataflow platforms such as Spark
and Flink allow programmers to manipulate sets consisting of
objects from a host programming language (often Java). Document databases such as MongoDB make use of hierarchical
interchange formats—most popularly JSON—which embody
a data model where individual records can themselves contain
sets of records. Systems such as Dremel and AsterixDB allow
complex nesting of data structures.

Clearly, no system designer would expect a system that
stores JSON objects as text to perform at the same level as a
system based upon a custom-built physical data model. The
question we ask is: How significant is the performance hit associated with choosing a particular physical implementation?
Is the choice going to result in a negligible performance cost,
or one that is debilitating? Unfortunately, there does not exist
a scientific study of the effect of physical complex model implementation on system performance in the literature. Hence
itis difficult for a system designer to fully understand performance implications of such choices. This paper is an attempt
to remedy that.
",14.265292616868656,13.277054644808747,184
SOCC_17_033.txt,15.536875764221588,13.556782444954045,SOCC,11258,"

Users of cloud services are presented with a bewildering choice of
VM types and the choice of VM can have significant implications
on performance and cost. In this paper we address the fundamental
problem of accurately and economically choosing the best VM for
a given workload and user goals. To address the problem of optimal VM selection, we present PARIS, a data-driven system that
uses a novel hybrid offline and online data collection and modeling
framework to provide accurate performance estimates with minimal data collection. PARIS is able to predict workload performance
for different user-specified metrics, and resulting costs for a wide
range of VM types and workloads across multiple cloud providers.
When compared to sophisticated baselines, including collaborative
filtering and a linear interpolation model using measured workload
performance on two VM types, PARIS produces significantly better
estimates of performance. For instance, it reduces runtime prediction error by a factor of 4 for some workloads on both AWS and
Azure. The increased accuracy translates into a 45% reduction in
user cost while maintaining performance.
",17.122413403193683,16.005487012987015,177
SOCC_17_034.txt,15.26180062924259,13.800982699768326,SOCC,9551,"

The virtualization of services with high-availability requirements
calls to revisit traditional operation and provisioning processes.
Providers are realizing services in software on virtual machines
instead of using dedicated appliances to dynamically adjust service
capacity to changing demands. Cloud orchestration systems control the number of service instances deployed to make sure each
service has enough capacity to meet incoming workloads. However,
determining the suitable build-out of a service is challenging as it
takes time to install new instances and excessive re-configurations
(i.e. scale in/out) can lead to decreased stability. In this paper we
present AidOps, a cloud orchestration system that leverages machine learning and domain-specific knowledge to predict the traffic
demand, optimizing service performance and cost. AidOps does
not require a conservative provisioning of services to cover for
the worst-case demand and significantly reduces operational costs
while still fulfilling service quality expectations. We have evaluated
our framework with real traffic using an enterprise application and
a communication service in a private cloud. Our results show up
to 4X improvement in service performance indicators compared
to existing orchestration systems. AidOps achieves up to 99.985%
availability levels while reducing operational costs at least by 20%.
",17.37919286519448,16.8879797979798,201
SOCC_17_035.txt,16.667560292552594,15.00185580293737,SOCC,11719,"

Cloud computing offers a cost-efficient data analytics platform.
However, due to the sensitive nature of data, many organizations
are reluctant to analyze their data in public clouds. Both softwarebased and hardware-based solutions have been proposed to address
the stalemate, yet all have substantial limitations. We observe that
a main issue cutting across all solutions is that they attempt to support confidentiality in data queries in a way transparent to queries.
We propose the novel abstraction of secure data types with corresponding annotations for programmers to conveniently denote
constraints relevant to security. These abstractions are leveraged by
novel compilation techniques in our system Cuttlefish to compute
data analytics queries in public cloud infrastructures while keeping
sensitive data confidential. Cuttlefish encrypts all sensitive data
residing in the cloud and employs partially homomorphic encryption schemes to perform operations securely, resorting however to
client-side completion, re-encryption, or secure hardware-based
re-encryption based on Intel’s SGX when available based on a novel
planner engine. Our evaluation shows that our prototype can execute all queries in standard benchmarks such as TPC-H and TPC-DS
with an average overhead of 2.34x and 1.69x respectively compared
to a plaintext execution that reveals all data.
",17.553077303434723,16.80601485148515,206
SOCC_17_036.txt,15.733087023513189,14.201606829578772,SOCC,14271,"

Cloud spot markets offer virtual machines (VMs) for a dynamic
price that is much lower than the fixed price of on-demand VMs. In
exchange, spot VMs expose applications to multiple forms of risk,
including price risk, or the risk that a VM’s price will increase relative to others. Since spot prices vary continuously across hundreds
of different types of VMs, flexible applications can mitigate price
risk by moving to the VM that currently offers the lowest cost. To
enable this flexibility, we present HotSpot, a resource container that
“hops” VMs—by dynamically selecting and self-migrating to new
VMs~as spot prices change. HotSpot containers define a migration
policy that lowers cost by determining when to hop VMs based on
the transaction costs (from vacating a VM early and briefly double
paying for it) and benefits (the expected cost savings). As a side effect of migrating to minimize cost, HotSpot is also able to reduce the
number of revocations without degrading performance. HotSpot
is simple and transparent: since it operates at the systems-level on
each host VM, users need only run an HotSpot-enabled VM image
to use it. We implement a HotSpot prototype on EC2, and evaluate it
using job traces from a production Google cluster. We then compare
HotSpot to using on-demand VMs and spot VMs (with and without
fault-tolerance) in EC2, and show that it is able to lower cost and
reduce the number of revocations without degrading performance.
",16.218646115125612,15.560561740890687,248
SOCC_17_037.txt,15.49659593492511,13.867338101240389,SOCC,12405,"

Increasingly, smart Network Interface Cards (sNICs) are being used.
in data centers to offload networking functions (NFs) from host
processors thereby making these processors available for tenant applications. Modern sNICs have fully programmable, energy-efficient
multi-core processors on which many packet processing functions,
including a full-blown programmable switch, can run. However,
having multiple switch instances deployed across the host hypervisor and the attached sNICs makes controlling them difficult and
data plane operations more complex.

This paper proposes a generalized SDN-controlled NF offload
architecture called UNO. It can transparently offload dynamically
selected host processors’ packet processing functions to sNICs by
using multiple switches in the host while keeping the data centerwide network control and management planes unmodified. UNO
exposes a single virtual control plane to the SDN controller and
hides dynamic NF offload behind a unified virtual management
plane. This enables UNO to make optimal use of host’s and sNIC’s
combined packet processing capabilities with local optimization
based on locally observed traffic patterns and resource consumption,
and without central controller involvement. Experimental results
based on areal UNO prototype in realistic scenarios show promising
results: it can save processing worth up to 8 CPU cores, reduce
power usage by up to 2x, and reduce the control plane overhead by
more than 50%.
",19.7143461543385,18.990757880617036,216
SOCC_17_038.txt,14.393378224677434,12.913404290033366,SOCC,11722,"

Developing Big Data Analytics workloads often involves trial and
error debugging, due to the unclean nature of datasets or wrong
assumptions made about data. When errors (e.g., program crash,
outlier results, etc.) arise, developers are often interested in identifying a subset of the input data that is able to reproduce the problem.
BIGSIFT is a new faulty data localization approach that combines
insights from automated fault isolation in software engineering and.
data provenance in database systems to find a minimum set of failureinducing inputs. BIGSIFT redefines data provenance for the purpose
of debugging using a test oracle function and implements several
unique optimizations, specifically geared towards the iterative nature
of automated debugging workloads. BIGSIFT improves the accuracy of fault localizability by several orders-of-magnitude (~ 107
to 107x) compared to Titian data provenance, and improves performance by up to 66x compared to Delta Debugging, an automated
fault-isolation technique. For each faulty output, BIGSIFT is able to
localize fault-inducing data within 62% of the original job running
time.
",18.243605946275583,18.851764705882356,171
SOCC_17_039.txt,15.212426185588015,13.441520569513202,SOCC,11908,"

Efficient snapshots are an important feature of modern storage
systems. However, the implicit sharing underlying most snapshot
implementations makes it difficult to answer basic questions about
the storage costs of individual snapshots. Traditional techniques for
answering these questions incur significant performance penalties
due to expensive metadata overheads. We present a novel probabilistic data structure, compatible with existing storage systems,
that can provide approximate answers about snapshot costs with
very low computational and storage overheads while achieving
better than 95% accuracy for real-world data sets.
",17.122413403193683,17.40808823529412,86
SOCC_17_040.txt,13.961865928403256,11.958462216806428,SOCC,13233,"

The increasing interest in the Internet-of-Things (IoT) suggests that
a new source of big data is imminent—the machines and sensors
in the IoT ecosystem. The fundamental characteristic of the data
produced by these sources is that they are inherently geospatial in
nature. In addition, they exhibit unprecedented and unpredictable
skews. Thus, big data systems designed for IoT applications must
be able to efficiently ingest, index and query spatial data having
heavy and unpredictable skews. Spatial indexing is well explored
area of research in literature, but little attention has been given to
the topic of efficient distributed spatial indexing.

In this paper, we propose SIFT, a distributed spatial index and
its implementation. Unlike systems that depend on load balancing
mechanisms that kick-in post ingestion, S1FrT tries to distribute the
incoming data along the distributed structure at indexing time and
thus incurs minimal rebalancing overhead. Sirt depends only on
an underlying key-value store, hence is implementable in many
existing big data stores. Our evaluations of SirT on a popular open
source data store show promising results—SirT achieves up to 8x
reduction in indexing overhead while simultaneously reducing the
query latency and index size by over 2x and 3x respectively, in a
distributed environment compared to the state-of-the-art.
",16.594172100314452,15.573987538940813,215
SOCC_17_041.txt,14.585138908057726,12.685093083066935,SOCC,10909,"

There is a trend in recent database research to pursue coordination
avoidance and weaker transaction isolation under a long-standing
assumption: concurrent serializable transactions under read-write
or write-write conflicts require costly synchronization, and thus
may incur a steep price in terms of performance. In particular, distributed transactions, which access multiple data items atomically,
are considered inherently costly. They require concurrency control for transaction isolation since both read-write and write-write
conflicts are possible, and they rely on distributed commitment
protocols to ensure atomicity in the presence of failures. This paper
presents serializable read-only and write-only distributed transactions as a counterexample to show that concurrent transactions
can be processed in parallel with low-overhead despite conflicts.
Inspired by the slotted ALOHA network protocol, we propose a
simpler and leaner protocol for serializable read-only write-only
transactions, which uses only one round trip to commit a transaction in the absence of failures irrespective of contention. Our
design is centered around an epoch-based concurrency control
(ECC) mechanism that minimizes synchronization conflicts and
uses a small number of additional messages whose cost is amortized
across many transactions. We integrate this protocol into ALOHAKV, a scalable distributed key-value store for read-only write-only
transactions, and demonstrate that the system can process close to
15 million read/write operations per second per server when each
transaction batches together thousands of such operations.
",20.537248953866403,20.550516717325234,236
SOCC_17_042.txt,15.141561071054781,14.512793339618245,SOCC,10441,"

The advent of Web 2.0 companies, such as Facebook, Google, and
Amazon with their insatiable appetite for vast amounts of structured,
semi-structured, and unstructured data, triggered the development
of Hadoop and related tools, e.g., YARN, MapReduce, and Pig, as
well as NoSQL databases. These tools form an open source softwate stack to support the processing of large and diverse data sets
on clustered systems to perform decision support tasks. Recently,
SQL is resurrecting in many of these solutions, e.g., Hive, Stinger,
Impala, Shark, and Presto. At the same time, RDBMS vendors are
adding Hadoop support into their SQL engines, e.g., IBM’s Big
SQL, Actian’s Vortex, Oracle’s Big Data SQL, and SAP’s HANA.
Because there was no industry standard benchmark that could measure the performance of SQL-based big data solutions, marketing
claims were mostly based on “cherry picked” subsets of the TPC-DS
benchmark to suit individual companies strengths, while blending
out their weaknesses. In this paper, we present and analyze our work
on modifying TPC-DS to fill the void for an industry standard benchmark that is able to measure the performance of SQL-based big
data solutions. The new benchmark was ratified by the TPC in early
2016. To show the significance of the new benchmark, we analyze
performance data obtained on four different systems running big
data, traditional RDBMS, and columnar in-memory architectures.
",16.52667757954773,15.398684210526323,237
SOCC_17_043.txt,13.511798168923544,11.729023599433738,SOCC,11474,"

Real-time sensor data enables diverse applications such as smart
metering, traffic monitoring, and sport analysis. In the Internet of
Things, billions of sensor nodes form a sensor cloud and offer data
streams to analysis systems. However, it is impossible to transfer all
available data with maximal frequencies to all applications. Therefore, we need to tailor data streams to the demand of applications.
We contribute a technique that optimizes communication costs
while maintaining the desired accuracy. Our technique schedules
reads across huge amounts of sensors based on the data-demands
of a huge amount of concurrent queries. We introduce user-defined
sampling functions that define the data-demand of queries and
facilitate various adaptive sampling techniques, which decrease
the amount of transferred data. Moreover, we share sensor reads
and data transfers among queries. Our experiments with real-world
data show that our approach saves up to 87% in data transmissions.
",13.205437297517054,12.228666666666669,151
SOCC_17_044.txt,14.977833315468889,14.070944699721284,SOCC,8964,"

Service providers want to reduce datacenter costs by consolidating workloads onto fewer servers. At the same time, customers
have performance goals, such as meeting tail latency Service Level
Objectives (SLOs). Consolidating workloads while meeting tail latency goals is challenging, especially since workloads in production
environments are often bursty. To limit the congestion when consolidating workloads, customers and service providers often agree
upon rate limits. Ideally, rate limits are chosen to maximize the
number of workloads that can be co-located while meeting each
workload’s SLO. In reality, neither the service provider nor customer
knows how to choose rate limits. Customers end up selecting rate
limits on their own in some ad hoc fashion, and service providers
are left to optimize given the chosen rate limits.

This paper describes WorkloadCompactor, a new system that
uses workload traces to automatically choose rate limits simultaneously with selecting onto which server to place workloads. Our
system meets customer tail latency SLOs while minimizing datacenter resource costs. Our experiments show that by optimizing
the choice of rate limits, WorkloadCompactor reduces the number of required servers by 30-60% as compared to state-of-the-art
approaches.
",15.322241378113624,14.247923211169283,193
SOCC_17_045.txt,15.098900015529278,13.271284478192758,SOCC,10469,"
Multi-tenant distributed systems composed of small services, such
as Service-oriented Architectures (SOAs) and Micro-services, raise
new challenges in attaining high performance and efficient resource
utilization. In these systems, a request execution spans tens to thousands of processes, and the execution paths and resource demands on
different services are generally not known when a request first enters
the system. In this paper, we highlight the fundamental challenges of
regulating load and scheduling in SOAs while meeting end-to-end
performance objectives on metrics of concern to both tenants and
operators. We design Wisp, a framework for building SOAs that
transparently adapts rate limiters and request schedulers systemwide according to operator policies to satisfy end-to-end goals while
responding to changing system conditions. In evaluations against production as well as synthetic workloads, Wisp successfully enforces
a range of end-to-end performance objectives, such as reducing average latencies, meeting deadlines, providing fairness and isolation,
and avoiding system overload.
",20.64404841556777,19.383749999999996,161
SOSP_17_001.txt,16.262276864404292,14.947029403410518,SOSP,10136,"

Deep learning (DL) systems are increasingly deployed in
safety- and security-critical domains including self-driving
cars and malware detection, where the correctness and predictability of a system’s behavior for corner case inputs are
of great importance. Existing DL testing depends heavily
on manually labeled data and therefore often fails to expose
erroneous behaviors for rare inputs.

We design, implement, and evaluate DeepXplore, the first
whitebox framework for systematically testing real-world DL
systems. First, we introduce neuron coverage for systematically measuring the parts of a DL system exercised by test
inputs. Next, we leverage multiple DL systems with similar
functionality as cross-referencing oracles to avoid manual
checking. Finally, we demonstrate how finding inputs for
DL systems that both trigger many differential behaviors and
achieve high neuron coverage can be represented as a joint
optimization problem and solved efficiently using gradientbased search techniques.

DeepXplore efficiently finds thousands of incorrect corner case behaviors (e.g., self-driving cars crashing into guard
rails and malware masquerading as benign software) in stateof-the-art DL models with thousands of neurons trained on
five popular datasets including ImageNet and Udacity selfdriving challenge data. For all tested DL models, on average,
DeepXplore generated one test input demonstrating incorrect
behavior within one second while running only on a commodity laptop. We further show that the test inputs generated by
DeepXplore can also be used to retrain the corresponding DL
model to improve the model’s accuracy by up to 3%.
",18.12316971661352,17.520246913580248,247
SOSP_17_002.txt,15.995799101359498,14.31243658552096,SOSP,11285,"

Complex and unforeseen failures in distributed systems must
be diagnosed and replicated in a development environment
so that developers can understand the underlying problem
and verify the resolution. System logs often form the only
source of diagnostic information, and developers reconstruct
a failure using manual guesswork. This is an unpredictable
and time-consuming process which can lead to costly service
outages while a failure is repaired.

This paper describes Pensieve, a tool capable of reconstructing near-minimal failure reproduction steps from log
files and system bytecode, without human involvement. Unlike existing solutions that use symbolic execution to search
for the entire path leading to the failure, Pensieve is based
on the Partial Trace Observation, which states that programmers do not simulate the entire execution to understand the
failure, but follow a combination of control and data dependencies to reconstruct a simplified trace that only contains
events that are likely to be relevant to the failure. Pensieve
follows a set of carefully designed rules to infer a chain of
causally dependent events leading to the failure symptom
while aggressively skipping unrelated code paths to avoid
the path-explosion overheads of symbolic execution models.
",20.10790988173199,19.32229166666667,193
SOSP_17_003.txt,16.229641115900964,15.102666815956358,SOSP,11446,"

This paper presents Canopy, Facebook's end-to-end performance tracing infrastructure. Canopy records causally related
performance data across the end-to-end execution path of
requests, including from browsers, mobile applications, and
backend services. Canopy processes traces in near real-time,
derives user-specified features, and outputs to performance
datasets that aggregate across billions of requests. Using Canopy,
Facebook engineers can query and analyze performance data
in real-time. Canopy addresses three challenges we have encountered in scaling performance analysis: supporting the
range of execution and performance models used by different components of the Facebook stack; supporting interactive ad-hoc analysis of performance data; and enabling deep
customization by users, from sampling traces to extracting
and visualizing features. Canopy currently records and processes over 1 billion traces per day. We discuss how Canopy has
evolved to apply to a wide range of scenarios, and present case
studies of its use in solving various performance challenges.

",18.243605946275583,16.265069124423963,157
SOSP_17_004.txt,14.562535193929989,12.402713420279962,SOSP,15150,"

Algorand is a new cryptocurrency that confirms transactions
with latency on the order of a minute while scaling to many
users. Algorand ensures that users never have divergent
views of confirmed transactions, even if some of the users
are malicious and the network is temporarily partitioned.
In contrast, existing cryptocurrencies allow for temporary
forks and therefore require a long time, on the order of an
hour, to confirm transactions with high confidence.

Algorand uses a new Byzantine Agreement (BA) protocol
to reach consensus among users on the next set of transactions. To scale the consensus to many users, Algorand
uses a novel mechanism based on Verifiable Random Functions that allows users to privately check whether they are
selected to participate in the BA to agree on the next set
of transactions, and to include a proof of their selection in
their network messages. In Algorand’s BA protocol, users
do not keep any private state except for their private keys,
which allows Algorand to replace participants immediately
after they send a message. This mitigates targeted attacks
on chosen participants after their identity is revealed.

We implement Algorand and evaluate its performance on
1,000 EC2 virtual machines, simulating up to 500,000 users.
Experimental results show that Algorand confirms transactions in under a minute, achieves 125x Bitcoin’s throughput,
and incurs almost no penalty for scaling to more users.

",16.728156217252725,15.662222222222223,230
SOSP_17_005.txt,14.317944279477452,12.858950331654846,SOSP,15273,"

It is challenging to simultaneously achieve multicore scalability and high disk throughput in a file system. For example, even for commutative operations like creating different
files in the same directory, current file systems introduce
cache-line conflicts when updating an in-memory copy of
the on-disk directory block, which limits scalability.

SCALEFS is a novel file system design that decouples the
in-memory file system from the on-disk file system using
per-core operation logs. This design facilitates the use of
highly concurrent data structures for the in-memory representation, which allows commutative operations to proceed
without cache conflicts and hence scale perfectly. SCALEFS
logs operations in a per-core log so that it can delay propagating updates to the disk representation (and the cache-line
conflicts involved in doing so) until an fsync. The fsync
call merges the per-core logs and applies the operations to
disk. SCALEFS uses several techniques to perform the merge
correctly while achieving good performance: timestamped
linearization points to order updates without introducing
cache-line conflicts, absorption of logged operations, and
dependency tracking across operations.

Experiments with a prototype of SCALEFS show that its
implementation has no cache conflicts for 99% of test cases
of commutative operations generated by CoMMUTER, scales
well on an 80-core machine, and provides on-disk performance that is comparable to that of Linux ext4.

",17.267425705330172,16.951315789473686,229
SOSP_17_006.txt,14.920566777182405,12.483917988626164,SOSP,12979,"

Videos are an increasingly utilized part of the experience
of the billions of people that use Facebook. These videos
must be uploaded and processed before they can be shared
and downloaded. Uploading and processing videos at our
scale, and across our many applications, brings three key
requirements: low latency to support interactive applications;
a flexible programming model for application developers
that is simple to program, enables efficient processing, and
improves reliability; and robustness to faults and overload.
This paper describes the evolution from our initial monolithic
encoding script (MES) system to our current Streaming Video
Engine (SVE) that overcomes each of the challenges. SVE
has been in production since the fall of 2015, provides lower
latency than MES, supports many diverse video applications,
and has proven to be reliable despite faults and overload.

",18.878054631974784,17.141894736842108,134
SOSP_17_007.txt,15.992057485125688,14.096493340783557,SOSP,10832,"

Distributed storage systems aim to provide strong consistency and isolation guarantees on an architecture that is partitioned across multiple shards for scalability and replicated for
fault tolerance. Traditionally, achieving all of these goals has
required an expensive combination of atomic commitment
and replication protocols — introducing extensive coordination overhead. Our system, Eris, takes a different approach.
It moves a core piece of concurrency control functionality,
which we term multi-sequencing, into the datacenter network
itself. This network primitive takes on the responsibility for
consistently ordering transactions, and a new lightweight
transaction protocol ensures atomicity.

The end result is that Eris avoids both replication and transaction coordination overhead: we show that it can process a
large class of distributed transactions in a single round-trip
from the client to the storage system without any explicit coordination between shards or replicas in the normal case. It
provides atomicity, consistency, and fault tolerance with less
than 10% overhead — achieving throughput 3.6-35x higher
and latency 72-80% lower than a conventional design on
standard benchmarks.
",18.699421769314853,17.675154553049293,173
SOSP_17_008.txt,13.365552652539954,11.615960097919217,SOSP,13016,"

We present NetCache, a new key-value store architecture that
leverages the power and flexibility of new-generation programmable switches to handle queries on hot items and balance the load across storage nodes. NetCache provides high
aggregate throughput and low latency even under highlyskewed and rapidly-changing workloads. The core of NetCache is a packet-processing pipeline that exploits the capabilities of modern programmable switch ASICs to efficiently detect, index, cache and serve hot key-value items in
the switch data plane. Additionally, our solution guarantees
cache coherence with minimal overhead. We implement a
NetCache prototype on Barefoot Tofino switches and commodity servers and demonstrate that a single switch can process 2+ billion queries per second for 64K items with 16-byte
keys and 128-byte values, while only consuming a small portion of its hardware resources. To the best of our knowledge,
this is the first time that a sophisticated application-level
functionality, such as in-network caching, has been shown
to run at line rate on programmable switches. Furthermore,
we show that NetCache improves the throughput by 3-10x
and reduces the latency of up to 40% of queries by 50%, for
high-performance, in-memory key-value stores.
",16.785175570968402,16.226661951909477,202
SOSP_17_009.txt,14.204691704073003,12.038015764216798,SOSP,10041,"

Performance of in-memory key-value store (K VS) continues
to be of great importance as modern KVS goes beyond the
traditional object-caching workload and becomes a key infrastructure to support distributed main-memory computation
in data centers. Recent years have witnessed a rapid increase
of network bandwidth in data centers, shifting the bottleneck
of most KVS from the network to the CPU. RDMA-capable
NIC partly alleviates the problem, but the primitives provided
by RDMA abstraction are rather limited. Meanwhile, programmable NICs become available in data centers, enabling
in-network processing. In this paper, we present KV-Direct, a
high performance KVS that leverages programmable NIC to
extend RDMA primitives and enable remote direct key-value
access to the main host memory.

SUSTC

We develop several novel techniques to maximize the through
put and hide the latency of the PCIe connection between
the NIC and the host memory, which becomes the new bottleneck. Combined, these mechanisms allow a single NIC
KV-Direct to achieve up to 180 M key-value operations per
second, equivalent to the throughput of tens of CPU cores.
Compared with CPU based KVS implementation, KV-Direct
improves power efficiency by 3x, while keeping tail latency
below 10 ys. Moreover, KV-Direct can achieve near linear
scalability with multiple NICs. With 10 programmable NIC
cards in a commodity server, we achieve 1.22 billion KV
operations per second, which is almost an order-of-magnitude
improvement over existing systems, setting a new milestone
for a general-purpose in-memory key-value store.
",16.404322709996244,15.412749999999999,258
SOSP_17_010.txt,15.204842972311614,13.402664103161658,SOSP,9771,"
Cloud research to date has lacked data on the characteristics of the production virtual machine (VM) workloads of
large cloud providers. A thorough understanding of these
characteristics can inform the providers’ resource management systems, e.g. VM scheduler, power manager, server
health manager. In this paper, we first introduce an extensive characterization of Microsoft Azure’s VM workload,
including distributions of the VMs’ lifetime, deployment size,
and resource consumption. We then show that certain VM
behaviors are fairly consistent over multiple lifetimes, ie.
history is an accurate predictor of future behavior. Based on
this observation, we next introduce Resource Central (RC),
a system that collects VM telemetry, learns these behaviors
offline, and provides predictions online to various resource
managers via a general client-side library. As an example of
RC’s online use, we modify Azure’s VM scheduler to leverage
predictions in oversubscribing servers (with oversubscribable VM types), while retaining high VM performance. Using
real VM traces, we then show that the prediction-informed
schedules increase utilization and prevent physical resource
exhaustion. We conclude that providers can exploit their
workloads’ characteristics and machine learning to improve
resource management substantially.

“Cortez and Bonde contributed equally. Muzio was an intern at Microsoft.
",16.24694786949722,14.275763728657132,202
SOSP_17_011.txt,13.797635671814707,11.612425485085108,SOSP,10644,"

MittOS provides operating system support to cut millisecondlevel tail latencies for data-parallel applications. In MittOS,
we advocate a new principle that operating system should
quickly reject IOs that cannot be promptly served. To achieve this, MittOS exposes a fast rejecting SLO-aware interface wherein applications can provide their SLOs (e.g., IO
deadlines). If MittOS predicts that the IO SLOs cannot be
met, MittOS will promptly return EBUSY signal, allowing the
application to failover (retry) to another less-busy node without waiting. We build MittOS within the storage stack (disk,
SSD, and OS cache managements), but the principle is extensible to CPU and runtime memory managements as well.
MittOS’ no-wait approach helps reduce IO completion time
up to 35% compared to wait-then-speculate approaches.
",15.247664890283005,13.952380952380953,128
SOSP_17_012.txt,16.047462695819497,14.526404043674564,SOSP,10190,"

In today’s data analytics frameworks, many users struggle
to reason about the performance of their workloads. Without an understanding of what factors are most important to
performance, users can’t determine what configuration parameters to set and what hardware to use to optimize runtime.
This paper explores a system architecture designed to make
it easy for users to reason about performance bottlenecks.
Rather than breaking jobs into tasks that pipeline many resources, as in today’s frameworks, we propose breaking jobs
into monotasks: units of work that each use a single resource.
We demonstrate that explicitly separating the use of different
resources simplifies reasoning about performance without
sacrificing performance. Monotasks provide job completion
times within 9% of Apache Spark for typical scenarios, and
lead to a model for job completion time that predicts runtime under different hardware and software configurations
with at most 28% error. Furthermore, separating the use of
different resources allows for new optimizations to improve
performance.
",16.785175570968402,15.554595086442223,161
SOSP_17_013.txt,17.10201993099718,15.956281751334313,SOSP,13768,"

Nested virtualization, the ability to run a virtual machine
inside another virtual machine, is increasingly important
because of the need to deploy virtual machines running software stacks on top of virtualized cloud infrastructure. As
ARM servers make inroads in cloud infrastructure deployments, supporting nested virtualization on ARM is a key
requirement, which has been met recently with the introduction of nested virtualization support to the ARM architecture.
We build the first hypervisor to use ARM nested virtualization support and show that despite similarities between ARM
and x86 nested virtualization support, performance on ARM
is much worse than on x86. This is due to excessive traps
to the hypervisor caused by differences in non-nested virtualization support. To address this problem, we introduce
a novel paravirtualization technique to rapidly prototype
architectural changes for virtualization and evaluate their
performance impact using existing hardware. Using this technique, we propose Nested Virtualization Extensions for ARM
(NEVE), a set of simple architectural changes to ARM that
can be used by software to coalesce and defer traps by logging the results of hypervisor instructions until the results
are actually needed by the hypervisor or virtual machines.
We show that NEVE allows hypervisors running real application workloads to provide an order of magnitude better
performance than current ARM nested virtualization support and up to three times less overhead than x86 nested
virtualization. NEVE will be included in ARMv8.4, the next
version of the ARM architecture.
",18.10804710084791,18.3234243697479,240
SOSP_17_014.txt,16.05072292204017,15.050502328233922,SOSP,10853,"

Containers are in great demand because they are lightweight
when compared to virtual machines. On the downside, containers offer weaker isolation than VMs, to the point where
people run containers in virtual machines to achieve proper
isolation. In this paper, we examine whether there is indeed
a strict tradeoff between isolation (VMs) and efficiency (containers). We find that VMs can be as nimble as containers, as
long as they are small and the toolstack is fast enough.

We achieve lightweight VMs by using unikernels for specialized applications and with Tinyx, a tool that enables
creating tailor-made, trimmed-down Linux virtual machines.
By themselves, lightweight virtual machines are not enough
to ensure good performance since the virtualization control
plane (the toolstack) becomes the performance bottleneck.
We present LightVM, a new virtualization solution based
on Xen that is optimized to offer fast boot-times regardless
of the number of active VMs. LightVM features a complete
redesign of Xen’s control plane, transforming its centralized
operation to a distributed one where interactions with the
hypervisor are reduced to a minimum. LightVM can boot a
VM in 2.3ms, comparable to fork/exec on Linux (1ms), and
two orders of magnitude faster than Docker. LightVM can
pack thousands of LightVM guests on modest hardware with
memory and CPU usage comparable to that of processes.
",15.322241378113624,13.728310502283104,222
SOSP_17_015.txt,15.1823352235754,13.667918166030855,SOSP,11540,"
Low-power microcontrollers lack some of the hardware features and memory resources that enable multiprogrammable
systems. Accordingly, microcontroller-based operating systems have not provided important features like fault isolation,
dynamic memory allocation, and flexible concurrency. However, an emerging class of embedded applications are software
platforms, rather than single purpose devices, and need these
multiprogramming features. Tock, a new operating system for
low-power platforms, takes advantage of limited hardwareprotection mechanisms as well as the type-safety features
of the Rust programming language to provide a multiprogramming environment for microcontrollers. Tock isolates
software faults, provides memory protection, and efficiently
manages memory for dynamic application workloads written
in any language. It achieves this while retaining the dependability requirements of long-running applications.
",18.422482065455632,18.170000000000005,121
SOSP_17_016.txt,15.314118708295105,13.680712643771788,SOSP,12057,"

This paper describes an approach to designing, implementing, and formally verifying the functional correctness of an
OS kernel, named Hyperkernel, with a high degree of proof
automation and low proof burden. We base the design of Hyperkernel’s interface on xv6, a Unix-like teaching operating
system. Hyperkernel introduces three key ideas to achieve
proof automation: it finitizes the kernel interface to avoid
unbounded loops or recursion; it separates kernel and user
address spaces to simplify reasoning about virtual memory;
and it performs verification at the LLVM intermediate representation level to avoid modeling complicated C semantics.

We have verified the implementation of Hyperkernel with
the Z3 SMT solver, checking a total of 50 system calls and
other trap handlers. Experience shows that Hyperkernel can
avoid bugs similar to those found in xv6, and that the verification of Hyperkernel can be achieved with a low proof burden.

",18.878054631974784,18.18068965517242,147
SOSP_17_017.txt,15.443327601672028,14.312938091945977,SOSP,13110,"

DFSCQ is the first file system that (1) provides a precise
specification for fsync and fdatasync, which allow applications to achieve high performance and crash safety, and
(2) provides a machine-checked proof that its implementation meets this specification. DFSCQ’s specification captures
the behavior of sophisticated optimizations, including logbypass writes, and DFSCQ’s proof rules out some of the
common bugs in file-system implementations despite the
complex optimizations.

The key challenge in building DFSCQ is to write a specification for the file system and its internal implementation
without exposing internal file-system details. DFSCQ introduces a metadata-prefix specification that captures the
properties of fsync and fdatasync, which roughly follows
the behavior of Linux ext4. This specification uses a notion of tree sequences—logical sequences of file-system tree
states—for succinct description of the possible states after
a crash and to describe how data writes can be reordered
with respect to metadata updates. This helps application
developers prove the crash safety of their own applications,
avoiding application-level bugs such as forgetting to invoke
fsync on both the file and the containing directory.

An evaluation shows that DFSCQ achieves 103 MB/s on
large file writes to an SSD and durably creates small files at
a rate of 1,618 files per second. This is slower than Linux
ext4 (which achieves 295 MB/s for large file writes and 4,977
files/s for small file creation) but much faster than two recent verified file systems, Yggdrasil and FSCQ. Evaluation
results from application-level benchmarks, including TPC-C
on SQLite, mirror these microbenchmarks.
",17.410965686947208,17.494713740458014,267
SOSP_17_018.txt,16.018446282223994,14.567428291135737,SOSP,15163,"

Intel SGX promises powerful security: an arbitrary number
of user-mode enclaves protected against physical attacks and
privileged software adversaries. However, to achieve this,
Intel extended the x86 architecture with an isolation mechanism approaching the complexity of an OS microkernel,
implemented by an inscrutable mix of silicon and microcode.
While hardware-based security can offer performance and
features that are difficult or impossible to achieve in pure
software, hardware-only solutions are difficult to update,
either to patch security flaws or introduce new features.
Komodo illustrates an alternative approach to attested,
on-demand, user-mode, concurrent isolated execution. We
decouple the core hardware mechanisms such as memory
encryption, address-space isolation and attestation from the
management thereof, which Komodo delegates to a privileged software monitor that in turn implements enclaves.
The monitor’s correctness is ensured by a machine-checkable
proof of both functional correctness and high-level security
properties of enclave integrity and confidentiality. We show
that the approach is practical and performant with a concrete
implementation of a prototype in verified assembly code on
ARM TrustZone. Our ultimate goal is to achieve security
equivalent to or better than SGX while enabling deployment
of new enclave features independently of CPU upgrades.
The Komodo specification, prototype implementation, and
proofs are available at https://github.com/Microsoft/Komodo.
",19.72995580123306,18.42119496855346,218
SOSP_17_019.txt,13.8264182529491,11.758104443987097,SOSP,16391,"

Recently, there is an increasing interest in building datacenter applications with RDMA because of its low-latency,
high-throughput, and low-CPU-utilization benefits. However, RDMA is not readily suitable for datacenter applications.
It lacks a flexible, high-level abstraction; its performance
does not scale; and it does not provide resource sharing or
flexible protection. Because of these issues, it is difficult to
build RDMA-based applications and to exploit RDMA’s performance benefits.

To solve these issues, we built LITE, a Local Indirection
TiEr for RDMA in the Linux kernel that virtualizes native
RDMaA into a flexible, high-level, easy-to-use abstraction
and allows applications to safely share resources. Despite
the widely-held belief that kernel bypassing is essential to
RDMA’s low-latency performance, we show that using a
kernel-level indirection can achieve both flexibility and lowlatency, scalable performance at the same time. To demonstrate the benefits of LITE, we developed several popular
datacenter applications on LITE, including a graph engine, a
MapReduce system, a Distributed Shared Memory system,
and a distributed atomic logging system. These systems are
easy to build and deliver good performance. For example, our
implementation of PowerGraph uses only 20 lines of LITE
code, while outperforming PowerGraph by 3.5x to 5.6x.
",17.631426480028413,15.212850241545897,212
SOSP_17_020.txt,15.337958798965484,13.818216769342516,SOSP,9170,"

This paper focuses on the efficient scheduling on multicore
systems of very fine-grain networked tasks, which are the
typical building block of online data-intensive applications.
The explicit goal is to deliver high throughput (millions of
remote procedure calls per second) for tail latency servicelevel objectives that are a small multiple of the task size.

We present ZYGOS, a system optimized for ps-scale, inmemory computing on multicore servers. It implements a
work-conserving scheduler within a specialized operating
system designed for high request rates and a large number of network connections. ZYGOS uses a combination
of shared-memory data structures, multi-queue NICs, and
inter-processor interrupts to rebalance work across cores.

For an aggressive service-level objective expressed at the
99¢* percentile, ZYGOS achieves 75% of the maximum possible load determined by a theoretical, zero-overhead model
(centralized queueing with FCFS) for 10s tasks, and 88%
for 25s tasks.

We evaluate ZYGOS with a networked version of Silo, a
state-of-the-art in-memory transactional database, running
TPC-C. For a service-level objective of 1000s latency at the
99¢* percentile, ZYGOS can deliver a 1.63x speedup over
Linux (because of its dataplane architecture) and a 1.26x
speedup over IX, a state-of-the-art dataplane (because of its
work-conserving scheduler).
",17.553077303434723,15.985206422018347,219
SOSP_17_021.txt,15.121434318409172,13.746491677234555,SOSP,10129,"

We revisit the question of delegation vs. synchronized access
to shared memory, and show through analysis and demonstration that delegation can be much faster than locking under a range of common circumstances. Starting from first
principles, we propose fast, fly-weight delegation (ffwd). The
highly optimized design of ffwd allows it to significantly outperform prior work on delegation, while retaining the scalability advantage.

In experiments with 6 benchmark applications, and 6
shared data structures, running on four different multi-socket
systems with up to 128 hardware threads, we compare ffwd
to a selection of lock, combining, lock-free, software transactional memory and delegation designs. Overall, we find
that ffwd often offers a simple and highly competitive alternative to existing work. By definition, the performance of a
fully delegated data structure is limited by the single-thread
throughput of said data structure. However, due to cache
effects, many data structures offer their best performance
when confined to a single thread. With an efficient delegation
mechanism, we approach this single-threaded performance
in a multi-threaded setting. In application-level benchmarks,
we see improvements up to 100% over the next best solution tested (RCL), and multiple micro-benchmarks show improvements in the 5-10x range.
",15.760457277294734,14.597799671592778,204
SOSP_17_022.txt,14.864054540163394,13.835138710918663,SOSP,11413,"

The forwarding behavior of computer networks is governed
by the configuration of distributed routing protocols and access filters—collectively known as the network control plane.
Unfortunately, control plane configurations are often buggy,
causing networks to violate important policies: e.g., specific
traffic classes (defined in terms of source and destination
endpoints) should always be able to reach their destination,
or always traverse a waypoint. Manually repairing these configurations is daunting because of their inter-twined nature
across routers, traffic classes, and policies.

Inspired by recent work in automatic program repair, we
introduce CPR, a system that automatically computes correct,
minimal repairs for network control planes. CPR casts configuration repair as a MaxSMT problem whose constraints are
based on a digraph-based representation of a control plane’s
semantics. Crucially, this representation must capture the
dependencies between traffic classes arising from the crosstraffic-class nature of control plane constructs. The MaxSMT
formulation must account for these dependencies whilst also
accounting for all policies and preferring repairs that minimize the size (e.g., number of lines) of the configuration
changes. Using configurations from 96 data center networks,
we show that CPR produces repairs in less than a minute for
98% of the networks, and these repairs requiring changing
the same or fewer lines of configuration than hand-written
repairs in 79% of cases.

",16.373557378465907,16.905389908256883,222
SOSP_17_023.txt,14.705776124085325,13.021128603730926,SOSP,10117,"

Large scale streaming systems aim to provide high throughput
and low latency. They are often used to run mission-critical
applications, and must be available 24x7. Thus such systems
need to adapt to failures and inherent changes in workloads,
with minimal impact on latency and throughput. Unfortunately, existing solutions require operators to choose between
achieving low latency during normal operation and incurring minimal impact during adaptation. Continuous operator
streaming systems, such as Naiad and Flink, provide low
latency during normal execution but incur high overheads
during adaptation (e.g., recovery), while micro-batch systems,
such as Spark Streaming and FlumeJava, adapt rapidly at the
cost of high latency during normal operations.

Our key observation is that while streaming workloads
require millisecond-level processing, workload and cluster
properties change less frequently. Based on this, we develop
Drizzle, a system that decouples the processing interval from
the coordination interval used for fault tolerance and adaptability. Our experiments on a 128 node EC2 cluster show that
on the Yahoo Streaming Benchmark, Drizzle can achieve endto-end record processing latencies of less than 100ms and can
get 2—3x lower latency than Spark. Drizzle also exhibits better
adaptability, and can recover from failures 4x faster than Flink
while having up to 13x lower latency during recovery.
",16.860833078287435,15.699304897314377,213
SOSP_17_024.txt,14.51531993257381,12.752595207093766,SOSP,10690,"

Scalable in-memory key-value stores provide low-latency
access times of a few microseconds and perform millions of
operations per second per server. With all data in memory,
these systems should provide a high level of reconfigurability.
Ideally, they should scale up, scale down, and rebalance load
more rapidly and flexibly than disk-based systems. Rapid
reconfiguration is especially important in these systems since
a) DRAM is expensive and b) they are the last defense against
highly dynamic workloads that suffer from hot spots, skew,
and unpredictable load. However, so far, work on in-memory
key-value stores has generally focused on performance and
availability, leaving reconfiguration as a secondary concern.

We present Rocksteady, a live migration technique for the
RAMCloud scale-out in-memory key-value store. It balances
three competing goals: it migrates data quickly, it minimizes
response time impact, and it allows arbitrary, fine-grained
splits. Rocksteady migrates 758 MB/s between servers under
high load while maintaining a median and 99.9"" percentile
latency of less than 40 and 250 ps, respectively, for concurrent
operations without pauses, downtime, or risk to durability
(compared to 6 and 45 ps during normal operation). To do
this, it relies on pipelined and parallel replay and a lineagelike approach to fault-tolerance to defer re-replication costs
during migration. Rocksteady allows RAMCloud to defer all
repartitioning work until the moment of migration, giving it
precise and timely control for load balancing.
",16.03029750255766,15.058792531120336,243
SOSP_17_025.txt,13.50242879768243,11.782896330822417,SOSP,11041,"

Atom is an anonymous messaging system that protects against
traffic-analysis attacks. Unlike many prior systems, each
Atom server touches only a small fraction of the total messages routed through the network. As a result, the system’s
capacity scales near-linearly with the number of servers. At
the same time, each Atom user benefits from “best possible”
anonymity: a user is anonymous among ail honest users of
the system, even against an active adversary who monitors
the entire network, a portion of the system’s servers, and any
number of malicious users. The architectural ideas behind
Atom have been known in theory, but putting them into practice requires new techniques for (1) avoiding heavy generalpurpose multi-party computation protocols, (2) defeating
active attacks by malicious servers at minimal performance
cost, and (3) handling server failure and churn.

Atom is most suitable for sending a large number of short
messages, as in a microblogging application or a high-security
communication bootstrapping (“dialing”) for private messaging systems. We show that, on a heterogeneous network
of 1,024 servers, Atom can transit a million Tweet-length
messages in 28 minutes. This is over 23x faster than prior
systems with similar privacy guarantees.
",15.742502247213078,15.936592639593908,201
SOSP_17_026.txt,15.252549635892432,13.654306361270905,SOSP,13676,"
 CCS CONCEPTS

Private communication over the Internet remains a challenging problem. Even if messages are encrypted, it is hard to
deliver them without revealing metadata about which pairs
of users are communicating. Scalable anonymity systems,
such as Tor, are susceptible to traffic analysis attacks that
leak metadata. In contrast, the largest-scale systems with
metadata privacy require passing all messages through a
small number of providers, requiring a high operational cost
for each provider and limiting their deployability in practice.

This paper presents Stadium, a point-to-point messaging
system that provides metadata and data privacy while scaling its work efficiently across hundreds of low-cost providers
operated by different organizations. Much like Vuvuzela,
the current largest-scale metadata-private system, Stadium
achieves its provable guarantees through differential privacy
and the addition of noisy cover traffic. The key challenge in
Stadium is limiting the information revealed from the many
observable traffic links of a highly distributed system, without requiring an overwhelming amount of noise. To solve
this challenge, Stadium introduces techniques for distributed
noise generation and differentially private routing as well as
a verifiable parallel mixnet design where the servers collaboratively check that others follow the protocol. We show that
Stadium can scale to support 4x more users than Vuvuzela
using servers that cost an order of magnitude less to operate
than Vuvuzela nodes.
",18.243605946275583,17.716666666666672,225
SOSP_17_027.txt,17.06497828871816,16.215147430936423,SOSP,15059,"
The large-scale monitoring of computer users’ software
activities has become commonplace, e.g., for application
telemetry, error reporting, or demographic profiling. This
paper describes a principled systems architecture—Encode,
Shuffle, Analyze (ESA)—for performing such monitoring
with high utility while also protecting user privacy. The ESA
design, and its PROCHLO implementation, are informed by
our practical experiences with an existing, large deployment
of privacy-preserving software monitoring.

With ESA, the privacy of monitored users’ data is guaranteed by its processing in a three-step pipeline. First, the data
is encoded to control scope, granularity, and randomness.
Second, the encoded data is collected in batches subject to
a randomized threshold, and blindly shuffled, to break linkability and to ensure that individual data items get “lost in the
crowd” of the batch. Third, the anonymous, shuffled data is
analyzed by a specific analysis engine that further prevents
statistical inference attacks on analysis results.

ESA extends existing best-practice methods for sensitivedata analytics, by using cryptography and statistical techniques to make explicit how data is elided and reduced in
precision, how only common-enough, anonymous data is analyzed, and how this is done for only specific, permitted purposes. As a result, ESA remains compatible with the established workflows of traditional database analysis.

Strong privacy guarantees, including differential privacy, can be established at each processing step to defend
against malice or compromise at one or more of those steps.
PROCHLO develops new techniques to harden those steps,
including the Stash Shuffle, a novel scalable and efficient
oblivious-shuffling algorithm based on Intel’s SGX, and new
applications of cryptographic secret sharing and blinding.
We describe ESA and PROCHLO, as well as experiments
that validate their ability to balance utility and privacy.
",18.153369283396113,17.139965034965034,289
SOSP_17_028.txt,13.387887519781358,11.637381553959216,SOSP,13168,"

Current hardware and application storage trends put immense
pressure on the operating system’s storage subsystem. On the
hardware side, the market for storage devices has diversified
to a multi-layer storage topology spanning multiple orders of
magnitude in cost and performance. Above the file system,
applications increasingly need to process small, random IO on
vast data sets with low latency, high throughput, and simple
crash consistency. File systems designed for a single storage
layer cannot support all of these demands together.

We present Strata, a cross-media file system that leverages
the strengths of one storage media to compensate for weaknesses of another. In doing so, Strata provides performance,
capacity, and a simple, synchronous IO model all at once,
while having a simpler design than that of file systems constrained by a single storage device. At its heart, Strata uses a
log-structured approach with a novel split of responsibilities
among user mode, kernel, and storage layers that separates
the concerns of scalable, high-performance persistence from
storage layer management. We quantify the performance benefits of Strata using a 3-layer storage hierarchy of emulated
NVM, a flash-based SSD, and a high-density HDD. Strata has
20-30% better latency and throughput, across several unmodified applications, compared to file systems purpose-built for
each layer, while providing synchronous and unified access
to the entire storage hierarchy. Finally, Strata achieves up
to 2.8x better throughput than a block-based 2-layer cache
provided by Linux’s logical volume manager.
",15.774802946060372,15.255387096774193,252
SOSP_17_029.txt,14.406428423947709,13.112660177490511,SOSP,9724,"

Emerging fast, persistent memories will enable systems that
combine conventional DRAM with large amounts of nonvolatile main memory (NVMM) and provide huge increases
in storage performance. Fully realizing this potential requires
fundamental changes in how system software manages, protects, and provides access to data that resides in NVMM. We
address these needs by describing an NVMM-optimized file
system called NOVA-Fortis that is both fast and resilient in
the face of corruption due to media errors and software bugs.
We identify and propose solutions for the unique challenges
in adding fault tolerance to an NVMM file system, adapt
state-of-the-art reliability techniques to an NVMM file system, and quantify the performance and storage overheads
of these techniques. We find that NOVA-Fortis’ reliability
features consume 14.8% of the storage for redundancy and reduce application-level performance by between 2% and 38%
compared to the same file system with the features removed.
NOVA-Fortis outperforms DAX-aware file systems without
reliability features by 1.5x on average. It outperforms reliable, block-based file systems running on NVMM by 3x on
average.
",16.26309291913925,15.491758241758244,185
SOSP_17_030.txt,12.88937861273083,10.975503810159953,SOSP,11823,"
Key-value stores such as LevelDB and RocksDB offer excellent write throughput, but suffer high write amplification.
The write amplification problem is due to the Log-Structured
Merge Trees data structure that underlies these key-value
stores. To remedy this problem, this paper presents a novel
data structure that is inspired by Skip Lists, termed Fragmented Log-Structured Merge Trees (FLSM). FLSM introduces the notion of guards to organize logs, and avoids
rewriting data in the same level. We build PebblesDB, a highperformance key-value store, by modifying HyperLevelDB
to use the FLSM data structure. We evaluate PebblesDB using micro-benchmarks and show that for write-intensive
workloads, PebblesDB reduces write amplification by 2.4-3×
compared to RocksDB, while increasing write throughput by
6.7×. We modify two widely-used NoSQL stores, MongoDB
and HyperDex, to use PebblesDB as their underlying storage
engine. Evaluating these applications using the YCSB benchmark shows that throughput is increased by 18-105% when
using PebblesDB (compared to their default storage engines)
while write IO is decreased by 35-55%.
",12.815532586354998,12.29909090909091,177
SOSP_17_031.txt,16.05624545781473,14.487484968582987,SOSP,9797,"

System administrators have unlimited access to system resources. As the Snowden case highlighted, these permissions
can be exploited to steal valuable personal, classified, or commercial data. This problem is exacerbated when a third party
administers the system. For example, a bank outsourcing
its IT would not want to allow administrators access to the
actual data. We propose WatchIT: a strategy that constrains
IT personnel’s view of the system and monitors their actions.
To this end, we introduce the abstraction of perforated containers — while regular Linux containers are too restrictive
to be used by system administrators, by “punching holes” in
them, we strike a balance between information security and
required administrative needs. Following the principle of
least privilege, our system predicts which system resources
should be accessible for handling each IT issue, creates a
perforated container with the corresponding isolation, and
deploys it as needed for fixing the problem.

Under this approach, the system administrator retains superuser privileges, however only within the perforated container limits. We further provide means for the administrator
to bypass the isolation, but such operations are monitored
and logged for later analysis and anomaly detection.

We provide a proof-of-concept implementation of our
strategy, which includes software for deploying perforated
containers, monitoring mechanisms, and changes to the
Linux kernel. Finally, we present a case study conducted
on the IT database of IBM Research in Israel, showing that
our approach is feasible.
",17.122413403193683,15.78947775628627,237
SOSP_17_032.txt,13.756191576519068,11.817760875596019,SOSP,10427,"

To reduce memory pressure, modern operating systems and
hypervisors such as Linux/KVM deploy page-level memory
fusion to merge physical memory pages with the same content
(i.e., page fusion). A write to a fused memory page triggers a
copy-on-write event that unmerges the page to preserve correct semantics. While page fusion is crucial in saving memory
in production, recent work shows significant security weaknesses in its current implementations. Attackers can abuse
timing side channels on the unmerge operation to leak sensitive data such as randomized pointers. Additionally, they can
exploit the predictability of the merge operation to massage
physical memory for reliable Rowhammer attacks. In this paper, we present VUsion, a secure page fusion system. VUsion
can stop all the existing and even new classes of attack, where
attackers leak information by side-channeling the merge operation or massage physical memory via predictable memory
reuse patterns. To mitigate information disclosure attacks, we
ensure attackers can no longer distinguish between fused and
non-fused pages. To mitigate memory massaging attacks, we
ensure fused pages are always allocated from a high-entropy
pool. Despite its secure design, our comprehensive evaluation shows that VUsion retains most of the memory saving
benefits of traditional memory fusion with negligible performance overhead while maintaining compatibility with other
advanced memory management features.
",17.00531248756302,16.02484403669725,220
SOSP_17_033.txt,14.902020281022882,12.853964456596035,SOSP,14504,"

You put a program on a concurrent server, but you don’t trust
the server; later, you get a trace of the actual requests that
the server received from its clients and the responses that
it delivered. You separately get logs from the server; these
are untrusted. How can you use the logs to efficiently verify
that the responses were derived from running the program
on the requests? This is the Efficient Server Audit Problem,
which abstracts real-world scenarios, including running a
web application on an untrusted provider. We give a solution
based on several new techniques, including simultaneous
replay and efficient verification of concurrent executions. We
implement the solution for PHP web applications. For several
applications, our verifier achieves 5.6-10.9x speedup versus
simply re-executing, with <10% overhead for the server.
",15.151101081350806,12.48168443496802,137
SOSP_17_034.txt,13.800106183461786,11.857461670350144,SOSP,14583,"

When systems fail in production environments, log data is
often the only information available to programmers for
postmortem debugging. Consequently, programmers’ decision on where to place a log printing statement is of crucial
importance, as it directly affects how effective and efficient
postmortem debugging can be. This paper presents Log20, a
tool that determines a near optimal placement of log printing
statements under the constraint of adding less than a specified amount of performance overhead. Log20 does this in
an automated way without any human involvement. Guided
by information theory, the core of our algorithm measures
how effective each log printing statement is in disambiguating code paths. To do so, it uses the frequencies of different
execution paths that are collected from a production environment by a low-overhead tracing library. We evaluated Log20
on HDFS, HBase, Cassandra, and ZooKeeper, and observed
that Log20 is substantially more efficient in code path disambiguation compared to the developers’ manually placed log
printing statements. Log20 can also output a curve showing
the trade-off between the informativeness of the logs and
the performance slowdown, so that a developer can choose
the right balance.
",16.827784334635936,14.974605263157894,191
SOSP_17_035.txt,16.583600425367063,15.497750304119908,SOSP,9811,"

Diagnosing concurrency bugs—the process of understanding
the root causes of concurrency failures—is hard. Developers depend on reproducing concurrency bugs to diagnose
them. Traditionally, systems that attempt to reproduce concurrency bugs record fine-grained thread schedules of events
(e.g., shared memory accesses) that lead to failures. Recording schedules incurs high runtime performance overhead
and scales poorly, making existing techniques unsuitable in
production.

In this paper, we formulate the coarse interleaving hypothesis, which states that the events leading to many concurrency
bugs are coarsely interleaved. Therefore, a fine-grained and
expensive recording is unnecessary for diagnosing such concurrency bugs. We test the coarse interleaving hypothesis
by studying 54 bugs in 13 systems and find that it holds in all
cases. In particular, the time elapsed between events leading
to concurrency bugs is on average 5 orders of magnitude
greater than what is used today in fine-grained recording.

Using the coarse interleaving hypothesis, we develop Lazy
Diagnosis, a hybrid dynamic-static interprocedural pointer
and type analysis to diagnose the root causes of concurrency bugs. Our Lazy Diagnosis prototype, SNoRLAx, relies
on commodity hardware to track thread interleavings at a
coarse granularity. SNoRLAx does not require any source
code changes and can diagnose complex concurrency bugs
in real large-scale systems (MySQL, httpd, memcached, etc.)
with full accuracy and an average runtime performance overhead of below 1%. Broadly, we believe that our findings can
be used to build more efficient in-production bug detection
and record/replay techniques.
",16.52667757954773,14.890941295546558,249
SOSP_17_036.txt,14.851767151593908,12.792780259783239,SOSP,11362,"

Network reliability is critical for large clouds and online
service providers like Microsoft. Our network is large, heterogeneous, complex and undergoes constant churns. In such
an environment even small issues triggered by device failures, buggy device software, configuration errors, unproven
management tools and unavoidable human errors can quickly
cause large outages. A promising way to minimize such
network outages is to proactively validate all network operations in a high-fidelity network emulator, before they are
carried out in production. To this end, we present CrystalNet, a cloud-scale, high-fidelity network emulator. It runs
real network device firmwares in a network of containers
and virtual machines, loaded with production configurations.
Network engineers can use the same management tools and
methods to interact with the emulated network as they do
with a production network. CrystalNet can handle heterogeneous device firmwares and can scale to emulate thousands
of network devices in a matter of minutes. To reduce resource
consumption, it carefully selects a boundary of emulations,
while ensuring correctness of propagation of network changes.
Microsoft’s network engineers use CrystalNet on a daily basis
to test planned network operations. Our experience shows
that CrystaINet enables operators to detect many issues that
could trigger significant outages.
",16.018793980428352,14.704491449144914,204
SOSP_17_037.txt,13.733397781393357,12.421694685528198,SOSP,11196,"

Applications like social networking, urban monitoring and
market feed processing require stateful stream query: a query
consults not only streaming data but also stored data to extract timely information; useful information from streaming
data also needs to be continuously and consistently integrated into stored data to serve inflight and future queries.
However, prior streaming systems either focus on stream
computation, or are not stateful, or cannot provide low latency and high throughput to handle the fast-evolving linked
data and increasing concurrency of queries.

This paper presents Wukong+S, a distributed stream
querying engine that provides sub-millisecond stateful query
at millions of queries per-second over fast-evolving linked
data. Wukong+S uses an integrated design that combines
the stream processor and the persistent store with efficient
state sharing, which avoids the cross-system cost and suboptimal query plan in conventional composite designs (e.g.,
Storm/Heron+Wukong). Wukong+S uses a hybrid store to
differentially manage timeless data and timing data accordingly and provides an efficient stream index with localityaware partitioning to facilitate fast access to streaming data.
Wukong+S further provides decentralized vector timestamps
with bounded snapshot scalarization to scale with nodes and
massive queries at efficient memory usage.

We have designed Wukong+S conforming to the RDF
data model and Continuous SPARQL (C-SPARQL) query
interface and have implemented Wukong+S by extending a
state-of-the-art static RDF store (namely Wukong). Evaluation on an 8-node RDMA-capable cluster using LSBench and CityBench shows that Wukong+S significantly outperforms existing system designs (e.g., CSPARQL-engine, Storm/Heron+Wukong, and Spark Streaming/Structured Streaming) for both latency and throughput, usually at the scale of
orders of magnitude.
",17.971250198000288,20.15370689655173,284
SOSP_17_038.txt,13.686356604155645,11.997296721033806,SOSP,10056,"

Classical query optimization relies on a predefined set of
rewrite rules to re-order and substitute SQL operators at
a logical level. This paper proposes Birrz, a system that
can synthesize efficient query-specific operators using automated program reasoning. Biirz uses static analysis to
identify sub-queries as potential targets for optimization. For
each sub-query, it constructs a template that defines a large
space of possible operator implementations, all restricted to
have linear time and space complexity. Birrz then employs
program synthesis to instantiate the template and obtain a
data-parallel operator implementation that is functionally
equivalent to the original sub-query up to a bound on the
input size.

Program synthesis is an undecidable problem in general
and often difficult to scale, even for bounded inputs. Birrz
therefore uses a series of analyses to judiciously use program
synthesis and incrementally construct complex operators.

We integrated Biirz with existing big-data query languages by embedding the synthesized operators back into
the query as User Defined Operators. We evaluated Burrz on
several production queries from MicrosorT running on two
state-of-the-art query engines: SPARKSQL as well as ScopE,
the big-data engine of MicrosorT. Biirz produces correct
optimizations despite the synthesis being bounded. The resulting queries have much more succinct query plans and demonstrate significant performance improvements on both
big-data systems (1.3x — 4.7x).
",17.238542476582836,16.30187610619469,229
SOSP_17_039.txt,15.395240507379167,14.025753993949504,SOSP,11526,"

SummaryStore is an approximate time-series store, designed
for analytics, capable of storing large volumes of time-series
data (~1 petabyte) on a single node; it preserves high degrees of query accuracy and enables near real-time querying
at unprecedented cost savings. SummaryStore contributes
time-decayed summaries, a novel abstraction for summarizing data streams, along with an ingest algorithm to continually merge the summaries for efficient range queries;
in conjunction, it returns reliable error estimates alongside
the approximate answers, supporting a range of machine
learning and analytical workloads. We successfully evaluated SummaryStore using real-world applications for forecasting, outlier detection, and Internet traffic monitoring; it
can summarize aggressively with low median errors, 0.1 to
10%, for different workloads. Under range-query microbenchmarks, it stored 1 PB synthetic stream data (1024 1TB streams),
on a single node, using roughly 10 TB (100x compaction)
with 95%-ile error below 5% and median cold-cache query
latency of 1.3s (worst case latency under 70s).

",21.85959125089889,23.149444444444445,164
SP_17_001.txt,15.70194151513515,14.015457791634422,SP,14277,"
Abstract—The analysis of real-world protocols, in particular
key exchange protocols and protocols building on these protocols, is a very complex, error-prone, and tedious task. Besides
the complexity of the protocols itself, one important reason for
this is that the security of the protocols has to be reduced to the
security of the underlying cryptographic primitives for every
protocol time and again.

We would therefore like to get rid of reduction proofs for
real-world key exchange protocols as much as possible and in
many cases altogether, also for higher-level protocols which use
the exchanged keys. So far some first steps have been taken in
this direction. But existing work is still quite limited, and, for
example, does not support Diffie-Hellman (DH) key exchange,
a prevalent cryptographic primitive for real-world protocols.

In this paper, building on work by Kiisters and Tuengerthal,
we provide an ideal functionality in the universal composability
setting which supports several common cryptographic primitives, including DH key exchange. This functionality helps to
avoid reduction proofs in the analysis of real-world protocols
and often eliminates them completely. We also propose a new
general ideal key exchange functionality which allows higherlevel protocols to use exchanged keys in an ideal way. As a
proof of concept, we apply our framework to three practical
DH key exchange protocols, namely ISO 9798-3, SIGMA, and
OPTLS.
",16.728156217252725,15.257536231884057,231
SP_17_002.txt,15.25860716329358,13.837082015644867,SP,10814,"
Abstract—Both the operational and academic security communities have used dynamic analysis sandboxes to execute malware
samples for roughly a decade. Network information derived
from dynamic analysis is frequently used for threat detection,
network policy, and incident response. Despite these common
and important use cases, the efficacy of the network detection
signal derived from such analysis has yet to be studied in depth.
This paper seeks to address this gap by analyzing the network
communications of 26.8 million samples that were collected over
a period of five years.

Using several malware and network datasets, our large scale
study makes three core contributions. (1) We show that dynamic
analysis traces should be carefully curated and provide a rigorous
methodology that analysts can use to remove potential noise from
such traces. (2) We show that Internet miscreants are increasingly
using potentially unwanted programs (PUPs) that rely on a
surprisingly stable DNS and IP infrastructure. This indicates
that the security community is in need of better protections
against such threats, and network policies may provide a solid
foundation for such protections. (3) Finally, we see that, for the
vast majority of malware samples, network traffic provides the
earliest indicator of infection—several weeks and often months
before the malware sample is discovered. Therefore, network
defenders should rely on automated malware analysis to extract
indicators of compromise and not to build early detection systems.
",17.353723509956247,15.90260869565218,232
SP_17_003.txt,16.22300421283755,14.793019987384692,SP,12698,"
Abstract—Industrial robots, automated manufacturing, and
efficient logistics processes are at the heart of the upcoming
fourth industrial revolution. While there are seminal studies on
the vulnerabilities of cyber-physical systems in the industry, as
of today there has been no systematic analysis of the security of
industrial robot controllers.

We examine the standard architecture of an industrial robot
and analyze a concrete deployment from a systems security
standpoint. Then, we propose an attacker model and confront
it with the minimal set of requirements that industrial robots
should honor: precision in sensing the environment, correctness
in execution of control logic, and safety for human operators.

Following an experimental and practical approach, we then
show how our modeled attacker can subvert such requirements
through the exploitation of software vulnerabilities, leading to
severe consequences that are unique to the robotics domain.

We conclude by discussing safety standards and security
challenges in industrial robotics.
",19.117987234576393,18.29387417218543,152
SP_17_004.txt,15.340260330743934,13.582091261059727,SP,11162,"
Abstract—Anecdotes, news reports, and policy briefings collectively suggest that Internet censorship practices are pervasive.
The scale and diversity of Internet censorship practices makes it
difficult to precisely monitor where, when, and how censorship
occurs, as well as what is censored. The potential risks in
performing the measurements make this problem even more
challenging. As a result, many accounts of censorship begin—and
end—with anecdotes or short-term studies from only a handful
of vantage points.

We seek to instead continuously monitor information about
Internet reachability, to capture the onset or termination of
censorship across regions and ISPs. To achieve this goal, we
introduce Augur, a method and accompanying system that utilizes
TCP/IP side channels to measure reachability between two
Internet locations without directly controlling a measurement
vantage point at either location. Using these side channels,
coupled with techniques to ensure safety by not implicating
individual users, we develop scalable, statistically robust methods
to infer network-layer filtering, and implement a corresponding
system capable of performing continuous monitoring of global
censorship. We validate our measurements of Internet-wide
disruption in nearly 180 countries over 17 days against sites
known to be frequently blocked; we also identify the countries
where connectivity disruption is most prevalent.
",18.7741,17.594247572815537,207
SP_17_005.txt,15.08689647197243,12.89491724343106,SP,8197,"
Software deobfuscation is a crucial activity in security analysis and especially in malware analysis. While standard
static and dynamic approaches suffer from well-known shortcomings, Dynamic Symbolic Execution (DSE) has recently been
proposed as an interesting alternative, more robust than static
analysis and more complete than dynamic analysis. Yet, DSE addresses only certain kinds of questions encountered by a reverser,
namely feasibility questions. Many issues arising during reverse,
e.g. detecting protection schemes such as opaque predicates, fall
into the category of infeasibility questions. We present BackwardBounded DSE, a generic, precise, efficient and robust method
for solving infeasibility questions. We demonstrate the benefit
of the method for opaque predicates and call stack tampering,
and give some insight for its usage for some other protection
schemes. Especially, the technique has successfully been used
on state-of-the-art packers as well as on the government-grade
X-Tunnel malware - allowing its entire deobfuscation. BackwardBounded DSE does not supersede existing DSE approaches, but
rather complements them by addressing infeasibility questions
in a scalable and precise manner. Following this line, we propose
sparse disassembly, a combination of Backward-Bounded DSE
and static disassembly able to enlarge dynamic disassembly in
a guaranteed way, hence getting the best of dynamic and static
disassembly. This work paves the way for robust, efficient and
precise disassembly tools for heavily-obfuscated binaries.

",17.693802365651003,16.2307668161435,225
SP_17_006.txt,14.471254511092166,13.017565878933393,SP,12476,"
Currently, no major browser fully checks for
TLS/SSL certificate revocations. This is largely due to the fact
that the deployed mechanisms for disseminating revocations
(CRLs, OCSP, OCSP Stapling, CRLSet, and OneCRL) are
each either incomplete, insecure, inefficient, slow to update, not
private, or some combination thereof. In this paper, we present
CRLite, an efficient and easily-deployable system for proactively
pushing all TLS certificate revocations to browsers. CRLite
servers aggregate revocation information for all known, valid TLS
certificates on the web, and store them in a space-efficient filter
cascade data structure. Browsers periodically download and use
this data to check for revocations of observed certificates in realtime. CRLite does not require any additional trust beyond the
existing PKI, and it allows clients to adopt a fail-closed security
posture even in the face of network errors or attacks that make
revocation information temporarily unavailable.

We present a prototype of CRLite that processes TLS certificates gathered by Rapid7, the University of Michigan, and
Google’s Certificate Transparency on the server-side, with a
Firefox extension on the client-side. Comparing CRLite to an
idealized browser that performs correct CRL/OCSP checking,
we show that CRLite reduces latency and eliminates privacy
concerns. Moreover, CRLite has low bandwidth costs: it can
represent all certificates with an initial download of 10 MB (less
than 1 byte per revocation) followed by daily updates of 580 KB
on average. Taken together, our results demonstrate that complete
TLS/SSL revocation checking is within reach for all clients.

",16.887214914478655,16.29461752988048,253
SP_17_007.txt,15.244938860580252,13.65635368117919,SP,11104,"
We present Catena, an efficiently-verifiable Bitcoin
witnessing scheme. Catena enables any number of thin clients,
such as mobile phones, to efficiently agree on a log of applicationspecific statements managed by an adversarial server. Catena
implements a log as an OP_RETURN transaction chain and
prevents forks in the log by leveraging Bitcoin’s security against
double spends. Specifically, if a log server wants to equivocate it
has to double spend a Bitcoin transaction output. Thus, Catena
logs are as hard to fork as the Bitcoin blockchain: an adversary
without a large fraction of the network’s computational power
cannot fork Bitcoin and thus cannot fork a Catena log either.
However, different from previous Bitcoin-based work, Catena
decreases the bandwidth requirements of log auditors from 90
GB to only tens of megabytes. More precisely, our clients only
need to download all Bitcoin block headers (currently less than
35 MB) and a small, 600-byte proof for each statement in a block.
We implement Catena in Java using the bitcoinj library and use it
to extend CONIKS, a recent key transparency scheme, to witness
its public-key directory in the Bitcoin blockchain where it can be
efficiently verified by auditors. We show that Catena can secure
many systems today, such as public-key directories, Tor directory
servers and software transparency schemes.

",15.760457277294734,14.587859327217128,221
SP_17_008.txt,15.273674410999728,14.296401416265319,SP,13642,"
The effectiveness of the Android permission system fundamentally hinges on the user’s correct understanding of the capabilities of the permissions being granted. In
this paper, we show that both the end-users and the security
community have significantly underestimated the dangerous
capabilities granted by the SYSTEM_ALERT_WINDOW and
the BIND_ACCESSIBILITY_SERVICE permissions: while it is
known that these are security-sensitive permissions and they
have been abused individually (e.g., in UI redressing attacks,
accessibility attacks), previous attacks based on these permissions
rely on vanishing side-channels to time the appearance of overlay
UI, cannot respond properly to user input, or make the attacks
literally visible. This work, instead, uncovers several design shortcomings of the Android platform and shows how an app with
these two permissions can completely control the UI feedback
loop and create devastating attacks. In particular, we demonstrate
how such an app can launch a variety of stealthy, powerful
attacks, ranging from stealing user’s login credentials and security PIN, to the silent installation of a God-mode app with all
permissions enabled, leaving the victim completely unsuspecting.

To make things even worse, we note that when installing
an app targeting a recent Android SDK, the list of its
required permissions is not shown to the user and that
these attacks can be carried out without needing to lure
the user to knowingly enable any permission. In fact, the
SYSTEM_ALERT_WINDOW permission is automatically
granted for apps installed from the Play Store and our
experiment shows that it is practical to lure users to unknowingly
grant the BIND_ACCESSIBILITY_SERVICE permission by
abusing capabilities from the SYSTEM_ALERT_WINDOW
permission. We evaluated the practicality of these attacks by
performing a user study: none of the 20 human subjects that took
part of the experiment even suspected they had been attacked.
We also found that it is straightforward to get a proof-of-concept
app requiring both permissions accepted on the official store.

We responsibly disclosed our findings to Google.
Unfortunately, since these problems are related to design
issues, these vulnerabilities are still unaddressed. We conclude
the paper by proposing a novel defense mechanism, implemented
as an extension to the current Android API, which would protect
Android users and developers from the threats we uncovered.

",19.1951169018618,19.64640316205534,372
SP_17_009.txt,15.301448107967438,14.244751504826223,SP,15677,"
 We present the design, implementation and information flow verification of CoSMeDis, a distributed social
media platform. The system consists of an arbitrary number
of communicating nodes, deployable at different locations over
the Internet. Its registered users can post content and establish
intra-node and inter-node friendships, used to regulate access
control over the posts. The system’s kernel has been verified
in the proof assistant Isabelle(HOL and automatically extracted
as Scala code. We formalized a framework for composing a
class of information flow security guarantees in a distributed
system, applicable to input/output automata. We instantiated this
framework to confidentiality properties for CoSMeDis’s sources
of information: posts, friendship requests, and friendship status.

",16.52667757954773,15.981548672566372,116
SP_17_010.txt,16.273760237266103,14.781033312102892,SP,13063,"
Potentially dangerous cryptography errors are welldocumented in many applications. Conventional wisdom suggests
that many of these errors are caused by cryptographic Application Programming Interfaces (APIs) that are too complicated,
have insecure defaults, or are poorly documented. To address this
problem, researchers have created several cryptographic libraries
that they claim are more usable; however, none of these libraries
have been empirically evaluated for their ability to promote
more secure development. This paper is the first to examine
both how and why the design and resulting usability of different
cryptographic libraries affects the security of code written with
them, with the goal of understanding how to build effective
future libraries. We conducted a controlled experiment in which
256 Python developers recruited from GitHub attempt common
tasks involving symmetric and asymmetric cryptography using
one of five different APIs. We examine their resulting code for
functional correctness and security, and compare their results
to their self-reported sentiment about their assigned library.
Our results suggest that while APIs designed for simplicity
can provide security benefits—reducing the decision space, as
expected, prevents choice of insecure parameters—simplicity is
not enough. Poor documentation, missing code examples, and a
lack of auxiliary features such as secure key storage, caused
even participants assigned to simplified libraries to struggle
with both basic functional correctness and security. Surprisingly,
the availability of comprehensive documentation and easy-touse code examples seems to compensate for more complicated
APIs in terms of functionally correct results and participant
reactions; however, this did not extend to security results. We
find it particularly concerning that for about 20% of functionally
correct tasks, across libraries, participants believed their code
was secure when it was not.

Our results suggest that while new cryptographic libraries
that want to promote effective security should offer a simple,
convenient interface, this is not enough: they should also, and
perhaps more importantly, ensure support for a broad range of
common tasks and provide accessible documentation with secure,
easy-to-use code examples.

",20.921347159928953,20.661668693009123,330
SP_17_011.txt,14.941153466044359,13.675361664189449,SP,14135,"
Tor is vulnerable to network-level adversaries who
can observe both ends of the communication to deanonymize
users. Recent work has shown that Tor is susceptible to the
previously unknown active BGP routing attacks, called RAPTOR
attacks, which expose Tor users to more network-level adversaries. In this paper, we aim to mitigate and detect such active
routing attacks against Tor. First, we present a new measurement
study on the resilience of the Tor network to active BGP prefix
attacks. We show that ASes with high Tor bandwidth can be less
resilient to attacks than other ASes. Second, we present a new
Tor guard relay selection algorithm that incorporates resilience
of relays into consideration to proactively mitigate such attacks.
We show that the algorithm successfully improves the security
for Tor clients by up to 36% on average (up to 166% for certain
clients). Finally, we build a live BGP monitoring system that
can detect routing anomalies on the Tor network in real time by
performing an AS origin check and novel detection analytics. Our
monitoring system successfully detects simulated attacks that are
modeled after multiple known attack types as well as a real-world
hijack attack (performed by us), while having low false positive
rates.

",14.232682905230785,13.785040650406504,206
SP_17_012.txt,14.562087133866179,12.45327322052658,SP,12867,"
Cryptographic functions have been commonly
abused by malware developers to hide malicious behaviors,
disguise destructive payloads, and bypass network-based firewalls. Now-infamous crypto-ransomware even encrypts victim’s
computer documents until a ransom is paid. Therefore, detecting cryptographic functions in binary code is an appealing
approach to complement existing malware defense and forensics.
However, pervasive control and data obfuscation schemes make
cryptographic function identification a challenging work. Existing
detection methods are either brittle to work on obfuscated
binaries or ad hoc in that they can only identify specific cryptographic functions. In this paper, we propose a novel technique
called bit-precise symbolic loop mapping to identify cryptographic
functions in obfuscated binary code. Our trace-based approach
captures the semantics of possible cryptographic algorithms
with bit-precise symbolic execution in a loop. Then we perform
guided fuzzing to efficiently match boolean formulas with known
reference implementations. We have developed a prototype called
CryptoHunt and evaluated it with a set of obfuscated synthetic
examples, well-known cryptographic libraries, and malware.
Compared with the existing tools, CryptoHunt is a general
approach to detecting commonly used cryptographic functions
such as TEA, AES, RC4, MD5, and RSA under different control
and data obfuscation scheme combinations.

Keywords-Cryptographic Function Detection; Obfuscated Binaries; Symbolic Execution.

",17.64278748958908,16.178776223776225,210
SP_17_013.txt,14.925009322480655,13.603103036175714,SP,14658,"
JavaScript, like many high-level languages, relies on runtime systems written in low-level C and C++. For example, the
Node.js runtime system gives JavaScript code access to the underlying filesystem, networking, and I/O by implementing utility functions in C++. Since C++’s type system, memory model, and execution
model differ significantly from JavaScript’s, JavaScript code must
call these runtime functions via intermediate binding layer code that
translates type, state, and failure between the two languages. Unfortunately, binding code is both hard to avoid and hard to get right.
This paper describes several types of exploitable errors that binding code creates, and develops both a suite of easily-to-build static
checkers to detect such errors and a backwards-compatible, lowoverhead API to prevent them. We show that binding flaws are a
serious security problem by using our checkers to craft 81 proof-ofconcept exploits for security flaws in the binding layers of the Node.js
and Chrome, runtime systems that support hundreds of millions of
users. As one practical measure of binding bug severity, we were
awarded $6,000 in bounties for just two Chrome bug reports.

",15.903189008614273,15.473210603829163,192
SP_17_014.txt,14.527041285562625,12.378125979147537,SP,10316,"
This paper studies information flows via timing
channels in the presence of automatic memory management. We
construct a series of example attacks that illustrate that garbage
collectors form a shared resource that can be used to reliably
leak sensitive information at a rate of up to 1 byte/sec on a
contemporary general-purpose computer. The created channel is
also observable across a network connection in a datacenter-like
setting. We subsequently present a design of automatic memory
management that is provably resilient against such attacks.

",16.827784334635936,15.160116279069765,87
SP_17_015.txt,15.771976985368216,14.684020383438835,SP,12038,"
SSL/TLS is the most commonly deployed family of
protocols for securing network communications. The security
guarantees of SSL/TLS are critically dependent on the correct
validation of the X.509 server certificates presented during the
handshake stage of the SSL/TLS protocol. Hostname verification
is a critical component of the certificate validation process that
verifies the remote server’s identity by checking if the hostname
of the server matches any of the names present in the X.509
certificate. Hostname verification is a highly complex process
due to the presence of numerous features and corner cases such
as wildcards, IP addresses, international domain names, and so
forth. Therefore, testing hostname verification implementations
present a challenging task.

In this paper, we present HVLearn, a novel black-box testing
framework for analyzing SSL/TLS hostname verification implementations, which is based on automata learning algorithms.
HVLearn utilizes a number of certificate templates, i.e., certificates with a common name (CN) set to a specific pattern, in
order to test different rules from the corresponding specification.
For each certificate template, HVLearn uses automata learning
algorithms to infer a Deterministic Finite Automaton (DFA) that
describes the set of all hostnames that match the CN of a given
certificate. Once a model is inferred for a certificate template,
HVLearn checks the model for bugs by finding discrepancies
with the inferred models from other implementations or by
checking against regular-expression-based rules derived from the
specification. The key insight behind our approach is that the
acceptable hostnames for a given certificate template form a
regular language. Therefore, we can leverage automata learning
techniques to efficiently infer DFA models that accept the
corresponding regular language.

We use HVLearn to analyze the hostname verification implementations in a number of popular SSL/TLS libraries and
applications written in a diverse set of languages like C, Python,
and Java. We demonstrate that HVLearn can achieve on average 11.21% higher code coverage than existing black/gray-box
fuzzing techniques. By comparing the DFA models inferred by
HVLearn, we found 8 unique violations of the RFC specifications
in the tested hostname verification implementations. Several
of these violations are critical and can render the affected
implementations vulnerable to active man-in-the-middle attacks.

",17.19993927845102,16.577567567567574,374
SP_17_016.txt,16.22351483246531,15.11089773684943,SP,11908,"
While the Java runtime is installed on billions of
devices and servers worldwide, it remains a primary attack vector
for online criminals. As recent studies show, the majority of all
exploited Java vulnerabilities comprise incorrect or insufficient
implementations of access-control checks. This paper for the
first time studies the problem in depth. As we find, attacks are
enabled by shortcuts that short-circuit Java’s general principle of
stack-based access control. These shortcuts, originally introduced
for ease of use and to improve performance, cause Java to
elevate the privileges of code implicitly. As we show, this creates
many pitfalls for software maintenance, making it all too easy
for maintainers of the runtime to introduce blatant confuseddeputy vulnerabilities even by just applying normally semanticspreserving refactorings.

How can this problem be solved? Can one implement Java’s
access control without shortcuts, and if so, does this implementation remain usable and efficient? To answer those questions, we
conducted a tool-assisted adaptation of the Java Class Library
(JCL), avoiding (most) shortcuts and therefore moving to a
fully explicit model of privilege elevation. As we show, the
proposed changes significantly harden the JCL against attacks:
they effectively hinder the introduction of new confused-deputy
vulnerabilities in future library versions, and successfully restrict
the capabilities of attackers when exploiting certain existing
vulnerabilities. We discuss usability considerations, and through
a set of large-scale experiments show that with current JVM
technology such a faithful implementation of stack-based access
control induces no observable performance loss.

",16.581925556534415,16.174728002920777,252
SP_17_017.txt,14.616828203974428,12.734107815965,SP,17359,"
As the most successful cryptocurrency to date,
Bitcoin constitutes a target of choice for attackers. While many
attack vectors have already been uncovered, one important vector
has been left out though: attacking the currency via the Internet
routing infrastructure itself. Indeed, by manipulating routing
advertisements (BGP hijacks) or by naturally intercepting traffic,
Autonomous Systems (ASes) can intercept and manipulate a large
fraction of Bitcoin traffic.

This paper presents the first taxonomy of routing attacks and
their impact on Bitcoin, considering both small-scale attacks,
targeting individual nodes, and large-scale attacks, targeting the
network as a whole. While challenging, we show that two key
properties make routing attacks practical: (i) the efficiency of
routing manipulation; and (ii) the significant centralization of
Bitcoin in terms of mining and routing. Specifically, we find that
any network attacker can hijack few (<100) BGP prefixes to
isolate ~50% of the mining power—even when considering that
mining pools are heavily multi-homed. We also show that on-path
network attackers can considerably slow down block propagation
by interfering with few key Bitcoin messages.

We demonstrate the feasibility of each attack against the
deployed Bitcoin software. We also quantify their effectiveness on
the current Bitcoin topology using data collected from a Bitcoin
supernode combined with BGP routing data.

The potential damage to Bitcoin is worrying. By isolating parts
of the network or delaying block propagation, attackers can cause
a significant amount of mining power to be wasted, leading to
revenue losses and enabling a wide range of exploits such as
double spending. To prevent such effects in practice, we provide
both short and long-term countermeasures, some of which can
be deployed immediately.

",15.796290986955235,15.044999999999998,279
SP_17_018.txt,15.223379164692432,13.576325249090154,SP,14392,"
Modern vehicles are required to comply with a
range of environmental regulations limiting the level of emissions
for various greenhouse gases, toxins and particulate matter. To
ensure compliance, regulators test vehicles in controlled settings
and empirically measure their emissions at the tailpipe. However,
the black box nature of this testing and the standardization
of its forms have created an opportunity for evasion. Using
modern electronic engine controllers, manufacturers can programmatically infer when a car is undergoing an emission test
and alter the behavior of the vehicle to comply with emission
standards, while exceeding them during normal driving in favor
of improved performance. While the use of such a defeat device
by Volkswagen has brought the issue of emissions cheating to the
public’s attention, there have been few details about the precise
nature of the defeat device, how it came to be, and its effect on
vehicle behavior.

In this paper, we present our analysis of two families of
software defeat devices for diesel engines: one used by the
Volkswagen Group to pass emissions tests in the US and Europe,
and a second that we have found in Fiat Chrysler Automobiles. To
carry out this analysis, we developed new static analysis firmware
forensics techniques necessary to automatically identify known
defeat devices and confirm their function. We tested about 900
firmware images and were able to detect a potential defeat device
in more than 400 firmware images spanning eight years. We
describe the precise conditions used by the firmware to detect a
test cycle and how it affects engine behavior. This work frames
the technical challenges faced by regulators going forward and
highlights the important research agenda in providing focused
software assurance in the presence of adversarial manufacturers.

",17.80541091248751,17.183580419580426,288
SP_17_019.txt,13.836496954880928,12.261427979851316,SP,11776,"
Online underground economy is an important channel that connects the merchants of illegal products and their
buyers, which is also constantly monitored by legal authorities. As
one common way for evasion, the merchants and buyers together
create a vocabulary of jargons (called “black keywords” in this
paper) to disguise the transaction (e.g., “smack” is one street
name for “heroin” [1]). Black keywords are often “unfriendly” to
the outsiders, which are created by either distorting the original
meaning of common words or tweaking other black keywords.
Understanding black keywords is of great importance to track
and disrupt the underground economy, but it is also prohibitively
difficult: the investigators have to infiltrate the inner circle of
criminals to learn their meanings, a task both risky and timeconsuming.

In this paper, we make the first attempt towards capturing
and understanding the ever-changing black keywords. We investigated the underground business promoted through blackhat
SEO (search engine optimization) and demonstrate that the black
keywords targeted by the SEOers can be discovered through a
fully automated approach. Our insights are two-fold: first, the
pages indexed under black keywords are more likely to contain
malicious or fraudulent content (¢.g., SEO pages) and alarmed
by off-the-shelf detectors; second, people tend to query multiple
similar black keywords to find the merchandise. Therefore, we
could infer whether a search keyword is “black” by inspecting the
associated search results and then use the related search queries
to extend our findings. To this end, we built a system called
KDES (Keywords Detection and Expansion System), and applied
it to the search results of Baidu, China’s top search engine. So
far, we have already identified 478,879 black keywords which
were clustered under 1,522 core words based on text similarity.
We further extracted the information like emails, mobile phone
numbers and instant messenger IDs from the pages and domains
relevant to the underground business. Such information helps us
gain better understanding about the underground economy of
China in particular.

In addition, our work could help search engine vendors purify
the search results and disrupt the channel of the underground
market. Our co-authors from Baidu compared our results with
their blacklist, found many of them (e.g., long-tail and obfuscated
keywords) were not in it, and then added them to Baidu’s internal
blacklist.

*Corresponding author.

",15.202697889610203,14.683305699481867,392
SP_17_020.txt,15.4636220980932,13.461192041205564,SP,13348,"
Despite a great deal of work to improve the TLS
PKI, CA misbehavior continues to occur, resulting in unauthorized certificates that can be used to mount man-in-themiddle attacks against HTTPS sites. CAs lack the incentives
to invest in higher security, and the manual effort required
to report a rogue certificate deters many from contributing to
the security of the TLS PKI. In this paper, we present IKP, a
platform that automates responses to unauthorized certificates
and provides incentives for CAs to behave correctly and for
others to report potentially unauthorized certificates. Domains
in IKP specify criteria for their certificates, and CAs specify
reactions such as financial penalties that execute in case of
unauthorized certificate issuance. By leveraging smart contracts
and blockchain-based consensus, we can decentralize IKP while
still providing automated incentives. We describe a theoretical
model for payment flows and implement IKP in Ethereum to
show that decentralizing and automating PKIs with financial
incentives is both economically sound and technically viable.

",19.454632303725965,17.51878048780488,165
SP_17_021.txt,16.56747360367024,15.35585198997195,SP,15044,"
Authorization bugs, when present in online social
networks, are usually caused by missing or incorrect authorization checks and can allow attackers to bypass the online
social network’s protections. Unfortunately, there is no practical
way to fully guarantee that an authorization bug will never
be introduced—even with good engineering practices—as a web
application and its data model become more complex. Unlike
other web application vulnerabilities such as XSS and CSRF,
there is no practical general solution to prevent missing or
incorrect authorization checks.

In this paper we propose INVARIANT DETECTOR (IVD), a
defense-in-depth system that automatically learns authorization
rules from normal data manipulation patterns and distills them
into likely invariants. These invariants, usually learned during
the testing or pre-release stages of new features, are then used to
block any requests that may attempt to exploit bugs in the social
network’s authorization logic. IVD acts as an additional layer of
defense, working behind the scenes, complementary to privacy
frameworks and testing.

We have designed and implemented IvD to handle the unique
challenges posed by modern online social networks. IvD is
currently running at Facebook, where it infers and evaluates
daily more than 200,000 invariants from a sample of roughly
500 million client requests, and checks the resulting invariants
every second against millions of writes made to a graph database
containing trillions of entities. Thus far IVD has detected several
high impact authorization bugs and has successfully blocked
attempts to exploit them before code fixes were deployed.

",16.32212239822248,16.714859437751006,253
SP_17_022.txt,16.385893523027924,14.995408810505378,SP,14199,"
Since the first whole-genome sequencing, the
biomedical research community has made significant steps towards a more precise, predictive and personalized medicine.
Genomic data is nowadays widely considered privacy-sensitive
and consequently protected by strict regulations and released
only after careful consideration. Various additional types of
biomedical data, however, are not shielded by any dedicated legal
means and consequently disseminated much less thoughtfully.
This in particular holds true for DNA methylation data as one
of the most important and well-understood epigenetic element
influencing human health.

In this paper, we show that, in contrast to the aforementioned
belief, releasing one’s DNA methylation data causes privacy issues
akin to releasing one’s actual genome. We show that already
a small subset of methylation regions influenced by genomic
variants are sufficient to infer parts of someone’s genome, and to
further map this DNA methylation profile to the corresponding
genome. Notably, we show that such re-identification is possible
with 97.5% accuracy, relying on a dataset of more than 2500
genomes, and that we can reject all wrongly matched genomes
using an appropriate statistical test. We provide means for
countering this threat by proposing a novel cryptographic scheme
for privately classifying tumors that enables a privacy-respecting
medical diagnosis in a common clinical setting. The scheme
relies on a combination of random forests and homomorphic
encryption, and it is proven secure in the honest-but-curious
model. We evaluate this scheme on real DNA methylation data,
and show that we can keep the computational overhead to
acceptable values for our application scenario.

",19.38786093064905,17.980527131782953,263
SP_17_023.txt,15.206647944696208,14.045038956614068,SP,17385,"
The record layer is the main bridge between TLS
applications and internal sub-protocols. Its core functionality
is an elaborate form of authenticated encryption: streams
of messages for each sub-protocol (handshake, alert, and
application data) are fragmented, multiplexed, and encrypted
with optional padding to hide their lengths. Conversely, the subprotocols may provide fresh keys or signal stream termination
to the record layer.

Compared to prior versions, TLS 1.3 discards obsolete
schemes in favor of a common construction for Authenticated
Encryption with Associated Data (AEAD), instantiated with
algorithms such as AES-GCM and ChaCha20-Poly1305. It
differs from TLS 1.2 in its use of padding, associated data
and nonces. It also encrypts the content-type used to multiplex
between sub-protocols. New protocol features such as early
application data (0-RTT and 0.5-RTT) and late handshake
messages require additional keys and a more general model
of stateful encryption.

We build and verify a reference implementation of the
TLS record layer and its cryptographic algorithms in F*,
a dependently typed language where security and functional
guarantees can be specified as pre- and post-conditions. We
reduce the high-level security of the record layer to cryptographic assumptions on its ciphers. Each step in the reduction
is verified by typing an F* module; for each step that involves
a cryptographic assumption, this module precisely captures the
corresponding game.

We first verify the functional correctness and injectivity
properties of our implementations of one-time MAC algorithms
(Poly1305 and GHASH) and provide a generic proof of their
security given these two properties. We show the security of
a generic AEAD construction built from any secure one-time
MAC and PRE. We extend AEAD, first to stream encryption,
then to length-hiding, multiplexed encryption. Finally, we build
a security model of the record layer against an adversary
that controls the TLS sub-protocols. We compute concrete
security bounds for the AES_128 GCM, AES_256_GCM,
and CHACHA20_POLY1305 ciphersuites, and derive recommended limits on sent data before re-keying.

We plug our implementation of the record layer into the
miTLS library, confirm that they interoperate with Chrome
and Firefox, and report initial performance results. Combining
our functional correctness, security, and experimental results,
we conclude that the new TLS record layer (as described in
RFCs and cryptographic standards) is provably secure, and
we provide its first verified implementation.
",16.562452216208204,14.960135746606337,394
SP_17_024.txt,14.180560134087926,12.932061188522898,SP,11918,"
Within the next few years, billions of IoT devices will
densely populate our cities. In this paper we describe a new
type of threat in which adjacent IoT devices will infect each
other with a worm that will spread explosively over large areas
in a kind of nuclear chain reaction, provided that the density
of compatible IoT devices exceeds a certain critical mass. In
particular, we developed and verified such an infection using
the popular Philips Hue smart lamps as a platform. The worm
spreads by jumping directly from one lamp to its neighbors,
using only their built-in ZigBee wireless connectivity and their
physical proximity. The attack can start by plugging in a single
infected bulb anywhere in the city, and then catastrophically
spread everywhere within minutes, enabling the attacker to
turn all the city lights on or off, permanently brick them,
or exploit them in a massive DDOS attack. To demonstrate
the risks involved, we use results from percolation theory to
estimate the critical mass of installed devices for a typical city
such as Paris whose area is about 105 square kilometers: The
chain reaction will fizzle if there are fewer than about 15,000
randomly located smart lights in the whole city, but will spread
everywhere when the number exceeds this critical mass (which
had almost certainly been surpassed already).

To make such an attack possible, we had to find a way
to remotely yank already installed lamps from their current
networks, and to perform over-the-air firmware updates. We
overcame the first problem by discovering and exploiting a
major bug in the implementation of the Touchlink part of
the ZigBee Light Link protocol, which is supposed to stop
such attempts with a proximity test. To solve the second
problem, we developed a new version of a side channel attack to
extract the global AES-CCM key (for each device type) that
Philips uses to encrypt and authenticate new firmware. We
used only readily available equipment costing a few hundred
dollars, and managed to find this key without seeing any actual
updates. This demonstrates once again how difficult it is to get
security right even for a large company that uses standard
cryptographic techniques to protect a major product.

",17.122413403193683,17.01199413489736,374
SP_17_025.txt,13.558296425130745,11.41374341327991,SP,11480,"
Recent large-scale deployments of differentially private algorithms employ the local model for
privacy (sometimes called PRAM or randomized response),
where data are randomized on each individual’s device
before being sent to a server that computes approximate,
aggregate statistics. The server need not be trusted for
privacy, leaving data control in users’ hands.

For an important class of convex optimization problems (including logistic regression, support vector machines, and the Euclidean median), the best known locally
differentially-private algorithms are highly interactive,
requiring as many rounds of back and forth as there are
users in the protocol.

We ask: how much interaction is necessary to optimize
convex functions in the local DP model? Existing lower
bounds either do not apply to convex optimization, or say
nothing about interaction.

We provide new algorithms which are either noninteractive or use relatively few rounds of interaction. We
also show lower bounds on the accuracy of an important
class of noninteractive algorithms, suggesting a separation
between what is possible with and without interaction.

Keywords-Differential privacy, local differential privacy,
convex optimization, oracle complexity.

",17.267425705330172,16.88637640449438,180
SP_17_026.txt,13.853158555486573,11.801897848157608,SP,14446,"
Order-preserving encryption and its generalization order-revealing encryption (OPE/ORE) allow sorting,
performing range queries, and filtering data — all while only
having access to ciphertexts. But OPE and ORE ciphertexts
necessarily leak information about plaintexts, and what level
of security they provide in practice has been unclear.

In this work, we introduce new leakage-abuse attacks
that recover plaintexts from OPE/ORE-encrypted databases.
Underlying our new attacks is a framework in which we cast
the adversary’s challenge as a non-crossing bipartite matching
problem. This allows easy tailoring of attacks to a specific
scheme’s leakage profile. In a case study of customer records,
we show attacks that recover 99% of first names, 97% of last
names, and 90% of birthdates held in a database, despite all
values being encrypted with the OPE scheme most widely used
in practice.

We also show the first attack against the recent frequencyhiding Kerschbaum scheme, to which no prior attacks have
been demonstrated. Our attack recovers frequently occurring
plaintexts most of the time.

",14.37465228746014,13.521029411764705,173
SP_17_027.txt,16.07323005883362,14.288342086443325,SP,11275,"
We provide the first machine-checked proof of
privacy-related properties (including ballot privacy) for an
electronic voting protocol in the computational model. We
target the popular Helios family of voting protocols, for which
we identify appropriate levels of abstractions to allow the
simplification and convenient reuse of proof steps across many
variations of the voting scheme. The resulting framework
enables machine-checked security proofs for several hundred
variants of Helios and should serve as a stepping stone for the
analysis of further variations of the scheme.

In addition, we highlight some of the lessons learned regarding the gap between pen-and-paper and machine-checked
proofs, and report on the experience with formalizing the
security of protocols at this scale.

",19.287186520377343,17.954607438016534,122
SP_17_028.txt,14.408870158948705,13.230890523493834,SP,13596,"
We quantitatively investigate how machine learning
models leak information about the individual data records on
which they were trained. We focus on the basic membership
inference attack: given a data record and black-box access to
a model, determine if the record was in the model’s training
dataset. To perform membership inference against a target model,
we make adversarial use of machine learning and train our own
inference model to recognize differences in the target model’s
predictions on the inputs that it trained on versus the inputs
that it did not train on.

We empirically evaluate our inference techniques on classification models trained by commercial “machine learning as a
service” providers such as Google and Amazon. Using realistic
datasets and classification tasks, including a hospital discharge
dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership
inference attacks. We then investigate the factors that influence
this leakage and evaluate mitigation strategies.

",17.879347455551382,17.23,163
SP_17_029.txt,15.078756770707528,14.169787784836299,SP,11822,"
In this paper we present a simple and reliable
authentication method for mobile devices equipped with multitouch screens such as smart phones, tablets and laptops. Users
are authenticated by performing specially designed multi-touch
gestures with one swipe on the touchscreen. During this process,
both hand geometry and behavioral characteristics are
recorded in the multi-touch traces and used for authentication.
By combining both geometry information and behavioral
characteristics, we overcome the problem of behavioral
variability plaguing many behavior based authentication
techniques — which often leads to less accurate authentication or
poor user experience — while also ensuring the discernibility of
different users with possibly similar handshapes. We evaluate
the design of the proposed authentication method thoroughly
using a large multi-touch dataset collected from 161 subjects
with an elaborately designed procedure to capture behavior
variability. The results demonstrate that the fusion of
behavioral information with hand geometry features produces
effective resistance to behavioral variability over time while at
the same time retains discernibility. Our approach achieves
EER of 5.84% with only 5 training samples and the
performance is further improved to EER of 1.88% with enough
training. Security analyses are also conducted to demonstrate
that the proposed method is resilient against common
smartphone authentication threats such as smudge attack,
shoulder surfing attack and statistical attack. Finally, user
acceptance of the method is illustrated via a usability study.

Keywords—Multi-touch Gesture; Mobile Authentication;
Hand Geometry; Behavioral Variability; Usable Security

",18.1352568364455,18.63065546218488,241
SP_17_030.txt,16.570473258099756,15.944934004750781,SP,11628,"
Differential testing uses similar programs as crossreferencing oracles to find semantic bugs that do not exhibit
explicit erroneous behaviors like crashes or assertion failures.
Unfortunately, existing differential testing tools are domainspecific and inefficient, requiring large numbers of test inputs
to find a single bug. In this paper, we address these issues by
designing and implementing NEZHA, an efficient input-formatagnostic differential testing framework. The key insight behind
NEZHA’s design is that current tools generate inputs by simply
borrowing techniques designed for finding crash or memory
corruption bugs in individual programs (e.g., maximizing code
coverage). By contrast, NEZHA exploits the behavioral asymmetries between multiple test programs to focus on inputs that are
more likely to trigger semantic bugs. We introduce the notion of
6-diversity, which summarizes the observed asymmetries between
the behaviors of multiple test applications. Based on 6-diversity,
we design two efficient domain-independent input generation
mechanisms for differential testing, one gray-box and one blackbox. We demonstrate that both of these input generation schemes
are significantly more efficient than existing tools at finding
semantic bugs in real-world, complex software.

NEZHA’s average rate of finding differences is 52 times and 27
times higher than that of Frankencerts and Mucerts, two popular
domain-specific differential testing tools that check SSL/TLS
certificate validation implementations, respectively. Moreover,
performing differential testing with NEZHA results in 6 times
more semantic bugs per tested input, compared to adapting
state-of-the-art general-purpose fuzzers like American Fuzzy Lop
(AFL) to differential testing by running them on individual test
programs for input generation.

NEZHA discovered 778 unique, previously unknown discrepancies across a wide variety of applications (ELF and XZ
parsers, PDF viewers and SSL/TLS libraries), many of which
constitute previously unknown critical security vulnerabilities. In
particular, we found two critical evasion attacks against ClamAV,
allowing arbitrary malicious ELF/XZ files to evade detection. The
discrepancies NEZHA found in the X.509 certificate validation
implementations of the tested SSL/TLS libraries range from
mishandling certain types of KeyUsage extensions, to incorrect
acceptance of specially crafted expired certificates, enabling manin-the-middle attacks. All of our reported vulnerabilities have
been confirmed and fixed within a week from the date of
reporting.

",18.922397154172714,18.325559006211183,372
SP_17_031.txt,16.044025835787497,14.628846429914208,SP,10341,"
Code reuse attacks exploiting memory disclosure
vulnerabilities can bypass all deployed mitigations. One promising defense against this class of attacks is to enable executeonly memory (XOM) protection on top of fine-grained address
space layout randomization (ASLR). However, recent works
implementing XOM, despite their efficacy, only protect programs
that have been (re)built with new compiler support, leaving
commercial-off-the-shelf (COTS) binaries and source-unavailable
programs unprotected.

We present the design and implementation of NORAX, a
practical system that retrofits XOM into stripped COTS binaries
on AArch64 platforms. Unlike previous techniques, NORAX
requires neither source code nor debugging symbols. NORAX
statically transforms existing binaries so that during runtime
their code sections can be loaded into XOM memory pages
with embedded data relocated and data references properly
updated. NORAX allows transformed binaries to leverage the
new hardware-based XOM support—a feature widely available
on AArché4 platforms (e.g., recent mobile devices) yet virtually
unused due to the incompatibility of existing binaries. Furthermore, NORAX is designed to co-exist with other COTS binary
hardening techniques, such as in-place randomization (IPR). We
apply NORAX to the commonly used Android system binaries
running on SAMSUNG Galaxy S6 and LG Nexus 5X devices. The
results show that NORAX on average slows down the execution
of transformed binaries by 1.18% and increases their memory
footprint by 2.21%, suggesting NORAX is practical for real-world
adoption.

",17.122413403193683,16.04075862068966,236
SP_17_032.txt,15.280336497341423,13.86568515977888,SP,16365,"
The computer security community has advocated
widespread adoption of secure communication tools to counter
mass surveillance. Several popular personal communication tools
(e.g., WhatsApp, iMessage) have adopted end-to-end encryption,
and many new tools (e.g., Signal, Telegram) have been launched
with security as a key selling point. However it remains unclear
if users understand what protection these tools offer, and if they
value that protection. In this study, we interviewed 60 participants about their experience with different communication tools
and their perceptions of the tools’ security properties. We found
that the adoption of secure communication tools is hindered by
fragmented user bases and incompatible tools. Furthermore, the
vast majority of participants did not understand the essential
concept of end-to-end encryption, limiting their motivation to
adopt secure tools. We identified a number of incorrect mental
models that underpinned these beliefs.

",17.28802050969988,15.196210739614997,144
SP_17_033.txt,13.498915824936905,11.033278365235258,SP,13046,"
The Trusted Platform Module (TPM) is an international standard for a security chip that can be used for the
management of cryptographic keys and for remote attestation.
The specification of the most recent TPM 2.0 interfaces for
direct anonymous attestation unfortunately has a number of
severe shortcomings. First of all, they do not allow for security
proofs (indeed, the published proofs are incorrect). Second, they
provide a Diffie-Hellman oracle w.r.t. the secret key of the TPM,
weakening the security and preventing forward anonymity of
attestations. Fixes to these problems have been proposed, but
they create new issues: they enable a fraudulent TPM to encode
information into an attestation signature, which could be used
to break anonymity or to leak the secret key. Furthermore,
all proposed ways to remove the Diffie-Hellman oracle either
strongly limit the functionality of the TPM or would require
significant changes to the TPM 2.0 interfaces. In this paper we
provide a better specification of the TPM 2.0 interfaces that
addresses these problems and requires only minimal changes to
the current TPM 2.0 commands. We then show how to use the
revised interfaces to build g-SDH- and LRSW-based anonymous
attestation schemes, and prove their security. We finally discuss
how to obtain other schemes addressing different use cases such
as key-binding for U-Prove and e-cash.

",15.616094167265928,14.661309523809525,231
SP_17_034.txt,13.378128160328068,11.864870976244983,SP,15889,"
Secure multiparty computation enables a set of
parties to securely carry out a joint computation of their private
inputs without revealing anything but the output. In the past
few years, the efficiency of secure computation protocols has
increased in leaps and bounds. However, when considering the
case of security in the presence of malicious adversaries (who
may arbitrarily deviate from the protocol specification), we
are still very far from achieving high efficiency. In this paper,
we consider the specific case of three parties and an honest
majority. We provide general techniques for improving efficiency
of cut-and-choose protocols on multiplication triples and utilize
them to significantly improve the recently published protocol of
Furukawa et al. (ePrint 2016/944), We reduce the bandwidth of
their protocol down from 10 bits per AND gate to 7 bits per AND
gate, and show how to improve some computationally expensive
parts of their protocol. Most notably, we design cache-efficient
shuffling techniques for implementing cut-and-choose without
randomly permuting large arrays (which is very slow due to continual cache misses). We provide a combinatorial analysis of our
techniques, bounding the cheating probability of the adversary.
Our implementation achieves a rate of approximately 7.15 billion
AND gates per second on a cluster of three 20-core machines with
a 10Gbps network. Thus, we can securely compute 212,000 AES
encryptions per second (which is hundreds of times faster than
previous work for this setting). Our results demonstrate that
high-throughput secure computation for malicious adversaries is
possible.

",16.887214914478655,15.591624505928856,256
SP_17_035.txt,15.726055397185803,13.51006793543431,SP,12036,"
Embedded systems are ubiquitous in every aspect of
modern life. As the Internet of Thing expands, our dependence
on these systems increases. Many of these interconnected systems
are and will be low cost bare-metal systems, executing without an
operating system. Bare-metal systems rarely employ any security
protection mechanisms and their development assumptions (unrestricted access to all memory and instructions), and constraints
(runtime, energy, and memory) makes applying protections
challenging.

To address these challenges we present EPOXY, an LLVMbased embedded compiler. We apply a novel technique, called
privilege overlaying, wherein operations requiring privileged
execution are identified and only these operations execute in
privileged mode. This provides the foundation on which codeintegrity, adapted control-flow hijacking defenses, and protections for sensitive IO are applied. We also design fine-grained
randomization schemes, that work within the constraints of baremetal systems to provide further protection against control-flow
and data corruption attacks.

These defenses prevent code injection attacks and ROP attacks
from scaling across large sets of devices. We evaluate the
performance of our combined defense mechanisms for a suite of
75 benchmarks and 3 real-world IoT applications. Our results for
the application case studies show that EPOXY has, on average,
a 1.8% increase in execution time and a 0.5% increase in energy
usage.

",16.4712008295341,14.897029449423822,216
SP_17_036.txt,14.508428754510337,12.775906149913034,SP,13207,"
Protecting vast quantities of data poses a
daunting challenge for the growing number of organizations that collect, stockpile, and monetize it. The ability to
distinguish data that is actually needed from data collected
‘just in case” would help these organizations to limit the
latter’s exposure to attack. A natural approach might be to
monitor data use and retain only the working-set of in-use
data in accessible storage; unused data can be evicted to a
highly protected store. However, many of today’s big data
applications rely on machine learning (ML) workloads that
are periodically retrained by accessing, and thus exposing
to attack, the entire data store. Training set minimization
methods, such as count featurization, are often used to
limit the data needed to train ML workloads to improve
performance or scalability.

We present Pyramid, a limited-exposure data management system that builds upon count featurization to
enhance data protection. As such, Pyramid uniquely introduces both the idea and proof-of-concept for leveraging
training set minimization methods to instill rigor and selectivity into big data management. We integrated Pyramid
into Spark Velox, a framework for ML-based targeting
and personalization. We evaluate it on three applications
and show that Pyramid approaches state-of-the-art models
while training on less than 1% of the raw data.

",16.18397175987059,15.727619047619054,220
SP_17_037.txt,14.462730054096745,13.002027569712254,SP,13078,"
Bias-resistant public randomness is a critical component in many (distributed) protocols. Generating public randomness is hard, however, because active adversaries may behave
dishonestly to bias public random choices toward their advantage. Existing solutions do not scale to hundreds or thousands
of participants, as is needed in many decentralized systems.
We propose two large-scale distributed protocols, RandHound
and RandHerd, which provide publicly-verifiable, unpredictable,
and unbiasable randomness against Byzantine adversaries. RandHound relies on an untrusted client to divide a set of randomness
servers into groups for scalability, and it depends on the pigeonhole principle to ensure output integrity, even for non-random,
adversarial group choices. RandHerd implements an efficient,
decentralized randomness beacon. RandHerd is structurally
similar to a BFT protocol, but uses RandHound in a one-time
setup to arrange participants into verifiably unbiased random
secret-sharing groups, which then repeatedly produce random
output at predefined intervals. Our prototype demonstrates that
RandHound and RandHerd achieve good performance across
hundreds of participants while retaining a low failure probability
by properly selecting protocol parameters, such as a group size
and secret-sharing threshold. For example, when sharding 512
nodes into groups of 32, our experiments show that RandHound
can produce fresh random output after 240 seconds. RandHerd,
after a setup phase of 260 seconds, is able to generate fresh
random output in intervals of approximately 6 seconds. For this
configuration, both protocols operate at a failure probability of
at most 0.08% against a Byzantine adversary.

",17.122413403193683,16.7092697466468,246
SP_17_038.txt,14.41977969349837,12.772731219201138,SP,14729,"
Machine learning is widely used in practice to produce predictive models for applications such as image processing,
speech and text recognition. These models are more accurate
when trained on large amount of data collected from different
sources. However, the massive data collection raises privacy
concerns.

In this paper, we present new and efficient protocols for
privacy preserving machine learning for linear regression, logistic
regression and neural network training using the stochastic
gradient descent method. Our protocols fall in the two-server
model where data owners distribute their private data among
two non-colluding servers who train various models on the joint
data using secure two-party computation (2PC). We develop new
techniques to support secure arithmetic operations on shared
decimal numbers, and propose MPC-friendly alternatives to nonlinear functions such as sigmoid and softmax that are superior
to prior work.

We implement our system in C++. Our experiments validate
that our protocols are several orders of magnitude faster than
the state of the art implementations for privacy preserving linear
and logistic regressions, and scale to millions of data samples
with thousands of features. We also implement the first privacy
preserving system for training neural networks.

",16.18397175987059,14.92940778341794,196
SP_17_039.txt,15.88208642425673,14.186470388925901,SP,14970,"
Augmented reality (AR) technologies, such as Microsoft’s HoloLens head-mounted display and AR-enabled car
windshields, are rapidly emerging. AR applications provide users
with immersive virtual experiences by capturing input from a
user’s surroundings and overlaying virtual output on the user’s
perception of the real world. These applications enable users to
interact with and perceive virtual content in fundamentally new
ways. However, the immersive nature of AR applications raises
serious security and privacy concerns. Prior work has focused
primarily on input privacy risks stemming from applications with
unrestricted access to sensor data. However, the risks associated
with malicious or buggy AR output remain largely unexplored.
For example, an AR windshield application could intentionally or
accidentally obscure oncoming vehicles or safety-critical output of
other AR applications. In this work, we address the fundamental
challenge of securing AR output in the face of malicious or buggy
applications. We design, prototype, and evaluate Arya, an AR
platform that controls application output according to policies
specified in a constrained yet expressive policy framework. In
doing so, we identify and overcome numerous challenges in
securing AR output.

",17.238542476582836,15.85628961748634,187
SP_17_040.txt,13.71755260446913,11.717218042067788,SP,16679,"
Abstract—Full-text search systems, such as Elasticsearch and
Apache Solr, enable document retrieval based on keyword
queries. In many deployments these systems are multi-tenant,
meaning distinct users’ documents reside in, and their queries
are answered by, one or more shared search indexes. Large
deployments may use hundreds of indexes across which user
documents are randomly assigned. The results of a search
query are filtered to remove documents to which a client should
not have access.
We show the existence of exploitable side channels in modern
multi-tenant search. The starting point for our attacks is a
decade-old observation that the TF-IDF scores used to rank
search results can potentially leak information about other
users’ documents. To the best of our knowledge, no attacks
have been shown that exploit this side channel in practice,
and constructing a working side channel requires overcoming
numerous challenges in real deployments. We nevertheless
develop a new attack, called STRESS (Search Text RElevance
Score Side channel), and in so doing show how an attacker
can map out the number of indexes used by a service, obtain
placement of a document within each index, and then exploit
co-tenancy with all other users to (1) discover the terms in
other tenants’ documents or (2) determine the number of
documents (belonging to other tenants) that contain a term of
interest. In controlled experiments, we demonstrate the attacks
on popular services such as GitHub and Xen.do. We conclude
with a discussion of countermeasures.
Keywords-side channels; SaaS security; elasticsearch
",14.811377253095038,13.206005726556906,256
SP_17_041.txt,14.71195831764322,12.441981017152532,SP,12983,"
Programs that take highly-structured files as inputs
normally process inputs in stages: syntax parsing, semantic checking, and application execution. Deep bugs are often hidden in the
application execution stage, and it is non-trivial to automatically
generate test inputs to trigger them. Mutation-based fuzzing generates test inputs by modifying well-formed seed inputs randomly
or heuristically. Most inputs are rejected at the early syntax parsing stage. Differently, generation-based fuzzing generates inputs
from a specification (e.g., grammar). They can quickly carry the
fuzzing beyond the syntax parsing stage. However, most inputs
fail to pass the semantic checking (e.g., violating semantic rules),
which restricts their capability of discovering deep bugs.

In this paper, we propose a novel data-driven seed generation
approach, named Skyfire, which leverages the knowledge in the
vast amount of existing samples to generate well-distributed seed
inputs for fuzzing programs that process highly-structured inputs.
Skyfire takes as inputs a corpus and a grammar, and consists of
two steps. The first step of Skyfire learns a probabilistic contextsensitive grammar (PCSG) to specify both syntax features and
semantic rules, and then the second step leverages the learned
PCSG to generate seed inputs.

We fed the collected samples and the inputs generated by
Skyfire as seeds of AFL to fuzz several open-source XSLT and
XML engines (i.e., Sablotron, libxslt, and libxml2). The results
have demonstrated that Skyfire can generate well-distributed
inputs and thus significantly improve the code coverage (i.e., 20%
for line coverage and 15% for function coverage on average)
and the bug-finding capability of fuzzers. We also used the
inputs generated by Skyfire to fuzz the closed-source JavaScript
and rendering engine of Internet Explorer 11. Altogether, we
discovered 19 new memory corruption bugs (among which there
are 16 new vulnerabilities) and 32 denial-of-service bugs.

",15.719379583869454,14.304887218045113,309
SP_17_042.txt,14.972633797675908,13.378726928395036,SP,13760,"
Permission systems are the main defense that mobile
platforms, such as Android and iOS, offer to users to protect
their private data from prying apps. However, due to the
tension between usability and control, such systems have several
limitations that often force users to overshare sensitive data. We
address some of these limitations with SmarPer, an advanced
permission mechanism for Android. To address the rigidity of
current permission systems and their poor matching of users’
privacy preferences, SmarPer relies on contextual information
and machine learning methods to predict permission decisions
at runtime. Note that the goal of SmarPer is to mimic the
users’ decisions, not to make privacy-preserving decisions per se.
Using our SmarPer implementation, we collected 8,521 runtime
permission decisions from 41 participants in real conditions. With
this unique data set, we show that using an efficient Bayesian
linear regression model results in a mean correct classification
rate of 80% (43%). This represents a mean relative reduction
of approximately 50% in the number of incorrect decisions
when compared with a user-defined static permission policy,
i.e., the model used in current permission systems. SmarPer also
focuses on the suboptimal trade-off between privacy and utility;
instead of only “allow” or “deny” type of decisions, SmarPer
also offers an “obfuscate” option where users can still obtain
utility by revealing partial information to apps. We implemented
obfuscation techniques in SmarPer for different data types and
evaluated them during our data collection campaign. Our results
show that 73% of the participants found obfuscation useful and
it accounted for almost a third of the total number of decisions.
In short, we are the first to show, using a large dataset of real in
situ permission decisions, that it is possible to learn users’ unique
decision patterns at runtime using contextual information while
supporting data obfuscation; this is an important step towards
automating the management of permissions in smartphones.

",17.5058628484301,16.711645569620256,319
SP_17_043.txt,15.261660483591253,13.804728777367771,SP,12755,"
Protected database search systems cryptographically
isolate the roles of reading from, writing to, and administering the
database. This separation limits unnecessary administrator access
and protects data in the case of system breaches. Since protected
search was introduced in 2000, the area has grown rapidly;
systems are offered by academia, start-ups, and established
companies.

However, there is no best protected search system or set of
techniques. Design of such systems is a balancing act between
security, functionality, performance, and usability. This challenge
is made more difficult by ongoing database specialization, as
some users will want the functionality of SQL, NoSQL, or
NewSQL databases. This database evolution will continue, and
the protected search community should be able to quickly provide
functionality consistent with newly invented databases.

At the same time, the community must accurately and clearly
characterize the tradeoffs between different approaches. To address these challenges, we provide the following contributions:

1) An identification of the important primitive operations
across database paradigms. We find there are a small
number of base operations that can be used and combined
to support a large number of database paradigms.

2) An evaluation of the current state of protected search
systems in implementing these base operations. This evaluation describes the main approaches and tradeoffs for each
base operation. Furthermore, it puts protected search in
the context of unprotected search, identifying key gaps in
functionality.

3) An analysis of attacks against protected search for different
base queries.

4) A roadmap and tools for transforming a protected search
system into a protected database, including an open-source
performance evaluation platform and initial user opinions
of protected search.

Index Terms—searchable symmetric encryption, property pre
serving encryption, database search, oblivious random access
memory, private information retrieval

",16.753131548150495,14.948055555555559,289
SP_17_044.txt,14.070395841329763,12.376880274186167,SP,12418,"

The idea of a paperless office has been dreamed of for
more than three decades. However, nowadays printers are
still one of the most essential devices for daily work and
common Internet users. Instead of removing them, printers
evolved from simple devices into complex network computer systems, installed directly into company networks, and
carrying considerable confidential data in their print jobs.
This makes them to an attractive attack target.

In this paper we conduct a large scale analysis of printer
attacks and systematize our knowledge by providing a general methodology for security analyses of printers. Based
on our methodology, we implemented an open-source tool
called PRinter Exploitation Toolkit (PRET). We used PRET
to evaluate 20 printer models from different vendors and
found all of them to be vulnerable to at least one of the
tested attacks. These attacks included, for example, simple
Denial-of-Service (DoS) attacks or skilled attacks, extracting
print jobs and system files.

On top of our systematic analysis we reveal novel insights that enable attacks from the Internet by using advanced cross-site printing techniques, combined with printer
CORS spoofing. Finally, we show how to apply our attacks
to systems beyond typical printers like Google Cloud Print
or document processing websites.

",14.696529576185021,13.523611650485439,207
SP_17_045.txt,16.394221277690917,15.094849297578218,SP,15212,"
The past ten years has seen increasing calls to make
security research more “scientific”. On the surface, most agree
that this is desirable, given universal recognition of “science” as a
positive force. However, we find that there is little clarity on what
“scientific” means in the context of computer security research,
or consensus on what a “Science of Security” should look like. We
selectively review work in the history and philosophy of science
and more recent work under the label “Science of Security”.
We explore what has been done under the theme of relating
science and security, put this in context with historical science,
and offer observations and insights we hope may motivate further
exploration and guidance. Among our findings are that practices
on which the rest of science has reached consensus appear little
used or recognized in security, and a pattern of methodological
errors continues unaddressed.

Index Terms—security research; science of security; history of
science; philosophy of science; connections between research and
observable world.
",16.26309291913925,15.126041308089501,167
SP_17_046.txt,18.160150981133366,17.911920455224074,SP,14648,"
Malware sandboxes, widely used by antivirus
companies, mobile application marketplaces, threat detection
appliances, and security researchers, face the challenge of
environment-aware malware that alters its behavior once it detects that it is being executed on an analysis environment. Recent
efforts attempt to deal with this problem mostly by ensuring that
well-known properties of analysis environments are replaced with
realistic values, and that any instrumentation artifacts remain
hidden. For sandboxes implemented using virtual machines, this
can be achieved by scrubbing vendor-specific drivers, processes,
BIOS versions, and other VM-revealing indicators, while more
sophisticated sandboxes move away from emulation-based and
virtualization-based systems towards bare-metal hosts.

We observe that as the fidelity and transparency of dynamic
malware analysis systems improves, malware authors can resort
to other system characteristics that are indicative of artificial
environments. We present a novel class of sandbox evasion
techniques that exploit the ‘“‘wear and tear’’ that inevitably occurs
on real systems as a result of normal use. By moving beyond how
realistic a system looks like, to how realistic its past use looks like,
malware can effectively evade even sandboxes that do not expose
any instrumentation indicators, including bare-metal systems. We
investigate the feasibility of this evasion strategy by conducting
a large-scale study of wear-and-tear artifacts collected from real
user devices and publicly available malware analysis services. The
results of our evaluation are alarming: using simple decision trees
derived from the analyzed data, malware can determine that a
system is an artificial environment and not a real user device
with an accuracy of 92.86%. As a step towards defending against
wear-and-tear malware evasion, we develop statistical models that
capture a system’s age and degree of use, which can be used to
aid sandbox operators in creating system images that exhibit a
realistic wear-and-tear state.

",19.946969662950774,20.598058252427183,312
SP_17_047.txt,14.857976977654367,13.351282881785032,SP,12614,"
Online programming discussion platforms such as
Stack Overflow serve as a rich source of information for software
developers. Available information include vibrant discussions
and oftentimes ready-to-use code snippets. Previous research
identified Stack Overflow as one of the most important information sources developers rely on. Anecdotes report that
software developers copy and paste code snippets from those
information sources for convenience reasons. Such behavior
results in a constant flow of community-provided code snippets
into production software. To date, the impact of this behaviour
on code security is unknown.

We answer this highly important question by quantifying
the proliferation of security-related code snippets from Stack
Overflow in Android applications available on Google Play.
Access to the rich source of information available on Stack
Overflow including ready-to-use code snippets provides huge
benefits for software developers. However, when it comes to
code security there are some caveats to bear in mind: Due
to the complex nature of code security, it is very difficult to
provide ready-to-use and secure solutions for every problem.
Hence, integrating a security-related code snippet from Stack
Overfiow into production software requires caution and expertise.
Unsurprisingly, we observed insecure code snippets being copied
into Android applications millions of users install from Google
Play every day.

To quantitatively evaluate the extent of this observation, we
scanned Stack Overflow for code snippets and evaluated their
security score using a stochastic gradient descent classifier. In
order to identify code reuse in Android applications, we applied
state-of-the-art static analysis. Our results are alarming: 15.4%
of the 1.3 million Android applications we analyzed, contained
security-related code snippets from Stack Overflow. Out of these
97.9% contain at least one insecure code snippet.

",15.988067610193283,14.473694444444444,292
SP_17_048.txt,17.24680085743426,16.06096227277218,SP,17855,"
The X.509 Public-Key Infrastructure has long been
used in the SSL/TLS protocol to achieve authentication. A recent
trend of Internet-of-Things (IoT) systems employing small footprint SSL/TLS libraries for secure communication has further
propelled its prominence. The security guarantees provided by
X.509 hinge on the assumption that the underlying implementation rigorously scrutinizes X.509 certificate chains, and accepts
only the valid ones. Noncompliant implementations of X.509
can potentially lead to attacks and/or interoperability issues. In
the literature, black-box fuzzing has been used to find flaws
in X.509 validation implementations; fuzzing, however, cannot
guarantee coverage and thus severe flaws may remain undetected.
To thoroughly analyze X.509 implementations in small footprint
SSL/TLS libraries, this paper takes the complementary approach
of using symbolic execution.

We observe that symbolic execution, a technique proven to
be effective in finding software implementation flaws, can also
be leveraged to expose noncompliance in X.509 implementations.
Directly applying an off-the-shelf symbolic execution engine on
SSL/TLS libraries is, however, not practical due to the problem
of path explosion. To this end, we propose the use of SymCerts,
which are X.509 certificate chains carefully constructed with
a mixture of symbolic and concrete values. Utilizing SymCerts
and some domain-specific optimizations, we symbolically execute
the certificate chain validation code of each library and extract
path constraints describing its accepting and rejecting certificate
universes. These path constraints help us identify missing checks
in different libraries. For exposing subtle but intricate noncompliance with X.509 standard, we cross-validate the constraints
extracted from different libraries to find further implementation
flaws. Our analysis of 9 small footprint X.509 implementations
has uncovered 48 instances of noncompliance. Findings and
suggestions provided by us have already been incorporated by
developers into newer versions of their libraries.

",17.532861487889196,15.801605975723625,307
SP_17_049.txt,15.016789368414877,13.727626165512799,SP,11666,"
To improve the security of user-chosen Android
screen lock patterns, we propose a novel system-guided pattern
lock scheme called “SysPal’”’ that mandates the use of a small
number of randomly selected points while selecting a pattern.
Users are given the freedom to use those mandated points at any
position.

We conducted a large-scale online study with 1,717 participants to evaluate the security and usability of three SysPal
policies, varying the number of mandatory points that must be
used (upon selecting a pattern) from one to three. Our results
suggest that the two SysPal policies that mandate the use of one
and two points can help users select significantly more secure
patterns compared to the current Android policy: 22.58% and
23.19% fewer patterns were cracked. Those two SysPal policies,
however, did not show any statistically significant inferiority in
pattern recall success rate (the percentage of participants who
correctly recalled their pattern after 24 hours). In our lab study,
we asked participants to install our screen unlock application
on their own Android device, and observed their real-life phone
unlock behaviors for a day. Again, our lab study did not show
any statistically significant difference in memorability for those
two SysPal policies compared to the current Android policy.

",15.903189008614273,16.77485987696514,213
SP_17_050.txt,16.391878672849817,15.253892207267956,SP,16090,"
Current smartphone operating systems regulate application permissions by prompting users on an ask-on-first-use
basis. Prior research has shown that this method is ineffective
because it fails to account for context: the circumstances under
which an application first requests access to data may be vastly
different than the circumstances under which it subsequently
requests access. We performed a longitudinal 131-person field
study to analyze the contextuality behind user privacy decisions
to regulate access to sensitive resources. We built a classifier
to make privacy decisions on the user’s behalf by detecting
when context has changed and, when necessary, inferring privacy
preferences based on the user’s past decisions and behavior.
Our goal is to automatically grant appropriate resource requests
without further user intervention, deny inappropriate requests,
and only prompt the user when the system is uncertain of the
user’s preferences. We show that our approach can accurately
predict users’ privacy decisions 96.8% of the time, which is a
four-fold reduction in error rate compared to current systems.

",17.693802365651003,17.52872781065089,174
SP_17_051.txt,13.15418919139648,11.814148360216976,SP,13358,"
We present the password reset MitM (PRMitM)
attack and show how it can be used to take over user accounts.
The PRMitM attack exploits the similarity of the registration and
password reset processes to launch a man in the middle (MitM)
attack at the application level. The attacker initiates a password
reset process with a website and forwards every challenge to the
victim who either wishes to register in the attacking site or to
access a particular resource on it.

The attack has several variants, including exploitation of a
password reset process that relies on the victim’s mobile phone,
using either SMS or phone call. We evaluated the PRMitM
attacks on Google and Facebook users in several experiments,
and found that their password reset process is vulnerable to
the PRMitM attack. Other websites and some popular mobile
applications are vulnerable as well.

Although solutions seem trivial in some cases, our experiments
show that the straightforward solutions are not as effective as
expected. We designed and evaluated two secure password reset
processes and evaluated them on users of Google and Facebook.
Our results indicate a significant improvement in the security.

Since millions of accounts are currently vulnerable to the
PRMitM attack, we also present a list of recommendations for
implementing and auditing the password reset process.

",14.696529576185021,13.980116279069769,217
SP_17_052.txt,15.134265334995941,13.515225097513135,SP,13026,"
Remote Access Trojans (RATs) give remote attackers interactive control over a compromised machine. Unlike largescale malware such as botnets, a RAT is controlled individually
by a human operator interacting with the compromised machine
remotely. The versatility of RATs makes them attractive to actors
of all levels of sophistication: they’ve been used for espionage,
information theft, voyeurism and extortion. Despite their increasing use, there are still major gaps in our understanding of RATs
and their operators, including motives, intentions, procedures,
and weak points where defenses might be most effective.

In this work we study the use of DarkComet, a popular
commercial RAT. We collected 19,109 samples of DarkComet
malware found in the wild, and in the course of two, severalweek-long experiments, ran as many samples as possible in our
honeypot environment. By monitoring a sample’s behavior in
our system, we are able to reconstruct the sequence of operator
actions, giving us a unique view into operator behavior. We
report on the results of 2,747 interactive sessions captured in
the course of the experiment. During these sessions operators
frequently attempted to interact with victims via remote desktop,
to capture video, audio, and keystrokes, and to exfiltrate files
and credentials. To our knowledge, we are the first large-scale
systematic study of RAT use.

",16.728156217252725,15.304213836477988,217
SP_17_053.txt,14.478812142656036,13.137671870524205,SP,13618,"

Neural networks provide state-of-the-art results for most
machine learning tasks. Unfortunately, neural networks are
vulnerable to adversarial examples: given an input x and any
target classification ¢, it is possible to find a new input 2’
that is similar to x but classified as ¢. This makes it difficult
to apply neural networks in security-critical areas. Defensive
distillation is a recently proposed approach that can take an
arbitrary neural network, and increase its robustness, reducing
the success rate of current attacks’ ability to find adversarial
examples from 95% to 0.5%.

In this paper, we demonstrate that defensive distillation does
not significantly increase the robustness of neural networks
by introducing three new attack algorithms that are successful
on both distilled and undistilled neural networks with 100%
probability. Our attacks are tailored to three distance metrics
used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks
are often much more effective (and never worse). Furthermore,
we propose using high-confidence adversarial examples in
a simple transferability test we show can also be used to
break defensive distillation. We hope our attacks will be used
as a benchmark in future defense attempts to create neural
networks that resist adversarial examples.

",17.122413403193683,16.961153381642514,207
SP_17_054.txt,15.242720756875695,13.560104211266225,SP,14857,"
BulletProof Hosting (BPH) services provide criminal actors with technical infrastructure that is resilient to
complaints of illicit activities, which serves as a basic building
block for streamlining numerous types of attacks. Anecdotal
reports have highlighted an emerging trend of these BPH services
reselling infrastructure from lower end service providers (hosting
ISPs, cloud hosting, and CDNs) instead of from monolithic BPH
providers. This has rendered many of the prior methods of
detecting BPH less effective, since instead of the infrastructure
being highly concentrated within a few malicious Autonomous
Systems (ASes) it is now agile and dispersed across a larger set
of providers that have a mixture of benign and malicious clients.

In this paper, we present the first systematic study on this
new trend of BPH services. By collecting and analyzing a large
amount of data (25 Whois snapshots of the entire IPv4 address
space, 1.5 TB of passive DNS data, and longitudinal data from
several blacklist feeds), we are able to identify a set of new
features that uniquely characterizes BPH on sub-allocations and
are costly to evade. Based upon these features, we train a classifier
for detecting malicious sub-allocated network blocks, achieving
a 98% recall and 1.5% false discovery rates according to our
evaluation. Using a conservatively trained version of our classifier,
we scan the whole IPv4 address space and detect 39K malicious
network blocks. This allows us to perform a large-scale study of
the BPH service ecosystem, which sheds light on this underground
business strategy, including patterns of network blocks being
recycled and malicious clients migrating to different network
blocks, in an effort to evade IP address based blacklisting. Our
study highlights the trend of agile BPH services and points
to potential methods of detecting and mitigating this emerging
threat.

",18.36309006607702,18.113333333333333,298
SP_17_055.txt,14.938383527658218,13.420749689714857,SP,13963,"
The ecosystem of open source software (OSS) has
been growing considerably in size. In addition, code clones - code
fragments that are copied and pasted within or between software
systems - are also proliferating. Although code cloning may
expedite the process of software development, it often critically
affects the security of software because vulnerabilities and bugs
can easily be propagated through code clones. These vulnerable
code clones are increasing in conjunction with the growth of OSS,
potentially contaminating many systems. Although researchers
have attempted to detect code clones for decades, most of these
attempts fail to scale to the size of the ever-growing OSS code
base. The lack of scalability prevents software developers from
readily managing code clones and associated vulnerabilities.
Moreover, most existing clone detection techniques focus overly
on merely detecting clones and this impairs their ability to
accurately find “vulnerable” clones.

In this paper, we propose VUDDY, an approach for the scalable
detection of vulnerable code clones, which is capable of detecting
security vulnerabilities in large software programs efficiently
and accurately. Its extreme scalability is achieved by leveraging
function-level granularity and a length-filtering technique that
reduces the number of signature comparisons. This efficient
design enables VUDDY to preprocess a billion lines of code in
14 hour and 17 minutes, after which it requires a few seconds to
identify code clones. In addition, we designed a security-aware
ion technique that renders VUDDY resilient to common
modifications in cloned code, while preserving the vulnerable
conditions even after the abstraction is applied. This extends the
scope of VUDDY to identifying variants of known vulnerabilities,
with high accuracy. In this study, we describe its principles
and evaluate its efficacy and effectiveness by comparing it
with existing mechanisms and presenting the vulnerabilities it
detected. VUDDY outperformed four state-of-the-art code clone
detection techniques in terms of both scalability and accuracy,
and proved its effectiveness by detecting zero-day vulnerabilities
in widely used software systems, such as Apache HTTPD and
Ubuntu OS Distribution.

",17.205460502629933,16.36510725010725,334
SP_17_056.txt,15.253858892675822,13.876292018814219,SP,15746,"
TLS 1.3 is the next version of the Transport Layer
Security (TLS) protocol. Its clean-slate design is a reaction both
to the increasing demand for low-latency HTTPS connections
and to a series of recent high-profile attacks on TLS. The
hope is that a fresh protocol with modern cryptography will
prevent legacy problems; the danger is that it will expose
new kinds of attacks, or reintroduce old flaws that were fixed
in previous versions of TLS. After 18 drafts, the protocol is
nearing completion, and the working group has appealed to
researchers to analyze the protocol before publication. This
paper responds by presenting a comprehensive analysis of the
TLS 1.3 Draft-18 protocol.

We seek to answer three questions that have not been fully
addressed in previous work on TLS 1.3: (1) Does TLS 1.3
prevent well-known attacks on TLS 1.2, such as Logjam or the
Triple Handshake, even if it is run in parallel with TLS 1.2?
(2) Can we mechanically verify the computational security of
TLS 1.3 under standard (strong) assumptions on its cryptographic primitives? (3) How can we extend the guarantees of
the TLS 1.3 protocol to the details of its implementations?

To answer these questions, we propose a methodology
for developing verified symbolic and computational models
of TLS 1.3 hand-in-hand with a high-assurance reference
implementation of the protocol. We present symbolic ProVerif
models for various intermediate versions of TLS 1.3 and
evaluate them against a rich class of attacks to reconstruct
both known and previously unpublished vulnerabilities that
influenced the current design of the protocol. We present
a computational CryptoVerif model for TLS 1.3 Draft-18
and prove its security. We present RefTLS, an interoperable
implementation of TLS 1.0-1.3 and automatically analyze its
protocol core by extracting a ProVerif model from its typed
JavaScript code.

",16.32212239822248,14.600098039215688,320
SP_17_057.txt,14.099674790565548,12.439385986129576,SP,12527,"
Side channel attacks have been used to extract
critical data such as encryption keys and confidential user data
in a variety of adversarial settings. In practice, this threat
is addressed by adhering to a constant-time programming
discipline, which imposes strict constraints on the way in which
programs are written. This introduces an additional hurdle for
programmers faced with the already difficult task of writing
secure code, highlighting the need for solutions that give the
same source-level guarantees while supporting more natural
programming models.

We propose a novel type system for verifying that programs correctly implement constant-resource behavior. Our
type system extends recent work on automatic amortized resource analysis (AARA), a set of techniques that automatically
derive provable upper bounds on the resource consumption of
programs. We devise new techniques that build on the potential
method to achieve compositionality, precision, and automation.

A strict global requirement that a program always maintains
constant resource usage is too restrictive for most practical
applications. It is sufficient to require that the program’s
resource behavior remain constant with respect to an attacker
who is only allowed to observe part of the program’s state
and behavior. To account for this, our type system incorporates
information flow tracking into its resource analysis. This allows
our system to certify programs that need to violate the constanttime requirement in certain cases, as long as doing so does
not leak confidential information to attackers. We formalize
this guarantee by defining a new notion of resource-aware
noninterference, and prove that our system enforces it.

Finally, we show how our type inference algorithm can be
used to synthesize a constant-time implementation from one
that cannot be verified as secure, effectively repairing insecure
programs automatically. We also show how a second novel
AARA system that computes lower bounds on resource usage
can be used to derive quantitative bounds on the amount of
information that a program leaks through its resource use.
We implemented each of these systems in Resource Aware
ML, and show that it can be applied to verify constant-time
behavior in a number of applications including encryption
and decryption routines, database queries, and other resourceaware functionality.

Keywords-Language-based security; timing channels; information flow; resource analysis; static analysis

",16.647925096878797,15.936418230563007,376
SP_17_058.txt,17.515815610334123,16.309848621270415,SP,11476,"
In recent years, researchers have shown that unwanted web tracking is on the rise, as advertisers are trying to
capitalize on users’ online activity, using increasingly intrusive
and sophisticated techniques. Among these, browser fingerprinting has received the most attention since it allows trackers to
uniquely identify users despite the clearing of cookies and the
use of a browser’s private mode.

In this paper, we investigate and quantify the fingerprintability
of browser extensions, such as, AdBlock and Ghostery. We
show that an extension’s organic activity in a page’s DOM can
be used to infer its presence, and develop XHOUND, the first
fully automated system for fingerprinting browser extensions. By
applying XHOUND to the 10,000 most popular Google Chrome
extensions, we find that a significant fraction of popular browser
extensions are fingerprintable and could thus be used to supplement existing fingerprinting methods. Moreover, by surveying
the installed extensions of 854 users, we discover that many users
tend to install different sets of fingerprintable browser extensions
and could thus be uniquely, or near-uniquely identifiable by
extension-based fingerprinting. We use XHOUND’s results to
build a proof-of-concept extension-fingerprinting script and show
that trackers can fingerprint tens of extensions in just a few
seconds. Finally, we describe why the fingerprinting of extensions
is more intrusive than the fingerprinting of other browser and
system properties, and sketch two different approaches towards
defending against extension-based fingerprinting.

",18.903936251131103,17.65254273504274,240
SP_17_059.txt,15.77547851851303,14.014128835575587,SP,13334,"
Developing a remote exploit is not easy. It requires
a comprehensive understanding of a vulnerability and delicate
techniques to bypass defense mechanisms. As a result, attackers
may prefer to reuse an existing exploit and make necessary
changes over developing a new exploit from scratch. One such
adaptation is the replacement of the original shellcode (i.e., the
attacker-injected code that is executed as the final step of the
exploit) in the original exploit with a replacement shellcode,
resulting in a modified exploit that carries out the actions desired
by the attacker as opposed to the original exploit author. We call
this a shellcode transplant.

Current automated shellcode placement methods are insufficient because they over-constrain the replacement shellcode, and
so cannot be used to achieve shellcode transplant. For example,
these systems consider the shellcode as an integrated memory
chunk, and require that the execution path of the modified exploit
must be same as the original one. To resolve these issues, we
present ShellSwap, a system that uses symbolic tracing, with a
combination of shellcode layout remediation and path kneading to
achieve shellcode transplant. We evaluated the ShellSwap system
on a combination of 20 exploits and 5 pieces of shellcode that are
independently developed and different from the original exploit.
Among the 100 test cases, our system successfully generated
88% of the exploits.

",16.647925096878797,15.392324324324324,224
SP_17_060.txt,14.451079883609072,12.783394057119313,SP,14511,"
Cloud database systems such as Amazon RDS or
Google Cloud SQL enable the outsourcing of a large database to
a server who then responds to SQL queries. A natural problem
here is to efficiently verify the correctness of responses returned
by the (untrusted) server. In this paper we present vSQL, a
novel cryptographic protocol for publicly verifiable SQL queries
on dynamic databases. At a high level, our construction relies on
two extensions of the CMT interactive-proof protocol [Cormode
et al., 2012]: (i) supporting outsourced input via the use of a
polynomial-delegation protocol with succinct proofs, and (ii) supporting auxiliary input (i.e., non-deterministic computation) efficiently. Compared to previous verifiable-computation systems
based on interactive proofs, our construction has verification cost
polylogarithmic in the auxiliary input (which for SQL queries
can be as large as the database) rather than linear.

In order to evaluate the performance and expressiveness of
our scheme, we tested it on SQL queries based on the TPC-H
benchmark on a database with 6 x 10° rows and 13 columns.
The server overhead in our scheme (which is typically the
main bottleneck) is up to 120x lower than previous approaches
based on succinct arguments of knowledge (SNARKs), and
moreover we avoid the need for query-dependent pre-processing
which is required by optimized SNARK-based schemes. In our
construction, the server/client time and the communication cost
are comparable to, and sometimes smaller than, those of existing
customized solutions which only support specific queries.

",19.78447435784618,18.498300000000004,251
SPAA_17_001.txt,13.944706459483431,11.83372237784003,SPAA,4749,"

We show that the basic semidefinite programming relaxation value
of any constraint satisfaction problem can be computed in NC;
that is, in parallel polylogarithmic time and polynomial work. As
a complexity-theoretic consequence we get that MIP1[k,c,s] ¢
PSPACE provided s/c < (.62 — o(1))k/2, resolving a question
of Austrin, Hastad, and Pass. Here MIP 1[k,c, s] is the class of languages decidable with completeness c and soundness s by an interactive proof system with k provers, each constrained to communicate
just 1 bit.
",17.122413403193683,16.250434782608696,91
SPAA_17_002.txt,12.158586141664845,9.382638181818184,SPAA,10039,"
 into the cache, paying a fixed cost, while a request to a cached item

We initiate the study of a natural and practically relevant new
variant of online caching where the to-be-cached items can have
dependencies. We assume that the universe is a tree T and items
are tree nodes; we require that if a node v is cached then the whole
subtree T(v) rooted at v is cached as well. This theoretical problem
finds an immediate application in the context of forwarding table
optimization in IP routing and software-defined networks.

We present an elegant online deterministic algorithm TC for
this problem, and rigorously prove that its competitive ratio is
O(HEIGHT(T) - Koni /(KonL — Koptr + 1)), where konz and kopr
denote the cache sizes of an online and the optimal offline algorithm,
respectively. The result is optimal up to a factor of O(HEIGHT(T)).
",15.381575749822971,15.177403973509936,151
SPAA_17_003.txt,13.746643868522511,13.220012444780192,SPAA,8652,"
In Web applications served by datacenter nowadays, the incast
congestion at the front-end server seriously degrades the data request latency performance due to the vast data transmissions from
a large number data servers for a data request in a short time.
Previous incast congestion control methods usually consider the
direct data transmissions from data servers to the front-end server,
which makes it difficult to control the sending speed or adjust workloads due to the transient transmission of only a few data objects
from each data server. In this paper, we propose a Swarm-based
Incast Congestion Control (SICC) system. SICC forms all target
data servers of one request in the same rack into a swarm. In each
swarm, a data server (called hub) is selected to forward all data
objects to the front-end server, so that the number of data servers
concurrently connected to the front-end server is reduced, which
avoids the incast congestion. Also, the continuous data transmission
from hubs to the front-end server facilitates the development of
other strategies to further control the incast congestion. To fully utilize the bandwidth, SICC uses a two-level data transmission speed
control method to adjust the data transmission speeds of hubs. A
query redirection method further reduces the request latency by
balancing the transmission remaining times between hubs. Our experiments in simulation and on a real cluster demonstrate that SICC
outperforms other incast control methods in improving throughput
and reducing the data request latency.
",15.470042427545799,15.378440860215054,249
SPAA_17_004.txt,11.367040862066833,8.269684981625836,SPAA,11512,"

We formulate and study an optimization problem that arises in
the energy management of data centers and, more generally, multiprocessor environments. Data centers host a large number of
heterogeneous servers. Each server has an active state and several
standby/sleep states with individual power consumption rates. The
demand for computing capacity varies over time. Idle servers may
be transitioned to low-power modes so as to rightsize the pool of
active servers. The goal is to find a state transition schedule for
the servers that minimizes the total energy consumed. On a small
scale the same problem arises in multi-core architectures with heterogeneous processors on a chip. One has to determine active and
idle periods for the cores so as to guarantee a certain service and
minimize the consumed energy.

For this power/capacity management problem, we develop two
main results. We use the terminology of the data center setting.
First, we investigate the scenario that each server has two states,
ie. an active state and a sleep state. We show that an optimal
solution, minimizing energy consumption, can be computed in
polynomial time by a combinatorial algorithm. The algorithm resorts to a single-commodity min-cost flow computation. Second,
we study the general scenario that each server has an active state
and multiple standby/sleep states. We devise a t-approximation algorithm that relies on a two-commodity min-cost flow computation.
Here tr is the number of different server types. A data center has a
large collection of machines but only a relatively small number of
different server architectures. Moreover, in the optimization one
can assign servers with comparable energy consumption to the
same class. Technically, both of our algorithms involve non-trivial
flow modification procedures. In particular, given a fractional twocommodity flow, our algorithm executes advanced rounding and
flow packing routines.
",14.043286433261986,12.216516393442621,306
SPAA_17_005.txt,14.744731972837496,12.519604078187477,SPAA,9770,"

Many large-scale scientific computations require eigenvalue solvers
in a scaling regime where efficiency is limited by data movement.
We introduce a parallel algorithm for computing the eigenvalues
of a dense symmetric matrix, which performs asymptotically less
communication than previously known approaches. We provide
analysis in the Bulk Synchronous Parallel (BSP) model with additional consideration for communication between a local memory and cache. Given sufficient memory to store c copies of the
symmetric matrix, our algorithm requires @(-c) less interprocessor communication than previously known algorithms, for any
c < p'/3 when using p processors. The algorithm first reduces the
dense symmetric matrix to a banded matrix with the same eigenvalues. Subsequently, the algorithm employs successive reduction to
O(log p) thinner banded matrices. We employ two new parallel algorithms that achieve lower communication costs for the full-to-band
and band-to-band reductions. Both of these algorithms leverage a
novel OR factorization algorithm for rectangular matrices.

",17.693802365651003,16.53401898734177,158
SPAA_17_006.txt,12.668636303438202,9.794401765998341,SPAA,12936,"

In this paper, we study an online Flexible Job Scheduling (FJS)
problem. The input of the problem is a set of jobs, each having an
arrival time, a starting deadline and a processing length. Each job
has to be started by the scheduler between its arrival and its starting
deadline. Once started, the job runs for a period of the processing
length without interruption. The target is to minimize the span of
all the jobs — the time duration in which at least one job is running.
We study online FJS under both the non-clairvoyant and clairvoyant
settings. In the non-clairvoyant setting, the processing length of
each job is not known for scheduling purposes. We first establish
a lower bound of : on the competitive ratio of any deterministic
online scheduler, where p is the max/min job processing length ratio.
Then, we propose two O()-competitive schedulers: Batch and
Batch+. The Batch+ scheduler is proved to have a tight competitive
ratio of (uz + 1). In the clairvoyant setting, the processing length
of each job is known at its arrival and can be used for scheduling purposes. We establish a lower bound of “>— on the competitive
ratio of any deterministic online scheduler, and propose two O(1)competitive schedulers: Classify-by-Duration Batch+ and Profit. The
Profit scheduler can achieve a competitive ratio of 4+2V2. Our work
lays the foundation for extending several online job scheduling
problems in cloud and energy-efficient computing to jobs that have
laxity in starting.

",14.348710955821954,10.577846025569766,252
SPAA_17_007.txt,13.311480706831134,10.768830384555454,SPAA,10212,"

Leader election is one of the basic problems in distributed computing. This is a symmetry breaking problem: all nodes of a network
must agree on a single node, called the leader. If the nodes of the
network have distinct labels, then such an agreement means that all
nodes have to output the label of the elected leader. For anonymous
networks, the task of leader election is formulated as follows: every
node v of the network must output a simple path, which is coded
as a sequence of port numbers, such that all these paths end at
a common node, the leader. In this paper, we study deterministic
leader election in arbitrary anonymous networks.

It is well known that deterministic leader election is impossible
in some networks, regardless of the allocated amount of time, even
if nodes know the map of the network. This is due to possible
symmetries in it. However, even in networks in which it is possible
to elect a leader knowing the map, the task may be still impossible
without any knowledge, regardless of the allocated time. On the
other hand, for any network in which leader election is possible
knowing the map, there is a minimum time, called the election
index, in which this can be done. Informally, the election index of
a network is the minimum depth at which views of all nodes are
distinct. Our aim is to establish tradeoffs between the allocated
time t and the amount of information that has to be given a priori
to the nodes to enable leader election in time r in all networks
for which leader election in this time is at all possible. Following
the framework of algorithms with advice, this information (a single
binary string) is provided to all nodes at the start by an oracle
knowing the entire network. The length of this string is called the
size of advice. For a given time 7 allocated to leader election, we give
upper and lower bounds on the minimum size of advice sufficient
to perform leader election in time t.

We focus on the two sides of the time spectrum. For the smallest
possible time, which is the election index of the network, we show


that the minimum size of advice is linear in the size n of the network,
up to polylogarithmic factors. On the other hand, we consider
large values of time: larger than the diameter D by a summand,
respectively, linear, polynomial, and exponential in the election
index; for these values, we prove tight bounds on the minimum
size of advice, up to multiplicative constants. We also show that
constant advice is not sufficient for leader election in all graphs,
regardless of the allocated time.
",14.474971422386794,12.95391832229581,454
SPAA_17_008.txt,12.459604104472351,10.338640031600928,SPAA,7035,"

We present improved bounds on the cover time of the coalescingbranching random walk process COBRA. The COBRA process, introduced in [Dutta et al, SPAA 2013], can be viewed as spreading a
single item of information throughout an undirected graph in synchronised rounds. In each round, each vertex which has received
the information in the previous round (possibly simultaneously
from more than one neighbour and possibly not for the first time),
‘pushes’ the information to b randomly selected neighbours. The
COBRA process is typically studied for integer branching rates
b > 2 (with the case b = 1 corresponding to a random walk). The
aim of the process is to propagate the information quickly, but with
a limited number of transmissions per vertex per round.

The cover time of COBRA is defined as the expected number of
rounds until each vertex has received the information at least once.
Our main results are a bound of O(m + (dmax)? log n) = O(n? log n)
on the COBRA cover time for an arbitrary connected graph with
n vertices, m edges and the maximum vertex degree dmax, and a
bound of O((r?+r/(1—A)) log n) for r-regular connected graphs with
the second eigenvalue A. Our bounds improve the O(n!/* log n)
and O((r*/¢2) log” n) bounds shown in [Mitzenmacher et al., SPAA
2016], where ¢ is the conductance of the graph, and complement
the O((1/(1 — A))? logn) bound shown in [Cooper et al., PODC
2016]. We obtain our bounds by analysing the process called Biased
Infection with Persistent Source (BIPS), which was introduced in
[Cooper et al., PODC 2016] as a dual process for COBRA .
",12.28987398476788,10.06998746867168,279
SPAA_17_009.txt,12.461060037856363,9.759925253632108,SPAA,9609,"

We investigate scheduling algorithms for distributed transactional
memory systems where transactions residing at nodes of a communication graph operate on shared, mobile objects. A transaction
requests the objects it needs, executes once those objects have been
assembled, and then possibly forwards those objects to other waiting transactions. Minimizing execution time in this model is known
to be NP-hard for arbitrary communication graphs, and also hard
to approximate within any factor smaller than the size of the graph.
Nevertheless, networks on chips, multi-core systems, and clusters
are not arbitrary. Here, we explore efficient execution schedules
in specialized graphs likely to arise in practice: Clique, Line, Grid,
Cluster, Hypercube, Butterfly, and Star. In most cases, when individual transactions request k objects, we obtain solutions close to a
factor O(k) from optimal, yielding near-optimal solutions for constant k. These execution times approximate the TSP tour lengths of
the objects in the graph. We show that for general networks, even
for two objects (k = 2), it is impossible to obtain execution time
close to the objects’ optimal TSP tour lengths, which is why it is
useful to consider more realistic network models. To our knowledge, this is the first attempt to obtain provably fast schedules for
distributed transactional memory.
",15.903189008614273,14.412857142857145,210
SPAA_17_010.txt,16.458089807249756,14.432906040835146,SPAA,11723,"

Interactive services send redundant requests to multiple different
replicas to meet stringent tail latency requirements. These additional (reissue) requests mitigate the impact of non-deterministic
delays within the system and thus increase the probability of receiving an on-time response.

There are two existing approaches of using reissue requests to
reduce tail latency. (1) Reissue requests immediately to one or more
replicas, which multiplies the load and runs the risk of overloading
the system. (2) Reissue requests if not completed after a fixed delay.
The delay helps to bound the number of extra reissue requests, but
it also reduces the chance for those requests to respond before a
tail latency target.

We introduce a new family of reissue policies, Single-Time
/ Random (SINGLER), that reissue requests after a delay d with
probability g. SINGLER employs randomness to bound the reissue
rate, while allowing requests to be reissued early enough so they
have sufficient time to respond, exploiting the benefits of both
immediate and delayed reissue of prior work. We formally prove,
within a simplified analytical model, that SINGLER is optimal even
when compared to more complex policies that reissue multiple
times.

To use SINGLER for interactive services, we provide efficient algorithms for calculating optimal reissue delay and probability from
response time logs through data-driven approach. We apply iterative adaptation for systems with load-dependent queuing delays.
The key advantage of this data-driven approach is its wide applicability and effectiveness to systems with various design choices and
workload properties.

We evaluated SIncLER policies thoroughly. We use simulation
to illustrate its internals and demonstrate its robustness to a wide
range of workloads. We conduct system experiments on the Redis key-value store and Lucene search server. The results show
that for utilizations ranging from 40-60%, SINGLER reduces the
99th-percentile latency of Redis by 30-70% by reissuing only 2% of
requests, and the 99th-percentile latency of Lucene by 15-25% by
reissuing 1% only.

",16.678067442207542,14.501114296636086,328
SPAA_17_011.txt,12.620693757574305,10.193403656850375,SPAA,8726,"

Many modern datacenter applications involve large-scale computations composed of multiple data flows that need to be completed
over a shared set of distributed resources. Such a computation
completes when all of its flows complete. A useful abstraction for
modeling such scenarios is a coflow, which is a collection of flows
(e.g., tasks, packets, data transmissions) that all share the same
performance goal.

In this paper, we present the first approximation algorithms for
scheduling coflows over general network topologies with the objective of minimizing total weighted completion time. We consider two
different models for coflows based on the nature of individual flows:
circuits, and packets. We design constant-factor polynomial-time
approximation algorithms for scheduling packet-based coflows with
or without given flow paths, and circuit-based coflows with given
flow paths. Furthermore, we give an O(log n/log log n)-approximation
polynomial time algorithm for scheduling circuit-based coflows
without given flow paths (here n is the number of network edges).

We obtain our results by developing a general framework for
coflow schedules, based on interval-indexed linear programs, which
may extend to other coflow models and objective functions and may
also yield improved approximation bounds for specific network scenarios. We also present an experimental evaluation of our approach
for circuit-based coflows that show a performance improvement of
at least %22 on average over competing heuristics.

",16.45884130781739,16.0783114992722,231
SPAA_17_012.txt,12.716812326660733,10.098545393554161,SPAA,9807,"

Recent years have witnessed an increasing popularity of algorithm
design for distributed data, largely due to the fact that massive
datasets are often collected and stored in different locations. In the
distributed setting communication typically dominates the query
processing time. Thus it becomes crucial to design communication
efficient algorithms for queries on distributed data. Simultaneously,
it has been widely recognized that partial optimizations, where
we are allowed to disregard a small part of the data, provide us
significantly better solutions. The motivation for disregarded points
often arise from noise and other phenomena that are pervasive in
large data scenarios.

In this paper we focus on partial clustering problems, k-center,
k-median and k-means, in the distributed model, and provide algorithms with communication sublinear of the input size. As a consequence we develop the first algorithms for the partial k-median and
means objectives that run in subquadratic running time. We also
initiate the study of distributed algorithms for clustering uncertain
data, where each data point can possibly fall into multiple locations
under certain probability distribution.

",17.410965686947208,16.505416666666665,178
SPAA_17_013.txt,13.89102880384122,12.717701058340158,SPAA,6504,"

Graph clustering is a fundamental computational problem with
a number of applications in algorithm design, machine learning,
data mining, and analysis of social networks. Over the past decades,
researchers have proposed a number of algorithmic design methods
for graph clustering. However, most of these methods are based
on complicated spectral techniques or convex optimisation, and
cannot be applied directly for clustering many networks that occur
in practice, whose information is often collected on different sites.
Designing a simple and distributed clustering algorithm is of great
interest, and has wide applications for processing big datasets.

In this paper we present a simple and distributed algorithm
for graph clustering: for a wide class of graphs that are characterised by a strong cluster-structure, our algorithm finishes in a
poly-logarithmic number of rounds, and recovers a partition of the
graph close to an optimal partition. The main component of our
algorithm is an application of the random matching model of load
balancing, which is a fundamental protocol in distributed computing and has been extensively studied in the past 20 years. Hence, our
result highlights an intrinsic and interesting connection between
graph clustering and load balancing.

At a technical level, we present a purely algebraic result characterising the early behaviours of load balancing processes for graphs
exhibiting a cluster-structure. We believe that this result can be
further applied to analyse other gossip processes, such as rumour
spreading and averaging processes.
",18.001758247042904,17.08383753501401,239
SPAA_17_014.txt,12.111977943835491,10.06100166284481,SPAA,10198,"

In this paper we focus on the Clairvoyant Dynamic Bin Packing
(DBP) problem, which extends the classical online bin packing
problem in that items arrive and depart over time and the departure
time of an item is known upon its arrival. The problem naturally
arises when handling cloud-based networks. We focus specifically
on the MinUsageTime cost function which aims to minimize the
overall usage time of all bins that are opened during the packing
og p

log log p
is defined as the ratio between the maximal and minimal durations

of all items. We improve the upper bound by giving an O(./log y)competitive algorithm. We then provide a matching lower bound
of Q(,/log yz) on the competitive ratio of any online algorithm, thus
closing the gap with regards to this problem. We then focus on
what we call the class of aligned inputs and give a O(log log :)competitive algorithm for this case, beating the lower bound of
the general case by an exponential factor. Surprisingly enough,
the analysis of our algorithm that we present, is closely related to
various properties of binary strings.

process. Earlier work has shown a O(

 

) upper bound where pz

CCS CONCEPTS

*Theory of computation — Scheduling algorithms; Online
algorithms;

KEYWORDS
Online Algorithms; Dynamic Bin Packing; Clairvoyant Setting;
Competitive Ratio; Analysis of Algorithms

ACM Reference format:

Yossi Azar and Danny Vainstein. 2017. Tight Bounds for Clairvoyant Dynamic Bin Packing. In Proceedings of SPAA’17, July 24-26, 2017, Washington,
DC, USA, , 10 pages.

DOL: http://dx.doi.org/10.1145/3087556.3087570

",14.90622815163357,13.29177165354331,262
SPAA_17_015.txt,13.010974588893799,10.829697665105964,SPAA,7721,"

Distributed property testing in networks has been introduced
by Brakerski and Patt-Shamir (2011), with the objective of
detecting the presence of large dense sub-networks in a distributed manner. Recently, Censor-Hillel et al. (2016) have
shown how to detect 3-cycles in a constant number of rounds
by a distributed algorithm. In a follow up work, Fraigniaud
et al. (2016) have shown how to detect 4-cycles in a constant
number of rounds as well. However, the techniques in these
latter works were shown not to generalize to larger cycles Cy
with k > 5. In this paper, we completely settle the problem
of cycle detection, by establishing the following result. For
every k > 3, there exists a distributed property testing algorithm for C,-freeness, performing in a constant number of
rounds. All these results hold in the classical CONGEST model
for distributed network computing. Our algorithm is 1-sided
error. Its round-complexity is O(1/e) where e € (0,1) is the
property testing parameter measuring the gap between legal
and illegal instances.

",13.731508374201272,11.613220973782774,177
SPAA_17_016.txt,13.479272038954477,11.197464409628193,SPAA,13413,"

We analyze the caching overhead incurred by a class of multithreaded algorithms when scheduled by an arbitrary scheduler.
We obtain bounds that match or improve upon the well-known
O(Q + S- (M/B)) caching cost for the randomized work stealing
(RWS) scheduler, where S is the number of steals, Q is the sequential
caching cost, and M and B are the cache size and block (or cache
line) size respectively.
",15.903189008614273,15.822222222222226,72
SPAA_17_017.txt,12.109541578739464,9.453285626989999,SPAA,10148,"

In sensitive applications, machines need to be periodically calibrated to ensure that they run to high standards. Creating an efficient schedule on these machines requires attention to two metrics:
ensuring good throughput of the jobs, and ensuring that not too
much cost is spent on machine calibration.

In this paper we examine flow time as a metric for scheduling
with calibrations. While previous papers guaranteed that jobs would
meet a certain deadline, we relax that constraint to a tradeoff: we
want to balance how long the average job waits with how many
costly calibrations we need to perform.

One advantage of this metric is that it allows for online schedules
(where an algorithm is unaware of a job until it arrives). Thus we
give two types of results. We give an efficient offline algorithm
which gives the optimal schedule on a single machine for a set
of jobs which are known ahead of time. We also give online algorithms which adapt to jobs as they come. Our online algorithms
are constant competitive for unweighted jobs on single or multiple
machines, and constant-competitive for weighted jobs on a single
machine.

",13.5591,11.771754385964915,191
SPAA_17_018.txt,13.78413511481179,12.028023838270155,SPAA,9633,"

We study the problem of efficiently optimizing submodular functions under cardinality constraints in distributed setting. Recently,
several distributed algorithms for this problem have been introduced which either achieve a sub-optimal solution or they run
in super-constant number of rounds of computation. Unlike previous work, we aim to design distributed algorithms in multiple
rounds with almost optimal approximation guarantees at the cost
of outputting a larger number of elements. Toward this goal, we
present a distributed algorithm that, for any e > 0 and any con
stant r, outputs a set S of O(rk/ er) items in r rounds, and achieves
a (1 — €)-approximation of the value of the optimum set with k
items. This is the first distributed algorithm that achieves an approximation factor of (1 — e) running in less than log 1 number of
rounds. We also prove a hardness result showing that the output
of any 1 — € approximation distributed algorithm limited to one
distributed round should have at least 2(k/e) items. In light of this
hardness result, our distributed algorithm in one round, r = 1, is
asymptotically tight in terms of the output size. We support the
theoretical guarantees with an extensive empirical study of our
algorithm showing that achieving almost optimum solutions is
indeed possible in a few rounds for large-scale real datasets.

",16.975882523387877,16.40897222222222,222
SPAA_17_019.txt,13.76565470535455,11.65233296419343,SPAA,5733,"

We introduce the mobile server problem, inspired by current trends
to move computational tasks from cloud structures to multiple
devices close to the end user. An example for this are embedded systems in autonomous cars that communicate in order to coordinate
their actions.

Our model is a variant of the classical Page Migration Problem.
More formally, we consider a mobile server holding a data page. The
server can move in the Euclidean space (of arbitrary dimension).
In every round, requests for data items from the page pop up at
arbitrary points in the space. The requests are served, each at a cost
of the distance from the requesting point and the server, and the
mobile server may move, at a cost D times the distance traveled for
some constant D. We assume a maximum distance m the server is
allowed to move per round.

We show that no online algorithm can achieve a competitive
ratio independent of the length of the input sequence in this setting.
Hence we augment the maximum movement distance of the online
algorithms to (1 + 6) times the maximum distance of the offline
solution. We provide a deterministic algorithm which is simple
to describe and works for multiple variants of our problem. The
algorithm achieves almost tight competitive ratios independent of
the length of the input sequence.

KEYWORDS

page migration; online algorithms; competitive analysis; resource
augmentation

ACM Reference format:

Bjérn Feldkord and Friedhelm Meyer auf der Heide. 2017. The Mobile Server
Problem. In Proceedings of SPAA °17, Washington, DC, USA, July 24-26, 2017,
7 pages.

https://doi.org/10.1145/3087556.3087575

*This work was partially supported by the German Research Foundation (DFG) within
the Collaborative Research Centre “On-The-Fly Computing” (SFB 901).

 

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.

SPAA 717, July 24-26, 2017, Washington, DC, USA

© 2017 Association for Computing Machinery.

ACM ISBN 978-1-4503-4593-4/17/07. ..$15.00
https://doiorg/10.1145/3087556.3087575

313

Friedhelm Meyer auf der Heide
Heinz Nixdorf Institut & Computer Science Dept.,
Paderborn University
Furstenallee 11
Paderborn, Germany
fmadh@upb.de

",14.130154551723667,11.543021561771564,439
SPAA_17_020.txt,13.475747884409046,11.0307595329235,SPAA,8696,"

Depth first search (DFS) tree is a fundamental data structure for
solving various graph problems. The classical algorithm [37] for
building a DFS tree requires O(n + n) time for a given undirected
graph G having n vertices and m edges. Recently, Baswana et al. [5]
presented a simple algorithm for updating the DFS tree of an undirected graph after an edge/vertex update in O(n) | time. However, their algorithm is strictly sequential. We present an algorithm
achieving similar bounds that can be easily adopted to the parallel
environment.

In the parallel environment, a DFS tree can be computed from
scratch using O(m) processors in expected O(1) time [2] on an
EREW PRAM, whereas the best deterministic algorithm takes O(-/n)
time [2, 17] on a CRCW PRAM. Our algorithm can be used to develop optimal time (upto poly log n factors) deterministic parallel
algorithms for maintaining fully dynamic DFS and fault tolerant
DFS of an undirected graph.

(1) Parallel Fully Dynamic DFS:
Given any arbitrary online sequence of vertex or edge
updates, we can maintain a DFS tree of an undirected graph
in O(1) time per update using m processors on an EREW
PRAM.
Parallel Fault tolerant DFS:
An undirected graph can be preprocessed to build a data
structure of size O(m), such that for any set of k updates
(where k is constant) in the graph, a DFS tree of the updated
graph can be computed in O(1) time using n processors on
an EREW PRAM. For constant k, this is also work optimal
(upto poly log n factors).

(2)

Moreover, our fully dynamic DFS algorithm provides, in a seamless manner, nearly optimal (upto poly log n factors) algorithms for
maintaining a DFS tree in the semi-streaming environment and
a restricted distributed model. These are the first parallel, semistreaming and distributed algorithms for maintaining a DFS tree in
the dynamic setting.
",16.728156217252725,14.810403726708078,321
SPAA_17_021.txt,13.299539793500593,11.188360334533751,SPAA,9140,"

We consider a problem of routing on directed paths and
trees to a single destination, with rate-limited, adversarial
traffic. In particular, we focus on local buffer management
algorithms that ensure no packet loss, while minimizing the
size of the required buffers.

While a centralized algorithm for the problem that uses
constant-sized buffers has been recently shown [21], there is
no known local algorithm that achieves a sub-linear buffer size.
In this paper we show tight bounds for the maximum buffer
size needed by ¢-local algorithms for information gathering
on directed paths and trees, where an algorithm is called
£-local if the decision made by each node v depends only on
the sizes of the buffers at most £ hops away from v.

We show three main results:

e a lower bound of 2(clogn/é) for all @local algorithms on
both directed and undirected paths, where c is an upper
bound on the link capacity and injection rate.

e asurprisingly simple 1-local algorithm for directed paths
that uses buffers of size O(logn), when c = 1.

e a natural 2-local extension of this algorithm to directed
trees, for c= 1, with the same asymptotic bound.

Our O(log 7) lower bound is significantly lower than the
Q(n) lower bound for greedy algorithms, and perhaps surprisingly, there is a matching upper bound. The algorithm that
achieves it can be summarized in two lines: If the size of your
buffer is odd, forward a message if your successor’s buffer
size is equal or lower. If your buffer size is even, forward a
message only if your successor’s buffer size is strictly lower.
For trees, a simple arbitration between siblings is added.
",15.616094167265928,15.837831603229528,287
SPAA_17_022.txt,12.758008452781535,10.125178881462357,SPAA,9052,"

We consider a scheduling problem on m identical processors sharing an arbitrarily divisible resource. In addition to assigning jobs to
processors, the scheduler must distribute the resource among the
processors (e.g., for three processors in shares of 20%, 15%, and 65%)
and adjust this distribution over time. Each job j comes with a size
pj € Randa resource requirement r; > 0. Jobs do not benefit when
receiving a share larger than 7; of the resource. But providing them
with a fraction of the resource requirement causes a linear decrease
in the processing efficiency. We seek a (non-preemptive) job and
resource assignment minimizing the makespan.

Our main result is an efficient approximation algorithm which
achieves an approximation ratio of 2+ 1/(m-—2). It can be improved
to an (asymptotic) ratio of 1+1/(m-—1) if all jobs have unit size. Our
algorithms also imply new results for a well-known bin packing
problem with splittable items and a restricted number of allowed
item parts per bin.

Based upon the above solution, we also derive an approximation algorithm with similar guarantees for a setting in which we
introduce so-called tasks each containing several jobs and where
we are interested in the average completion time of tasks (a task is
completed when all its jobs are completed).
",15.774802946060372,12.944837104072402,219
SPAA_17_023.txt,14.690669111478286,12.331717183818313,SPAA,5344,"

Strassen’s algorithm (1969) was the first sub-cubic matrix multiplication algorithm. Winograd (1971) improved its complexity by a
constant factor. Many asymptotic improvements followed. Unfortunately, most of them have done so at the cost of very large, often
gigantic, hidden constants. Consequently, Strassen-Winograd’s
oO (n!°ee 7) algorithm often outperforms other matrix multiplication algorithms for all feasible matrix dimensions. The leading
coefficient of Strassen-Winograd’s algorithm was believed to be
optimal for matrix multiplication algorithms with 2 x 2 base case,
due to a lower bound of Probert (1976).

Surprisingly, we obtain a faster matrix multiplication algorithm,
with the same base case size and asymptotic complexity as StrassenWinograd’s algorithm, but with the coefficient reduced from 6
to 5. To this end, we extend Bodrato’s (2010) method for matrix
squaring, and transform matrices to an alternative basis. We prove a
generalization of Probert’s lower bound that holds under change of
basis, showing that for matrix multiplication algorithms with a 2x2
base case, the leading coefficient of our algorithm cannot be further
reduced, hence optimal. We apply our technique to other Strassenlike algorithms, improving their arithmetic and communication
costs by significant constant factors.
",14.811377253095038,14.424747998115873,199
SPAA_17_024.txt,14.895726920698603,13.30367206287617,SPAA,11042,"
 parallel machines such as Pregel [36], GraphLab [32, 33], Power
Existing graph-processing frameworks let users develop efficient implementations for many graph problems, but none of them support
efficiently bucketing vertices, which is needed for bucketing-based
graph algorithms such as A-stepping and approximate set-cover.
Motivated by the lack of simple, scalable, and efficient implementations of bucketing-based algorithms, we develop the Julienne
framework, which extends a recent shared-memory graph processing framework called Ligra with an interface for maintaining a
collection of buckets under vertex insertions and bucket deletions.

We provide a theoretically efficient parallel implementation of
our bucketing interface and study several bucketing-based algorithms that make use of it (either bucketing by remaining degree
or by distance) to improve performance: the peeling algorithm for
k-core (coreness), A-stepping, weighted breadth-first search, and
approximate set cover. The implementations are all simple and concise (under 100 lines of code). Using our interface, we develop the
first work-efficient parallel algorithm for k-core in the literature
with nontrivial parallelism.

We experimentally show that our bucketing implementation
scales well and achieves high throughput on both synthetic and
real-world workloads. Furthermore, the bucketing-based algorithms
written in Julienne achieve up to 43x speedup on 72 cores with
hyper-threading over well-tuned sequential baselines, significantly
outperform existing work-inefficient implementations in Ligra, and
either outperform or are competitive with existing special-purpose
parallel codes for the same problem. We experimentally study our
implementations on the largest publicly available graphs and show
that they scale well in practice, processing real-world graphs with
billions of edges in seconds, and hundreds of billions of edges in a
few minutes. As far as we know, this is the first time that graphs
at this scale have been analyzed in the main memory of a single
multicore machine.

",19.838815038463903,19.632640692640695,309
SPAA_17_025.txt,14.259171400199996,12.793844709331992,SPAA,9932,"

A common approach for designing scalable algorithms for massive data sets is to distribute the computation across, say k, machines and process the data using limited communication between
them. A particularly appealing framework here is the simultaneous
communication model whereby each machine constructs a small
representative summary of its own data and one obtains an approximate/exact solution from the union of the representative summaries.
If the representative summaries needed for a problem are small,
then this results in a communication-efficient and round-optimal (requiring essentially no interaction between the machines) protocol.
Some well-known examples of techniques for creating summaries
include sampling, linear sketching, and composable coresets. These
techniques have been successfully used to design communication
efficient solutions for many fundamental graph problems. However,
two prominent problems are notably absent from the list of successes, namely, the maximum maiching problem and the minimum
vertex cover problem. Indeed, it was shown recently that for both
these problems, even achieving a modest approximation factor of
polylog(n) requires using representative summaries of size Q(n?)
ie. essentially no better summary exists than each machine simply
sending its entire input graph.

The main insight of our work is that the intractability of matching and vertex cover in the simultaneous communication model is
inherently connected to an adversarial partitioning of the underlying graph across machines. We show that when the underlying
graph is randomly partitioned across machines, both these problems admit randomized composable coresets of size O(n) that yield an
O(1)-approximate solution’, In other words, a small subgraph of the
input graph at each machine can be identified as its representative
summary and the final answer then is obtained by simply running
any maximum matching or minimum vertex cover algorithm on
these combined subgraphs. This results in an O(1)-approximation
",18.14513906131404,18.053034316676705,303
SPAA_17_026.txt,14.937361750782799,13.617034167614715,SPAA,10489,"

The performance gap between memory and CPU has grown exponentially. To bridge this gap, hardware architects have proposed
near-memory computing (also called processing-in-memory, or
PIM), where a lightweight processor (called a PIM core) is located
close to memory. Due to its proximity to memory, a memory access from a PIM core is much faster than that from a CPU core.
New advances in 3D integration and die-stacked memory make
PIM viable in the near future. Prior work has shown significant
performance improvements by using PIM for embarrassingly parallel and data-intensive applications, as well as for pointer-chasing
traversals in sequential data structures. However, current server
machines have hundreds of cores, and algorithms for concurrent
data structures exploit these cores to achieve high throughput and
scalability, with significant benefits over sequential data structures.
Thus, it is important to examine how PIM performs with respect to
modern concurrent data structures and understand how concurrent
data structures can be developed to take advantage of PIM.

This paper is the first to examine the design of concurrent data
structures for PIM. We show two main results: (1) naive PIM data
structures cannot outperform state-of-the-art concurrent data structures, such as pointer-chasing data structures and FIFO queues, (2)
novel designs for PIM data structures, using techniques such as
combining, partitioning and pipelining, can outperform traditional
concurrent data structures, with a significantly simpler design.
",17.251386760058843,15.786666666666669,237
SPAA_17_027.txt,13.773296193376591,11.92634068241708,SPAA,8864,"
Motivated by the asymmetric read and write costs of emerging
non-volatile memory technologies, we study lower bounds for
the problems of sorting, permuting and multiplying a sparse ma-
trix by a dense vector in the asymmetric external memory model
(AEM). Given an AEM with internal (symmetric) memory of size M,
transfers between symmetric and asymmetric memory in blocks of
size B and the ratio ω between write and read costs, we show ωN N
Ω(min{N, B logωM B }) lower bound for the cost of permut- B
ing N input elements. This lower bound also applies to the problem
of sorting N elements. This proves that the existing sorting algo-
rithms in the AEM model are optimal to within a constant factor
for reasonable ranges of parameters N , M, B, and ω. We also show a lower bound of Ω min H, B logωM max{δ,M} for the cost B
of multiplying an N × N matrix with at most H = δ N non-empty entries by a vector with N elements.
",16.15616582465906,16.10618181818182,171
SPAA_17_028.txt,13.859919134684453,12.067255926558769,SPAA,4490,"

Randomized binary exponential backoff (BEB) is a popular algorithm for coordinating access to a shared channel. With an operational history exceeding four decades, BEB is currently an important
component of several wireless standards.

Despite this track record, prior theoretical results indicate that
under bursty traffic (1) BEB yields poor makespan and (2) superior
algorithms are possible. To date, the degree to which these findings
manifest in practice has not been resolved.

To address this issue, we examine one of the strongest cases
against BEB: n packets that simultaneously begin contending for
the wireless channel. Using Network Simulator 3, we compare
against more recent algorithms that are inspired by BEB, but whose
makespan guarantees are superior. Surprisingly, we discover that
these newer algorithms significantly underperform.

Through further investigation, we identify as the culprit a flawed
but common abstraction regarding the cost of collisions. Our experimental results are complemented by analytical arguments that the
number of collisions — and not solely makespan — is an important
metric to optimize. We argue that these findings have implications
for the design of contention-resolution algorithms.
",16.15616582465906,14.925078212290504,180
SPAA_17_029.txt,12.395454908077003,10.245171844605952,SPAA,6802,"
Maximum coverage and minimum set cover problems—here collec-
tively called coverage problems—have been studied extensively in
streaming models. However, previous research not only achieves
suboptimal approximation factors and space complexities but also
study a restricted set-arrival model which makes an explicit or
implicit assumption on oracle access to the sets, ignoring the com-
plexity of reading and storing the whole set at once. In this paper,
we address the above shortcomings and present algorithms with
improved approximation factor and improved space complexity,
and prove that our results are almost tight. Moreover, unlike most of
the previous work, our results hold in a more general edge-arrival
model.

More specifically, consider an instance with n sets, together
covering m elements. Information arrives in the form of “edges”
from sets to elements (denoting membership) in arbitrary order.

(1) We present (almost) optimal approximation algorithms for
maximum coverage and minimum set cover problems in the
streaming model with an (almost) optimal space complexity
of ˜O(n); i.e., the space is independent of the size of the sets or
the size of the ground set of elements. These results not only
improve the best known algorithms for the set-arrival model,
but also are the first such algorithms for the more powerful
edge-arrival model.

(2) In order to achieve the above results, we introduce a new
general sketching technique for coverage functions: One can
apply this sketching scheme to convert an α-approximation
algorithm for a coverage problem to a (1−ε)α-approximation
algorithm for the same problem in streaming model.
(3) We show the significance of our sketching technique by
ruling out the possibility of solving coverage problems via
accessing (as a black box) a (1±ε)-approximate oracle (e.g., a
sketch function) that estimates the coverage function on any
subfamily of the sets. Finally, we show that our streaming
algorithms achieve an almost optimal space complexity.
",17.845785984406827,16.97108413325805,318
SPAA_17_030.txt,14.940474016741927,12.466550219576508,SPAA,8054,"

Iterative wavefront algorithms for evaluating dynamic programming recurrences exploit optimal parallelism but show poor cache
performance. Tiled-iterative wavefront algorithms achieve optimal
cache complexity and high parallelism but are cache-aware and
hence are not portable and not cache-adaptive. On the other hand,
standard cache-oblivious recursive divide-and-conquer algorithms
have optimal serial cache complexity but often have low parallelism
due to artificial dependencies among subtasks.

Recently, we introduced cache-oblivious recursive wavefront
(COW) algorithms, which do not have any artificial dependencies, but they are too complicated to develop, analyze, implement,
and generalize. Though COW algorithms are based on fork-join
primitives, they extensively use atomic operations for ensuring
correctness, and as a result, performance guarantees (i.e., parallel
running time and parallel cache complexity) provided by state-ofthe-art schedulers (e.g., the randomized work-stealing scheduler)
for programs with fork-join primitives do not apply. Also, extensive
use of atomic locks may result in high overhead in implementation.

In this paper, we show how to systematically transform standard
cache-oblivious recursive divide-and-conquer algorithms into recursive wavefront algorithms to achieve optimal parallel cache complexity and high parallelism under state-of-the-art schedulers for
fork-join programs. Unlike COW algorithms these new algorithms
do not use atomic operations. Instead, they use closed-form formulas to compute the time when each divide-and-conquer function
must be launched in order to achieve high parallelism without losing cache performance. The resulting implementations are arguably
much simpler than implementations of known COW algorithms.
We present theoretical analyses and experimental performance and
scalability results showing a superiority of these new algorithms
over existing algorithms.
",19.917542046945165,18.726453546453545,276
SPAA_17_031.txt,15.113753985570302,13.11604773604774,SPAA,8033,"
 (i.e., duration and number of locations accessed): smaller transac
In this paper, we introduce revocable reservations, a transactional
memory mechanism to reserve locations in one transaction and
check whether they are unchanged in a subsequent transaction
without preventing reserved locations from being reclaimed in the
interim. We describe several implementations of revocable reservations, and show how to use revocable reservations to implement
lists and trees with a transactional analog to hand-over-hand locking. Our evaluation of these data structures shows that revocable
reservations allow precise and immediate reclamation within transactional data structures, without sacrificing scalability or introducing excessive latency.
",22.641843272026104,23.01600000000001,102
SYSTOR_17_001.txt,14.925106808931726,12.936203973560154,SYSTOR,7191,"
Multi-stream SSDs can isolate data with different life time
to disparate erase blocks, thus reduce garbage collection
overhead and improve overall SSD performance. Applications are responsible for management of these device-level
steams such as stream open/close and data-to-stream mapping. This requires application changes, and the engineer deploying the solution needs to be able to individually identify
the streams in their workload. Furthermore, when multiple
applications are involved, such as in VM or containerized
environments, stream management becomes more complex
due to the limited number of streams a device can support,
for example, allocating streams to applications or sharing
streams across applications will cause additional overhead.

To address these issues and reduce the overhead of stream
management, this paper proposes automatic stream management algorithms that operate under the application layer.
Our stream assignment techniques, called AutoStream, is
based on run time workload detection and independent of
the application(s). We implement our AutoStream prototype in NVMe Linux device driver and our performance evaluation shows up to 60% reduction on WAF(Write Amplification Factor) and up to 237% improvement on performance
compared to a conventional SSD device.
",18.848423458724294,17.2915037593985,191
SYSTOR_17_002.txt,13.975832892998461,12.758857368544529,SYSTOR,9220,"
We present MemEC, an erasure-coding-based in-memory
key-value (KV) store that achieves high availability and fast
recovery while keeping low data redundancy across storage
servers. MemEC is specifically designed for workloads dominated by small objects. By encoding objects in entirety,
MemEC is shown to incur 60% less storage redundancy for
small objects than existing replication- and erasure-codingbased approaches. It also supports graceful transitions between decentralized requests in normal mode (i.e., no failures) and coordinated requests in degraded mode (i.e., with
failures). We evaluate our MemEC prototype via testbed experiments under read-heavy and update-heavy YCSB workloads. We show that MemEC achieves high throughput and
low latency in both normal and degraded modes, and supports fast transitions between the two modes.
",14.790194502661404,14.907800000000002,128
SYSTOR_17_003.txt,14.468890406678216,12.80166452812669,SYSTOR,4254,"
While flash-based SSDs have much higher access speed than
hard disks, they have an Achilles heel, which is the service of
write requests. Not only is writing slower than reading, but
also it can incur expensive garbage collection operations and
reduce SSDs’ lifetime. The deduplication technique can help
to avoid writing data objects whose contents have been on
the disk. A typical object is the disk block, for which a blocklevel deduplication scheme can help identify duplicate ones
and avoid their writing. For the technique to be effective,
data written to the disk must not only be the same as those
currently on the disk but also be block-aligned.

In this work, we will show that many deduplication opportunities are lost due to block misalignment, leading to a
substantially large number of unnecessary writes. As case studies, we develop a scheme to retain alignments of the data
that are read from the disk in the file modifications by using small additional spaces for two important applications,
a log-based key-value store (e.g., FAWN) and an LSM-tree
based key-value store (e.g., LevelDB). Our experiments show
that the proposed scheme can achieve up to 4.5X and 26% of
throughput improvement for FAWN and LevelDB systems,
respectively, with a less than 5% space overhead.
",13.624085052395262,13.240925925925929,220
SYSTOR_17_004.txt,15.980485743188602,14.558465834085702,SYSTOR,8036,"
Over the last few years, GPUs have become common in computing. However, current GPUs are not designed for a shared
environment like a cloud, creating a number of challenges
whenever a GPU must be multiplexed between multiple users.
In particular, the round-robin scheduling used by today’s
GPUs does not distribute the available GPU computation
time fairly among applications. Most of the previous work
addressing this problem resorted to scheduling all GPU computation in software, which induces high overhead. While
there is a GPU scheduler called NEON which reduces the
scheduling overhead compared to previous work, NEON’s
accounting mechanism frequently disables GPU access for
all but one application, resulting in considerable overhead if
that application does not saturate the GPU by itself.

In this paper, we present LoGA, a novel accounting mechanism for GPU computation time. LoGA monitors the GPU’s
state to detect GPU-internal context switches, and infers the
amount of GPU computation time consumed by each process
from the time between these context switches. This method
allows LoGA to measure GPU computation time consumed
by applications while keeping all applications running concurrently. As a result, LoGA achieves a lower accounting
overhead than previous work, especially for applications that
do not saturate the GPU by themselves. We have developed
a prototype which combines LoGA with the pre-existing
NEON scheduler. Experiments with that prototype have
shown that LoGA induces no accounting overhead while still
delivering accurate measurements of applications’ consumed
GPU computation time.
",17.122413403193683,14.823204172876306,248
SYSTOR_17_005.txt,16.169344447983246,14.908691169868863,SYSTOR,6666,"
Over the last few years, Graphics Processing Units (GPUs)
have become popular in computing, and have found their
way into a number of cloud platforms. However, integrating
a GPU into a cloud environment requires the cloud provider
to efficiently virtualize the GPU. While several research
projects have addressed this challenge in the past, few of
these projects attempt to properly enable sharing of GPU
memory between multiple clients: To date, GPUswap is the
only project that enables sharing of GPU memory without inducing unnecessary application overhead, while maintaining
both fairness and high utilization of GPU memory. However,
GPUswap includes only a rudimentary swapping policy, and
therefore induces a rather large application overhead.

In this paper, we work towards a practicable swapping
policy for GPUs. To that end, we analyze the behavior of
various GPU applications to determine their memory access
patterns. Based on our insights about these patterns, we
derive a swapping policy that includes a developer-assigned
priority for each GPU buffer in its swapping decisions. Experiments with our prototype implementation show that a
swapping policy based on buffer priorities can significantly
reduce the swapping overhead.
",17.267425705330172,15.990421122994654,188
SYSTOR_17_006.txt,15.110477519087777,13.071929603444993,SYSTOR,2988,"
Tracing and profiling low-level kernel functions (e.g. as
found in the process scheduler) is a challenging task, though,
necessary in both research and production in order to acquire detailed insights and achieve peak performance. Several kernel functions are known to be not traceable because
of architectural limitations, whereas tracking other functions
causes side effects and skews profiling results.

In this paper, we present a novel, simulation-based approach to analyze the behavior and performance of kernel
functions. Kernel code is executed on a simulated hardware
platform avoiding the bias caused by collecting the tracing
data within the system under observation. From the flat
call trace generated by the simulator, we reconstruct the entire call graph and enrich it with detailed profiling statistics.
Specifying regions of interest enables developers to systematically explore the system behavior and identify performance
bottlenecks. As case study, we analyze the process scheduler of the Linux kernel. We are interested in quantifying
the synchronization overhead caused by a growing number
of CPU cores in a custom, semi-partitioned scheduler design. Conventional tracing methods were not able to obtain
measurements with the required accuracy and granularity.
",16.594172100314452,15.38835978835979,191
SYSTOR_17_007.txt,13.310731037718625,11.277597157649378,SYSTOR,4641,"
Building self-healing SSDs is proven feasible by recent studies. When the stress of a block becomes critical, it can be
healed to remove part of the stress. However, with wear
leveling, all blocks are evenly worn and have similar stress,
and all blocks could undergo the healing process within a
short period of time. The intensive heal operations, called
heal storms, cause highly unpredictable I/O performance
and storage reliability. Inspired by the even distribution
of erase counts under wear leveling, we propose to operate
wear leveling on virtual erase counts instead of real erase
counts. When the balance among virtual erase counts is
achieved through wear leveling, all real erase counts become
evenly dispersed in a controlled interval. In this way, blocks
will undergo healing at different times. Virtual erase counts
are progressively adjusted such that all blocks reach their
endurance limit when the SSD permanently retires. Our results show that our approach successfully resolved the heal
storm problem without impacting on the SSD lifespan.
",13.901157680251561,11.431097804391218,168
SYSTOR_17_008.txt,14.458674264970316,12.987897120574274,SYSTOR,7101,"
During the past decade, network and storage devices
have undergone rapid performance improvements, delivering ultra-low latency and several Gbps of bandwidth. Nevertheless, current network and storage stacks fail to deliver
this hardware performance to the applications, often due
to the loss of IO efficiency from stalled CPU performance.
While many efforts attempt to address this issue solely on
either the network or the storage stack, achieving highperformance for networked-storage applications requires a
holistic approach that considers both.

In this paper, we present FlashNet, a software IO stack
that unifies high-performance network properties with flash
storage access and management. FlashNet builds on
RDMA principles and abstractions to provide a direct, asynchronous, end-to-end data path between a client and remote
flash storage. The key insight behind FlashNet is to codesign the stack’s components (an RDMA controller, a flash
controller, and a file system) to enable cross-stack optimizations and maximize IO efficiency. In micro-benchmarks,
FlashNet improves 4kB network IOPS by 38.6% to 1.22M,
decreases access latency by 43.5% to 50.4 secs, and prolongs the flash lifetime by 1.6-5.9x for writes. We illustrate
the capabilities of FlashNet by building a Key-Value store,
and porting a distributed data store that uses RDMA on
it. The use of FlashNet’s RDMA API improves the performance of KV store by 2x, and requires minimum changes
for the ported data store to access remote flash devices.
",16.04434344847333,15.036666666666669,245
SYSTOR_17_009.txt,15.637032967257301,14.141082474226803,SYSTOR,6609,"
General purpose GPU (GPGPU) computing in virtualized
environments leverages PCI passthrough to achieve GPU
performance comparable to bare-metal execution. However,
GPU passthrough prevents service administrators from performing virtual machine migration between physical hosts.

Crane is a new technique for virtualizing OpenCL-based
GPGPU computing that achieves within 5.25% of passthrough GPU performance while supporting VM migration. Crane interposes a virtualization-aware OpenCL library that makes it possible to reclaim and subsequently
reassign physical GPUs to a VM without terminating the
guest or its applications. Crane also enables continued GPU
operation while the VM is undergoing live migration by
transparently switching between GPU passthrough operation and API remoting.
",18.669449996058646,16.9802962962963,110
SYSTOR_17_010.txt,12.970525349166948,11.041582494199673,SYSTOR,5478,"
Despite the growing popularity of enterprise virtual desktop infrastructure (VDI), little is known about its storage traffic characteristics. In addition, no prior work
has considered the detailed characteristics of virtual machine (VM) behavior on VDI. In this paper, we analyze the enterprise storage traffic on commercial office
VDI using designated VMs. For 28 consecutive days,
we gathered various types of traces, including a usage
questionnaire and active and passive measurements. To
characterize the storage traffic, we focused on two perspectives: fibre channel (FC) traffic and VM behavior. From the FC traffic perspective, we found that
read traffic is dominant, although the applications are
similar to those in a previous small-scale VDI. In particular, the write response time of large transactions,
e.g.,128 KiB, is strongly affected by a slight decrease in
cache hits during an update storm. From the VM behavior, we found that all active user VMs generate only
25% of traffic. Although a few VMs generate massive
traffic, their impact is small. These characteristics are
unique in comparison with the small-scale VDI. Our results have significant implications for designing the next
generation of VDI and improving its performance.
",14.696529576185021,12.609458333333336,194
SYSTOR_17_011.txt,14.825238343923356,13.424375153971045,SYSTOR,8077,"
The exponential growth of data produced, the ever faster
and ubiquitous connectivity, and the collaborative processing tools lead to a clear shift of data stores from local
servers to the cloud. This migration occurring across different application domains and types of users—individual
or corporate—raises two immediate challenges. First, outsourcing data introduces security risks, hence protection
mechanisms must be put in place to provide guarantees such
as privacy, confidentiality and integrity. Second, there is no
“one-size-fits-all” solution that would provide the right level
of safety or performance for all applications and users, and
it is therefore necessary to provide mechanisms that can be
tailored to the various deployment scenarios.

In this paper, we address both challenges by introducing
SAFEFS, a modular architecture based on software-defined
storage principles featuring stackable building blocks that
can be combined to construct a secure distributed file system. SAFEFS allows users to specialize their data store to
their specific needs by choosing the combination of blocks
that provide the best safety and performance tradeoffs. The
file system is implemented in user space using FUSE and can
access remote data stores. The provided building blocks notably include mechanisms based on encryption, replication,
and coding. We implemented SAFEF'S and performed indepth evaluation across a range of workloads. Results reveal
that while each layer has a cost, one can build safe yet efficient storage architectures. Furthermore, the different combinations of blocks sometimes yield surprising tradeoffs.
",16.018793980428352,14.840852508487362,243
SYSTOR_17_012.txt,14.737562186601131,12.839581818539116,SYSTOR,6727,"
One of the main motivations for the shift to the Cloud (and
the more recent shift of telco operators into NFV) is cost
reduction due to high utilization of infrastructure resources.
However, achieving high utilization in practical scenarios is
complex since the term “resources” covers diferent orthogonal aspects, such as server CPU, storage (or disk) usage and
network capacity, and the workload characterization varies
over time and over diferent users.

In this paper we study the placement of Virtual Machines
(VMs) that implement services over the physical inrastructure, trying to understand what makes a placement scheme
better than others in the overall utilization of the various
resources. We show that the multidimensional case is inherently diferent from the single dimension case, and develop
novel placement heuristics to address the speciic challenges.
We then show, by extensive evaluation over real data, that
operators can signiicantly improve their resource utilization
by selecting the most appropriate placement policy, according to their system speciications and the deployed services.
In particular, two of our new heuristics that dynamically
change the placement logic according to the amount of available (unused) resources are shown to perform very well in
many practical scenarios.
",20.267338824336647,20.079871794871796,196
SYSTOR_17_013.txt,17.415498893119754,15.38723578394551,SYSTOR,7414,"
As the number of cores increases in a single chip processor,
several challenges arise: wire delays, contention or out-of
chip accesses, and core heterogeneity. In order to address
these issues and the applications demands, uture large-scale
many-core processors are expected to be organized as a collection of NUMA clusters of heterogeneous cores. In this
work we propose a scheduler that takes into account the
non-uniorm memory latency, the heterogeneity of the cores,
and the contention to the memory controller to ind the best
matching core or the application’s memory and compute
requirements. Scheduler decisions are based on an on-line
classiication process that determines applications requirements either as memory- or compute-bound. We evaluate
our proposed scheduler on the 48-core Intel SCC using applications rom SPEC CPU2006 benchmark suite. Our results
show that even when all cores are busy, migrating processes
to cores that match better the requirements of applications
results in overall perormance improvement. In particular
we observed a reduction of the execution time from 15% to
36% compared to a random static scheduling policy.
",17.77360955136429,15.942904498816102,183
SYSTOR_17_014.txt,15.375520658892356,13.9097100333295,SYSTOR,4752,"
Storage disaggregation separates compute and storage to di
erent nodes in order to allow for independent resource scaling and thus, better hardware resource utilization. While
disaggregation o hard-drives storage is a common practice,
NVM-SSD (i.e., PCle-based SSD) disaggregation is considered more challenging. This is because SSDs are signiicantly
aster than hard drives, so the latency overheads (due to
both network and CPU processing) as well as the extra
compute cycles needed or the offloading stack become much
more pronounced.

In this work we characterize the overheads o NVMe-SSD
disaggregation. We show that NVMe-over-Fabrics (NVM)
—a recently-released remote storage protocol speciication —
reduces the overheads o remote access to a bare minimum,
thus greatly increasing the cost-eficiency of Flash disaggregation. Specifically, while recent work showed that SSD
storage disaggregation via iSCSI degrades application-level
throughput by 20%, we report on negligible perormance
degradation with NVM — both when using stress-tests as
well as with a more-realistic KV-store workload.
",16.52667757954773,16.454216867469885,168
SYSTOR_17_015.txt,14.985637630196734,13.767165606293592,SYSTOR,8295,"
Misconfigurations in the storage systems can lead to business losses due to system downtime with substantial people
resources invested into troubleshooting. Hence, aster troubleshooting of software misconfigurations has been critically
important or the customers as well as the vendors.

This paper introduces a ramework and a tool called Dexter, which embraces the recent trend of viewing systems as
data to derive the troubleshooting clues. Dexter provides
quick insights into the problem root cause and possible resolution by solely using the storage system logs. This diferentiates Dexter from other previously known approaches
which complement log analysis with source code analysis,
execution traces etc. Furthermore, Dexter analyzes command history logs rom the sick system after it has been
healed and predicts the exact command(s) which resolved
the problem. Dexter’s approach is simple and can be applied
to other sotware systems with diagnostic logs for immediate
problem detection without any pre-trained models.

Evaluation on 600 real customer support cases shows 90%
accuracy in root causing and over 65% accuracy in inding
an exact resolution or the misconfiguration problem. Results show up to 60% noise reduction in system logs and
at least 10z savings in case resolution times, bringing down
the troubleshooting times rom days to minutes at times.
Dexter runs 24x7 in the NetApp’s® support data center.

The paper also presents insights rom study on thousands
o real customer support cases over thousands of deployed
systems over the period o 1.5 years. These investigations
uncover acts that cause potential delays in customer case
resolutions and inluence Dexter’s design.
",14.672994598444667,14.382596899224804,262
SYSTOR_17_016.txt,15.625770258735145,14.943897033330064,SYSTOR,3390,"
Exploiting the cloud storage hierarchy both within and
across data-centers o different cloud providers empowers
Internet applications to choose data centers (DCs) and storage services based on storage needs. However, using multiple storage services across multiple data centers brings a
complex data placement problem that depends on a large
number o factors including, e.g., desired goals, storage and
network characteristics, and pricing policies. In addition,
dynamics e.g., changing user locations and access patterns,
make it impossible to determine the best data placement
statically. In this paper, we present TripS, a lightweight
system that considers both data center locations and storage tiers to determine the data placement or geo-distributed
storage systems. Such systems make use o TripS by providing inputs including SLA, consistency model, fault tolerance,
latency inormation, and cost information. With given inputs, TripS models and solves the data placement problem
using mixed integer linear programming (MILP) to determine data placement. In addition, to adapt quickly to dynamics, we introduce the notion o Target Locale List (TLL),
a pro-active approach to avoid expensive re-evaluation o
the optimal placement. The TripS prototype is running on
Wiera, a policy driven geo-distributed storage system, to
show how a storage system can easily utilize TripS or data
placement. We evaluate TripS/Wiera on multiple data centers o AWS and Azure. The results show that TripS/Wiera
can reduce cost 14.96% ~ 98.1% based on workloads in comparison with other works’ approaches and can handle both
short- and long-term dynamics to avoid SLA violations.
",16.404322709996244,15.780098039215687,259
VEE_17_001.txt,15.720984838503217,14.146311442243348,VEE,8448,"
The ability to quickly set up and tear down a virtual machine
is critical for today’s cloud elasticity, as well as in numerous other scenarios: guest migration/consolidation, event-driven invocation of micro-services, dynamically adaptive
unikernel-based applications, micro-reboots for security or
stability, etc.

In this paper, we focus on the process of setting up/freeing the hypervisor and host control layer data structures at
boot/destruction time, showing that it does not scale in current virtualization solutions. In addition to the direct overhead of long VM set-up/destruction times, we demonstrate
by experimentation the indirect costs on real world auto scaling systems. Focusing on the popular Xen hypervisor, we
identify three critical issues hindering the scalability of the
boot and destruction processes: serialized boot, unscalable
interactions with the Xenstore at guest creation time, and
remote NUMA memory scrubbing at destruction time. For
each of these issues we present the design and implementation of a solution in the Xen infrastructure: parallel boot with
fine-grained locking, caching of Xenstore data, and local
NUMA scrubbing. We evaluate these solutions using microbenchmarks, macro-benchmarks, and real world datacenter
traces. Results show that our work improves the current Xen
implementation by a significant factor, for example macrobenchmarks indicate a speedup of more than 4X in high-load
scenarios.
",19.5731925562951,19.181402359108784,220
VEE_17_002.txt,15.91205682572437,15.09694988192891,VEE,8040,"
Video streaming dominates the Internet’s overall traffic mix,
with reports stating that it will constitute 90% of all consumer traffic by 2019. Most of this video is delivered by
Content Delivery Networks (CDNs), and, while they optimize QoE metrics such as buffering ratio and start-up time,
no single CDN provides optimal performance. In this paper
we make the case for elastic CDNs, the ability to build virtual CDNs on-the-fly on top of shared, third-party infrastructure at a scale. To bring this idea closer to reality we begin
by large-scale simulations to quantify the effects that elastic
CDNs would have if deployed, and build and evaluate MiniCache, a specialized, minimalistic virtualized content cache
that runs on the Xen hypervisor. MiniCache is able to serve
content at rates of up to 32 Gb/s and handle up to 600K reqs/sec on a single CPU core, as well as boot in about 90 milliseconds on x86 and around 370 milliseconds on ARM32.
",17.80541091248751,16.44606060606061,167
VEE_17_003.txt,16.973123705539894,15.372956283637052,VEE,7022,"
Live migration of a virtual machine (VM) is a powerful technique with benefits of server maintenance, resource management, dynamic workload re-balance, etc. Modern research
has effectively reduced the VM live migration (VMLM) time
to dozens of milliseconds, but live migration still exhibits
failures if it cannot terminate within the given time constraint. The ability to predict this type of failure can avoid
wasting networking and computing resources on the VM
migration, and the associated system performance degradation caused by wasting these resources. The cost of VM
live migration highly depends on the application workload
of the VM, which may undergo frequent changes. At the
same time, the available system resources for VM migration
can also change substantially and frequently. To account for
these issues, we present a solution called MigVisor, which
can accurately predict the behaviour of VM migration using
working-set model. This can enable system managers to predict the migration cost and enhance the system management
efficacy. The experimental results prove the design suitability and show that the MigVisor has a high prediction accuracy since the average relative error between the predicted
value and the measured value is only 6.2%~9%.

",17.267425705330172,15.519326923076928,196
VEE_17_004.txt,14.911310289592361,12.935535738181262,VEE,9865,"
Content based page sharing techniques improve memory efficiency in virtualized systems by identifying and merging
identical pages. Kernel Same-page Merging (KSM), a Linux
kernel utility for page sharing, sequentially scans memory
pages of virtual machines to deduplicate pages. Sequential scanning of pages has several undesirable side effects—
wasted CPU cycles when no sharing opportunities exist, and
rate of discovery of sharing being dependent on the scanning rate and corresponding CPU availability. In this work,
we exploit presence of GPUs on modern systems to enable
rapid memory sharing through targeted scanning of pages.
Our solution, Catalyst, works in two phases, the first where
pages of virtual machines are processed by the GPU to identify likely pages for sharing and a second phase that performs page-level similarity checks on a targeted set of shareable pages. Opportunistic usage of the GPU to produce sharing hints enables rapid and low-overhead duplicate detection, and sharing of memory pages in virtualization environments. We evaluate Catalyst against various benchmarks and
workloads to demonstrate that Catalyst can achieve higher
memory sharing in lesser time compared to different scan
rate configurations of KSM, at lower or comparable compute costs.

Keywords Memory Deduplication, Graphics Processing
Units, Virtualization, KSM;

",17.553077303434723,17.69133706467662,202
VEE_17_005.txt,15.309547326138986,13.78986086805033,VEE,6441,"
Computer systems are increasingly featuring powerful parallel devices with the advent of many-core CPUs and GPUs.
This offers the opportunity to solve computationally-intensive
problems at a fraction of the time traditional CPUs need.
However, exploiting heterogeneous hardware requires the
use of low-level programming language approaches such as
OpenCL, which is incredibly challenging, even for advanced
programmers.

On the application side, interpreted dynamic languages
are increasingly becoming popular in many domains due to
their simplicity, expressiveness and flexibility. However, this
creates a wide gap between the high-level abstractions offered to programmers and the low-level hardware-specific
interface. Currently, programmers must rely on high performance libraries or they are forced to write parts of their application in a low-level language like OpenCL. Ideally, nonexpert programmers should be able to exploit heterogeneous
hardware directly from their interpreted dynamic languages.

In this paper, we present a technique to transparently
and automatically offload computations from interpreted dynamic languages to heterogeneous devices. Using just-intime compilation, we automatically generate OpenCL code
at runtime which is specialized to the actual observed data
types using profiling information. We demonstrate our technique using R, which is a popular interpreted dynamic language predominately used in big data analytic. Our experimental results show the execution on a GPU yields speedups
of over 150x compared to the sequential FastR implementation and the obtained performance is competitive with manually written GPU code. We also show that when taking into
account start-up time, large speedups are achievable, even
when the applications run for as little as a few seconds.

",17.78686850342507,16.43492366412214,263
VEE_17_006.txt,16.519594535130093,15.669089517239808,VEE,4067,"
Real-time 3D space understanding is becoming prevalent
across a wide range of applications and hardware platforms.
To meet the desired Quality of Service (QoS), computer vision applications tend to be heavily parallelized and exploit
any available hardware accelerators. Current approaches to
achieving real-time computer vision, evolve around programming languages typically associated with High Performance
Computing along with binding extensions for OpenCL or
CUDA execution.

Such implementations, although high performing, lack
portability across the wide range of diverse hardware resources and accelerators. In this paper, we showcase how a
complex computer vision application can be implemented
within a managed runtime system. We discuss the complexities of achieving high-performing and portable execution
across embedded and desktop configurations. Furthermore,
we demonstrate that it is possible to achieve the QoS target
of over 30 frames per second (FPS) by exploiting FPGA and
GPGPU acceleration transparently through the managed
runtime system.

Keywords GPU Acceleration, Java Virtual Machines, Heterogeneous Runtime Systems, SLAM, Computer Vision
",17.553077303434723,16.811545031055903,162
VEE_17_007.txt,16.302644064483335,14.010626561383003,VEE,10001,"
This paper presents a novel framework that enables practical event-driven monitoring for untrusted virtual machine
monitors (VMMs) in cloud computing. Unlike previous approaches for VMM monitoring, our framework neither relies on a higher privilege level nor requires any special hardware support. Instead, we place the trusted monitor at the
same privilege level and in the same address space with
the untrusted VMM to achieve superior efficiency, while
proposing a unique mutual-protection mechanism to ensure
the integrity of the monitor. Our security analysis demonstrates that our framework can provide high-assurance for
event-driven VMM monitoring, even if the highest-privilege
VMM is fully compromised. The experimental results show
that our framework only incurs trivial performance overhead for enforcing event-driven monitoring policies, exhibiting tremendous performance improvement on previous approaches.

",19.882160675589997,19.038687022900763,132
VEE_17_008.txt,17.007305480390183,15.563356292484045,VEE,7418,"
With DevOps automation and an everything-as-code approach to lifecycle management for cloud-native applications, challenges emerge from an operational visibility and
control perspective. Once a VM is deployed in production it
typically becomes a hands-off entity in terms of restrictions
towards inspecting or tuning it, for the fear of negatively
impacting its operation. We present CIVIC (Cloning and
Injection based VM Inspection for Cloud), a new mechanism that enables safe inspection of unmodified production
VMs on-the-fly. CIVIC restricts all impact and side-effects
of inspection or analysis operations inside a live clone of
the production VM. New functionality over the replicated
VM state is introduced using code injection. In this paper,
we describe the design and implementation of our solution
over KVM/QEMU. We demonstrate four of its use-cases(i) safe reuse of system monitoring agents, (ii) impact-heavy
problem diagnostics and troubleshooting, (iii) attaching an
intrusive anomaly detector to a live service, and (iv) live
tuning of a webserver’s configuration parameters. Our evaluation shows CIVIC is nimble and lightweight in terms of
memory footprint as well as clone activation time (6.5s),
and has a low impact on the original VM (< 10%).
",17.267425705330172,15.813762562814073,201
VEE_17_009.txt,15.11039641203255,13.836534718159314,VEE,10598,"
We introduce Sky, an extension to the VMM that gathers
insights and information by intercepting system calls made
by guest applications. We show how Sky gains three specific
insights — guest file-size information, metadata-data distinction, and file-content hints — and uses said information to
enhance virtualized-storage performance. By caching small
files and metadata with higher priority, Sky reduces the runtime by 2.3 to 8.8 times for certain workloads. Sky also
achieves 4.5 to 18.7 times reduction in the runtime of an
open-source block-layer deduplication system by exploiting
hints about file contents. Sky works underneath both Linux
and FreeBSD guests, as well as under a range of file systems,
thus enabling portable and general VMM-level optimization
underneath a wide range of storage stacks.
",16.15616582465906,14.571118110236224,132
VEE_17_010.txt,16.056260896705627,14.046877703243133,VEE,8097,"
Cloud computing has become indispensable in today’s computer landscape. The flexibility it offers for customers as
well as for providers has become a crucial factor for large
parts of the computer industry. Virtualization is the key technology that allows for sharing of hardware resources among
different customers. The controlling software component,
called hypervisor, provides a virtualized view of the computer resources and ensures separation of different guest virtual machines. However, this important cornerstone of cloud
computing is not necessarily trustworthy or bug-free. To mitigate this threat AMD introduced Secure Encrypted Virtualization, short SEV, which transparently encrypts a virtual
machines memory.

In this paper we analyse to what extend the proposed features can resist a malicious hypervisor and discuss the tradeoffs imposed by additional protection mechanisms. To do so,
we developed a model of SEV’s security capabilities based
on the available documentation as actual silicon implementations are not yet on the market.

We found that the first proposed version of SEV is not
up to the task owing to three design shortcomings. First the
virtual machine control block is not encrypted and handled
directly by the hypervisor, allowing it to bypass VM memory
encryption by executing conveniently chosen gadgets. Secondly, the general purpose registers are not encrypted upon
vmexit, leaking potentially sensitive data. Finally, the control
over the nested pagetables allows a malicious hypervisor to
closely monitor the execution state of a VM and attack it
with memory replay attacks.
",17.219254097808864,15.367500000000003,243
VEE_17_011.txt,14.505850648804664,12.772131885718405,VEE,6345,"
Recent code reuse attacks are able to circumvent various address space layout randomization (ASLR) techniques by exploiting memory disclosure vulnerabilities. To mitigate sophisticated code reuse attacks, we proposed a light-weight
virtual machine, RERANZ, which deployed a novel continuous binary code re-randomization to mitigate memory disclosure oriented attacks. In order to meet security and performance goals, costly code randomization operations were
outsourced to a separate process, called the “shuffling process”. The shuffling process continuously flushed the old
code and replaced it with a fine-grained randomized code
variant. RERANZ repeated the process each time an adversary might obtain the information and upload a payload. Our
performance evaluation shows that RERANZ Virtual Machine incurs a very low performance overhead. The security
evaluation shows that RERANZ successfully protect the Nginx web server against the Blind-ROP attack.
",16.084390811093357,15.587142857142858,137
VEE_17_012.txt,15.369778171954977,13.637121769964157,VEE,8504,"
This paper extends the concepts behind cloud services to
offer hypervisor-based reliability and security monitors for
cloud virtual machines. Cloud VMs can be heterogeneous
and as such guest OS parameters needed for monitoring
can vary across different VMs and must be obtained in
some way. Past work involves running code inside the VM,
which is unacceptable for a cloud environment. We solve this
problem by recognizing that there are common OS design
patterns that can be used to infer monitoring parameters from
the guest OS. We extract information about the cloud user’s
guest OS with the user’s existing VM image and knowledge
of OS design patterns as the only inputs to analysis. To
demonstrate the range of monitoring functionality possible
with this technique, we implemented four sample monitors:
a guest OS process tracer, an OS hang detector, a return-touser attack detector, and a process-based keylogger detector.
",15.470042427545799,14.843993288590607,152
VEE_17_013.txt,15.22537391370664,13.12463019230806,VEE,7960,"
Storage consolidation in a virtualized environment introduces numerous duplications in virtual disks and imposes
considerable pressure on disk I/O and caching. In this paper,
we present a content look-aside buffer (CLB) approach for
simultaneously providing redundancy-free virtual disk I/O
and caching. CLB attaches persistent fingerprints to virtual
disk blocks, which enables detection of I/O redundancy before disk access. At run time, CLB exploits content pages
already present in the guest disk caches to service the redundant reads through page sharing, thus eliminating both
redundant I/O requests and redundant disk cache copies.
For write requests, CLB uses a group invalidating writeback protocol for updating fingerprints to support crash consistency while minimizing disk write overhead. By implementing and evaluating a CLB prototype on KVM hypervisor, we demonstrate that CLB delivers considerably improved I/O performance with realistic workloads. Our CLB
prototype improves the throughput of sequential and random
read on duplicate data by 4.1x and 26.2x, respectively. For
typical read-intensive workloads, such as booting VM and
launching application, CLB’s I/O deduplication and cache
deduplication eliminates 94.9%-98.5% of read requests and
saves 50%-100% cache memory in each VM, respectively.
Compared with the QEMU’s raw virtual disk format, CLB
improves the per-disk VM density by 8x—16x. For mixed
read-write workloads, the cost of on-line fingerprint updating offsets the read benefit; nevertheless, CLB substantially
improves overall performance.
",17.467979349516824,15.614,243
VEE_17_014.txt,16.893702381594764,15.55887297006051,VEE,9121,"
Current computer architectures — ARM, MIPS, PowerPC,
SPARC, x86 — have evolved from a 32-bit architecture to
a 64-bit one. Computer architects often consider whether it
could be possible to eliminate hardware support for a subset
of the instruction set as to reduce hardware complexity,
which could improve performance, reduce power usage and
accelerate processor development. This paper considers the
scenario where we want to eliminate 32-bit hardware support
from the ARMV8 architecture.

Dynamic binary translation can be used for this purpose
and generally comes in one of two forms: application-level
translators that translate a single user mode process on top of
a native operating system, and system-level translators that
translate an entire operating system and all its processes.

Application-level translators can have good performance
but is not totally transparent; system-level translators may be
100% compatible but performance suffers. HyperMAMBOX64 uses a new approach that gets the best of both worlds,
being able to mn the translator as an application under the
hypervisor but still react to the behavior of guest operating
systems. It works with complete transparency with regards
to the virtualized system whilst delivering performance close
to that provided by hardware execution.

A key factor in the low overhead of HyperMAMBO-X64
is its deep integration with the virtualization and memory
management features of ARMV8. These are exploited to support caching of translations across multiple address spaces
while ensuring that translated code remains consistent with
the source instructions it is based on. We show how these attributes are achieved without sacrificing either performance
or accuracy.
",17.916177094544413,16.24632061068702,263
VEE_17_015.txt,14.68414872111617,12.385054758910247,VEE,11250,"
Once compromising the hypervisor, remote or local adversaries can easily access other customers’ sensitive data in
the memory and context of guest virtual machines (VMs).
VM isolation is an efficient mechanism for protecting the
memory of guest VMs from unauthorized access. However, previous VM isolation systems either modify hardware
architecture or introduce a software module without being
protected, and most of them focus on the x86 architecture.

This paper proposes HA-VMSI, a lightweight hardwareassisted VM isolation approach for ARM, to provide runtime protection of guest VMs, even with a compromised hypervisor. In the ARM TrustZone secure world, a thin security monitor is introduced as HA-VMSI’s entire TCB. Hence,
the security monitor is much less vulnerable and safe from
attacks that can compromise the hypervisor. The key of HAVMSI is decoupling the functions of memory isolation among VMs from the hypervisor into the security monitor.
As a result, the hypervisor can only update the Stage-2 page
tables of VMs via the security monitor, which inspects and
approves each new mapping. It is worth noting that HAVMSI is more secure and effective than current software approaches, and more flexible and compatible than hardware
approaches. We have implemented a prototype for KVM hypervisor with multiple Linux as guest OSes on Juno board.
The security assessment and performance evaluation show
that HA-VMSLI is effective, efficient and practical.
",16.908762484321528,14.654657589106929,229
VEE_17_016.txt,15.161552105721036,13.440000437724233,VEE,8908,"
Ubiquitous mobile platforms such as Android rely on managed language run-time environments, also known as language virtual machines (VMs), to run a diverse range of user
applications (apps). Each app runs in its own private VM instance, and each VM makes its own private local decisions in
managing its use of processor and memory resources. Moreover, the operating system and the hardware do not communicate their low-level decisions regarding power management with the high-level app environment. This lack of coordination across layers and across apps restricts more effective global use of resources on the device.

We address this problem by devising and implementing a global memory manager service for Android that optimizes memory usage, run-time performance, and power
consumption globally across all apps running on the device. The service focuses on the impact of garbage collection (GC) along these dimensions, since GC poses a significant overhead. within managed run-time environments.
Our prototype collects system-wide statistics from all running VMs, makes centralized decisions about memory management across apps and across software layers, and also collects garbage centrally. Furthermore, the global memory manager coordinates with the power manager to tune collector scheduling. In our evaluation, we illustrate the impact of such a central memory management service in reducing total energy consumption (up to 18%) and increasing throughput (up to 12%), and improving memory utilization and adaptability to user activities.
",18.001758247042904,16.789928469241776,234
VEE_17_017.txt,16.175231248876575,15.09634867074276,VEE,6831,"
Hypervisor-based virtualization solutions reveal good security and isolation, while container-based solutions make applications and workloads more portable and distributed in
an effective, standardized and repeatable way. Therefore,
nested virtualization based computing environments (e.g.,
container over virtual machine), which inherit the capabilities from both solutions, are becoming more and more attractive in clouds (e.g., running Docker over Amazon EC2
VMs). Recent studies have shown that running applications
in either VMs or containers still has significant overhead,
especially for I/O intensive workloads. This motivates us
to investigate whether the nested virtualization based solution can be adopted to build high-performance computing
(HPC) clouds for running MPI applications efficiently and
where the bottlenecks lie. To eliminate performance bottlenecks, we propose a high-performance two-layer locality and NUMA aware MPI library, which is able to dynamically detect co-resident containers inside one VM as
well as detect co-resident VM inside one host at MPI runtime. Thus the MPI processes across different containers and
VMs can communicate to each other by shared memory or
Cross Memory Attach (CMA) channels instead of network
channel if they are co-resident. We further propose an enhanced NUMA aware hybrid design to utilize InfiniBand
loopback based channel to optimize large message transfer
across containers when they are running on different sockets. Performance evaluations show that compared with the
performance of the state-of-art (1Layer) design, our proposed enhance-hybrid design can bring up to 184%, 81%
and 12% benefit on point-to-point, collective operations, and
end applications. Compared with the default performance,
our enhanced-hybrid design delivers up to 184%, 85% and
16% performance improvement
",19.51006720791134,18.283956043956042,276
VEE_17_018.txt,15.987799890725611,14.331418080283534,VEE,6984,"
Page protection is often used to achieve memory access
monitoring in many applications, dealing with program 
analysis, checkpoint-based failure recovery, and garbage
collection in managed runtime systems. Typically, low over 
head access monitoring is limited by the relatively large
page-level granularity of memory management unit hardware support for virtual memory protection. In this paper,
we improve upon traditional page-level mechanisms by additionally using hardware support for virtualization in order
to achieve fine and flexible granularities that can be smaller
than a page. We first introduce a memory allocator based
on page protection that can achieve fine-grained monitoring.
Second, we explain how virtualization hardware support can
be used to achieve dynamic adjustment of the monitoring
granularity. In all, we propose a process-level virtual machine to achieve dynamic and fine-grained monitoring. Any
application can run on our process-level virtual machine
without modification. Experimental results for an incremental checkpoint tool provide a use-case to demonstrate our
work. Comparing with traditional page-based checkpoint,
our work can effectively reduce the amount of checkpoint
data and improve performance.
",16.18397175987059,15.788139963167588,182
