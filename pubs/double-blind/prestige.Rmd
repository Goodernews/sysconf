---
output: 
  bookdown::pdf_book:
    keep_tex: true
    template: main.tex
    fig_caption: true
title: Evaluating the relationships between author reputation and single-blind peer review
bibliography: ../sysconf.bib
csl: ieee.csl
abstract: |
 | yada
 |  yada
 |  yada
---

```{r code = readLines("../load_data.R"), echo=F, message=F}
```


```{r setup, echo=F, message=F, warning=F, cache=F}
library("GGally")
library("kableExtra")

ppl <-  roles %>%
  left_join(persons) %>%
  filter(role == "author") %>%
  mutate(paper_cites = citedby / npubs)

# There are some 31 authors with 1000+ npubs, which would mean nearly one publication per week for 20 years. Clearly, many of their pubs on GS are not theirs or not real pubs. So we remove or impute those. We also remove those where the ratio of npubs to s2npubs isn't too large.

THRESH_RATIO <- 3
S2_THRESH = 1000

ppl <- ppl %>% mutate(good_npub = !is.na(npubs) & npubs / s2npubs < THRESH_RATIO)

good_npubs = !is.na(ppl$npubs) &
             !is.na(ppl$hindex) &
              ppl$npubs / ppl$s2npubs < THRESH_RATIO &
              ppl$s2npubs < S2_THRESH
#              ppl$s2npubs / ppl$npubs < THRESH_RATIO

all_good <- ppl %>%
  filter(!is.na(npubs), !is.na(hindex), !is.na(s2npubs), npubs / s2npubs < THRESH_RATIO, s2npubs / npubs < THRESH_RATIO) %>%
  group_by(name, gs_email) %>%
  summarize(npubs = first(npubs), s2npubs = min(first(s2npubs), S2_THRESH), hindex = first(hindex))


model.npubs_from_s2npubs <- lm(data = all_good, npubs ~ s2npubs)
model.hindex_from_s2npubs <- lm(data = all_good, hindex ~ s2npubs)

ppl.best_source <- ppl %>%
  mutate(npubs = ifelse(good_npubs, npubs, predict(model.npubs_from_s2npubs, data.frame(s2npubs = c(pmin(s2npubs, S2_THRESH)))))) %>%
  mutate(hindex = ifelse(good_npubs, hindex, predict(model.hindex_from_s2npubs, data.frame(s2npubs = c(pmin(s2npubs, S2_THRESH))))))

ppl.only_imputed <- ppl.best_source %>%
  filter(!good_npubs)

ppl.not_imputed <- setdiff(ppl.best_source, ppl.only_imputed)

# No good, beause these people are often real, experienced researchers, but GS inflates their publication count.
ppl.filtered <- ppl %>%
  filter(good_npubs)


# No good, because the median s2npubs for is.na(npubs) is 15, not zero. It's much higher for !is.na(npubs) however.
ppl.flat <- ppl %>%
  mutate(npubs = ifelse(is.na(npubs), 1, npubs),
         hindex = ifelse(is.na(hindex), 0, hindex))

model.npubs <- lm(data = ppl, npubs ~ as_author)
model.hindex <- lm(data = ppl, hindex ~ as_author)

# No good, because the intercept at zero is about 50 publications
ppl.linear <- ppl %>%
  mutate(npubs = ifelse(is.na(npubs), predict(model.npubs, data.frame(as_author=c(as_author))), npubs),
         hindex = ifelse(is.na(hindex), predict(model.hindex, data.frame(as_author=c(as_author))), hindex))

famous.thresh = 40

# Only the people/papers with H-index and no "famous" co-authors on the same paper
ppl.notfamous <- ppl.filtered %>%
  group_by(as.factor(key)) %>%
  mutate(maxh = max(hindex)) %>%
  filter(maxh <= famous.thresh) %>%
  ungroup()

# All the people/papers with at least one famous author per paper:
ppl.famous <- ppl.filtered %>%
  group_by(as.factor(key)) %>%
  mutate(maxh = max(hindex)) %>%
  filter(maxh > famous.thresh) %>%
  ungroup()


ppl.mismatched <- ppl.filtered %>%
  group_by(as.factor(key)) %>%
  mutate(maxh = max(hindex), maxp = max(npubs), maxhi = which.max(hindex), maxpi = which.max(npubs)) %>%
  filter(maxhi != maxpi) %>%
  ungroup()

mismatched_pairs <- ppl.mismatched %>%
  filter(npubs==maxp | hindex==maxh) %>%
  select(c(key,name, conf, hindex, npubs)) %>%
  left_join(select(all_confs, c(key, double_blind)), by=c("conf"="key"))

experience <- ppl.filtered %>%
    group_by(as.factor(key)) %>%
    summarize(conf = as.character(first(conf)), 
              coauthors = n(),
              max_npubs_idx = which.max(npubs), max_h_idx = which.max(hindex),
              max_papers = max(as_author), min_papers = min(as_author), mean_papers = mean(as_author), median_papers = median(as_author), total_papers = sum(as_author),
              max_s2npubs = max(s2npubs),
              max_h = max(hindex), min_h = min(hindex), mean_h = mean(hindex), median_h = median(hindex), total_h = sum(hindex),
              max_npubs = max(npubs), min_npubs = min(npubs), mean_npubs = mean(npubs), median_npubs = median(npubs), total_npubs = sum(npubs)) %>%
    left_join(select(all_confs, c(key, double_blind)), by = c("conf" = "key"))

db_confs = filter(all_confs, double_blind)
sb_confs = filter(all_confs, !double_blind)

# over_h: compuate how many papers in SB and DB conferences have at least one author with H-index at thresh or higher
over_h <- function(thresh)
{
  experience$over_thresh <- experience$max_h >= thresh
  c("db" = nrow(filter(experience, over_thresh, double_blind)),
    "sb" = nrow(filter(experience, over_thresh, !double_blind)),
    "p.value" = chisq.test(table(experience$over_thresh, experience$double_blind))$p.value
   )
}

over_npubs <- function(thresh)
{
  experience$over_thresh <- experience$max_npubs >= thresh
  c("db" = nrow(filter(experience, over_thresh, double_blind)),
    "sb" = nrow(filter(experience, over_thresh, !double_blind)),
    "p.value" = chisq.test(table(experience$over_thresh, experience$double_blind))$p.value
   )
}

# prestige_vs_db: For a given metric and aggregation function, compute the difference in means and median between SB and DB
prestige_vs_db <- function(who,                # Population to work on
                           metric = "hindex",  # Which primary prestige metric to use
                           aggreg = "max",     # How to aggregate authors across paper
                           logar = FALSE,      # Whether to take the log(metric+1) value or just the metric.
                           control = ""        # What conference parameters to control for
                          )
{
  fun <- get(aggreg)
  experience <- who %>%
    group_by(as.factor(key)) %>%
    summarize(conf = as.character(first(conf)),
              prestige = ifelse(logar, fun(log(!!rlang::sym(metric) + 1)), fun(!!rlang::sym(metric))),
              .groups = 'drop')  %>%
    left_join(all_confs, by = c("conf" = "key"))
  
  sb <- filter(experience, double_blind == F)
  db <- filter(experience, double_blind == T)
  means = ifelse(control == "", report_test(t.test(sb$prestige, db$prestige), p_option = "stars"), NA)
  medians = ifelse(control == "", paste0(round(median(sb$prestige)-median(db$prestige), 2),
                                         report_test(wilcox.test(sb$prestige, db$prestige), p_option = "stars", show_stat = F)), NA)
  
  form = "prestige ~ double_blind"
  if (control != "") {
    form = paste(form, "+", control)
  }
  model <- summary(lm(data = experience, formula = form))
  k = round(model$coefficients[2,1], 2)
  p = model$coefficients[2,4]
  stars = ifelse(p < 0.001, "***", ifelse(p < 0.01, "**", ifelse(p < 0.05, "*", "")))
  r2 = model$r.squared
  
  c("Metric" = metric,
    "Seniority" = aggreg,
    "Log" = logar,
    "Controls" = control,
    "Means" = means,
    "Medians" = medians,
    "Coefficient" = paste0(k, stars))
}

# report_db: for a given population and metric, report difference of means and medians for the metric between SB and DB
report_db <- function(who, metric) {
  experience <- who %>%
    group_by(as.factor(key)) %>%
    summarize(conf = as.character(first(conf)), 
              coauthors = n(),
              max_papers = max(as_author), min_papers = min(as_author), mean_papers = mean(as_author), median_papers = median(as_author), total_papers = sum(as_author),
              max_h = max(hindex), min_h = min(hindex), mean_h = mean(hindex), median_h = median(hindex), total_h = sum(hindex),
              max_npubs = max(npubs), min_npubs = min(npubs), mean_npubs = mean(npubs), median_npubs = median(npubs), total_npubs = sum(npubs)) %>%
    left_join(select(all_confs, c(key, double_blind)), by = c("conf" = "key"))

  sb <- filter(experience, double_blind == F)
  db <- filter(experience, double_blind == T)
  paste0("Mean SB: ", round(mean(sb[[metric]]), 2),
         " Mean DB: ", round(mean(db[[metric]]), 2),
         " Median SB: ", round(median(sb[[metric]]), 2),
         " Median DB: ", round(median(db[[metric]]), 2),
        " (", report_test(t.test(sb[[metric]], db[[metric]])), " -- ",
        report_test(wilcox.test(sb[[metric]], db[[metric]])), ")")
}

# is_famous_vs_db: For a given population, metric, and threshold, return ratios of papers that include a famous author
is_famous_vs_db <- function(who, metric = "hindex", thresh=40) {
  papers <- who %>%
    group_by(as.factor(key)) %>%
    summarize(conf = as.character(first(conf)),
              includes_famous = (max(!!rlang::sym(metric), na.rm = T) >= thresh),
              .groups = 'drop')  %>%
    left_join(select(all_confs, c(key, double_blind)), by = c("conf" = "key"))
  
  paste0("For ", metric, ">=", thresh, " ",
         pct(nrow(filter(papers, double_blind, includes_famous)), nrow(filter(papers, double_blind))),
         "% of single-blind papers include famous, vs. ",
         pct(nrow(filter(papers, !double_blind, includes_famous)), nrow(filter(papers, !double_blind))),
         "% of double-blind (",
         report_test(chisq.test(table(papers$double_blind, papers$includes_famous)), p_option = "stars"),
         ")")
}
```


# Introduction {#sec:intro}


<!-- send to?
  PLOS One
  PeerJ/CS
  IEEE Access
  J. of the Assoc. for Inf. Science. and Tech. (https://asistdl.onlinelibrary.wiley.com/journal/23301643)
  Journal of Information Science (https://journals.sagepub.com/home/jis)
  Quant. Science Studies (https://www.mitpressjournals.org/loi/qss)
  Scientometrics (https://www.springer.com/journal/11192)
  Trans. Prof. Comm. (https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=47) 
  Journal of Scientometric Research
  JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY
 -->

* Double-blind reviewing has been suggested as a way to reduce bias in the review process, including "prestige bias".
* Various studies found evidence for and against prestige bias in single-blind reviewing
* We found that even for the same data, the way you evaluate and measure this relationship can produce different and even contradictory results to this question.
* Emphasis: we're not trying to decide the question of prestige bias for our dataset, but rather look at the factors that can affect the outcome of such an evaluation, including:

  - How to address outliers and missing data (filter, flatten, impute, log)
  - How to measure an author's reputation? (H-index, Npubs x 2, # papers in dataset)
  - How to aggregate reputation across a group of all co-authors, since decision is on a per-paper basis? (max / mean / median / sum / min / any>thresh / seniority bands)
  - How to correct for conference reputation
  
* How do our results compare to past findings in related work?


<!--
Common research question: Is there a prestige bias in systems conference?
In other words, are prestigious authors (or institutes) more likely to be accepted to a conference than lesser-known authors?

We cannot answer this question directly without information on rejected papers, so instead we ask a weaker question:
Is there a statistically signigicant difference in the rates of famous authors published in single-blind or double-blind conferences?

The information we collected measures publication rates, not acceptance rates, so our observations do not lead to conclusive claims on which papers/authors are accepted more. Higher publication rate of certain authors in certain conferences does not necessarily imply a higher acceptance rate, because, for example, these authors could be submitting more papers to these conferences. But the two metrics are nevertheless related: all else being equal, a higher acceptance rate will imply a higher publication rate.


## Challenges: {-}

 - How do we measure researcher prestige? H-index? npubs? s2npubs? other?
 - Selection bias: most authors with missing H-index data are also less famous.
 - Survivorship bias: The submission rate of famous people in S.B and D.B conferences may not be the same as the acceptance rate.
 - Confounding conference variables: Other conference factors, like conference prestige, may be associated with S.B/D.B but much more influential in the submission choice of famous authors.
 - Computation of S2npubs is bad for people with shared names (predominatly Chinese), and therefore the imputation is as well.

-->

# Materials and methods {#sec:data}

The primary dataset we analyze comes from a hand-curated collection of `r nrow(all_confs)` peer-reviewed systems conferences from a single publication year (2017).
In CS, and in particular in its more applied fields such as systems, original scientific results are typically first published in peer-reviewed conferences [@franceschet10:role; @freyne10:relative; @patterson99:evaluating; @vardi09:conferences], and then possibly in archival journals, sometimes years later [@vrettas15:conferences].
The conferences we selected include some of the most prestigious systems conferences (based on indirect measurements such as Google Scholar's metrics), as well as several smaller or less-competitive conferences for contrast (Table \@ref(tab:sys-confs)).
We chose to focus on a large cross-sectional set of conferences from a single publication year (2017), to reduce variations in time.
Our choice of which conferences belong to "systems" is necessarily subjective.^[For the purpose of this study, we define systems as the study and engineering of concrete computing systems, which includes research topics such as: operating systems, computer architectures, data storage and management, compilers, parallel and distributed computing, and computer networks.]
Not all systems papers from 2017 are included in our set, and some papers that are in our set may not necessarily be considered part of systems (for example, if they lean more towards algorithms or theory).
However, we believe that our cross-sectional set is both wide enough to represent the field well and focused enough to distinguish it from the rest of CS.
In total, our sample includes `r sum(all_confs$npapers)` peer-reviewed papers.

```{r sys-confs, echo=F, message=F, warning=F, cache=T}
tmp <- all_confs %>%
  mutate(Conference = gsub("_\\d*", "", conference)) %>%
  rename(Date = postdate, Papers = npapers, Authors = authors_num, Country = country) %>%
  mutate(Acceptance = round(acceptance_rate, 2)) %>%
  select(Conference, Date, Papers, Authors, Acceptance) %>%
  arrange(Conference)
  
cbind(tmp[1:(nrow(tmp)/2),], tmp[(1+nrow(tmp)/2):nrow(tmp),]) %>%
  knitr::kable(booktabs = T, linesep = "",
               align = c("|l", "c", "r", "r", "r|"),
               caption = "System conferences, including start date, number of published papers, total number of named authors, acceptance rate, and country code.") %>%
  kable_styling(font_size = 8)
```


For each of these conferences, we downloaded all papers and gathered information about all authors, program committee (PC) members, and other roles.
Conferences also do not generally offer information on authors' demographics, but we were able to unambiguously link approximately two thirds (`r pct(nrow(filter(persons, !is.na(hindex))), nrow(persons))`%) of
researchers in our dataset to a Google Scholar (GS) profile.
For each author and PC member, we collected all metrics in their GS profile, such as total previous publications (ca. 2017), H-index, etc.
Note that we found no GS profile for about a third of the researchers
(`r pct(sum(is.na(authors$hindex)), nrow(authors), 2)`%),
and these researchers appear to be less experienced than researchers with a GS profile.
We therefore also another proxy metric for author experience (total number of past publications) from another source, the Semantic Scholar database.

In addition to researcher information, we gathered various statistics on each conference, either from its web page, proceedings, or directly from its chairs.
For each conference, we collected data from the web and from program committee (PC) chairs, including review policies, important dates, the composition of its technical PC, and the number of submitted papers.
We also collected historical metrics from the Institute of Electrical and Electronics Engineers (IEEE), Association for Computing Machinery (ACM), and Google Scholar (GS) websites, including past citations, age, and total publications, and downloaded all `r nrow(papers)` papers.

## Statistics

For statistical testing, group means were compared pairwise using Welch's two-sample t-test; differences between distributions of two categorical variables were tested with $\chi^{2}$ test; and comparisons between two numeric properties of the same population were evaluated with Pearson's product-moment correlation. All statistical tests are reported with their p-values.

## Code and data availability

For reproducibility, all of the data and source code files for this paper can be found at [https://github.com/eitanf/sysconf/](https://github.com/eitanf/sysconf/).

# Cleaning the raw data {#sec:cleaning}


```{r metric-distribs, echo=F,  warning=F, message=F, fig.cap="Density plots of all reputation proxy metrics (log-scale)."}
metrics.summary <- ppl %>%
  pivot_longer(cols = c("npubs", "hindex", "citedby", "paper_cites", "hindex5y", "i10index", "i10index5y", "s2npubs", "as_author"), names_to = "Metric") %>%
  mutate(Metric = recode_factor(Metric,
                                npubs = "Total publications (GS)", hindex = "H-index", citedby = "Total citations",
                                s2npubs = "Total publications (S2)", as_author = "Papers in dataset", paper_cites = "Citations per paper",
                                i10index = "I-10 index", hindex5y = "H-index (5 years)", i10index5y = "I-10 index (5 years)")) %>%
  group_by(name, gs_email, Metric) %>%
  summarize(Metric = first(Metric), value = first(value), .groups = "keep") %>%
  group_by(Metric) %>%
  mutate(count = paste0("n=", sum(!is.na(value)))) %>%
  ungroup()

total_npubs = paste0("n=", nrow(filter(authors, !is.na(hindex))))
total_s2npubs = paste0("n=", nrow(filter(authors, !is.na(s2npubs))))

counts <- data.frame(value = c(2500, 150, 150000, 12000, 400, 10, 750, 90, 50),
                     count = c(3000, 1500, 3800, 6500, 2000, 5500, 2800, 1400, 2600),
                     Metric = levels(metrics.summary$Metric),
                     label = c(rep(total_npubs, 3), rep(total_s2npubs, 2), rep(total_npubs, 4))
)
                     
ggplot(metrics.summary, aes(x = value)) +
  geom_histogram() +
  geom_text(aes(x = Inf, y = Inf, label = count)) +
  facet_wrap(~Metric, scales = "free")

ggplot(metrics.summary, aes(x = value)) +
  geom_density() +
  scale_x_log10() +
  facet_wrap(~Metric, scales = "free")
```

```{r metric-corrs, echo=F, warning=F, cache=T, fig.cap="Correlogram of reputation metrics."}
cors <- ppl %>%
  select(npubs, hindex, citedby, paper_cites, hindex5y, i10index, i10index5y, s2npubs, as_author)

ggpairs(cors)

library(corrplot)
library(RColorBrewer)

corrplot(cor(cors, use="pairw"), type = "upper", order = "hclust", col=brewer.pal(n = 8, name = "RdYlBu"), tl.col="black", tl.srt=45)


```

 * All the distributions are right-skewed.
 * The two tails have the values we're most worried about: ambiguous names; expansive vs. mininmal inclusion of publications; inexperienced authors with no GS profile.
 * Sensitivity analysis
 * The correlation between npubs and s2npubs is much higher for ppl.fitered, probably because we removed the points where they disagree by a lot.

# Empirical results {#sec:results}

In this section we explore the observed relationships between prestige and double-blind reviewing across three design dimensions, or factors: metrics of reputation, aggregation of prestige across coauthors, and additional conference variables that could affect this relationship. Because of the complexity introduced by multiple independent factors, we start with a commonly set of choices for comparisons and investigate the effect of each factor and level in relation to this baseline. The baseline we chose is to approximate the reputation of a paper's coauthors by the maximum total number of prior publications of any coauthor and ignoring any confounding variables. This metric has been commonly used in other studies as well \cite{...}.

## Reputation metrics {#subsec:metrics}

Author reputation or prestige is an intangible, abstract concept. Nevertheless, quantitative studies like ours require a concrete approximation. As a proxy for the intangible reputation, most bibliometric studies employ either metrics based on publication counts [@cite; @cite; @cite] on citation counts [@cite; @cite; @cite], or both [@cite]. We next describe these metrics and their distributions in our dataset.


### Publication counts

Counting the number of past publications is ostensibly the most straightforward way to quantify how well-known an author is. After all, the more papers that carry their name, the more likely it is that their name is recognized by reviewers. But the relationship is hardly linear in practice:

 * Not all papers are equally well-known, read, or cited (a problem that the H-index attempts to address).
 * Systems papers vary significantly  in numbers of coathurs, making partial credit attribution particularly tricky, even with H-index \cite{h-frac}.
 * Deciding what counts as a "publication" can be subjective and vary from source to source. For example, do patents count? Arxiv preprints? blog posts?
 * Accurate collection of even the raw data is complicated by factors such as author name disambiguation, publication disambiguation, and noisy or crowd-sourced data sources. 
 
npubs/s2npubs, papers in dataset; from now on use only s2npubs? (because more data than npubs but correlated and cleaner; papers in dataset measure of present and not past reputation?)

### Citation-based metrics

H index,  i10, h5_index, total citations, citations/npubs

From now on, discard i10 and h5 because of correlations?


### Program committee metrics

As PC total
PC in this conference

## Group aggregations {#subsec:aggregation}

max, sum, min, mean, seniority tiers, any-famous (plot SB bias as function of threshold)
The last two avoid the problem of outliers on the right tail; using with s2npubs also avoids the problem of the missing data on the left end.

### Binary classification of fame

So far, we have treated the reputation or fame of researchers as a continuous variable. This treatment allowed us to answer the question of whether the likelihood of an author publish in SB proceedings increases proportionately with their fame. But the question of reputation bias may instead be more binary, based on the accept/reject outcome of a paper: either a paper is accepted based on name recogniction of the author or it is not. 

The quantitative question then becomes, what is the proprtion of accepted papers in SB and DB conferences for which at least one of the authors carries enough reputation to be recognized. Since we do not know a-priori what "enough reputation" means, we observe how these proportions change based on different reputation metrics and thresholds.

```{r rep-threshold-h, echo = F, warning = F, fig.cap = "Proportion of papers that have at least one author with an H-index value above the threshold."}
total_db = nrow(filter(experience, double_blind))
total_sb = nrow(filter(experience, !double_blind))
tbl <- data.frame(cbind("thresh" = 0, "db" = total_db, "sb" = total_sb, "p.value" = NA), stringsAsFactors = F, check.names = F)
for (t in seq(1, 50)) {
  tbl <- rbind(tbl, cbind("thresh" = t, t(over_h(t))))
}

tbl %>%
  mutate(sb = sb / total_sb, db = db / total_db) %>%
  pivot_longer(cols = c("sb", "db"), names_to = c("db_status"), values_to = "ratio") %>%
  ggplot(aes(x = thresh, y = ratio, color = db_status)) +
    geom_line()
```

Fig. \@ref(fig:rep-threshold-h) shows the difference in proportions of papers written by famous authors in either single- or double-blind conferences. It shows famous authors consistently over-represented in double-blind conferences by a nearly constant amount of some 5%--10%. Comparing these proportions across single- and double-blind conferences with a $\chi{}^2$-test yields p-values below 0.05 for `r pct(sum(tbl$p.value < 0.05, na.rm = T), nrow(tbl) - 1, 1)`% of the threshold values.

```{r rep-threshold-npubs, echo = F, warning = F, fig.cap = "Proportion of papers that have at least one author with at least as many publications as the threshold."}
total_db = nrow(filter(experience, double_blind))
total_sb = nrow(filter(experience, !double_blind))
tbl <- data.frame(cbind("thresh" = 0, "db" = total_db, "sb" = total_sb, "p.value" = NA), stringsAsFactors = F, check.names = F)
for (t in seq(1, 1000, by = 1)) {
  tbl <- rbind(tbl, cbind("thresh" = t, t(over_npubs(t))))
}

tbl %>%
  mutate(sb = sb / total_sb, db = db / total_db) %>%
  pivot_longer(cols = c("sb", "db"), names_to = c("db_status"), values_to = "ratio") %>%
  ggplot(aes(x = thresh, y = ratio, color = db_status)) +
    geom_line()
```

Add graphs with other metrics

### Banded classification of seniority

## Confounding variables {#subsec:confounding}

A conference chair's choice to employ double-blind reviewing is often connected to other conference factors that may also be connected to the experience of researchers submitting to it.
Double-blind conferences tend to have a longer history
(`r report_test(t.test(db_confs$age, sb_confs$age))`), 
a lower acceptance rate
(`r report_test(t.test(db_confs$acceptance_rate, sb_confs$acceptance_rate))`),
higher H5-index in GS
(`r report_test(t.test(db_confs$h5_index, sb_confs$h5_index))`),
longer papers
(`r report_test(t.test(db_confs$mean_pages, sb_confs$mean_pages))`),
more paper submissions
(`r report_test(t.test(db_confs$submissions, sb_confs$submissions))`),
with more coauthors per paper
(`r report_test(t.test(db_confs$submissions, sb_confs$submissions))`),
lower ratio of authors coming from the PC itself
(`r report_test(t.test(db_confs$pc_author_ratio, sb_confs$pc_author_ratio))`),
and more citations to past papers
(`r report_test(t.test(db_confs$past_citations, sb_confs$past_citations))`),
especially when normalized to citations per paper
(`r report_test(t.test(db_confs$mean_historical_citations, sb_confs$mean_historical_citations))`).

In other words, double-blind conferences themselves show several other distinct characteristics, many of which associated with better reputation. The higher implied prestige of the conference could be associated with the prestige of authors as well, confounding the relationship.
Although we do not have experimental data to infer a causal relationship between the two, the inclusion alone of confounding variables could weaken the observed association between double-blind conferences and author prestige.
Every one of these variables could potentially serve as a better predictor of author reputation than double-blind reviewing, which could complicate drawing conclusions on review bias from other observational studies as well.

To evaluate the effect of these confounding variabkes, we picked the author reputation metric with the strongest relationship to double-blind reviewing, --- , and recalculated the relationship while correcting for each variable.

Check with logistic regreshion (glm with method=binomial), contrast with linear model

# Discussion {#sec:discussion}

-- There are two "types" of senior researchers. Type A publish fewer but more cited papers, and type B just publishes a lot of papers, but have lower H-index than type A relative to npubs. The DB conferences show more type-A people, and the SB more type B. The DB conferences also average higher on various prestige metrics and are more competitive. This would be consistent with fewer, more cited papers. But it means you have to correct for conference prestige.

   * To demonstrate this, compute for each paper mean citations/paper (for either max_h person or max_npubs) and correlate with acceptance rate of conference.

-- Effects of other ways of cleaning the data, outliers.

-- Even the same metric, npubs, from different data sources (GS/S2) can yield very different distributions, because each database comes up with its own choices to inherently hard questions like "what constitutes a publication" and "how to disambiguate names".

-- Reccomendations? perhaps any-famous using npubs from a erliable/complete source? Correct for conference prestige by acceptance rate (or whatever the strongest correlator to DB was).

Double-blind reviewing is also associated with fewer authors that are PC members
(`r report_test(t.test(db_confs$pc_author_ratio, sb_confs$pc_author_ratio))`),
although it has no effect on the number of papers with at least one PC author
(`r report_test(t.test(db_confs$pc_paper_ratio, sb_confs$pc_paper_ratio))`),
--Could be less PC name recognition on review, but some conferences limit how many papers a PC may submit???



<!--
 - Who are the people with large discrepancy between s2npubs and npubs? People with common names that get merged with other researchers for too many pubs; or in GS case, counting many non-paper pubs.
 - Compute the % of papers for which max_h and max_npubs are the same person: `r pct(nrow(filter(experience, max_h_idx != max_npubs_idx)), nrow(experience), 3)`%.
 - Should we winsorize high npubs instead of impute? or just use H-index?
 - Median H-index is significantly different, but mean isn't? (H-index probably not as long-tailed)
 - What is the effect of outlier publishers? Try to remove the top_n h-index and retest.
 - There appears to be a prestige bias so that more senior authors (max npubs) are more prevalent in SB conference, and this doesn't seem to change based on how we treat the "bad npubs". But this isn't true for max H-index--why?
 There are `r ppl.filtered %>% group_by(as.factor(key)) %>% summarize(maxhi = which.max(hindex), maxpi = which.max(npubs)) %>% filter(maxhi != maxpi) %>% nrow()` papers where the max npubs author is not the same as the max h-index.
- The distributions of max_npubs and max_h are different. It appears that people with lots of npubs (> 500) that publish in single-blind conferences have relatively lower h-index. Perhaps that's because they publish in less competitive conferences.
 - What is the relationship between PC h-index (mean/median) and authors' h-index? PC-author-ratio vs max_h?
 
```{r distribs, echo=F}
  experience <- ppl.filtered %>%
    group_by(as.factor(key)) %>%
    summarize(conf = as.character(first(conf)), 
              coauthors = n(),
              max_npubs_idx = which.max(npubs), max_h_idx = which.max(hindex),
              max_papers = max(as_author), min_papers = min(as_author), mean_papers = mean(as_author), median_papers = median(as_author), total_papers = sum(as_author),
              max_s2npubs = max(s2npubs),
              max_h = max(hindex), min_h = min(hindex), mean_h = mean(hindex), median_h = median(hindex), total_h = sum(hindex),
              max_npubs = max(npubs), min_npubs = min(npubs), mean_npubs = mean(npubs), median_npubs = median(npubs), total_npubs = sum(npubs)) %>%
    left_join(select(all_confs, c(key, double_blind)), by = c("conf" = "key"))
#ggplot(experience, aes(x=max_npubs, color=double_blind)) + geom_density()
#ggplot(experience, aes(x=max_h, color=double_blind)) + geom_density()
```


 - tier people into junior, medium, senior and count no. of people

```{r seniority bands, echo=F}
experience$band = case_when(
  experience$mean_h < 13 ~ "junior",
  experience$mean_h > 18 ~ "senior",
  TRUE ~ "mid-career"
)
experience$band = as.factor(experience$band)

#report_test(chisq.test(table(experience$double_blind, experience$band)))

filter(experience, double_blind) %>% select(band) %>% summary()
filter(experience, !double_blind) %>% select(band) %>% summary()
```

Looking at seniority bands by H-index confirms what we saw with max-H: more senior authors in double-blind, not single-blind conferences.


 - Correlate all conference factors to double_blind: 
 
 `r all_confs %>% select(double_blind, h5_index,month_of_year,review_days,mean_pages,submissions,rebuttal,age,mean_historical_citations,mean_historical_length,h5_median,pc_size,pc_author_ratio,mean_authors_per_paper,acceptance_rate,pc_paper_ratio,mean_review_load) %>% cor(use="pair")`
 

Refer to survey paper [@frachtenberg20:survey]:

Many author factors do not appear to significantly interact with double-blindness, such as: the gender, position, and research experience (based on H-index).
In terms of review scores, there do not appear to be large differences in the mean and median grades, but the distribution of grades in double-blind reviews appears wider for some categories, especially technical merit.

We doubt that double-blinding adequately explains all these phenomena [@godlee98:peer; @mcnutt90:blinding]. Instead, we suggest looking at a third factor: the reputation or prestige of a conference. Prestige may be too abstract to measure accurately, but we did collect two proxy conference metrics: the acceptance rate and H5-index (as measured by GS).


 
-->


# Related work {#sec:related}

 * [@tomkins17:reviewer] Performed an experiment on the WSDM'17 submissions: two PCs, one for SB and one for DB, with two from each PC reviewing each paper. They look at the following covariates: gender of first author (and others); sector of majority authors; paper predominantly from US authors (most common country); famous author (npubs >= 100, of which at least three from WSDM); country homophily with reviewer (we don't have that info); top-50 university, any author (based on http://www.topuniversities.com/); top company, any author (Google/Microsoft/Yahoo/Facebook). 
 
 * [@blank91:effects] (quoted from [@tomkins17:reviewer, related work]): "Perhaps the best-known experimental study of single-blind vs.double-blind reviewing behavior, and to our knowledge the onlycontrolled  experiment  in  this  area  other  than  our  own,  is  thestudy by Rebecca Blank (15). Over several years, 1,498 paperswere randomly assigned to single-blind vs. double-blind review-ing condition. While Blank performs detailed analyses of manyfacets of the data, we may summarize part of the high-level find-ings  as  follows.  First,  authors  at  top  or  bottom  institutions  donot see significant differences in acceptance decisions based onreviewing model, but authors at midtier institutions perform bet-ter in a single-blind setting, as do foreign authors and those out-side academia. Second, there is a mild indication, not statisticallysignificant, that women do slightly better in double-blind review."
 
 * [@okike16:single] (quoted from [@tomkins17:reviewer, related work]): "performed an ingenious study con-structing  an  artificial  submission  proposing  a  study  of  the  effi-cacy  of  training  to  improve  communication  in  the  operatingroom. The fabricated study was submitted to an orthopedics jour-nal  and  listed  as  authors  two  past  presidents  of  the  AmericanAcademy of Orthopedic Surgeons. With the involvement of thejournal, the study was sent to 256 reviewers, of whom 119 com-pleted  the  review,  split  between  single-blind  and  double-blindconditions. The results showed that single-blind reviewers weresignificantly more favorable toward the paper."
 
 * [@tung06:impact] looked retrospectively at VLDB and SIGMOD papers (SIGMOD is in our data set), to compare the ratio of accepted papers from famous people over time, before and after starting double-blind reviewing. Unlike a previous study, it found that when looking at median papers/famous as opposed to mean, a significant drop is noticed when switching to DB (no p-value computed). Like our approach, it looks only at accepted papers; unlike our approach, it's longitudinal, not cross-sectional.
 
 * [@fisher94:effects] performed a RCT on 57 consecutive submissions to J. of Dev. and Behavioral Pediatrics, where each paper got 2 SB and 2 DB reviews. Review grades and acceptance decisions were correlated against max npubs and lead-author npubs. It found that SB prefers authors with higher npubs.
 
 * [@peters82:peer]: A famous (infamous?) study about bias in peer review favoring famous insititutes. In an ethically controvertial experiment, they've retitled 12 famous papers with fictitious authors and institutes. Three were detected as resubmissions, and eight of the remaining nine failed SB peer review.

 * [@snodgrass06:single] offers a literature review of the question and concludes that the results on prestige bias are mixed.
 
 * [nicholas17:early] Early career researchers appear to have a positive experience overall with peer review. They actualy aim for high-impact journals and are influential in choosing where to publish.
 
 * SC'17 (in our dataset) offers a [rationale](https://sc17.supercomputing.org/conference-overview/technical-papers/sc17-double-blind-review-policy/) for double-blind reviewing that mentions "ample evidence" that DB reduces bias and improves citations.
 
 * [World University Rankings 2017](https://www.timeshighereducation.com/world-university-rankings/2017/reputation-ranking#!/page/0/length/25/sort_by/rank/sort_order/asc/cols/stats) can be a source of data to assess institution bias.

 * [@koltun21:hindex] Argues that The h-index is no longer an effective correlate of scientific reputation. See also here: [https://arxiv.org/pdf/1108.3901](https://arxiv.org/pdf/1108.3901).

 *  [@bhagat18:data] has addressed some similiar methodological issues in measuring women's representation.


# Conclusion {#sec:conclusion}

 - Add recommendations? perhaps to discussion?
 - Nearly all the metrics tested show higher prestige in single-blind, suggesting prestige bias.
 - The only exception is the metric max H-index (H-index of most senior author per paper), which is lower in single-blind (p=0.0055).
 - This reversal isn't changed whether we drop the NA H-index, flatten them, or linearly impute them from # papers or S2npubs (at least for median; also for mean when eliminating extremely high imputed values).
 - The higher prestige bias survives when controlling for conference prestige (either with h5_hindex or with I(mean_historical_citations/age)). But not when controlling for pc_author_ratio and pc_paper_ratio.


### Acknowledgements {-}

# References {-}



