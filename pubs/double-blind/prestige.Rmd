---
output: 
  bookdown::pdf_book:
    keep_tex: true
    template: main.tex
    fig_caption: true
title: Evaluating the relationships between author reputation and single-blind peer review
bibliography: ../sysconf.bib
csl: ieee.csl
abstract: |
 | ...
---

```{r code = readLines("../load_data.R"), echo=F, message=F}
```


```{r setup, echo=F, message=F, warning=F, cache=F}
library("GGally")
library("kableExtra")

ppl <-  roles %>%
  left_join(persons) %>%
  filter(role == "author") %>%
  mutate(paper_cites = citedby / npubs)

# There are some 31 authors with 1000+ npubs, which would mean nearly one publication per week for 20 years. Clearly, many of their pubs on GS are not theirs or not real pubs. So we remove or impute those. We also remove those where the ratio of npubs to s2npubs isn't too large.

THRESH_RATIO <- 3
S2_THRESH = 1000

ppl <- ppl %>% mutate(good_npub = !is.na(npubs) & npubs / s2npubs < THRESH_RATIO)

good_npubs = !is.na(ppl$npubs) &
             !is.na(ppl$hindex) &
              ppl$npubs / ppl$s2npubs < THRESH_RATIO &
              ppl$s2npubs < S2_THRESH
#              ppl$s2npubs / ppl$npubs < THRESH_RATIO

all_good <- ppl %>%
  filter(!is.na(npubs), !is.na(hindex), !is.na(s2npubs), npubs / s2npubs < THRESH_RATIO, s2npubs / npubs < THRESH_RATIO) %>%
  group_by(name, gs_email) %>%
  summarize(npubs = first(npubs), s2npubs = min(first(s2npubs), S2_THRESH), hindex = first(hindex))


model.npubs_from_s2npubs <- lm(data = all_good, npubs ~ s2npubs)
model.hindex_from_s2npubs <- lm(data = all_good, hindex ~ s2npubs)

ppl.best_source <- ppl %>%
  mutate(npubs = ifelse(good_npubs, npubs, predict(model.npubs_from_s2npubs, data.frame(s2npubs = c(pmin(s2npubs, S2_THRESH)))))) %>%
  mutate(hindex = ifelse(good_npubs, hindex, predict(model.hindex_from_s2npubs, data.frame(s2npubs = c(pmin(s2npubs, S2_THRESH))))))

ppl.only_imputed <- ppl.best_source %>%
  filter(!good_npubs)

ppl.not_imputed <- setdiff(ppl.best_source, ppl.only_imputed)

# No good, because these people are often real, experienced researchers, but GS inflates their publication count.
ppl.filtered <- ppl %>%
  filter(good_npubs)


# No good, because the median s2npubs for is.na(npubs) is 15, not zero. It's much higher for !is.na(npubs) however.
ppl.flat <- ppl %>%
  mutate(npubs = ifelse(is.na(npubs), 1, npubs),
         hindex = ifelse(is.na(hindex), 0, hindex))

model.npubs <- lm(data = ppl, npubs ~ as_author)
model.hindex <- lm(data = ppl, hindex ~ as_author)

# No good, because the intercept at zero is about 50 publications
ppl.linear <- ppl %>%
  mutate(npubs = ifelse(is.na(npubs), predict(model.npubs, data.frame(as_author=c(as_author))), npubs),
         hindex = ifelse(is.na(hindex), predict(model.hindex, data.frame(as_author=c(as_author))), hindex))

famous.thresh = 40

# Only the people/papers with h-index and no "famous" co-authors on the same paper
ppl.notfamous <- ppl.filtered %>%
  group_by(as.factor(key)) %>%
  mutate(maxh = max(hindex)) %>%
  filter(maxh <= famous.thresh) %>%
  ungroup()

# All the people/papers with at least one famous author per paper:
ppl.famous <- ppl.filtered %>%
  group_by(as.factor(key)) %>%
  mutate(maxh = max(hindex)) %>%
  filter(maxh > famous.thresh) %>%
  ungroup()


ppl.mismatched <- ppl.filtered %>%
  group_by(as.factor(key)) %>%
  mutate(maxh = max(hindex), maxp = max(npubs), maxhi = which.max(hindex), maxpi = which.max(npubs)) %>%
  filter(maxhi != maxpi) %>%
  ungroup()

mismatched_pairs <- ppl.mismatched %>%
  filter(npubs==maxp | hindex==maxh) %>%
  select(c(key,name, conf, hindex, npubs)) %>%
  left_join(select(all_confs, c(key, double_blind)), by=c("conf"="key"))

experience <- ppl.filtered %>%
    group_by(as.factor(key)) %>%
    summarize(conf = as.character(first(conf)), 
              coauthors = n(),
              max_npubs_idx = which.max(npubs), max_h_idx = which.max(hindex),
              max_papers = max(as_author), min_papers = min(as_author), mean_papers = mean(as_author), median_papers = median(as_author), total_papers = sum(as_author),
              max_s2npubs = max(s2npubs),
              max_h = max(hindex), min_h = min(hindex), mean_h = mean(hindex), median_h = median(hindex), total_h = sum(hindex),
              max_npubs = max(npubs), min_npubs = min(npubs), mean_npubs = mean(npubs), median_npubs = median(npubs), total_npubs = sum(npubs)) %>%
    left_join(select(all_confs, c(key, double_blind)), by = c("conf" = "key"))

any_pc <- ppl.filtered %>%
    group_by(as.factor(key)) %>%
    summarize(conf = as.character(first(conf)), any_pc = sum(as_pc) > 0) %>%
    left_join(select(all_confs, c(key, double_blind)), by = c("conf" = "key"))

db_confs = filter(all_confs, double_blind)
sb_confs = filter(all_confs, !double_blind)

# over_h: compute how many papers in SB and DB conferences have at least one author with h-index at thresh or higher
over_h <- function(thresh)
{
  experience$over_thresh <- experience$max_h >= thresh
  c("db" = nrow(filter(experience, over_thresh, double_blind)),
    "sb" = nrow(filter(experience, over_thresh, !double_blind)),
    "p.value" = chisq.test(table(experience$over_thresh, experience$double_blind))$p.value
   )
}

over_npubs <- function(thresh)
{
  experience$over_thresh <- experience$max_npubs >= thresh
  c("db" = nrow(filter(experience, over_thresh, double_blind)),
    "sb" = nrow(filter(experience, over_thresh, !double_blind)),
    "p.value" = chisq.test(table(experience$over_thresh, experience$double_blind))$p.value
   )
}

# prestige_vs_db: For a given metric and aggregation function, compute the difference in means and median between SB and DB
prestige_vs_db <- function(who,                # Population to work on
                           metric = "hindex",  # Which primary prestige metric to use
                           aggreg = "max",     # How to aggregate authors across paper
                           logar = FALSE,      # Whether to take the log(metric+1) value or just the metric.
                           control = ""        # What conference parameters to control for
                          )
{
  fun <- get(aggreg)
  experience <- who %>%
    group_by(as.factor(key)) %>%
    summarize(conf = as.character(first(conf)),
              prestige = ifelse(logar, fun(log(!!rlang::sym(metric) + 1)), fun(!!rlang::sym(metric))),
              .groups = 'drop')  %>%
    left_join(all_confs, by = c("conf" = "key"))
  
  sb <- filter(experience, double_blind == F)
  db <- filter(experience, double_blind == T)
  means = ifelse(control == "", report_test(t.test(sb$prestige, db$prestige), p_option = "stars"), NA)
  medians = ifelse(control == "", paste0(round(median(sb$prestige)-median(db$prestige), 2),
                                         report_test(wilcox.test(sb$prestige, db$prestige), p_option = "stars", show_stat = F)), NA)
  
  form = "prestige ~ double_blind"
  if (control != "") {
    form = paste(form, "+", control)
  }
  model <- summary(lm(data = experience, formula = form))
  k = round(model$coefficients[2,1], 2)
  p = model$coefficients[2,4]
  stars = ifelse(p < 0.001, "***", ifelse(p < 0.01, "**", ifelse(p < 0.05, "*", "")))
  r2 = model$r.squared
  
  c("Metric" = metric,
    "Seniority" = aggreg,
    "Log" = logar,
    "Controls" = control,
    "Means" = means,
    "Medians" = medians,
    "Coefficient" = paste0(k, stars))
}

# report_db: for a given population and metric, report difference of means and medians for the metric between SB and DB
report_db <- function(who, metric) {
  experience <- who %>%
    group_by(as.factor(key)) %>%
    summarize(conf = as.character(first(conf)), 
              coauthors = n(),
              max_papers = max(as_author), min_papers = min(as_author), mean_papers = mean(as_author), median_papers = median(as_author), total_papers = sum(as_author),
              max_h = max(hindex), min_h = min(hindex), mean_h = mean(hindex), median_h = median(hindex), total_h = sum(hindex),
              max_i10 = max(i10index),
              max_citedby = max(citedby),
              max_cites_per_paper = max(citedby / npubs),
              max_pcs = max(as_pc),
              max_npubs = max(npubs), min_npubs = min(npubs), mean_npubs = mean(npubs), median_npubs = median(npubs), total_npubs = sum(npubs),
              max_s2npubs = max(s2npubs), min_s2npubs = min(s2npubs), mean_s2npubs = mean(s2npubs), median_s2npubs = median(s2npubs), total_s2npubs = sum(s2npubs)) %>%
    left_join(select(all_confs, c(key, double_blind)), by = c("conf" = "key"))

  sb <- filter(experience, double_blind == F)
  db <- filter(experience, double_blind == T)
  paste0("mean SB: ", round(mean(sb[[metric]]), 2),
         ", mean DB: ", round(mean(db[[metric]]), 2)
#         , " Median SB: ", round(median(sb[[metric]]), 2)
#         , " Median DB: ", round(median(db[[metric]]), 2)
#         , " ("
         , "; ", report_test(t.test(sb[[metric]], db[[metric]]))
#         , " -- "
#         , report_test(wilcox.test(sb[[metric]], db[[metric]]))
#         ")"
        )
}

# is_famous_vs_db: For a given population, metric, and threshold, return ratios of papers that include a famous author
is_famous_vs_db <- function(who, metric = "hindex", thresh=40) {
  papers <- who %>%
    group_by(as.factor(key)) %>%
    summarize(conf = as.character(first(conf)),
              includes_famous = (max(!!rlang::sym(metric), na.rm = T) >= thresh),
              .groups = 'drop')  %>%
    left_join(select(all_confs, c(key, double_blind)), by = c("conf" = "key"))
  
  paste0("For ", metric, ">=", thresh, " ",
         pct(nrow(filter(papers, double_blind, includes_famous)), nrow(filter(papers, double_blind))),
         "% of single-blind papers include famous, vs. ",
         pct(nrow(filter(papers, !double_blind, includes_famous)), nrow(filter(papers, !double_blind))),
         "% of double-blind (",
         report_test(chisq.test(table(papers$double_blind, papers$includes_famous)), p_option = "stars"),
         ")")
}
```

<!-------------------------------------------------------------------------------------------->
<!-------------------------------------------------------------------------------------------->

# Introduction {#sec:intro}


<!-- send to?
  PLOS One
  PeerJ/CS
  IEEE Access
  J. of the Assoc. for Inf. Science. and Tech. (https://asistdl.onlinelibrary.wiley.com/journal/23301643)
  Journal of Information Science (https://journals.sagepub.com/home/jis)
  Quant. Science Studies (https://www.mitpressjournals.org/loi/qss)
  Scientometrics (https://www.springer.com/journal/11192)
  Trans. Prof. Comm. (https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=47) 
  Journal of Scientometric Research
  JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY
 -->

Numerous studies attempt to answer the question of whether prestige bias exists in a particular discipline of science [blank91:blind; @budden08:double; @cox19:cases; @fisher94:effects; @kalavar21:single; @okike16:single; @peters82:peer; @seeber17:newcomers; @tomkins17:reviewer].
This paper began as one of these studies, focusing on the field of computer systems.
We soon realized that our particular dataset was insufficient to provide conclusive evidence for or against prestige bias.
We also realized that the limitations we encountered may actually be shared by many of these studies and had not been well documented before.
The main focus of our study this became the exploration of how subtle changes in the data and methods used to answer this question can radically change the resulting outcome.

But first, what is prestige bias, and why should we care about it?
Scientists normally publish their findings in peer-reviewed venues such as journals and conferences.
Their work is judged by a small set of peers from their field, who typically remain anonymous so that their critique can remain free from author influence.
This so-called single-blind (SB) reviewing has been widely accepted as the baseline standard of credibility in the scientific process [@hames08:peer].
However, single-blind reviewing still lets reviewers know the identities of the work's authors.
Several researchers have argued that this knowledge can affect the review outcomes in subjective ways.
In particular, when reviewers encounter a famous author or affiliation they may subconsciously experience authority bias, where higher accuracy is attributed to the opinion of authority figures [@ramos17:biases]. 
This bias, also known as "the halo effect", "status bias", or prestige bias", the term we use in this paper.
It has been implicated as an important but unscientific factor in the review process, which in turn can leads to bad science and lower credibility for the scientific process as a whole [@tomkins17:reviewer]

A commonly suggested antidote to prestige bias is double-blind (DB) reviewing, where authors' identities and affiliations are hidden from reviewers.
Not all venues employ DB reviewing, both for practical and principled considerations.
However, the coexistence of SB and DB reviewing (sometimes in the same venue) opens the door to comparative studies of the effects of prestige bias.
For example, the association between reviewing policy and various biases has been assessed in fields as disparate as economics [blank91:blind], behavioral ecology [@budden08:double; @cox19:cases], ophthalmology [@kalavar21:single], medicine [@fisher94:effects; @okike16:single], psychology [@peters82:peer], and computer science [@seeber17:newcomers; @tomkins17:reviewer].
Some of these studies have found conclusive evidence for or against the existence of bias, sometimes in the same field.
Often, these studies were not directly comparable because of various differences in the data, methodologies, or metrics used.
One limitation of these studies is that there are various ways to estimate a person's name recognition, and they all yield different results.
Another limitation is that the reviewing policy of a venue is not always independent from other factors, so DB reviewing could be confounded with other factors that affect submission or acceptance decisions.
When we tried to apply similar methodologies to another broad field of research, computer systems, we discovered that the outcome can vary significantly based on the methodology and metrics used.

The main contribution of this paper is therefore an analysis of the sensitivity of the association between reviewing policy and prestige bias.
To emphasize, this study does not attempt to directly address the question of the existence of prestige bias in this field.
Instead, we focus on exposing and analyzing the factors that could change the outcomes of studies that do attempt to address this question.
These factors include questions such as: how to estimate the prestige or reputation of authors? where should data be sourced from, and how should it be cleaned? and how should we correct for confounding variables like conference prestige?
A meaningful analysis of review bias requires a careful consideration of these factors, and a cognizant effort to address the sensitivity of the outcomes to these choices.
Understanding these factors can help accomplish more reliable and stable results in future studies on bias in general, and prestige bias in particular.

The rest of this paper is organized as follows.
We start by describing the extensive dataset we collected on our field of focus in Sec. \@ref(sec:data).
As is often the case with real-world data, it is noisy and contains significant outliers.
We describe the treatment of these outliers and the cleaning of this dataset in general in Sec. \@ref(sec:cleaning).
The next section then describes empirical evidence for and against prestige bias, varying based on the types of metrics and statistical analyses performed.
This section demonstrates how contradictory but statistically significant results can be obtained by different approaches, despite working on the very same dataset.
We elaborate and discuss these findings and practical recommendations in Sec. \@ref(sec:discussion).
Finally, we review related work in Sec. \@ref(sec:related) and conclude in Sec. \@ref(sec:conclusion).


<!--

## Challenges: {-}

 - How do we measure researcher prestige? h-index? npubs? s2npubs? other?
 - Selection bias: most authors with missing h-index data are also less famous.
 - Survivorship bias: The submission rate of famous people in S.B and D.B conferences may not be the same as the acceptance rate.
 - Confounding conference variables: Other conference factors, like conference prestige, may be associated with S.B/D.B but much more influential in the submission choice of famous authors.
 - Computation of S2npubs is bad for people with shared names (predominantly Chinese), and therefore the imputation is as well.

-->

<!-------------------------------------------------------------------------------------------->
<!-------------------------------------------------------------------------------------------->

# Materials and methods {#sec:data}

The primary dataset we analyze comes from a hand-curated collection of `r nrow(all_confs)` peer-reviewed systems conferences from a single publication year (2017).
In CS, and in particular in its more applied fields such as systems, original scientific results are typically first published in peer-reviewed conferences [@franceschet10:role; @freyne10:relative; @patterson99:evaluating; @vardi09:conferences], and then possibly in archival journals, sometimes years later [@vrettas15:conferences].
The conferences we selected include some of the most prestigious systems conferences (based on indirect measurements such as Google Scholar's metrics), as well as several smaller or less-competitive conferences for contrast (Table \@ref(tab:sys-confs)).
We chose to focus on a large cross-sectional set of conferences from a single publication year (2017), to reduce variations in time.
Our choice of which conferences belong to "systems" is necessarily subjective.^[For the purpose of this study, we define systems as the study and engineering of concrete computing systems, which includes research topics such as: operating systems, computer architectures, data storage and management, compilers, parallel and distributed computing, and computer networks.]
Not all systems papers from 2017 are included in our set, and some papers that are in our set may not necessarily be considered part of systems (for example, if they lean more towards algorithms or theory).
However, we believe that our cross-sectional set is both wide enough to represent the field well and focused enough to distinguish it from the rest of CS.
In total, our sample includes `r sum(all_confs$npapers)` accepted peer-reviewed papers.

```{r sys-confs, echo=F, message=F, warning=F, cache=T}
tmp <- all_confs %>%
  mutate(Conference = gsub("_\\d*", "", conference)) %>%
  rename(Date = postdate, Papers = npapers, Authors = authors_num, Country = country) %>%
  mutate(Acceptance = round(acceptance_rate, 2)) %>%
  select(Conference, Date, Papers, Authors, Acceptance) %>%
  arrange(Conference)
  
cbind(tmp[1:(nrow(tmp)/2),], tmp[(1+nrow(tmp)/2):nrow(tmp),]) %>%
  knitr::kable(booktabs = T, linesep = "",
               align = c("|l", "c", "r", "r", "r|"),
               caption = "System conferences, including start date, number of published papers, total number of named authors, acceptance rate, and country code.") %>%
  kable_styling(font_size = 8)
```

To address decisively the question of prestige bias in computer systems, we would need to compare the prestige of accepted authors to that of rejected authors, for which we have no information.
The information we collected measures publication rates for authors, not acceptance rates, so our observations do not lead to conclusive claims on which papers/authors are accepted more.
Higher publication rate of certain authors does not necessarily imply a higher acceptance rate, because, for example, these authors could be submitting more papers to these conferences.
But the two metrics are nevertheless related: all else being equal, a higher acceptance rate will imply a higher publication rate.
So instead, we concentrate not on the existence of prestige bias, but on a related question:
Is there a statistically significant difference in the rates of famous authors published in single-blind or double-blind conferences?

To answer this question, we collected extensive information to help us estimate the prestige of authors and conferences.
For each conference, we downloaded all papers and gathered information about all authors, program committee (PC) members, and other roles.
Conferences do not generally offer information on authors' demographics, but we were able to unambiguously link approximately two thirds (`r pct(nrow(filter(persons, !is.na(hindex))), nrow(persons))`%) of
researchers in our dataset to a Google Scholar (GS) profile.
For each author and PC member, we collected all metrics in their GS profile, such as total previous publications (ca. 2017), h-index, etc.
Note that we found no GS profile for about a third of the researchers
(`r pct(sum(is.na(authors$hindex)), nrow(authors), 2)`%),
and these researchers appear to be less experienced than researchers with a GS profile.
We therefore also another proxy metric for author experience (total number of past publications) from another source, the Semantic Scholar database.

In addition to researcher information, we gathered various statistics on each conference, either from its web page, proceedings, or directly from its chairs.
For each conference, we collected data from the web and from program committee (PC) chairs, including review policies, important dates, the composition of its technical PC, and the number of submitted papers.
We also collected historical metrics from the Institute of Electrical and Electronics Engineers (IEEE), Association for Computing Machinery (ACM), and Google Scholar (GS) websites, including past citations, age, and total publications, and downloaded all `r nrow(papers)` papers.

<!-------------------------------------------------------------------------------------------->
## Statistics

For statistical testing, group means were compared pairwise using Welch's two-sample t-test; differences between distributions of two categorical variables were tested with $\chi^{2}$ test; and comparisons between two numeric properties of the same population were evaluated with Pearson's product-moment correlation. All statistical tests are reported with their p-values.

<!-------------------------------------------------------------------------------------------->
## Code and data availability

For reproducibility, all of the data and source code files for this paper can be found at [https://github.com/eitanf/sysconf/](https://github.com/eitanf/sysconf/).

<!-------------------------------------------------------------------------------------------->
<!-------------------------------------------------------------------------------------------->

# Cleaning the raw data {#sec:cleaning}


```{r metric-distribs, echo=F, warning=F, message=F, fig.cap="Density plots of all reputation proxy metrics (log-scale)."}
metrics.summary <- ppl %>%
  pivot_longer(cols = c("npubs", "hindex", "citedby", "paper_cites", "hindex5y", "i10index", "i10index5y", "s2npubs", "as_author"), names_to = "Metric") %>%
  mutate(Metric = recode_factor(Metric,
                                npubs = "Total publications (GS)", hindex = "h-index", citedby = "Total citations",
                                s2npubs = "Total publications (S2)", as_author = "Papers in dataset", paper_cites = "Citations per paper",
                                i10index = "I-10 index", hindex5y = "h-index (5 years)", i10index5y = "I-10 index (5 years)")) %>%
  group_by(name, gs_email, Metric) %>%
  summarize(Metric = first(Metric), value = first(value), .groups = "keep") %>%
  group_by(Metric) %>%
  mutate(count = paste0("n=", sum(!is.na(value)))) %>%
  ungroup()

total_npubs = paste0("n=", nrow(filter(authors, !is.na(hindex))))
total_s2npubs = paste0("n=", nrow(filter(authors, !is.na(s2npubs))))

counts <- data.frame(value = c(2500, 150, 150000, 12000, 400, 10, 750, 90, 50),
                     count = c(3000, 1500, 3800, 6500, 2000, 5500, 2800, 1400, 2600),
                     Metric = levels(metrics.summary$Metric),
                     label = c(rep(total_npubs, 3), rep(total_s2npubs, 2), rep(total_npubs, 4))
)
                     
ggplot(metrics.summary, aes(x = value)) +
  geom_histogram() +
  geom_text(aes(x = Inf, y = Inf, label = count)) +
  facet_wrap(~Metric, scales = "free")

ggplot(metrics.summary, aes(x = value)) +
  geom_density() +
  scale_x_log10() +
  facet_wrap(~Metric, scales = "free")
```

```{r metric-corrs, echo=F, warning=F, cache=T, fig.cap="Correlogram of reputation metrics."}
cors <- ppl %>%
  select(npubs, hindex, citedby, paper_cites, hindex5y, i10index, i10index5y, s2npubs, as_author)

ggpairs(cors)

library(corrplot)
library(RColorBrewer)

corrplot(cor(cors, use="pairw"), type = "upper", order = "hclust", col=brewer.pal(n = 8, name = "RdYlBu"), tl.col="black", tl.srt=45)
```

 * All the distributions are right-skewed.
 * The two tails have the values we're most worried about: ambiguous names; expansive vs. minimal inclusion of publications; inexperienced authors with no GS profile.
 * Sensitivity analysis
 * The correlation between npubs and s2npubs is much higher for ppl.filtered, probably because we removed the points where they disagree by a lot.

<!-------------------------------------------------------------------------------------------->
<!-------------------------------------------------------------------------------------------->

# Empirical results {#sec:results}

In this section we explore the observed relationships between reputation and DB reviewing across three design dimensions, or factors: metrics of reputation, aggregation of prestige across coauthors, and additional conference variables that could affect this relationship. Because of the complexity introduced by multiple independent factors, we start with a commonly set of choices for comparisons and investigate the effect of each factor and level in relation to this baseline. The baseline we chose is to approximate the reputation of a paper's coauthors by the maximum total number of prior publications of any coauthor and ignoring any confounding variables. This metric has been commonly used in other studies as well \cite{from related work}.

<!-------------------------------------------------------------------------------------------->
## Reputation metrics {#subsec:metrics}

Author reputation is an intangible, abstract concept. Nevertheless, the quantitative studies that we focus on require a concrete approximation. As a proxy for the intangible reputation, most bibliometric studies employ either metrics based on productivity (publication counts), on impact, (citation metrics), or both [@carpenter14:metrics; @dettori19:measuring]. We next describe these metrics and their distributions in our dataset. This list is certainly not exhaustive; the debate on their relative merits and effectiveness is an ongoing research topic, and new and improved metrics are proposed constantly. But this list represents some of the most commonly used metrics, and as our results show, is sufficient to surface problems in the context of reviewing policy. 


<!---------------------------------------------------->
### Publication counts

Counting the number of past publications is ostensibly the most straightforward way to quantify how well-known an author is. After all, the more papers that carry their name, the more likely it is that their name is recognized by reviewers. But the relationship is hardly linear in practice:

 * Not all papers are equally well-known, read, or cited (a problem that the impact-based metrics attempt to address).
 * Successful young researchers may be receiving significant name recognition without having yet accumulated a long publication record.
 * Deciding what counts as a "publication" can be subjective and vary from source to source. For example, do patents count? Arxiv preprints? blog posts?
 * Accurate collection of even the raw data is complicated by factors such as author name disambiguation, paper disambiguation, and noisy or crowd-sourced data sources.
 * Systems papers in particular vary significantly in numbers of coauthors because some implementations take large team effort. Credit attribution therefore becomes particularly tricky in this field, even with h-index [@koltun21:hindex].

We these limitations in mind, we start our analysis by looking at the count of previous publications for each author
For each accepted paper, we looked at the publication count of the most prolific author.
In our dataset, authors in SB conferences average a higher publication count based on GS than in DB conferences
(`r report_db(ppl.filtered, "max_npubs")`).
This relationship appears to support a hypothesis of prestige bias for these conferences (again, it is insufficient to conclude a causal relationship).
When we compare the publication counts from our other data source, S2, which covers all authors, we find similar evidence, albeit weaker
(`r report_db(ppl.best_source, "max_s2npubs")`).

We can also use a third source of paper counts that to differentiate between active and mostly inactive researchers, using only up-to-date statistics.
If we look only at the number of papers published in our cross-section of conferences from 2017, we find that SB conference authors actually average significantly fewer publications than those in DB conferences
(`r report_db(ppl.filtered, "max_papers")`).
This result could provide evidence against a prestige bias, but it could also point to a completely different hypothesis, such as authors preferring to submit to DB conferences for these specific conferences and point in time.
The evidence based on publication counts alone is inconclusive.


<!---------------------------------------------------->
### Citation-based metrics

We next turn our attention to another set of commonly used metrics, those incorporating citations, which is one measure of impact.
Citation-based metrics try to capture the fact that not all publications have the same influence on other researchers, and indirectly, on the author's reputation.
Note that the correlation between total past citations and total past papers is not particularly high, leading potentially to divergent results when evaluating prestige bias
(Pearson correlation of
$r=`r round(cor(ppl.filtered$npubs, ppl.filtered$citedby), 2)`$
when using GS paper counts,
$r=`r round(cor(ppl.filtered$s2npubs, ppl.filtered$citedby), 2)`$
with S2 data).

From GS, we collected for each author with an identifiable profile they total number of past citations, h-index, i10-index (number of papers with 10 or more citations), the h-index over the past 5 years and the i10-index over the past 5 years.
We omit the last two metrics from our discussion because of their high correlation with h-index and i10-index, respectively (Fig. \@ref(fig:metric-corrs)).
We also computed another metric from these statistics, citations per paper, by dividing total citations with total GS publications.

Once more, we look at the most senior coauthor in each paper group, and compare the averages across SB and DB conferences.
Of these, the citations-per-paper metric shows the strongest association with DB reviewing
(`r report_db(ppl.filtered, "max_cites_per_paper")`).
The h-index metric also is strongly indicated with higher values in DB conferences
(`r report_db(ppl.filtered, "max_h")`).

The total citations metric shows a much weaker association in the same direction
(`r report_db(ppl.filtered, "max_citedby")`),
while the i10 metric shows a weak association in the opposite direction.
(`r report_db(ppl.filtered, "max_i10")`).


<!---------------------------------------------------->
### Program committee metrics

Two other reputation metrics that are not regularly reported or measured are participation of the authors in review roles.
Many comparable studies focus on journal authors, where most reviewers remain unidentified, and editorial boards are relatively small.
But in CS, and systems in particular, where the main publication venue is conferences, most of the peer review is carried out by broad, publicized program committees (PCs).

Participation in a PC is related to an author's reputation in two ways.
First, to be invited to serve in a PC, an author needs to typically be regarded as an authority in the field.
We can extend this intuition to reputation metrics, either as a quantitative one---how many PC in our set an author belongs to---or a qualitative one, simply noting whether they participate in any PC at all.
For the former we observed that the most senior coauthors in each paper group average marginally more PC roles in DB conferences
(`r report_db(ppl.filtered, "max_pcs")`).
For the latter, we similarly observed that the ratio of papers authored by at least one member of a PC in our set is slightly higher for DB conferences than for SB
(`r pct(nrow(filter(any_pc, double_blind, any_pc)), nrow(filter(any_pc, double_blind)))`% vs.
`r pct(nrow(filter(any_pc, !double_blind, any_pc)), nrow(filter(any_pc, !double_blind)))`%;
`r report_test(chisq.test(table(any_pc$any_pc, any_pc$double_blind)))`).

Second, when a person submits a paper to a specific conference wherein they also serve on the PC, they are quite likely recognized by reviewers, who also compose the PC.
We can compute the percentage of papers in each conference where at least one of the authors participates in the PC for that same conference.
Comparing across review polices, we find no significant difference in the mean percentage for DB and SB conferences
(`r 100 * round(mean(filter(all_confs, double_blind)$pc_paper_ratio), 2)`% vs.
`r 100 * round(mean(filter(all_confs, !double_blind)$pc_paper_ratio), 2)`%;
`r report_test(t.test(filter(all_confs, double_blind)$pc_paper_ratio, filter(all_confs, !double_blind)$pc_paper_ratio))`).

In summary, none of the PC metrics for reputation shows a strong association with the review policy of the conference.

<!-------------------------------------------------------------------------------------------->
## Group aggregations {#subsec:aggregation}

max, sum, min, mean, first author, last author, seniority tiers, any-famous (plot SB bias as function of threshold)
The last two avoid the problem of outliers on the right tail; using with s2npubs also avoids the problem of the missing data on the left end.
Create table: x-axis is prestige metric (hindex, npubs, ...), y-axis is aggregation type; each cell shows a t-test statistic with stars, color coded for sign of t (white means no significant association, more red mean prestige bias, more blue means anti-prestige bias)

<!---------------------------------------------------->
### Binary classification of fame

So far, we have treated the reputation or fame of researchers as a continuous variable. This treatment allowed us to answer the question of whether the likelihood of an author publish in SB proceedings increases proportionately with their fame. But the question of reputation bias may instead be more binary, based on the accept/reject outcome of a paper: either a paper is accepted based on name recognition of the author or it is not. 

The quantitative question then becomes, what is the proportion of accepted papers in SB and DB conferences for which at least one of the authors carries enough reputation to be recognized. Since we do not know a-priori what "enough reputation" means, we observe how these proportions change based on different reputation metrics and thresholds.

```{r rep-threshold-h, echo = F, warning = F, fig.cap = "Proportion of papers that have at least one author with an h-index value above the threshold."}
total_db = nrow(filter(experience, double_blind))
total_sb = nrow(filter(experience, !double_blind))
tbl <- data.frame(cbind("thresh" = 0, "db" = total_db, "sb" = total_sb, "p.value" = NA), stringsAsFactors = F, check.names = F)
for (t in seq(1, 50)) {
  tbl <- rbind(tbl, cbind("thresh" = t, t(over_h(t))))
}

tbl %>%
  mutate(sb = sb / total_sb, db = db / total_db) %>%
  pivot_longer(cols = c("sb", "db"), names_to = c("db_status"), values_to = "ratio") %>%
  ggplot(aes(x = thresh, y = ratio, color = db_status)) +
    geom_line()
```

Fig. \@ref(fig:rep-threshold-h) shows the difference in proportions of papers written by famous authors in SB or DB conferences. It shows famous authors consistently over-represented in DB conferences by a nearly constant amount of some 5%--10%. Comparing these proportions across SB and DB conferences with a $\chi{}^2$-test yields p-values below 0.05 for `r pct(sum(tbl$p.value < 0.05, na.rm = T), nrow(tbl) - 1, 1)`% of the threshold values.

```{r rep-threshold-npubs, echo = F, warning = F, fig.cap = "Proportion of papers that have at least one author with at least as many publications as the threshold."}
total_db = nrow(filter(experience, double_blind))
total_sb = nrow(filter(experience, !double_blind))
tbl <- data.frame(cbind("thresh" = 0, "db" = total_db, "sb" = total_sb, "p.value" = NA), stringsAsFactors = F, check.names = F)
for (t in seq(1, 1000, by = 1)) {
  tbl <- rbind(tbl, cbind("thresh" = t, t(over_npubs(t))))
}

tbl %>%
  mutate(sb = sb / total_sb, db = db / total_db) %>%
  pivot_longer(cols = c("sb", "db"), names_to = c("db_status"), values_to = "ratio") %>%
  ggplot(aes(x = thresh, y = ratio, color = db_status)) +
    geom_line()
```

Add graphs with other metrics

<!---------------------------------------------------->
### Famous school/company

[@tomkins17:reviewer] any author (based on http://www.topuniversities.com/); top company, any author (Google/Microsoft/Yahoo/Facebook).
 * [World University Rankings 2017](https://www.timeshighereducation.com/world-university-rankings/2017/reputation-ranking#!/page/0/length/25/sort_by/rank/sort_order/asc/cols/stats) can be a source of data to assess institution bias.



<!---------------------------------------------------->
### Banded classification of seniority

<!-------------------------------------------------------------------------------------------->
## Confounding variables {#subsec:confounding}

A conference chair's choice to employ DB reviewing is often connected to other conference factors that may also be connected to the experience of researchers submitting to it.
DB conferences tend to have a longer history
(`r report_test(t.test(db_confs$age, sb_confs$age))`), 
a lower acceptance rate
(`r report_test(t.test(db_confs$acceptance_rate, sb_confs$acceptance_rate))`),
higher H5-index in GS
(`r report_test(t.test(db_confs$h5_index, sb_confs$h5_index))`),
longer papers
(`r report_test(t.test(db_confs$mean_pages, sb_confs$mean_pages))`),
more paper submissions
(`r report_test(t.test(db_confs$submissions, sb_confs$submissions))`),
with more coauthors per paper
(`r report_test(t.test(db_confs$submissions, sb_confs$submissions))`),
lower ratio of authors coming from the PC itself
(`r report_test(t.test(db_confs$pc_author_ratio, sb_confs$pc_author_ratio))`),
and more citations to past papers
(`r report_test(t.test(db_confs$past_citations, sb_confs$past_citations))`),
especially when normalized to citations per paper
(`r report_test(t.test(db_confs$mean_historical_citations, sb_confs$mean_historical_citations))`).

In other words, DB conferences themselves show several other distinct characteristics, many of which associated with better reputation. The higher implied prestige of the conference could be associated with the prestige of authors as well, confounding the relationship.
Although we do not have experimental data to infer a causal relationship between the two, the inclusion alone of confounding variables could weaken the observed association between DB conferences and author prestige.
Every one of these variables could potentially serve as a better predictor of author reputation than DB reviewing, which could complicate drawing conclusions on review bias from other observational studies as well.

To evaluate the effect of these confounding variables, we picked the author reputation metric with the strongest relationship to DB reviewing, --- , and recalculated the relationship while correcting for each variable.

Check with logistic regression (glm with method=binomial), contrast with linear model

<!-------------------------------------------------------------------------------------------->
<!-------------------------------------------------------------------------------------------->

# Discussion {#sec:discussion}

-- There are two "types" of senior researchers. Type A publish fewer but more cited papers, and type B just publishes a lot of papers, but have lower h-index than type A relative to npubs. The DB conferences show more type-A people, and the SB more type B. The DB conferences also average higher on various prestige metrics and are more competitive. This would be consistent with fewer, more cited papers. But it means you have to correct for conference prestige.

   * To demonstrate this, compute for each paper mean citations/paper (for either max_h person or max_npubs) and correlate with acceptance rate of conference.

-- Effects of other ways of cleaning the data, outliers.

-- Even the same metric, npubs, from different data sources (GS/S2) can yield very different distributions, because each database comes up with its own choices to inherently hard questions like "what constitutes a publication" and "how to disambiguate names".

-- Recommendations? perhaps any-famous using npubs from a reliable/complete source? Correct for conference prestige by acceptance rate (or whatever the strongest correlate to DB was).

DB reviewing is also associated with fewer authors that are PC members
(`r report_test(t.test(db_confs$pc_author_ratio, sb_confs$pc_author_ratio))`),
although it has no effect on the number of papers with at least one PC author
(`r report_test(t.test(db_confs$pc_paper_ratio, sb_confs$pc_paper_ratio))`),
--Could be less PC name recognition on review, but some conferences limit how many papers a PC may submit???

 * [nicholas17:early] Early career researchers appear to have a positive experience overall with peer review. They actually aim for high-impact journals and are influential in choosing where to publish.
 


<!--
 - Who are the people with large discrepancy between s2npubs and npubs? People with common names that get merged with other researchers for too many pubs; or in GS case, counting many non-paper pubs.
 - Compute the % of papers for which max_h and max_npubs are the same person: `r pct(nrow(filter(experience, max_h_idx != max_npubs_idx)), nrow(experience), 3)`%.
 - Should we winsorize high npubs instead of impute? or just use h-index?
 - Median h-index is significantly different, but mean isn't? (h-index probably not as long-tailed)
 - What is the effect of outlier publishers? Try to remove the top_n h-index and retest.
 - There appears to be a prestige bias so that more senior authors (max npubs) are more prevalent in SB conference, and this doesn't seem to change based on how we treat the "bad npubs". But this isn't true for max h-index--why?
 There are `r ppl.filtered %>% group_by(as.factor(key)) %>% summarize(maxhi = which.max(hindex), maxpi = which.max(npubs)) %>% filter(maxhi != maxpi) %>% nrow()` papers where the max npubs author is not the same as the max h-index.
- The distributions of max_npubs and max_h are different. It appears that people with lots of npubs (> 500) that publish in single-blind conferences have relatively lower h-index. Perhaps that's because they publish in less competitive conferences.
 - What is the relationship between PC h-index (mean/median) and authors' h-index? PC-author-ratio vs max_h?
 
```{r distribs, echo=F}
  experience <- ppl.filtered %>%
    group_by(as.factor(key)) %>%
    summarize(conf = as.character(first(conf)), 
              coauthors = n(),
              max_npubs_idx = which.max(npubs), max_h_idx = which.max(hindex),
              max_papers = max(as_author), min_papers = min(as_author), mean_papers = mean(as_author), median_papers = median(as_author), total_papers = sum(as_author),
              max_s2npubs = max(s2npubs),
              max_h = max(hindex), min_h = min(hindex), mean_h = mean(hindex), median_h = median(hindex), total_h = sum(hindex),
              max_npubs = max(npubs), min_npubs = min(npubs), mean_npubs = mean(npubs), median_npubs = median(npubs), total_npubs = sum(npubs)) %>%
    left_join(select(all_confs, c(key, double_blind)), by = c("conf" = "key"))
#ggplot(experience, aes(x=max_npubs, color=double_blind)) + geom_density()
#ggplot(experience, aes(x=max_h, color=double_blind)) + geom_density()
```


 - tier people into junior, medium, senior and count no. of people

```{r seniority bands, echo=F}
experience$band = case_when(
  experience$mean_h < 13 ~ "junior",
  experience$mean_h > 18 ~ "senior",
  TRUE ~ "mid-career"
)
experience$band = as.factor(experience$band)

#report_test(chisq.test(table(experience$double_blind, experience$band)))

filter(experience, double_blind) %>% select(band) %>% summary()
filter(experience, !double_blind) %>% select(band) %>% summary()
```

Looking at seniority bands by h-index confirms what we saw with max-H: more senior authors in double-blind, not single-blind conferences.


 - Correlate all conference factors to double_blind: 
 
 `r all_confs %>% select(double_blind, h5_index,month_of_year,review_days,mean_pages,submissions,rebuttal,age,mean_historical_citations,mean_historical_length,h5_median,pc_size,pc_author_ratio,mean_authors_per_paper,acceptance_rate,pc_paper_ratio,mean_review_load) %>% cor(use="pair")`
 

Refer to survey paper [@frachtenberg20:survey]:

Many author factors do not appear to significantly interact with double-blindness, such as: the gender, position, and research experience (based on h-index).
In terms of review scores, there do not appear to be large differences in the mean and median grades, but the distribution of grades in double-blind reviews appears wider for some categories, especially technical merit.

We doubt that double-blinding adequately explains all these phenomena [@godlee98:peer; @mcnutt90:blinding]. Instead, we suggest looking at a third factor: the reputation or prestige of a conference. Prestige may be too abstract to measure accurately, but we did collect two proxy conference metrics: the acceptance rate and H5-index (as measured by GS).


 
-->

<!-------------------------------------------------------------------------------------------->
<!-------------------------------------------------------------------------------------------->

# Related work {#sec:related}

<!-------------------------------------------------------------------------------------------->
## Prestige bias in peer review

The integrity of the peer-review process is of utmost importance to science and scientists, and is thus an active research area all of its own.
As an inherently human process, influenced by the opinions, perspective, and understanding of human reviewers, peer review is potentially subject to various cognitive biases.
Numerous studies attempt to identify, measure, and offer interventions for such biases.
However, the gold standard for such experiments, randomized controlled trials, is exceedingly difficult to carry out and generalize [@beverly13:findings; @ernst94:reviewer; @mahoney77:publication; @mcnutt90:blinding].
Aside from the usual ethical challenges of experimenting on humans, the properties of the scientific publication make repeatable and controllable experiments particularly difficult.
Papers are typically disallowed from concurrent submission, so A/B experimentation with the same papers is both complex and only partially controlled.
Moreover, actual scientific publications have potentially enormous impact on everyone involved, so the review process cannot be arbitrarily tweaked for the purposes of experimentation.

The upshot of such constraints is that there are relatively few studies on peer-review bias using representative controlled experiments, they are often limited in scope to one journal or conference, and depending on their data and methods, can reach incompatible conclusions [@parno17:SPsurvey; @shah18:design; @tomkins17:reviewer].
A literature survey from 15 years ago on the question of prestige bias found mixed results [@snodgrass06:single], as have we in the literature since then.

In one famous study, for example, Fisher et al performed a randomized trial on 57 consecutive submissions to the Journal of Development and Behavioral Pediatrics.
Each paper received 2 SB and 2 DB reviews, and acceptance decisions were compared to the number of publications of each paper's lead author and most-senior author.
The paper concludes that SB reviewing favors authors with higher publication counts.

A similar but more recent example experimented on the submissions to the WSDM'17 conference on data mining [@tomkins17:reviewer].
The experiment split the PC into two halves, one using SB reviewing and the other DB, with two papers from each PC reviewing each paper.
The study looked at the following covariates: author gender; sector of majority coauthors per paper; most common country of majority coauthors; country homophily with reviewers (we don't have that info); famous author (based on a criterion of at least 100 past publications, 3 of which in WSDM); and affiliation prestige (based on top-50 university list or top-4 company list).
Their findings suggest a strong bias in favor of authors with individual or affiliation prestige in SB reviews.
The paper also acknowledges the difficulty and rarity of such controlled experiments and points out to a single other study [@blank91:effects].
As Tomkins et al summarize: 

> Perhaps the best-known experimental study of SB vs. DB reviewing behavior, and to our knowledge the only controlled  experiment  in  this  area  other  than  our  own,  is  the study by Rebecca Blank (15). Over several years, 1,498 papers were randomly assigned to SB vs. DB reviewing condition. While Blank performs detailed analyses of many facets of the data, we may summarize part of the high-level findings  as  follows.  First,  authors  at  top  or  bottom  institutions  do not see significant differences in acceptance decisions based on reviewing model, but authors at midtier institutions perform better in a SB setting, as do foreign authors and those out-side academia. Second, there is a mild indication, not statistically significant, that women do slightly better in DB review. [@tomkins17:reviewer]
 
A different experimental approach, using a fabricated article listing two famous researchers as authors, found that "Reviewers were more likely to recommend acceptance when the prestigious authors’ names and institutions were visible... than when they were redacted...  and also gave higher ratings for the methods and other categories" [@okike16:single].

Yet another approach, with controversial ethical considerations, was reported in an study that retitled and resubmitted 12 famous papers in psychology with fictitious authors and institutes [@peters82:peer].
Three of those were detected as resubmissions, and eight of the remaining nine were rejected in SB peer review.

Closer to our field of study, Madden and DeWitt [@madden06:impact] looked retrospectively at VLDB and SIGMOD conference papers (SIGMOD is also in our dataset).
Similar to our approach, this observational study did not look at rejected papers or experiment with the review process itself.
While our study compares the publication ratios of famous authors across SB/DB conferences in the same field and point in time, this study compares them across time for the same two conferences, before and after they switched from SB to DB reviewing.
For the purpose of their study, "famous authors" were defined as those 25 "prolific" individuals with at least 20 past publications in these two conferences, including recent publications.
The comparison found that the mean publication rate of famous authors was substantially the same for SB years and DB years.

Interestingly, a followup analysis on the same dataset came to a contrary conclusion by simply comparing median publication rates instead of means [@tung06:impact].
The justification for using medians was that it is more robust to outliers, which for this dataset was particularly relevant because only one of the measured values fell above the mean.
This study not only demonstrates the sensitivity of similar conclusions to the metrics used, as we show as well, but also cautions that "here  are  probably a lot of other factors that must be taken into consideration  before  the  database  community  makes  a  final  choice  on  whether  to  continue  with  double  blind  review."


<!-------------------------------------------------------------------------------------------->
## Prestige metrics

Since we found no consensus in the literature on the existence of prestige bias in SB reviewing, or even on the methods to measure it, could we at least found a common answer to the smaller question, "how should we quantify research prestige?"

Unfortunately, we found no such answer either.
The field of bibliometrics is rich with studies comparing different and contradictory metrics to evaluate the productivity, prestige, and impact of scientific work and researchers, and we can only review a fraction of this discussion in the scope of this paper.

The highly influential h-index, for example, was proposed to combine productivity with echo, and to address specific shortcomings of the publications and citations count metrics, such as sensitivity to a single highly-cited paper ("one-hit wonders") and prolific authors in low-quality venues [@bornmann07:hindex; @hirsch05:index; @masic16:scientometric].

But some researchers argue that a few influential papers should count more than a bevy of poorly cited papers [@egghe06:theory; @masic16:scientometric] and that h-index is overly sensitive to the length of a researcher's career [@vinkler07:eminence].

Furthermore, the h-index has additional shortcomings of its own.
It can be inconsistent [@waltman12:inconsistency], or be manipulated with self-citations [@smith06:peer].
It also does not account properly for the magnitude of individual contributions in team papers [@koltun21:hindex; @masic16:scientometric].
This criticism applies to the total publications metric as well, and most other widely used reputation metrics.
Finally, even when deciding to use h-index, several studies found different counts in different databases (as have we).
They caution that more than one source should be used to compute h-index accurately, and that comparisons between researchers should be limited to the same data source [@barilan08:hindex; @degroote12:coverage].

Other citation-based reputation metrics have been proposed to overcome some of these limitations, such as g-index [@egghe06:theory], p-index [@senanayake14:pindex], I3 [@bornmann11:further], AR-index [@jin07:rindex], and even simply total citations.
All of these metrics have been found to correlate with each other (as we have also observed) and are generally sensitive to changes in number of publications as well [@cronin06:hindex; @ding20:exploring].
In fact, Bertoli-Barsotti and Lando built a model to predict one reputation metric based on the others, which shows that the relationships between metrics can be complex and nonlinear [@bertoli17:theoretical].
These relationships and model mean that most metrics are codependent, so adding reputation metrics to evaluate bias may not add as much signal as desired.

Nevertheless, we believe that it would be wrong to focus on a single metric without performing a sensitivity analysis with additional metrics that may provide counterevidence.
Carpenter et al also concluded, based on a comparison of traditional and emerging publication metrics, that no single metric is descriptive enough [@carpenter14:metrics].
Instead, they suggest that a complete picture of author reputation includes multiple metrics, including traditional productivity- and impact-level metrics, as well as document-level metrics such as views, downloads, tweets, and translations.
Standardizing and incorporating such \emph{altmetrics} as part of an author's reputation or name recognition could potentially improve the analyses of prestige bias in peer review.

<!-------------------------------------------------------------------------------------------->
<!-------------------------------------------------------------------------------------------->

# Conclusion {#sec:conclusion}

 - Add recommendations? perhaps to discussion?
 - Nearly all the metrics tested show higher prestige in single-blind, suggesting prestige bias.
 - The only exception is the metric max h-index (h-index of most senior author per paper), which is lower in single-blind (p=0.0055).
 - This reversal isn't changed whether we drop the NA h-index, flatten them, or linearly impute them from # papers or S2npubs (at least for median; also for mean when eliminating extremely high imputed values).
 - The higher prestige bias survives when controlling for conference prestige (either with h5_hindex or with I(mean_historical_citations/age)). But not when controlling for pc_author_ratio and pc_paper_ratio.

We hope that this paper provides a guideline and a roadmap to studies on bias on how to carefully address methodological issues that could limit the generalization or credibility of their results.

### Acknowledgements {-}

# References {-}



