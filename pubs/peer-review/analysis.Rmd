# Analysis {#sec:analysis}

In this section we address four higher-level questions that arise from our data.

## Gender Differences

Women represent only about 20--30% of CS researchers overall [@wang19:trends]. In our data, the percentage is about half that, with only
`r pct(nrow(filter(demographics, mapped_gender == "F")), nrow(filter(demographics, !is.na(mapped_gender))))`%
female survey respondents. What factors could explain this lower ratio?

One potential explanation is selection bias: women might be less inclined to respond to this survey. But the percentage of women across all authors,
`r pct(nrow(filter(good_genders, gender=="F")), nrow(good_genders))`%,
is very close.

Another explanation may be that women publish less than men in Systems. Indeed, women in our dataset did publish fewer papers on average:
`r round(mean(filter(persons, gender == "F")$as_author, na.rm = T), 2)` compared to men's
`r round(mean(filter(persons, gender == "M")$as_author, na.rm = T), 2)`.
But this gap is not large enough to explain the 2--3x representation gap with the rest of CS, and is not unique to Systems anyway [@elsevier17:gender].

A third explanation could be that female authors' papers are rejected at a higher rate than males'. We cannot test this hypothesis directly without data on rejected papers. However, three pieces of evidence counter this explanation.
First, limiting our observations to double-blind conferences, where reviewers presumably remain oblivious of the authors' gender, makes no statistically significant difference in the ratio of women.
<!---(`r db <- filter(roles, role == "author") %>% left_join(persons, by = c("name", "gs_email")) %>% left_join(all_confs, by = c("conf" = "conference")) %>% filter(double_blind == T, !is.na(gender));`) --->
Second, the same ratio also shows up when we limit our observations to lead authors only, where gender may be more prominent. And lastly,
we found no statistically significant evidence that women receive lower overall acceptance scores than men in their reviews,
<!---(`r report_t(t.test(filter(survey_with_confs, mapped_gender == "F")$gr_overall, filter(survey_with_confs, mapped_gender == "M")$gr_overall))`)--->
even when limiting to lead authors or single-blind reviews.
 This equitability extends to most other grade categories, except for originality
<!---(`r report_t(t.test(filter(survey_with_confs, mapped_gender == "F")$gr_originality, filter(survey_with_confs, mapped_gender == "M")$gr_originality))`), --->
and technical merit, especially in single-blind conferences.
<!---(`r report_t(t.test(filter(survey_with_confs, mapped_gender == "F", double_blind == F)$gr_technical, filter(survey_with_confs, mapped_gender == "M")$gr_technical))`). --->
In both categories, women scored significantly higher than men. 

In addition to these two grade differences, women also reported reviewers as somewhat more understanding, helpful, and fair than men did
(`r report_chi(chisq.test(table(survey_with_confs$gender, survey_with_confs$understanding)), 2)`,
`r report_chi(chisq.test(table(survey_with_confs$gender, survey_with_confs$helpfulness)), 2)`, and
`r report_chi(chisq.test(table(survey_with_confs$gender, survey_with_confs$fairness)), 2)`, respectively).
On the other hand, papers authored by women average more prior submissions: 
`r round(mean(filter(survey, mapped_gender == "F")$prior_subs, na.rm = T), 2)` compared to mens'
`r round(mean(filter(survey, mapped_gender == "M")$prior_subs, na.rm = T), 2)`, albeit weakly
(`r report_t(t.test(table(survey_with_confs$gender, survey_with_confs$prior_subs)))`).
Note, however, that review quality and prior submissions are strongly associated.  In other words, a paper with a longer submission history tends to rate higher on reviewer understanding, helpfulness, and fairness, regardless of gender.

In summary, our data does not reveal statistical gender differences in the review process, and in particular it does not help to explain the large gender gap in Systems. Addressing this problem may require focusing our attention elsewhere.


## English Proficiency

Another aspect of diversity in scientific publication is English-level proficiency [@lee13:bias; @murray19:gender]. All of the papers, reviews, and communications in our conference set were conducted in English, but not all authors or reviewers are native English speakers. The effective use of language can affect both reviewers' understanding of the works and authors' understanding of the reviews [@crovella08:openness; @editage18:perspectives; @flowerdew99:writing]. How does the author experience vary based on this factor?

At least in our dataset, the answer appears to be "not much". From an objective grading perspective, all but one of the review categories exhibit very similar distributions, both for teams with native English speakers and for teams with none. These categories include the presentation grade
(`r report_t(t.test(filter(survey_with_confs, any_english == F)$gr_present, filter(survey_with_confs, any_english == T)$gr_present))`),
where language skills presumably would make the most difference. The only exception was the originality grade, where teams with no native speakers averaged a normalized score a little higher than the native speakers' teams: 
(`r round(mean(filter(survey, any_english == F)$gr_originality, na.rm = T), 3)` vs.
`r round(mean(filter(survey, any_english == T)$gr_originality, na.rm = T), 3)`,
`r report_t(t.test(filter(survey_with_confs, any_english == F)$gr_originality, filter(survey_with_confs, any_english == T)$gr_originality), 3)`).

As for the subjective experience of authors, non-native speakers do feel differently about how well reviewers understand their work 
(`r report_chi(chisq.test(t(table(survey$native_english, survey$understanding))))`),
but perhaps not in the way we would expect: of those reviews with reportedly poor understanding, only
`r pct(nrow(filter(poor_understanding, any_english == F)), nrow(filter(survey_with_confs, !is.na(any_english))))`%
were from teams with no native English speakers, compared to
`r pct(nrow(filter(good_understanding, any_english == F)), nrow(filter(survey_with_confs, !is.na(any_english))))`%
non-native teams in the better-understood reviews. The overall rate of non-native teams among survey responses was
`r pct(nrow(filter(survey_with_confs, any_english == F)), nrow(filter(survey_with_confs, !is.na(any_english))))`%,
so clearly most of them did not feel misunderstood.

We can also attempt to look in the opposite direction: how does the English level of the reviewers affect how well understood the authors feel. We don't know who reviewed whose paper, or even the reviewer's native language or nationality. But we can try to estimate it by looking at their affiliation's country. We first guess the country of residence of reviewers by looking at their email affiliation, extract a country when possible, and look up whether this country includes English as one of its official languages. We then look at the conference PC overall demographics, and assign each conference a value corresponding to the percent of PC members affiliated with an English-speaking country. Program committees range from 91% English speakers (SOCC) to 24% (EuroPar), and average `r pct(mean(english_pcs$pct_english), 1, 1)`%. It turns out that this metric has no significant association with the reported understanding level of the reviews for the conference, which could merely be a reflection on the shortcomings of this statistic.

These negative results could suggest that in the overall picture of Systems research, English proficiency is merely one resource in the multidimensional skill set required to publish successfully [@bardi15:learning; @ferguson11:English], and that the binary distinction of native/non-native speaker may be inadequate to capture even this skill alone [@ferguson11:English].


## Double-Blind Reviewing {#subsec:double-blind}

Many author factors do not appear to significantly interact with double-blindness, such as: the gender, position, and research experience (based on H-index).
In terms of review scores, there do not appear to be large differences in the mean and median grades, but the distribution of grades in double-blind reviews appears wider for some categories, especially technical merit.

Double-blind reviewed conferences in our dataset do appear to accept longer papers by page count
(`r report_t(t.test(table(survey_with_confs$double_blind, survey_with_confs$mean_pages)))`)
with longer research history
(`r report_chi(chisq.test(table(survey_with_confs$double_blind, survey_with_confs$months_research)))`)
and more coauthors per paper
(`r report_t(t.test(table(survey_with_confs$double_blind, survey_with_confs$mean_authors_per_paper)))`).
These conferences are also more likely to allow rebuttals
(`r report_chi(chisq.test(table(survey_with_confs$double_blind, survey_with_confs$allow_rebuttal)))`),
and produce more reviews
(`r report_t(t.test(table(survey_with_confs$double_blind, survey_with_confs$reviews)), 2)`)
of longer length
(`r report_chi(chisq.test(table(survey_with_confs$double_blind, survey_with_confs$length)))`),
which in turn are deemed more helpful, fair, and understanding by authors (Sec. \@ref(subsec:quality)).


We doubt that double-blinding adequately explains all these phenomena [@godlee98:peer; @mcnutt90:blinding]. Instead, we propose a third variable: the reputation or prestige of a conference. Prestige may be too abstract to measure accurately, but we did collect two proxy conference metrics: the acceptance rate and H5-index (as measured by GS).
Indeed, the `r nrow(filter(all_confs, double_blind == T))` double-blind conferences in our set average a lower acceptance rate
(`r pct(mean(filter(all_confs, double_blind == T)$acceptance_rate, na.rm = T), 1, 1)`% vs.
`r pct(mean(filter(all_confs, double_blind == F)$acceptance_rate, na.rm = T), 1, 1)`%) and higher H5-index
(`r round(mean(filter(all_confs, double_blind == T)$h5_index, na.rm = T), 1)` vs.
`r round(mean(filter(all_confs, double_blind == F)$h5_index, na.rm = T), 1)`).

More competitive conferences also produce longer reviews, which are linked to better author evaluations of the reviews
(`r report_t(t.test(table(survey_with_confs$acceptance_rate, survey_with_confs$length)))`).
When correcting for conference quality, as measured by either metric, most of the previous double-blind associations fade or disappear. For example, the relationship between review length and double-blind reviews loses statistical significance when limiting ourselves to conferences with an acceptance rate of less then 0.2
(`r x <- filter(survey_with_confs, acceptance_rate >= 0.0, acceptance_rate < 0.2); report_chi(chisq.test(table(x$double_blind, x$length)))`),
or between 0.2 and 0.3
(`r x <- filter(survey_with_confs, acceptance_rate >= 0.2, acceptance_rate < 0.3); report_chi(chisq.test(table(x$double_blind, x$length)))`),
or higher than 0.3
(`r x <- filter(survey_with_confs, acceptance_rate >= 0.3, acceptance_rate < 0.9); report_chi(chisq.test(table(x$double_blind, x$length)))`).
Fig. \@ref(fig:acceptance-vs-length) depicts the relationships between these three variables.


Some studies found that a double-blind peer-review process can improve women's representation among authors [@budden08:double; @eaton19:gender; @lloyd90:gender; @wenneras01:nepotism], whereas other studies disagree [@ceci11:understanding; @lee13:bias; @ware08:peer]. Our own data does not show a significant difference in women's representation
(`r report_chi(chisq.test(table(survey_with_confs$double_blind, survey_with_confs$mapped_gender)))`).

```{r acceptance-vs-length, echo = F, message = F, warning = F, out.width = '100%', fig.cap = "Conference acceptance rate and reported review length. Observe that double-blind conferences tend to be more competitive, and more competitive conferences tend to have longer reviews."}
cbp <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#000000")
survey_with_confs %>%
  filter(!is.na(length), !is.na(acceptance_rate)) %>%
  filter(acceptance_rate > 0.14) %>% # Remove two outlier conferences
  group_by(conf, npapers, acceptance_rate, double_blind, length) %>%
  summarize(n = n()) %>%
  mutate(Review = factor(ifelse(double_blind, "Double blind", "Single blind"))) %>%
  arrange(acceptance_rate) %>%
  ggplot(aes(x = acceptance_rate, y = n, fill = forcats::fct_rev(length))) +
    geom_bar(position = "fill", stat = "identity", width = 0.005) +
    scale_x_continuous(labels = scales::percent_format()) +
    scale_y_continuous(labels = scales::percent_format()) +
    scale_fill_discrete(limits = c("1-2 Paragraphs", "Half a Page", "A Page", "Multiple Pages")) +
    facet_wrap(~ Review) +
    ylab("Reported review length (proportion)") +
    xlab("Conference acceptance rate") +
    labs(fill = "Length") +
    theme_light() +
    theme(legend.position = "top") +
    coord_flip()

#survey_with_confs %>%
#  filter(!is.na(length)) %>%
#  group_by(acceptance_rate, double_blind, length) %>%
#  summarize(n = n()) %>%
#  ggplot(aes(x = length, y = acceptance_rate, color = double_blind, size = n)) +
##         geom_boxplot(notch = T) +
#    geom_point(alpha = 0.5) +
#    geom_jitter(height = 0, width = 0.2) +
#    theme(legend.position = "top") +
#    scale_color_discrete(name = "Review policy", labels = c("Single blind", "Double blind")) +
#    xlab("Reported review length") +
#    ylab("Conference acceptance rate") +
#    coord_flip()
```

## Effect of Rebuttals {#subsec:rebuttal-analysis}

Many conferences do not employ a formal rebuttal process. And yet most
(`r pct(nrow(yes_rebut_help), nrow(yes_rebut_help) + nrow(no_rebut_help), 0)`%)
of the unique authors who were afforded a chance to respond to reviewers found the process helpful. Among the authors who responded to this question, there are some noteworthy differences between those who found rebuttals valuable and those who did not:

 Professors comprise only
`r pct(profs_yes, nrow(yes_rebut_help), 0)`%
of the respondents who found rebuttals helpful, compared to
`r pct(profs_no, nrow(no_rebut_help), 0)`%
among those who didn't
(`r report_chi(chisq.test(data.frame(c(profs_yes, profs_no), c(nrow(yes_rebut_help) - profs_yes, nrow(no_rebut_help) - profs_no))), 2)`).
In contradistinction, students found rebuttals more helpful
(`r pct(students_yes, nrow(yes_rebut_help), 0)`% vs.
`r pct(students_no, nrow(no_rebut_help), 0)`%,
`r report_chi(chisq.test(data.frame(c(students_yes, students_no), c(nrow(yes_rebut_help) - students_yes, nrow(no_rebut_help) - students_no))), 2)`),
perhaps because of their lack of experience.

More generally, the experience level of authors who found rebuttals helpful, as measured by median publications count from their GS profile, is about half that of those who did not
(`r yes <- filter(yes_rebut_help, !is.na(npubs)); no <- filter(no_rebut_help, !is.na(npubs)); round(median(yes$npubs), 0)` vs.
`r round(median(no$npubs), 0)`,
`r report_t(t.test(no$npubs, yes$npubs))`).
We've also collected information on which authors serve on PCs in any of our conferences, as another measure of experience. It agrees with the previous metrics: those authors satisfied with the rebuttal process serve on an average of `r round(mean(yes$as_pc), 1)` PCs, compared to `r round(mean(no$as_pc), 1)` PCs for those who weren't (`r report_t(t.test(no$as_pc, yes$as_pc))`). This is consistent with the mixed opinions we got directly from PC chairs on the question of rebuttals.

 Non-native English speakers were also more likely to find the rebuttals helpful
(`r pct(nonenglish_yes, nonenglish_yes + nonenglish_no, 0)`% vs.
`r pct(english_yes, english_yes + english_no, 0)`%,
`r report_chi(chisq.test(data.frame(c(english_yes, english_no), c(nonenglish_yes, nonenglish_no))))`),
perhaps because it allowed them to address gaps in communication. This difference also extends weakly to the entire team:
`r pct(sum(yes_rebut_help$any_english == F), nrow(filter(authors_with_rebut, any_english == F)), 0)`%
of responses where no team member was a native English speaker found the rebuttal helpful, vs.
`r pct(sum(yes_rebut_help$any_english == T), sum(yes_rebut_help$any_english == T) + sum(no_rebut_help$any_english == T), 0)`%
in responses from the other teams. Satisfaction with the rebuttal process is also correlated with the grade reviewers gave to a paper's presentation
(`r report_t(t.test(filter(survey, rebuttal_helpful == T)$gr_present, filter(survey, rebuttal_helpful == F)$gr_present), 3)`),
but the relationship appears counter-intuitive: the mean presentation grade given to authors satisfied with the rebuttals was
`r round(100 * mean(filter(survey, rebuttal_helpful == T)$gr_present, na.rm = T), 0)`%, compared to
`r round(100 * mean(filter(survey, rebuttal_helpful == F)$gr_present, na.rm = T), 0)`% among the dissatisfied authors.

<!---In 9 papers, different respondents responded had conflicting views on the helpfulness of the rebuttal, but these conflicts do not show any clear relationship to gender or position.--->
Rebuttal helpfulness does appear to be related to the conference. When limiting ourselves to the eleven conferences that had a formal rebuttal process and at least ten unique authors responding to this question, three conferences had higher than average dissatisfaction rate with the rebuttal process: ASPLOS, ISC, and SOSP. Conversely, in four conferences, no more than 8% of respondents were dissatisfied with the rebuttals: MICRO, PPoPP, SC, and PLDI.

