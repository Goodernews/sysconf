# Author survey results {#sec:survey}


## Demographic Questions {#subsec:demo}

We asked three demographic questions to evaluate their role in the review experience. We intentionally kept these questions to a minimum to reduce the risk of priming or other biases.

### Which best describes your position during 2017? {#question:position}

As shown in Fig. \@ref(fig:position-dist), about one third (`r pct(nrow(filter(demographics, position == "Student")), nrow(demographics))`%) of the respondents were students in 2017, another third or so professors of various ranks (`r pct(str_extract(demographics$position, "Professor") %>% na.omit() %>% length(), nrow(demographics))`%), and the rest were distributed between all other categories, including unknown.
For comparison, we looked at the inferred affiliation of `r nrow(good_affils)` total authors with an identifiable email affiliation. Of these, `r pct(sum(good_affils$sector == "COM"), nrow(good_affils))`% had an industry affiliation, compared to `r pct(nrow(filter(demographics, position == "Industry Researcher")), nrow(good_positions))`% of the non-NA survey respondents (`r report_chi(chisq.test(com_table))`). The difference for government researchers is a little larger:
`r pct(nrow(filter(demographics, position == "Government Researcher")), nrow(good_positions))`% by affiliation vs.
`r pct(sum(good_affils$sector == "GOV"), nrow(good_affils))`% among survey respondents,
but still not significant enough to suggest selection bias by position (`r report_chi(chisq.test(gov_table))`).

```{r position-dist, echo = F, message = F, out.width = '90%', warning = F, fig.cap = "Distribution of respondent positions."}
ggplot(demographics, aes(x = position)) +
         geom_histogram(stat = "count") + 
         scale_x_discrete(limits = c(NA, "Other", "Government Researcher", "Industry Researcher", "Professor", "Associate Professor", "Assistant Professor", "Postdoctoral Researcher", "Student")) +
  coord_flip() +
  ylab("Count") +
  xlab("Position") +
  theme_light()
```

Systems is a field with numerous practical applications and commercial implications. It is not surprising therefore to find a large proportion of researchers in industrial and government positions, diversifying authors across sectors.

### What is your gender?

Of the `r nrow(filter(demographics, !is.na(gender)))` non-NA responses,
`r nrow(filter(demographics, gender == "F"))`
(`r pct(nrow(filter(demographics, gender == "F")), sum(!is.na(demographics$gender)))`%)
chose "Female". In our hand-verified gender data of all authors,
`r pct(nrow(filter(good_genders, gender=="F")), nrow(good_genders))`%
were female. These two proportions are close
(`r report_chi(chisq.test(gender_table))`),
leading us to believe there is no significant selection bias by gender. Only a handful of respondents chose the non-binary option ("Other").


### What is your English level proficiency?

Of the `r nrow(filter(demographics, !is.na(native_english)))` non-NA respondents,
`r pct(nrow(filter(demographics, native_english == T)), sum(!is.na(demographics$native_english)))`%
of respondents chose "Native" for their English level proficiency. There appears to be no gender or position difference in the response to this question.

We also asked (and checked) for each paper whether there was any native English speaker among its coauthors. From this question, we estimate that about `r pct(sum(survey$any_english == T, na.rm = T), sum(!is.na(survey$any_english)), 0)`% of papers had at least one native speaking author.

## Paper History

### How many months did it take to research and write? {#question:mor}


```{r mor.dist, echo = F, message = F, warning = F}
levels(survey$months_research)[5] <- "13+"
df <- survey %>%
  group_by(paper_id) %>%
  summarize(mor = max(ordered(months_research))) %>%
  select(mor) %>%
  freq_and_prop(usena = "ifany")
df$Response = ordered(df$Response, levels = c("1-3", "4-6", "7-9", "10-12", "13+", NA))

knitr::kable(t(arrange(df, Response)), booktabs = T, align = "rrrrr") %>%
  kable_styling(latex_options = c("scale_down"), position = "center", font_size = 6)
```

The answers to this question varied most among different responses to the same paper, although typically by no more than 3 months. The response to this question was not significantly associated with the team size (number of coauthors), or lead author's experience, gender, or sector.

### How many conferences/journals was it submitted to prior to this publication? {#question:priors}

```{r subs_dist, echo = F, message = F, warning = F}
tmp <- survey
tmp[!is.na(tmp$prior_subs) & tmp$prior_subs>4,]$prior_subs = 9  # Make table a little smaller
tmp$prior_subs <- factor(tmp$prior_subs)
levels(tmp$prior_subs)[6] <- "5+"
df <- tmp %>%
  group_by(paper_id) %>%
  summarize(prior = max(ordered(prior_subs))) %>%
  select(prior) %>%
  freq_and_prop(usena = "ifany")
df$Response = ordered(df$Response, levels = c("0", "1", "2", "3", "4", "5+", NA))
knitr::kable(t(arrange(df, Response)), booktabs = T, align = "r") %>%
  kable_styling(latex_options = c("scale_down"), position = "center", font_size = 8)
```

It is instructive to see that over 40% of papers with responses are rejected on their first submission [@solomon14:survey; @wallach11:rebooting], and can take as many as 12 attempts to reach publication. We also observed a tendency of papers with a longer submission history to have a longer research history (previous question), perhaps conflating the two variables.


### Please type in their names {#question:prior-names}

Because of the free-form responses to this question, quantitative analysis is challenging. As a case study, we turn our attention to computer architecture alone, for which four of the leading conferences are represented in our data set, and are of similar size and acceptance rates. We noted that most papers that had been previously rejected from these conferences, had been mostly submitted to one of these four as well. This table shows in each cell how many papers were rejected from that row's conference prior to acceptance in that column's conference.

```{r arch.rejections, echo=F, message=F, warning=F }
priors <- data.frame(check.names = F,
             "Previous conf" = c("ASPLOS", "HPCA", "ISCA", "MICRO"),
             "ASPLOS" = c(1, 0, 4, 5),
             "HPCA" = c(1, 2, 1, 2),
             "ISCA" = c(0, 6, 1, 12),
             "MICRO" = c(0, 11, 2, 3),
             "Total papers" = c(10, 28, 12, 24),
             "Acceptance rate" = c("17.5%", "22.3%", "16.8%", "18.6%"))

priors <- t(priors)
#rownames(priors)[1] <- "Previous"
#rownames(priors)[7] <- "Acceptance rate"
knitr::kable(priors, booktabs = T,  align = rep('r', 7)) %>%
#             caption = "Prior submission counts for architecture conferences. Each row counts how many papers were rejected from that conference prior to acceptance in that column's conference. The total rejections per conference row is self-reported by authors, and includes conferences outside these four. The overall acceptance rates are computed by dividing the number of submitted papers by the number of accepted papers in 2017.") %>%
  kable_styling(latex_options = c("scale_down"), position = "center", font_size = 6)
```


The fact that these relationships work both ways, i.e., that many papers were accepted after previously being rejected from equivalent (or even the very same) conferences, can be interpreted both positively and negatively. Some respondents expressed frustration that the peer-review process can appear arbitrary [@anderson08:towards; @francois15:arbit; @vardi09:conferences; @vines11:cointoss]. Other authors expressed that an effective peer review process provides feedback that improves the paper for the next submission. These mixed opinions, combined with the long resubmission history of so many papers, suggests that selection bias in this survey played a lesser role in deciding authors' experience one way or another.


## Rebuttal Process {#subsec:rebuttal-process}


### Did the conference allow you to address reviewers concerns before final acceptance notice?

Of the `r ar <- group_by(survey, response_id) %>% filter(!is.na(allow_rebuttal)) %>% summarize(f = first(allow_rebuttal)); nrow(ar)` non-NA responses,
`r pct(nrow(filter(ar, f == T)), nrow(ar))`%
chose "Yes".
Contrast this with the conferences, of which only `r sum(all_confs$rebuttal)` of our `r nrow(all_confs)` offered a formal rebuttal option
(`r pct(sum(all_confs$npapers * all_confs$rebuttal), sum(all_confs$npapers))`%
when weighted by papers).
The discrepancy may be explained by some authors who specifically mentioned answering "Yes" to this question despite the lack of a formal rebuttal policy, because the conference had a "provisional acceptance" policy or mandatory revisions guided by a PC "shepherd". Although this response type is clearly different than a formal rebuttal, limiting our analysis to formal rebuttals only does not meaningfully change their results.


`r rebutters <- filter(survey, allow_rebuttal == T) %>% group_by(name, paper_id) %>% summarize(use = first(use_rebuttal), grade = first(gr_overall), mor = first(months_research), gender = first(mapped_gender), native = first(native_english)); pct(nrow(filter(rebutters, use == T)), nrow(rebutters), 2)`%
of the "Yes" respondents also reported that they took advantage of the rebuttal option. The few who didn't received higher overall acceptance grades on average,
(`r pct(filter(rebutters, use == F)$grade %>% mean(na.rm = T), 1)`%
vs. 
`r pct(filter(rebutters, use == T)$grade %>% mean(na.rm = T), 1)`%),
possibly obviating the need to rebut [@daume15:naacl].

There were no statistically significant differences in responses to this question by position, English proficiency, or gender. Notably, however, only men answered "no" to this question
(`r sum(rebutters$gender == "M" & rebutters$use == F)` respondents), and that
`r pct(nrow(filter(rebutters, native == F, use == T)), nrow(filter(rebutters, native == F, !is.na(use))))`%
of non-native speakers opted to respond to reviews, vs.
`r pct(nrow(filter(rebutters, native == T, use == T)), nrow(filter(rebutters, native == T, !is.na(use))))`%
of native speakers
(`r report_chi(chisq.test(table(rebutters$native, rebutters$use)))`).

### Did you find the response process helpful?

Of the non-NA responses,
`r pct(nrow(filter(survey, rebuttal_helpful == T)), nrow(filter(survey, !is.na(rebuttal_helpful))))`%
were positive. This high percentage may be a little surprising, considering how many PC chairs and authors alike commented privately on how little difference rebuttals make [@daume15:naacl]. One cautionary reminder is that the survey and statistics exclude rejected papers, which could lead to survivorship bias. It is quite plausible that authors of rejected papers were less enthused about the rebuttal process. But even among authors of accepted papers there are some noteworthy differences between those who found rebuttals valuable and those who did not.

 Professors comprise only
`r pct(profs_yes, nrow(yes_rebut_help), 0)`%
of the respondents who found rebuttals helpful, compared to
`r pct(profs_no, nrow(no_rebut_help), 0)`%
among those who didn't
(`r report_chi(chisq.test(data.frame(c(profs_yes, profs_no), c(nrow(yes_rebut_help) - profs_yes, nrow(no_rebut_help) - profs_no))), 2)`).
In contradistinction, students found rebuttals more helpful
(`r pct(students_yes, nrow(yes_rebut_help), 0)`% vs.
`r pct(students_no, nrow(no_rebut_help), 0)`%,
`r report_chi(chisq.test(data.frame(c(students_yes, students_no), c(nrow(yes_rebut_help) - students_yes, nrow(no_rebut_help) - students_no))), 2)`),
perhaps because of their lack of experience.

More generally, the experience level of authors who found rebuttals helpful, as measured by median publications count from their GS profile, is about half that of those who did not
(`r yes <- filter(yes_rebut_help, !is.na(npubs)); no <- filter(no_rebut_help, !is.na(npubs)); round(median(yes$npubs), 0)` vs.
`r round(median(no$npubs), 0)`,
`r report_t(t.test(no$npubs, yes$npubs))`).
We have also collected information on which authors serve on PCs in any of our conferences, as another measure of experience. It agrees with the previous metrics: those authors satisfied with the rebuttal process serve on an average of `r round(mean(yes$as_pc), 1)` PCs, compared to `r round(mean(no$as_pc), 1)` PCs for those who were not (`r report_t(t.test(no$as_pc, yes$as_pc))`). This is consistent with the mixed opinions we got directly from PC chairs on the question of rebuttals.

 Non-native English speakers were also more likely to find the rebuttals helpful
(`r pct(nonenglish_yes, nonenglish_yes + nonenglish_no, 0)`% vs.
`r pct(english_yes, english_yes + english_no, 0)`%,
`r report_chi(chisq.test(data.frame(c(english_yes, english_no), c(nonenglish_yes, nonenglish_no))))`),
perhaps because it allowed them to address gaps in communication. This difference also extends weakly to the entire team:
`r pct(sum(yes_rebut_help$any_english == F), nrow(filter(authors_with_rebut, any_english == F)), 0)`%
of responses where no team member was a native English speaker found the rebuttal helpful, vs.
`r pct(sum(yes_rebut_help$any_english == T), sum(yes_rebut_help$any_english == T) + sum(no_rebut_help$any_english == T), 0)`%
in responses from the other teams.
<!--Satisfaction with the rebuttal process is also correlated with the grade reviewers gave to a paper's presentation
(`r report_t(t.test(filter(survey, rebuttal_helpful == T)$gr_present, filter(survey, rebuttal_helpful == F)$gr_present), 3)`),
but the relationship appears counter-intuitive: the mean presentation grade given to authors satisfied with the rebuttals was
`r round(100 * mean(filter(survey, rebuttal_helpful == T)$gr_present, na.rm = T), 0)`%, compared to
`r round(100 * mean(filter(survey, rebuttal_helpful == F)$gr_present, na.rm = T), 0)`% among the dissatisfied authors. -->

<!---In 9 papers, different respondents responded had conflicting views on the helpfulness of the rebuttal, but these conflicts do not show any clear relationship to gender or position.--->
Rebuttal helpfulness does appear to be related to the conference. When limiting ourselves to the eleven conferences that had a formal rebuttal process and at least ten unique authors responding to this question, three conferences had higher-than-average dissatisfaction rate with the rebuttal process: ASPLOS, ISC, and SOSP. Conversely, in four conferences, no more than 8% of respondents were dissatisfied with the rebuttals: MICRO, PPoPP, SC, and PLDI.

When asked to explain their previous answer, responses varied. The main themes that emerged from the positive responses were that rebuttals allowed for clarifications, increased review scores, and improved the communication of specific points in the paper. One PC chair also thought they elicit better initial reviews and better PC discussion. The main negative themes were that rebuttals rarely change reviewers' minds, and that the process was still opaque and arbitrary.

## Review Quality Assessment {#subsec:quality}

The following questions (one for each review and paper) were designed to assess the quality of the reviews.

### How many reviews did this paper receive?


```{r reviews.dist, echo = F, message = F, warning = F}
df <- survey %>%
  group_by(paper_id) %>%
  summarize(reviews = first(reviews)) %>%
  select(reviews) %>%
  freq_and_prop(usena = "ifany")
knitr::kable(t(df), booktabs=TRUE, align=c("rrrrrrr")) %>%
    kable_styling(latex_options = c("scale_down"), position = "center", font_size = 6)
```

The papers in our dataset average more than four reviews per paper, far better than the typical 2+ reviews in an average CS journal [@publons18:peer, p. 21]). This could partially explain the attractiveness of conferences over journals, at least in Systems. Authors were also asked to qualitatively approximate _how long each review_ was. It is encouraging to find over half of the responses showing one or more pages per review
(`r pct(nrow(filter(survey, length == "A Page" | length == "Multiple Pages")), sum(!is.na(survey$length)))`%),
whereas only about
`r pct(nrow(filter(survey, length == "1-2 Paragraphs")), sum(!is.na(survey$length)))`%
of reviews were reported to be less than half a page.

### How well did the reviewer understand the paper, in your estimation? {#question:understanding}


```{r understanding.dist, echo = F, message = F, warning = F}
df <- freq_and_prop(survey$understanding, usena = "ifany")
knitr::kable(df, booktabs = T, align = "lrr") %>%
  kable_styling(position = "center", font_size = 6)

poor_understanding <- filter(survey_with_confs, understanding == "Probably didn't read it" | understanding == "Misunderstood major points")
good_understanding <- filter(survey_with_confs, understanding != "Probably didn't read it" & understanding != "Misunderstood major points")
```

Of the reviews that missed major points or worse,
`r pct(nrow(filter(poor_understanding, length == "1-2 Paragraphs" | length == "Half a Page")), nrow(filter(poor_understanding, !is.na(length))))`%
were short, spanning half a page or less. This correlation demonstrates the relationship between review quality and length
(`r report_chi(chisq.test(table(survey$understanding, survey$length)))`) [@hames08:peer; @papagiannaki07:author]. But longer is not always better or necessary, as these short reviews still comprise
`r pct(nrow(filter(survey_with_confs, understanding == "Perfectly", length == "1-2 Paragraphs" | length == "Half a Page")), nrow(filter(survey_with_confs, understanding == "Perfectly", !is.na(length))))`%
of the "perfect understanding" reviews, whereas multi-page reviews only comprise
`r pct(nrow(filter(survey_with_confs, understanding == "Perfectly", length == "Multiple Pages")), nrow(filter(survey_with_confs, understanding == "Perfectly", !is.na(length))))`%.

As for paper history, the better-understood papers appear to have had a longer history, in terms of prior submissions
(`r report_t(t.test(poor_understanding$prior_subs, good_understanding$prior_subs), 2)`),
and in terms of months researched as well.
It is conceivable that previous rejections have helped improve the communication of a resubmitted paper. 

### How helpful did you find this review for improving the paper? {#question:helpfulness}


```{r helpfulness.dist, echo = F, message = F, warning = F}
df <- freq_and_prop(survey$helpfulness)
knitr::kable(df, booktabs = T, align = "lrr") %>%
  kable_styling(position = "center", font_size = 6)
```

The helpfulness of a review is closely linked to its reported level of understanding
(`r report_chi(chisq.test(table(survey$helpfulness, survey$understanding)), 2)`),
which in turn also implies that it is closely linked to the review's length
(`r report_chi(chisq.test(table(survey$helpfulness, survey$length)), 2)`).
This result is consistent with other surveys of journal authors [@editage18:perspectives; @sense19:peer].

### How fair would you say the review was?


```{r fairness.dist, echo = F, message = F, warning = F}
df <- freq_and_prop(survey$fairness)
knitr::kable(df, booktabs = T, align = "lrr") %>%
  kable_styling(position = "center", font_size = 6)
```

Fairness in reviews is a high priority for the Systems community [@jerger17:isca]. Once more, the data shows that the perception of a review's fairness is closely tied to that of its understanding
(`r report_chi(chisq.test(table(survey$fairness, survey$understanding)), 2)`)
and helpfulness
(`r report_chi(chisq.test(table(survey$fairness, survey$helpfulness)), 2)`).

Only
`r nrow(filter(survey, fairness == "Unfair" | fairness == "Very unfair"))` of non-NA responses
(`r pct(nrow(filter(survey, fairness == "Unfair" | fairness == "Very unfair")), nrow(filter(survey, !is.na(fairness))), 2)`%)
ranked a review as 'Unfair' or 'Very unfair'. However, this relatively low number may be distorted by survivorship bias more than for any other question.
Of these responses, SOSP stands out as the conference with most 'Unfair' reviews
(`r nrow(filter(survey, fairness == "Unfair" | fairness == "Very unfair", conf == "SOSP"))`, or
`r pct(nrow(filter(survey, fairness == "Unfair" | fairness == "Very unfair", conf == "SOSP")), nrow(filter(survey, !is.na(fairness), conf == "SOSP")), 2)`%)
and ICPE as the conference with the highest percentage
(`r nrow(filter(survey, fairness == "Unfair" | fairness == "Very unfair", conf == "ICPE"))`, or
`r pct(nrow(filter(survey, fairness == "Unfair" | fairness == "Very unfair", conf == "ICPE")), nrow(filter(survey, !is.na(fairness), conf == "ICPE")), 2)`%).
The only other unusual aspect of these responses is that only one came from a woman
(`r pct(nrow(filter(survey, gender == "F", fairness == "Unfair" | fairness == "Very unfair")), nrow(filter(survey, !is.na(gender), fairness == "Unfair" | fairness == "Very unfair")), 1)`%).

## Review Scores

We asked respondents to upload their reviews' text, or alternately to fill in the actual grades they received in the reviews of up to six reviews per paper and in seven different categories, when applicable. All grades were then normalized, so that the lowest grade in a category always received 0 and the highest always 1. The transcription of reviews, scaling, and calibration process were error-prone, possibly introducing some noise to these responses. The distributions of these normalized scores are depicted in Fig. \@ref(fig:review-grades).

```{r review-grades, echo = F, message = F, warning = F, out.width = '100%', fig.cap = "Normalized grades and response distribution. Diamonds represent mean scores. Bars represent median scores, with a notched 95-pct confidence. N is the number of scores received in each category." }
survey_with_confs %>%
  select(starts_with("gr_")) %>%
  rename(Overall = gr_overall, Technical = gr_technical, Presentation = gr_present, Impact = gr_impact, Originality = gr_originality, Relevance = gr_relevance, Confidence = gr_confidence) %>%
  melt() %>%
  rename(Category = variable, Grade = value) %>%
  filter(!is.na(Grade)) %>%
  group_by(Category) %>%
  add_count() %>%
  ungroup() %>%
  ggplot(aes(x = Category, y = Grade, color = Category)) +
         geom_text(aes(y = 1.0, label = paste0("N=",n)), vjust = -0.5) +
         geom_boxplot(notch = T) +
         stat_summary(fun.y=mean, geom="point", shape=23, size=4) +
         theme(legend.position = "none") +
         ylab("Normalized grade") 
```

One observation is that the "Relevance" grade appears mostly irrelevant, both because of its narrow distribution, and because of the low number of conferences that ask for it. Conceivably, an out-of-scope paper could simply get rejected and excluded from our dataset.

It is also not surprising that all papers average above 50% for all grades---after all, they have all been accepted [@vines11:cointoss]. The inter-quartile range for the overall grade is 0.5-0.75, meaning that half the papers probably got accepted with an overall recommendation somewhere between "weak accept" and "accept". Perhaps more surprisingly, about `r pct(20, 201,0)`% of the papers were accepted despite a low (< 0.5 average) acceptance recommendation, and about `r pct(43, 201,0)`% of the accepted papers had low reviewer confidence (<0.5 average).

It is illuminating to see that there is no correlation between a paper's overall grade and the number of past rejections
(`r report_cor(cor.test(survey$prior_subs, survey$gr_overall), 2)`).
Perhaps multiple submissions do not improve a paper, or if they do, they merely bring it to the levels where other accepted papers are evaluated.

