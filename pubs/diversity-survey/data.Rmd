# Methodology and Data {#sec:data}

Before issuing our survey, we collected data from various external sources to complement and corroborate its findings, starting with the conferences themselves. We selected `r nrow(all_confs)` conferences from the Systems and related areas. These peer-reviewed conferences include some of the most prestigious in the field, as well as others for comparison. They vary in scope and in size (from 7 to 151 papers), but all are rigorously peer-reviewed and all are from 2017. The complete list of conferences is given in Table \@ref(tab:sys-confs).

For each conference we collected data from the Web and program committee (PC) chairs, including review policies, important dates, the composition of its technical PC, and the number of submitted papers. We also collected historical metrics from the IEEE, ACM, and Google Scholar (GS) websites, including past citations, age, and total publications, and downloaded all `r nrow(papers)` papers. From the conference and paper text, we compiled the complete list of authors for all `r nrow(all_confs)` conferences (a total of `r nrow(authors)` unique persons), as well as their email addresses. These addresses not only served for the survey's distribution, but also to infer an author's affiliation, sector, and country of residence. If an email address was not shown in the paper, we attempted to infer authors' affiliation from their GS profile, when uniquely identifiable. These profiles also provide indirect metrics on author's research experience, such as their H-index [@hirsch05:index]. Finally, we also manually verified the gender of `r pct(nrow(filter(authors, !is.na(gender))), nrow(authors))`% of authors, by looking up their photos and pronouns on the web.^[We recognize that gender is a complex, non-binary identity that cannot be captured adequately by just photos or pronouns. However, the focus of this study is on perceived gender, not self-identification, which is often judged by the same simplistic criteria as photos and pronouns.]


```{r sys-confs, echo=F, message=F, warning=F, cache=T}
conf_counts <- survey %>%
  group_by(paper_id, conf) %>%
  summarize(n = n()) %>%
  group_by(conf) %>%
  summarize(Response = n()) %>%
  rbind(data.frame(conf = "HotI", Response = 0))

tmp <- data.frame(Name = gsub("_\\d\\d", "", as.character(all_confs$key)),
           Start = all_confs$postdate,
           Papers = all_confs$npapers)  %>%
    left_join(conf_counts, by = c("Name" = "conf")) %>%
    mutate(Response = pct(Response, Papers, 0)) %>%
    arrange(Name) %>%
#    arrange(desc(Response)) %>%
    mutate(Response = paste0(as.character(Response), "%"))

df <- cbind(tmp[1:(nrow(tmp)/2),], tmp[(1+nrow(tmp)/2):nrow(tmp),])

df %>%
    knitr::kable(format = "latex", longtable = F, booktabs = T, align = c('l', 'c', 'r', 'r', 'l', 'c', 'r', 'r'),
                 caption = "Conferences in our dataset with their start date, number of accepted papers, and survey response rate by papers", escape = T) %>%
    kable_styling(latex_options = c("scale_down", "repeat_header"), font_size = 7, position = "center")
```

We sent our survey to all 5919 valid email addresses during the summer of 2018, and `r nrow(raw_survey)` authors responded. We asked a few demographic questions, as well as questions about their paper and about review process, repeated for up to three distinct papers from our data set. Non-responses to a question were marked as "NA".

Of the `r nrow(unique(survey$paper_id))` papers,
`r survey %>% group_by(paper_id) %>% mutate(n = n_distinct(name)) %>% filter(n > 1) %>% select(paper_id) %>% unique() %>% nrow()`
had responses from more than one author. These responses were typically identical or off by one unit, and always tested statistically insignificant in aggregate. In five papers, the responses from different authors were so inconsistent across questions that we elided them from our data. These inconsistencies relate mostly to the paper's history, and may be the results of authors joining or leaving the project at different times.
<!---
```{r code-to-see-discrepancies}
# survey %>% group_by(paper_id) %>% summarize(tmax = range(as.numeric(months_research), na.rm = T)[2], tmin = range(as.numeric(months_research), na.rm = T)[1], diff = tmax-tmin)
```
-->


## Limitations {-}

Our methodology involves several limitations and tradeoffs that we address throughout the paper. First, by focusing only on Systems, we may be limiting the applicability of our findings to this subfield. By focusing on a single year, we cannot report trends. These choices were deliberate, so as to eliminate extraneous variability in our data. Our survey is also subject selection bias (representing only authors who responded to the survey and/or each question). Since we found no statistically significant demographic differences between all authors and survey respondents only, we believe the effect of this bias is benign (see also [@daume15:naacl; @papagiannaki07:author]). Additionally, the effort involved in compiling all the data in the preparation for the survey took nearly a year, by which time some authors had difficulty recalling some details, and generally lead to fewer responses.

Last, but certainly not least, is survivorship bias. Since we only polled authors of accepted papers, we have no information on all submitted papers.  Our survey data is insufficient to distinguish between the demographics of accepted and rejected authors, leaving the door open to undetected biases in the peer-review process. That said, we found no difference in the demographics of accepted papers between double-blind and single-blind papers, which reduces the likelihood that the demographic questions of the survey would be answered differently for rejected papers. This is not necessarily the case for the other survey sections on paper history and review process. The reader is therefore strongly advised to limit any conclusions they may draw from this data to accepted authors only. Even with this limitation, there is value in comparing the responses across different demographics within accepted authors.

Because of the complexity of the experimental design, there are few cotrolled studies that evaluate the peer-review process on both accepted and rejected papers, and they are typically limited in scope to one conference or journal [@tomkins17:reviewer]. We chose an observational approach for our study that lets us examine an entire field of study, but at the cost of surviorship bias and experimental control. We believe both approaches to be useful and complementary.
