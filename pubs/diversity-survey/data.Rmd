# Methodology and Data {#sec:data}

Before issuing our survey, we collected data from various external sources to complement and corroborate its findings, starting with the conferences themselves. We selected `r nrow(all_confs)` conferences from Systems and related areas. These peer-reviewed conferences include some of the most prestigious in the field, as well as others for comparison. They vary in scope and size (from 7 to 151 papers), but all are rigorously peer-reviewed and all are from 2017. The complete list of conferences is given in Table \@ref(tab:sys-confs).

For each conference we collected data from the Web and program committee (PC) chairs, including review policies, important dates, the composition of its technical PC, and the number of submitted papers. We also collected historical metrics from the IEEE, ACM, and Google Scholar (GS) websites, including past citations, age, and total publications, and downloaded all `r nrow(papers)` papers. From the conference and paper text, we compiled the complete list of authors for all `r nrow(all_confs)` conferences (a total of `r nrow(authors)` unique persons), as well as their email addresses. These addresses not only served for the survey's distribution, but also to infer an author's affiliation, sector, and country of residence. If an email address was not shown in the paper, we attempted to infer authors' affiliation from their GS profile, when uniquely identifiable. These profiles also provide indirect metrics on author's research experience, such as their H-index [@hirsch05:index]. Finally, we also manually verified the gender of `r pct(nrow(filter(authors, !is.na(gender))), nrow(authors))`% of authors, by looking up their photos and pronouns on the web.^[We recognize that gender is a complex, non-binary identity that cannot be captured adequately by just photos or pronouns. However, the focus of this study is on perceived gender, not self-identification, which is often judged by the same simplistic criteria.]


```{r sys-confs, echo=F, message=F, warning=F, cache=T}
conf_counts <- survey %>%
  group_by(paper_id, conf) %>%
  summarize(n = n()) %>%
  group_by(conf) %>%
  summarize(Response = n()) %>%
  rbind(data.frame(conf = "HotI", Response = 0))

tmp <- data.frame(Name = gsub("_\\d\\d", "", as.character(all_confs$key)),
           Start = all_confs$postdate,
           Papers = all_confs$npapers)  %>%
    left_join(conf_counts, by = c("Name" = "conf")) %>%
    mutate(Response = pct(Response, Papers, 0)) %>%
    arrange(Name) %>%
#    arrange(desc(Response)) %>%
    mutate(Response = paste0(as.character(Response), "%"))

df <- cbind(tmp[1:(nrow(tmp)/2),], tmp[(1+nrow(tmp)/2):nrow(tmp),])

df %>%
    knitr::kable(format = "latex", longtable = F, booktabs = T, align = c('l', 'c', 'r', 'r', 'l', 'c', 'r', 'r'),
                 caption = "Conferences in our dataset with their start date, number of accepted papers, and survey response rate by papers", escape = T) %>%
    kable_styling(latex_options = c("scale_down", "repeat_header"), font_size = 6, position = "center")
```

We sent our survey to all 5919 valid email addresses during the summer of 2018, and `r nrow(raw_survey)` authors responded. We asked a few demographic questions, as well as questions about their paper and about the review process, repeated for up to three distinct papers from our dataset. Non-responses to a question were marked as "NA".

Of the `r length(unique(survey$paper_id))+5` papers,
`r survey %>% group_by(paper_id) %>% mutate(n = n_distinct(name)) %>% filter(n > 1) %>% select(paper_id) %>% unique() %>% nrow()`
had responses from multiple authors.
The distribution of responses per paper was statistically similar to the distribution of co-authors per paper,
(`r dist_survey <- survey %>% group_by(paper_id) %>% summarize(nauthors = n()); dist_authors <- roles %>% filter(role == "author") %>% group_by(key) %>% summarize(nauthors = n()); report_t(t.test(dist_authors$nauthors, dist_survey$nauthors), 2)`), 
suggesting that authors were equally likely to respond to the survey, regardless of the paper.

Survey responses from different authors to the same paper typically identical or off by one unit, and always tested statistically insignificant in aggregate. In five papers, the responses from different authors were so inconsistent across questions that we elided them from our data. These inconsistencies relate mostly to the paper's history, and may be the results of authors joining or leaving the project at different times.

<!---
```{r code-to-see-discrepancies}
# survey %>% group_by(paper_id) %>% summarize(tmax = range(as.numeric(months_research), na.rm = T)[2], tmin = range(as.numeric(months_research), na.rm = T)[1], diff = tmax-tmin)
```
-->


## Limitations {-}

Our methodology involves several limitations and tradeoffs worthy of mention. First, by focusing only on Systems, we may be limiting the applicability of our findings to this subfield. By focusing on a single year, we cannot report trends. These choices were deliberate, so as to eliminate extraneous variability in our data. Second, our survey is subject to selection bias (representing only authors who responded to the survey or to each question). Since we found no statistically significant demographic differences between all authors and survey respondents only (Sec. \@ref(sec:analysis)), we believe the effect of this bias is minimal (see also [@daume15:naacl; @papagiannaki07:author]). Third, the effort involved in compiling all the data in preparation for the survey took nearly a year, by which time some authors have difficulty recalling some details, leading to fewer responses, as stated in their comments. Fourth, we collected gender data manually from the Web, which is prone to human errors. But we have empirically determined that our manually collected data is more accurate than gendered inferred automatically by given names.

Last, but certainly not least, is survivorship bias. Since we only polled authors of accepted papers, we have no information on all submitted papers.  Our survey data is insufficient to distinguish between the demographics of accepted and rejected authors, leaving the door open to undetected biases in the peer-review process. That said, we found no difference in the demographics of accepted papers between double-blind and single-blind papers, which reduces the likelihood that the demographic section of the survey would be answered differently for rejected papers. This is not necessarily the case for the other survey sections on paper history and review process. The reader is therefore cautioned to limit any conclusions they may draw from this data to accepted authors only. Nevertheless, there is still value in comparing the responses across different demographics within accepted authors.

We found very few controlled studies that evaluate the peer-review process on both accepted and rejected papers, and they are typically limited in scope to one conference or journal [@parno17:SPsurvey; @tomkins17:reviewer]. We chose an observational approach instead, which lets us examine an entire field of study, but at the cost of survivorship bias and experimental control. We believe both approaches to be useful and complementary.
