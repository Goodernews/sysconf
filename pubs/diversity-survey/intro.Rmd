# Introduction {#sec:intro}

Peer review is a cornerstone of modern scientific research. And yet understanding and improving this process is challenging, because it can be hard to experiment with [@beverly13:findings; @mahoney77:publication; @mcnutt90:blinding]. For example, reputable conferences disallow parallel submission, and even within the same conference we cannot design an experiment where the same paper is reviewed multiple times with fully controlled variations. Thus, decisions on peer-review policies are often based more on the opinions of editors or program chairs, and less based on hard data and evidence.

This article attempts to present such data and evidence from statistical observations on the peer-review process for a specific year (2017) and a specific subfield of research computing (computer systems or "Systems"). Like most subfields of computer science, the primary channel for publishing research results in Systems is peer-reviewed conferences [@franceschet10:role; @freyne10:relative; @patterson99:evaluating; @vardi09:conferences]. Reputable Systems conferences employ similar policies, such as giving each paper at least three blind reviews (where the identity of the specific reviewers is hidden from authors). But conferences can vary considerably in other aspects, such as double-blind reviews (authors' identities are hidden from reviewers), rebuttals, two-phase reviews, etc. These decisions can potentially have dramatic effects on both the quality of the conference and the experience of the authors, but there appear to be conflicting opinions on the effects and tradeoffs of these policies [@mainguy05:peer].

The primary goal of this paper therefore is to analyze the effects of two specific policies from the _conference author's perspective_. The main contribution of this paper is an exposition and description of a large-scale author survey of Systems researcher. This data should prove highly relevant to at least three groups of people: 1) research computing professionals working to better understand the publication process and its effect in their careers; conference chairs wishing to understand the effect of their policies on author experience and diversity; and 3) educators who may apply our findings on author response to the classroom, where peer and mentor feedback can also impact diversity [@ginsberg09:diversity].


In addition to discussing the specific survey distributions, we attempt to bring the survey results into context by combining it with other data from multiple sources to assess author diversity and potential biases. Among all the Systems researchers in our survey, we primarily focus on three demographic factors: gender, English proficiency, and career stage or experience in Systems.

By limiting our scope to conferences in a single subfield, we avoid the variability that might occur across a broader range of disciplines.^[Despite its large research output and enormous economic impact, we found no consensus definition for the field of "Systems". For the purposes of this article, we define it to be the study of computer hardware and software components, which includes research in operating systems, computer architectures, databases, parallel and distributed computing, and computer networks.]  This important subfield is known for poor gender diversity [@destefano18:micro; @fox06:engineering], which gives us a lens by which we can examine any magnified effects of review policy on diversity. Despite this focus, we aim to generalize our analysis and increase the statistical validity and robustness of our measurements by including a large number conferences and of authors in our study.

To the best of our knowledge, this is the first cross-sectional survey of authors across Systems conferences. Past studies has concentrated on either very wide journal authorship [@editage18:perspectives; @publons18:peer; @sense19:peer; @solomon14:survey] or a single conference [@beverly13:findings; @daume15:naacl; @papagiannaki07:author; @parno17:SPsurvey]. We cite these works throughout our study wherever relevant.

The next section discusses our methodology and limitations for the survey data. The third, main section is organized around the areas of the survey itself: demographic questions, questions about the history of the accepted paper the author submitted, and questions about the quality of the author feedback and reviewer feedback. Each subsection lists the survey questions in order, describes the statistics of the responses, and then includes a short discussion or correlation analysis as applicable. As an initial analysis of the survey, Sec. \@ref(sec:analysis) devles into questions of author diversity for which we have data. We acknowledge that the wealth of this data set leaves more questions unanswered than this expository paper allows, and we discuss some of our future work in the final section. As an additional contribution, most of our data and source code, except for individual survey responses will be made freely available upon publication, to allow others to pursue their own questions.
