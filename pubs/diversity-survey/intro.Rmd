# Introduction {#sec:intro}

Peer review is a cornerstone of modern scientific research. However, understanding and improving this process is challenging, because it can be hard to experiment with peer review [@beverly13:findings; @mahoney77:publication; @mcnutt90:blinding]. For example, reputable conferences disallow parallel submission, and even within the same conference we cannot design an experiment where papers are reviewed multiple times with fully controlled variations. Thus, decisions on peer-review policies are often based more on the opinions of editors or program chairs, and less on hard data and evidence.

This article presents data and evidence from statistical observations on the peer-review process for a specific year (2017) and a specific subfield of research computing (computer systems or "Systems").^[Despite its large research output and enormous economic impact, we found no consensus definition for the field of "Systems". For the purposes of this article, we define it to be the study of computer hardware and software components, which includes research in operating systems, computer architectures, databases, parallel and distributed computing, and computer networks.]  Like most subfields of Computer Science, the primary channel for publishing research results in Systems is peer-reviewed conferences [@fortnow09:growup; @franceschet10:role; @vardi09:conferences]. Many conference policies are similar, such as giving each paper at least three blind reviews (where the identity of the specific reviewers is hidden from authors). But conferences can vary considerably in other aspects, such as double-blind reviews, rebuttals, two-phase reviews, etc. These decisions can potentially have dramatic effects on both the quality of the conference and the experience of the authors, but there appear to be conflicting opinions on the effects and tradeoffs of these policies [@mainguy05:peer].

The primary goal of this paper therefore is to analyze the effects on the _conference author's experience_. The main contribution of this paper is an exposition and description of a large-scale author survey of Systems researcher. This data should prove highly relevant to at least three groups of people: 1) research computing professionals working to better understand the publication process and its effect in their careers; 2) conference chairs wishing to understand the effect of their policies on author experience and diversity; and 3) educators who may apply our findings on author response to the classroom, where peer and mentor feedback can also impact diversity [@ginsberg09:diversity].

A secondary goal of this paper is to investigate how survey responses relate to author diversity. To this end, we combine our survey data with other data from multiple sources to assess author diversity and potential biases. Specifically, we look at gender, English proficiency, research experience, and geography.

By limiting our scope to conferences in a single subfield, we avoid the variability that might occur across a broader range of disciplines. This important subfield is known for poor gender diversity [@destefano18:micro; @fox06:engineering], which gives us a lens by which we can examine any magnified effects of review policy on diversity. Despite this focus, we aim to generalize our analysis and increase the statistical validity and robustness of our measurements by including a large number conferences and of authors in our study.

To the best of our knowledge, this is the first cross-sectional survey of authors across Systems conferences. Past studies has concentrated on either very wide journal authorship [@editage18:perspectives; @publons18:peer; @sense19:peer; @solomon14:survey] or a single conference [@beverly13:findings; @daume15:naacl; @papagiannaki07:author; @parno17:SPsurvey]. We contrast these works with our findings throughout our study wherever relevant.

### Organization {-}
The next section discusses our methodology and limitations of the survey data. Sec. \@ref(sec:survey) describes the survey and is organized around the areas of the survey itself. Each subsection lists the survey questions in order, describes the statistics of the responses, and then includes a short discussion or correlation analysis as applicable.^[A longer discussion can be found at [http://sysconf.review/survey](http://sysconf.review/survey).] 
As an initial analysis of the survey, Sec. \@ref(sec:analysis) delves into questions of author diversity for which we have data. We believe that the wealth of this dataset leaves more questions unanswered than this expository paper allows, and we discuss some of our future work in the final section. As an additional contribution, most of our data and source code, except for individual survey responses will be made freely available upon publication, to allow others to pursue their own questions.
