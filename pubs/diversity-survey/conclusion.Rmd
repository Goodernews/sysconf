# Conclusion and Future Work {#sec:conclusion}

This paper presented a new survey of authors of Systems conferences, looking at two peer-review policies and two diversity aspects.
Our data suggests that many conference variables and author experience metrics appear to be affected by the blindness policy of the conference. But on deeper examination, double-blind reviewing policies are often conflated with the academic reputation of a conference. The question of whether double-blind reviews contribute to the prestige of a conference (or are a consequence of it) remains open, and we plan to explore it in future work.

Most respondents found the opportunity to respond to reviewers very helpful, even if it did not change their review scores. The implication for PC chairs, and by extension, educators, may be that while a response process to technical feedback is of little value to experienced practitioners, novices do find it overwhelmingly helpful. Students are well-represented in this survey, like in others [@parno17:SPsurvey]. Their inputs be be a useful data point for conferences with an educational mission, and could help shape their policies to better address this target audience. A related finding is that longer feedback is generally perceived as more helpful, understanding, and fair, which in turn may serve as another factor in improving students' experience.

Women represent an alarmingly small group of authors in Systems research, and this paper looked at whether the peer-review process plays a role in this underrepresentation, as has been found in some grant and job evaluations. For female authors of accepted papers, we found that their papers tend to have a slightly longer history. But we found little evidence of negative effects in the reviews they receive or experience they perceive, even when their identity is known to the reviewers. Non-native English speakers also appear to experience no specific adverse effects from peer review, and in fact often report more positively on their experiences than native speakers. These two negative results can help focus the diversity effort on other policies.


This dataset remains rich for exploration of the many questions that fell outside the scope of this paper. Some of the questions we plan to address in future work include:

* Why is the representation of women in Systems so low?

* What are the effects of double-blind reviewing on the quality of reviews, conferences, and papers?

* What other publication differences and commonalities exist between Systems and the rest of CS?

* How do review grade correlate across categories?

* How might reviewer workload affect our results?

* How do any of these factors affect the eventual success of a paper, as measured by awards or citations?

* How do we correct for, or address, the survivorship bias, so that the voices of rejected papers' authors can be incorporated into this data?

* How do we explain the subjectively better experience of non-native English speakers in our survey?
