# Conclusion and Future Work {#sec:conclusion}

This paper presented a new survey of conference authors, exposing the experience of authors across a large section of Computer Systems. A strong emphasis was made to examine responses across various author demographics, and we found no selection bias based on the authors' gender, experience, position, geographical region, or paper. We think these responses are representative to authors of accepted papers throughout the field of Systems, and can be used to inform future conference policies such as double-blind reviews and author rebuttal. The former remains an important research question, and we plan to explore it with our survey data in future work.

Most respondents found the opportunity to respond to reviewers very valuable, even if it did not change their review grades. The implication for PC chairs, and by extension, educators, may be that while a response process to technical feedback is of little value to experienced practitioners, novices do find it overwhelmingly helpful. Students are well-represented in this survey, and  their inputs be be a useful data point for conferences with an educational mission, and could help shape their policies to better address this target audience. A related finding is that longer feedback is generally perceived as more helpful, understanding, and fair, which in turn may serve as another factor in improving students' experience.

Overall, we found that published authors in Systems exhibit a good mix of work sectors, research experience, and English proficiency, but poor diversity across gender and geographical regions.
Women in particular represent an alarmingly small group of authors in Systems research, and this paper looked at whether the peer-review process plays a role in this underrepresentation, as has been found in some grant and job evaluations. For female authors of accepted papers, we found that their papers tend to have a slightly longer submission history. But we found little evidence of negative outcomes in the reviews they receive or experience they perceive, even when their identity is known to the reviewers. Non-native English speakers also appear to experience no specific adverse effects from peer review, and in fact often report more positively on their experiences than native speakers. These two negative results can help focus the diversity effort on other policies.


This dataset remains rich for exploration of the many questions that fell outside the scope of this paper, such as:

* Why is the representation of women in Systems so low?

* What are the effects of double-blind reviewing on the quality of reviews, conferences, and papers?

* What other publication differences and commonalities exist between Systems and the rest of CS?

* What are the relationships between the paper statistics we collected and author statistics for the lead author?

* How do review grades correlate across categories?

* How might reviewer workload affect our results?

* How do any of these factors affect the eventual success of a paper, as measured by awards or citations?

* How do we correct for, or address, the survivorship bias, so that the voices of rejected papers' authors can be incorporated into this data?

We plan to address these questions and others in subsequent research. Our hope is that by opening up all the non-private data we collected, we also open the door for other researchers to validate our results, extend them, or collaborate on future studies.
